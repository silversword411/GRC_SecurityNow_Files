GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#862

DATE:		March 15, 2022

TITLE:		QWACs On? or QWACs Off?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-862.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we briefly touch on last week's Patch Tuesday for both Windows and Android, the world's two most used operating systems.  We look at a recent emergency update to Firefox and the need to keep all of our systems' UEFI firmware up to date.  NVIDIA suffers a huge and quite embarrassing network breach, and ProtonMail handles their Russian customers correctly.  The Linux kernel has seen some challenging times recently, and Russia has decided to start signing website certificates.  Research was just published to put some numbers to WordPress add-ons' observably miserable security, and the European Union legislators who brought us GDPR and mandatory website cookie notifications are at it again.  What now?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's a lot to talk about.  A Patch Tuesday recap coming up.  NVIDIA gets hacked, and the hackers get a lot of data out of them.  We'll talk about a UEFI firmware flaw and the patch that's being put out.  And then a proposal from the European Union that's just QWACed.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 862, recorded Tuesday, March 15th, 2022:  QWACs On?  Or QWACs Off?



It's time for Security Now!, yay!  I look forward to this all week.  Our man of the hour, Mr. Steve Gibson.  Why are you laughing?  It's true?



STEVE GIBSON:  Oh, Leo.  You look forward to a vacation all week.



LEO:  Well, I do that also. 	But no, this is easily the most informative show on the network.  I mean, if you want to know what's going on in security, which is really the tip of the spear for all computing these days, you've got to listen to this show.



STEVE:  I thought about that, like it was 17.5 years ago that you said, hey, want to do a podcast about security?  And back then it was like, oh, well, okay, what's a podcast?  But also...



LEO:  And what's security?



STEVE:  I didn't know we'd have stuff to talk about for 17.5 years.  And actually some of the topics this week were from last week because I wanted to stay focused on the consequences of what is going on in Russia with cybersecurity.  And I've got a few things that couldn't fit in this week which we'll be talking about next week.  So yes, our cup runneth over with security problems.



So this is Episode 862 for the middle of March.  And I'm looking forward to next week because that's going to be 3/22/22.  But that's next week.  This is 3/15/22.  And of course we missed Pi Day, 3/14.  



LEO:  It's the day after Pi Day.  	



STEVE:  Yes.  So this one I titled it QWACs On?  Or QWACs Off?  And those old-timers among us will know about wax on, wax off.  We're going to briefly touch on last week's Patch Tuesday, both for Windows and Android, which of course are the world's two most used operating systems, one for desktop, one for mobile.  We look at a recent emergency update that Firefox received, and also the need to keep an eye out for upcoming UEFI firmware updates for our various machines.



A research firm plowed into a widely used SDK used by many firms, I'll enumerate them later, to develop their own custom UEFI firmware.  And as we've seen before, you make a mistake in the example code, everybody just copies it.  So anyway, we've got a lot of copy problems which need to get patched.  But the point is that it really does undermine the guarantees that UEFI is here to provide, keeping things from crawling into our operating systems before they've had a chance to get booted.



Also NVIDIA has suffered a huge and quite embarrassing network breach with some consequences for them.  ProtonMail handled, I think, their Russian customers correctly, so I just wanted to give them some props.  But Tutanota did also, yeah, it's actually even better.  The Linux kernel has been seeing some challenging times recently, so we're going to talk about that.  And Russia has decided to start signing their own website certificates.  And this is one of those what could possibly go wrong?



LEO:  Oh, boy.



STEVE:  Research was also just published to put some numbers to my constant WordPress grumbling about add-ons, the lack of security in their add-ons.  We now know exactly just how bad.  And I guess if I could encourage people to just use the root WordPress install and try not to embellish it with all the add-ons, that would be the takeaway from that.  And finally, the European Union legislators who brought us the GDPR and mandatory website cookie notifications are at it again with QWACs.  So we'll be talking about that.  And we do have a fun Picture of the Week from one of our listeners.



LEO:  And as you say, QWACs on, QWACs off.



STEVE:  QWACs off.  What could they have done now?



LEO:  I'm trying to find it, but I remember seeing on Reddit somebody wrote an app that ran in UEFI.  So I think it was Wordle or something.  So, I mean, because I guess UEFI there's enough space in there, you can put an app in there.



STEVE:  Yeah.



LEO:  So you boot your computer to Wordle, apparently, I guess.  I don't know why you'd want to do that, but you know.



STEVE:  Hopefully you can then exit and go back to your regularly scheduled OS.



LEO:  That's right.  I'm presuming there's a menu, I would hope, I would hope.  Picture of the Week time, Mr. G.



STEVE:  So, okay.  This is a one-off, or a one-of-a-kind.  One of our listeners was inspired by the question that I answered the other day about how one would detect a BGP hijack where an illicit cert was obtained on the fly immediately after the hijack in order to impersonate a website.  And of course the way is, as I said, to check the certificate's fingerprint because the fingerprint is something that cannot be copied.  It cannot be duplicated by someone who's spoofing it.  So a fraudulent certificate with the same domain name, affecting the same site, would have a different fingerprint.



So this enterprising individual took the current certificate fingerprints of his most important sites and created a label for a coffee mug.  So we have a coffee mug, a nice white ceramic coffee mug, around which is wrapped this label.  And the headline says:  "Honey, You've Left Thumbprints All Over My Cup."  And so he's got the certificate thumbprint for the Bank of Tampa.  Where do you imagine he lives?  Fidelity Investments.  TD Ameritrade.  PNC Financial Services, Synchrony Financial, the Bank of America, Mastercard, and as a nod to me, Gibson Research Corporation.



LEO:  I love it.  I love it.



STEVE:  And as I was looking at it, I was saying, oh, August 25th, '22 is when my cert expires.  Of course you notice nobody's - that's good to know - nobody's cert is expiring long from now.  And of course we know why, because they only get a year's life.



LEO:  They don't do long ones anymore, yeah.



STEVE:  Consequently, yeah.  So everybody's '22 and '23.  But anyway, I just thought this was a kick because if you were going to the site, and you were concerned that maybe you were not at the right place, you could just pick up your coffee cup, use it as a reference for the proper thumbprint for the certificate, and then, baby, you know you're talking to the right people.



LEO:  That's pretty funny.  That's great.  I always - I never do the whole number, but I look at the first four and the last four, and if they match I figure good enough.



STEVE:  Yeah.  It is a hash.  And so with any hash the chances of, let's see, if the first four and the last four, that's going to be on the order of 32 bits, so that's one in 4.3 billion...



LEO:  Good enough.



STEVE:  ...chance that, yeah, it's good enough.  Okay.  So we had Patch Tuesday last Tuesday.  And the world's dominant and increasingly rudderless desktop operating system, Windows, received fixes - boy, I tell you, Leo, listening to Paul and Mary Jo is just like, well, it's called Windows Weekly, and that's W-E-A-K-L-Y.  Boy.



LEO:  You know, it's kind of sad because if you have a show called Windows Weekly, you have to use Windows.



STEVE:  Yeah.



LEO:  But you get the feeling that maybe they're thinking, I wish I didn't have to use Windows.



STEVE:  Well, it is Mary Jo's world; right?



LEO:  Yeah.



STEVE:  I mean, so she's - that's really her thing.



LEO:  She has to.



STEVE:  But you really see Paul, like kind of like...



LEO:  Oh, groaning, groaning.  Well, you, too.  I mean, you have to because SpinRite is a Windows product.



STEVE:  Yeah.



LEO:  And so you have to use Windows.  But boy, Linux just looks better and better as Microsoft keeps making these mistakes.



STEVE:  Leo, this is not in the news today for me, but they're now experimenting with adding ads to...



LEO:  Ads, yeah, in Explorer.



STEVE:  Yes.



LEO:  Yeah, yeah, I saw that.  It was just like, what?  Stop.  You don't make enough money?  Is that it?  Is that the problem, Microsoft?



STEVE:  I know.  And I just, you know, I'm talking to you on a brand new Win10 setup.  And I had to remove Candy Crush Soda Saga.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  It's like, what?



LEO:  Why is it on there?



STEVE:  This is in my Start menu.  What is wrong with these people?



LEO:  Yeah.



STEVE:  Yeah.  Anyway, they fixed 71 - yes.  So I was just thinking the same thing.  I was thinking, you know, I mean, if anyone should be moving to Linux, it's I.  I should be moving to Linux.  But I do, I do have many things that are Windows only.  And, for example, the next development platform I'll be switching to for SpinRite 7, SpinRite will be running on a little custom 32-bit real-time operating system in order to get dual boot over UEFI and BIOS so that it can run either way.  Well, it supports remote debugging, which is the only way to debug, especially on a little turnkey RTOS, you know, it's not going to have a full desktop environment.  So I'll need to be reaching over to that machine and debugging on a separate machine.  That's Visual Studio.  That's what it supports.  It doesn't support other Linux-hosted environments.  It's Visual Studio on Windows specifically.



LEO:  Well, yeah.  And if you're going to do a security show you have to be on the insecure operating system so you can talk about it, I guess.



STEVE:  Yeah.



LEO:  Yeah.  And you can secure yourself.  You can get rid of Candy Crush Soda Saga.



STEVE:  Oh, no, it's not the security that I'm worried about.  Leo, I'm running on Windows 7.  Security's not a problem.  It's just the annoying things they do.  Paul was complaining that he'd been writing a book for several years.  His Windows had been watching him writing this book.  Yet when he searches, he uses the search for the book's title, trying to find some documents relative to it that are, like, named that, he gets Bing results from the web.  It's like, what?  It was like some steakhouse.  It's like, oh, he was writing about longhorn, and he was getting...



LEO:  Longhorn Steakhouse.



STEVE:  ...Longhorn Steakhouse results.



LEO:  Yes, that's right.



STEVE:  And it's like, hey, Windows, there's files named "Longhorn" right in front of you.  That's what I'm trying to have you help me find.  No.  How would you like your steak?  Rare?  Medium rare?  Oh, yeah.  Anyway, it's - so, yes, not for security.  But we're going to talk about security eventually here.



Okay.  So last week Windows fixed 71 known flaws, three of which had been publicly disclosed, none which were exploited.  But of course Microsoft calls these "zero-days" because someone surprised them about them.  Two of the three did have publicly disclosed exploits and their remote code execution.  So again, the advice is always going to be update as soon as you can.  29 of them were remote code execution (29 of the 71 were RCEs); 25 elevation of privileges; six information disclosures; four denial of services, meaning that you can crash something, and then it won't serve you; and then three each security feature bypass, which is so generic it's annoying.  Security feature bypass, gee, isn't that everything?



Anyway, and then we have spoofing vulnerabilities, three of those.  And then, oh, on top of that we did have 21 problems identified and fixed in Microsoft's Chromium-based Edge browser.  And I always kind of wonder, because we're not told, if those are things Microsoft did to Edge-ify it, or if those are in the Chromium core, and they're just fixed because everybody who uses Chromium got those 21 things fixed?  It's not clear from what Microsoft is saying.  So anyway, I trust everybody has patched their Windows and gone through their update cycle.  Last time it happened for me I had to switch computers because it stopped being fast enough to run Zoom.



LEO:  I'm liking this one, by the way.  And the picture's really good.



STEVE:  Oh, good.



LEO:  Are you using the same camera?



STEVE:  Everything's the same.  Same, same, yeah.



LEO:  Yeah, looks really good.  It's a higher quality image.



STEVE:  Well, I can even see, yes, I can even see that the image quality is higher, and the refresh rate is better.



LEO:  So it was bogging.  That's all.  It was just an old machine, too slow, yeah.



STEVE:  I'm sure it was.  I'm sure.  Okay.  Also, as I said, Android.  The most severe issues fixed, there was a critical security vulnerability in Android's system component that could lead to remote code execution with no additional privileges needed and no user interaction.  So those are the things that the people who are trying to get into other people's phones in targeted attacks are trying to use.  And of course Android is the majority mobile platform on the globe.  So update Android also, as always.



Just over a week ago, my Firefox browser jumped to update and restart itself.  Like it just runs statically over on my left-hand screen all the time, and it's sort of my reference browser.  And so it's rare that I see it, like, waving at me, hey, Gibson, restart me.  And so I did.  And that was to eliminate a pair of high-impact zero-day vulnerabilities, both of which Mozilla said were seen actively exploited in the wild.  And that's a little bit of a rare thing for Firefox because it's a little bit of a rare browser, increasingly rare these days.  Everybody's kind of gone Chromium-ized.  But bless their hearts, not Mozilla.



So there were two CVEs issued for these two zero-days.  I mean, these are like real zero-days; right?  They were found being actively exploited, 2022-26485 and 486.  They're both once again use-after-free flaws impacting the Extensible Stylesheet.  Sorry, Stylesheet.  I like Extensible Stylesheets.  I don't have anything against them.



LEO:  It's just hard to say.



STEVE:  Yeah.  Extensible Stylesheet Language Transform - oh, Leo, speaking of hard to say, wait till we get to the Russian Kremlin Gremlins.  There's a tongue twister.



LEO:  Oh, boy.



STEVE:  The Stylesheet Language Transformations (XSLT) parameter processing, and the WebGPU - easily said - interprocess communication, the IPC Framework.  This XSLT is an XML-based language used for the conversion of XML documents into web pages or PDF docs, which Firefox can do, and this WebGPU is the emerging standard that's expected to supplant the current WebGL JavaScript graphics library.  Both of those had problems.



In the case of this XSLT, it was found that the removal of an XSLT parameter during processing could lead to an exploitable use-after-free flaw.  And the problem is this could be done remotely.  And as for this use-after-free, as is often the case, we were talking about this recently, with dynamic memory management, a reference count is kept by the memory management system on behalf of the programmer so that they don't need to worry about it.  So that frees the programmer from needing to do any kind of reference counting manually.



And it's a convenience that the so-called "garbage collection" is done in the background without the programmer needing to bother.  But the only way the system knows it's safe to release and free an object that had been allocated is if and when that object's reference count drops back to zero.  So the first time it's - the first instance of a reference or actually every instance of a reference increments the reference count.  And so once there are no more references in context, the garbage collector says, oh, okay, nobody else is using this.  I'm going to let it go.



The problem is, and it's cool, but it's one of those inherently brittle technologies.  It's great when everything works exactly right.  But mistakes are very difficult to spot.  And normally it's the kind of thing that needs to be seen happening, which is why these problems generically, these use-after-free problems, keep recurring.



And I guess if our listeners wanted to explain the trouble to some non-techie, they could say that use-after-free bugs, which can sometimes be exploited to corrupt data and execute an attacker's code, arise from a confusion over which part of a program is responsible for freeing the memory.  So that's sort of, in a nutshell, that's the problem.  And again, modern languages doing more for their programmers behind the scenes; but when they make little mistakes, they can be leveraged.



So the other use-after-free glitch, because these were both that, was caused by an error when processing messages in the WebGPU's interprocess communication framework.  A remote attacker could trick their victim into opening a specially crafted web page, trigger this use-after-free error, and execute arbitrary code on the victim's system.  And as I mentioned, this XSLT vulnerability was exploitable in the same way, just open the wrong web page and BLAMMO. 



So last week you would have wanted to be at Firefox 97.0.2, which is what it updated me to.  But just now when I checked it wanted me to restart, which I did, which moved me from 98 to 98.0.1 on the desktop.  So you might want to check if you're using Firefox to make sure that you're current.  Again, almost certainly targeted attacks.  Remember the so-called Watering Hole attacks where something is done to lure a targeted victim to a website in order to get them compromised.  So sort of a variation on phishing.  But thus called a "watering hole"; right?



So anyway, Firefox.  We've talked a lot about the surprisingly daunting challenge of booting a computer while maintaining true provable security.  But this really should not be that surprising.  When you think about it, the reason computers have been such an incredible breakthrough and boon for mankind is that they are so flexible.  It's a machine that follows instructions.  The trouble arises because there's no way for it to know whose instructions it's following.  It doesn't care.  And its limitless vulnerability arises from its incredible flexibility.



And in security we have this concept of a chain of trust where that chain is anchored by a root of trust.  And this applies to booting up an operating system.  If the operating system is signed by its publisher, any single bit that's changed will break the signature, which is just a signed hash.  So the theory is the system's firmware simply needs to load the code that's been signed, and then verify the operating system's cryptographically secure signature before it turns control over to it.  Doing that will prevent anyone from modifying the operating system's code.  Not one bit can be changed because the code gets hashed and signed, and a bit will completely change the hash.



Which, Leo, is why you and I only check the first four digits, first and last four digits.  It's like, impossible for the majority of them to be changed.  In fact, we know, if you change one bit, on average half of the bits in the hash will be changed.  So thus our strategy.



Anyway, so the concept is good.  But the question is, if the UEFI is performing that work, who's making sure that it's doing it right?  On one hand, having the system's startup firmware code being firm and not hard creates a vulnerability because that means it can be altered.  On the other hand, since we appear to be unable to get it right, if it's firm it can at least be improved when problems with it are inevitably found, as they have in this case been.



So the good news is there are people who are focused upon improving the situation with firmware.  At the beginning of last month, a group of researchers from the firmware protection company Binarly discovered a raft of critical vulnerabilities in the so-called "InsydeH2O" (I-N-S-Y-D-E), InsydeH2O UEFI firmware.  It is a cross-vendor firmware used by many computer vendors including Fujitsu, Intel, AMD, Lenovo, Dell, ASUS, HP, Siemens, Microsoft, and Acer.  So, like, right, that's - who doesn't use them?  And actually I think that's a good thing.



Binarly found 23 flaws in this InsydeH2O UEFI firmware kit, essentially, most of them occurring in the software's System Management Mode code, which provides the system-wide functions such as power management and hardware control.  Since the System Management Mode privileges inherently exceed those of the OS kernel, which it is responsible for booting, any security issues there can have severe consequences for the vulnerable system.



The flaws that were found in this InsydeH2O firmware can collectively and individually be used to invalidate many hardware security features including Secure Boot and Intel's BootGuard, to install permanent and persistent software that cannot be easily erased, and to create backdoors and back channel communications to steal sensitive data.  As we've talked about this whole baseboard processing now, there's a whole processor separate from the Intel chip that we stick in a socket when we decide which model that we want, that runs the motherboard and all of its things.  And it's very capable.



So as I said, there were 23 flaws found with three of them obtaining CVSS scores of 9.8.  So this is in the baseband processing firmware.  Binarly's disclosure report explained that the root cause of the problem was found in, as I mentioned at the top of the show, the reference code associated with InsydeH2O's firmware framework.  In other words, as we've seen before in other contexts, all the manufacturers derive their own firmware from Insyde's firmware SDK reference code to develop their customized UEFI firmware.



And that's certainly reasonable.  They bought a license to do so.  That's what they did.  No reason to recreate the wheel.  The chances are when you do you're going to make mistakes.  So that's great.  And so frankly I'd rather have everyone working from a common codebase than each rolling their own, since getting this exactly right is crucially important.  And when it's fixed for one, it's fixed for all.



InSyde Software has released firmware updates to fix all the security vulnerabilities that were identified by Binarly, and they've published detailed bulletins to assign severity and descriptions for each flaw.  So all of the people using their kit.  Hopefully there are engineers right now that have been like working through this and working to update the firmware across all their products.  Of course we always have the problem of the inside UEFI stuff having been around for a long time, and products going out of support and no longer receiving firmware updates.  It's difficult to require everybody to support everything forever.  So these are really bad, and now they're becoming known.



So being individually customized firmware on a per product and per manufacture basis, they all have to be created, adopted, incorporated, downloaded, installed.  Every single vendor needs to do this.  And that way it's a little bit like the Log4j mess where it is something core to many different products and publishers.  So this is why I said at the top we can expect to see important firmware updates coming from many of our hardware vendors.



And I'm not suggesting that this will be a widespread attack or that that will result.  It's very likely that these would be used, you know, these are going to be sophisticated, targeted attacks.  You need to get onto the system first.  Then these are being used to achieve persistence, you know, getting down into the firmware.  So I did not look in detail to see what three of these things had a 9.8, but it is possible that this could be exploited remotely because there is now management technology in some of these motherboards which allows remote, over-the-network management of them.  So that may be why they're at 9.8.  Still, it just makes sense to keep your firmware updated.



And then, in addition to this, on top of these 23 problems, last week HP disclosed an additional 16 high-impact UEFI firmware vulnerabilities that Binarly had found which affect multiple HP models, and HP is a user of Insyde also.  And that included laptops, desktop computers, point-of-sale systems, and edge computing tools.  These flaws allow malware to survive hard drive replacement and operating system reinstallation.



So anyway, a long time ago we talked about how rootkits are able to hide in plain sight by doing something as simple as hooking an operating system's directory listing API, to simply remove references to any of its own files from the list.  So you do a dir, or any of your programs do a dir, and they don't see any of the files that are sitting in the directory right in front of them.  I remember it was the Sony rootkit that we talked about in the way long time ago.  And it's just unnerving to imagine that something that simple, you know, I mean, it shows how much we assume that our operating system is doing what we expect and what it should; where in fact, as I said because software is infinitely flexible, it's so easy for it not to be working the way we want it to.  So anyway, keep an eye on firmware updates.  And a huge thanks to Binarly for digging in, for taking the time to dig into Insyde's UEFI offerings and help make our stuff better.



LEO:  This seems like UEFI would be a really good vector for attacks because it's basically a mini operating system that runs before your system.  And it's persistent, as you point out.



STEVE:  It has a file system.



LEO:  Yeah.  They can do anything.  It's Turing complete.  I mean, it's written in C, and it's basically, if I were going to be a bad guy, that would be the best place to put malware because it would survive a reinstall and everything.  It wouldn't survive...



STEVE:  Yeah.



LEO:  It usually lives on a hard drive; right?  It would not survive a complete nuke of the hard drive.



STEVE:  No, no, it's in the onboard firmware.



LEO:  It's in the firmware?



STEVE:  Yes.



LEO:  There's a UEFI partition, though, on the drive that also contains code.



STEVE:  Yes.  And so that could be a problem, although these guys specifically said that this would survive a hard drive replacement.



LEO:  Oh.  All right.  So it is in the firmware.  Wow.  Yeah.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Yeah, nobody wants that.



LEO:  Yeah.



STEVE:  So NVIDIA suffered a serious network breach last month.  The hacker extortion group by the name of Lapsus$, L-A-P-S-U-S, then with an extra dollar sign appended to the end, claims to have exfiltrated, and all evidence is they probably did, about a terabyte of NVIDIA's very proprietary data during the attack.  They extorted them.  NVIDIA said go away.  So they began leaking NVIDIA's data online, as I said, after NVIDIA refused to negotiate.



On February 23rd, emails and NTLM, you know, NT LAN Manager, you know, Windows password hashes for 71,335 NVIDIA employees were leaked on the Internet.  Now, this was a bit puzzling since NVIDIA currently has only 18,975 employees.  But the hackers claimed to have had deep total access to NVIDIA's system, where they said they roamed around for about a week.  And so the presumption has been that the leaked credentials include deep past employee files, as well, which would explain the high count.



Then four days later, on February 27th, that hacking group Lapsus$ claimed it had been digging around for a week, and for some reason made a point of clearly stating that they are not state-sponsored, and that they are "not into politics AT ALL," in caps on the "AT ALL."  So such is the world we're in today.



Troy Hunt's "Have I Been Pwned" site now has the leaked data and noted that many of the hashes are being cracked and circulating within the hacking community.  So I'm sure that NVIDIA has already made sure that all of their employees have changed their passwords because otherwise that would even be worse.  



Lapsus$ also stole, I mean, apparently they just got everything.  Think about it.  I mean, we just glibly talk about a terabyte here, a terabyte there.  But that is a trillion bytes, a thousand billion bytes of data.  So it's probably everything.  And among the everything was some NVIDIA expired driver signing certificates.  And what's confusing to me is that the reports are, and they've been confirmed by many outlets, these expired driver signing certificates were immediately put to use signing malware components in order to get them past antivirus and Windows' own driver signing protections.  So expired.  What's not at all clear to me is why Windows would load any driver signed with an expired certificate?  But apparently it's doing that.



Code signing works differently from TLS web server certificate signing.  The reason for the difference is that servers and clients, web servers and clients, are by definition always online and talking to each other, and servers are thus always able to present a certificate that's currently valid, so that's what we force them to do.  But that's not the case with code; right?  A driver might be signed 10 years ago, and the certificate with which it was signed will have long since expired, that particular certificate.  So we want drivers to be valid in the future, even if a machine is not online.  So there's no, like, online OCSP-style verification.  And the point is, was the certificate valid when it was signed?



So that's what we did.  We solved this problem by changing the requirement, the validity requirement for code signing.  We require that code is signed by a certificate that is valid at the time the code was signed.  But that means that we need to know when the code was signed.  And that information is incorporated by requiring co-signing of the certificate by a valid certificate authority which offers a timestamping service.  So at the time the code is signed, the signing computer obtains a certified timestamp from a timestamping service which is all bundled into the final signature.  So it's sort of like having an online notary saying, yes, here I am, I'm asserting that this is the current date and time that all of this is happening.  So this attests to the time that the signing was done.



Now, Bleeping Computer showed the digital signature's property page of a piece of malware signed with NVIDIA's expired cert.  It shows that there was no timestamp included.  And in fact you don't need to include a timestamp.  That is to say, the act of signing doesn't require a timestamp.  But the entity trusting the signature is supposed to require it.  I mean, that's the whole point.  But all I can guess is that for the sake of compatibility, Microsoft doesn't enforce timestamping for at least some uses.  And "some" apparently includes kernel drivers because that's what was being signed by these expired NVIDIA certificates.  They've immediately been blacklisted.  There are now, you know, we will not trust anything signed with these expired certs.  But, boy, you know, it took somebody using them maliciously to make that happen.  Which to me just seems crazy.  But anyway, that seems to be what's going on.



And also on March 1st, the Lapsus$ group demanded that NVIDIA open source their proprietary drivers for Windows, Mac, and Linux.  I have a picture in the show notes of their ransom demand.  Dated March 1st, it says:  "After evaluating" - this is Lapsus$ talking, the bad guys.  "After evaluating our position and NVIDIA's, we decided to add one more requirement.  We request that NVIDIA commits to" - in all caps - "COMPLETELY OPEN SOURCE (and distribute under a FOSS license) their GPU drives for Windows, macOS, and Linux, from now and forever.



"If this request is not met, on Friday we will release the" - and now we have in caps - "COMPLETE SILICON, GRAPHICS, AND COMPUTER CHIPSET FILES for all recent NVIDIA GPUs, including the RTX 3090 Ti and UPCOMING REVISIONS!"  And they say:  "Of course, this includes all files with extensions such as .v, .vx, .vg, and more."



And so they finish:  "So, NVIDIA, the choice is yours.  Either, one, officially make current and all future drivers for all cards open source, while keeping the Verilog and chipset trade secrets - well," they said, "secret, or not make the drivers open source, making us" - right, they're being made - "to release the entire silicon chip files so that everyone not only knows your driver's secrets, but also your most closely guarded trade secrets for graphics and computer chipsets, too.  You have until Friday.  You decide."



LEO:  Oh, lord.



STEVE:  I know.  Now, this .v, .vx, and .vg file extensions are for Verilog, which is the Hardware Description Language (HDL) used to model electronic systems.  It's the most commonly used in the design and manufacture of integrated circuits, and it has become an industry standard, standardized as IEEE Standard 1364.  This means that these guys have the designs of NVIDIA's chips.  Which, yes, NVIDIA certainly intended to be kept inside.  And, you know, while at first this seems a bit breathtaking, it's unclear what real value those would be.



I mean, maybe there's proprietary stuff that could be reverse engineered out of them.  But no other reputable foundry is going to make NVIDIA clone chips.  They'd be litigated into nonexistence, for one thing.  And, well, I was thinking about this, maybe Russia would, except that Russia doesn't have the process technology to make any state-of-the-art chips.  They're unable to do it.  So NVIDIA's designs would be of little use to them.



Last year, NVIDIA also came up with a technology known as Lite Hash Rate, or LHR, which reduces the value of its GPUs for cryptocurrency mining. Their aim is to make a very powerful graphic processor, but one which would not be very good at cryptomining, in order that, you know, gamers can actually have a chance to buy some of these things, rather than them all being sucked up by the cryptominers.  And apparently this also annoyed the hackers, who have said they want the hash rate limiters to be removed from the chips.



So my feeling is mostly this is a huge embarrassment for NVIDIA.  And it is a perfect example of just how much damage can be done when all of a company's proprietary value takes electronic form, as NVIDIA's is, and thus can be downloaded, shared, and unfortunately escape from their control.  Wow.



LEO:  By the way, I just wanted to show you something that came up in the Discord.  Did you see this?  In Crystal City, Virginia?



STEVE:  Yup.



LEO:  Instead of telling you when it's safe to cross the street, the walk signs in Crystal City, Virginia are telling you "Change your password.  Change your password.  Change your" - I'm going to have to turn on the sound.  But clearly somebody got...



CLIP:  Change password.  Change password.



STEVE:  That's brilliant.



LEO:  Clearly somebody got into the system.  And instead of being malicious, they just thought they'd have some fun.  Which I think is great.



STEVE:  Yeah, I think it's actually wonderful.  It's a little bit of a Rickroll.



LEO:  Exactly.



STEVE:  I mentioned that I wanted to just give ProtonMail, which I know is a very popular email service among our listeners, a bit of a thumbs-up.  Last week I grumbled about Namecheap's unilateral, and I felt questionable decision to dishonor their previous prepaid commitments to their DNS domain registrants who were at the time given one week to find another DNS provider before their domain name resolution would be summarily terminated.  I think ProtonMail did the right thing.



Thanks to Bleeping Computer, who was able to - actually I think one of their employees is a ProtonMail user who received a notification, preemptive notification from ProtonMail that said:  "Dear Proton community member.  As you may have heard, Mastercard, Visa, American Express, PayPal, and numerous other financial institutions have announced that payments into and out of Russia will soon be cut off."  And they said, "(if this hasn't happened already).  This means paying for your Proton subscription with your current payment method will likely soon be impossible.



"While many companies have announced that they will no longer serve Russian customers, at Proton our mission is to defend online freedom everywhere.  We remain committed to serving the people of Russia for as long as possible.  The present difficulties, however, may take some time to resolve.  If you are a Proton subscriber, we suggest renewing your subscription or purchasing credits before all forms of payment are cut off in the coming days.



"We are committed to not cutting off any users in Russia for financial reasons for as long as possible during this difficult time.  If you are able to use alternative payment methods, your support will ensure that we can continue to serve the Russian people in many years to come.  Thank you again for supporting our mission.  Best regards, the Proton Team."  And they also in this provided a list of alternative payment methods, including cryptocurrency, that could be used by Russian users to renew their subscriptions.



So I thought that was, you know, although yes, it's a little self-serving, they're saying hey, you know, pay in advance quickly if you don't want services to be cut off, to me their heart was more in the right place than Namecheap.  I'll also note, however that separately, for those based in Russia, Belarus, or Ukraine, another encrypted email provider, Tutanota, has offered to renew subscriptions free of charge for users unable to make payments during the current situation.  So some companies are doing the right thing.



Linux has been having a rough time of it lately.  The last year has seen this ever more popular server and desktop operating system hit with revelations of multiple high-profile elevation of privilege vulnerabilities.  There have been problems in Linux's iSCSI subsystem, in the kernel, in the Extended Berkeley Packet Filter and, as we talked about at the time, in the Policy Kit's Polkit pkexec component.  And there have been two additional recent problems.



The first bears the unfortunate name "Dirty Pipe" and was discovered by a guy named Max Kellerman who discovered, mostly by accident - well, okay, something happened by accident.  His actual discovery was certainly not that.  But this event was an accident.  It was an important locally exploitable vulnerability which was introduced into the Linux kernel at v5.8.  Because the exploitation of the flaw allows overwriting data in arbitrary read-only files, it could be put to some very creative and not generally beneficial use, leading to privilege escalation because unprivileged processes could be able to inject code into root processes.  So in some ways this is reminiscent of the 2016 "Dirty Cow" Linux vulnerability, though this one is even easier to exploit.  But it does explain, although it doesn't forgive, this vulnerability's name, Dirty Pipe.



So in Max's original posting, he explains how this all began.  And I'll just share the beginning of this because it was lengthy.  He said:  "It all started a year ago with a support ticket about corrupt files.  A customer complained that the access logs they downloaded could not be decompressed.  And indeed, there was a corrupt log file on one of the log servers.  It could be decompressed, but gzip reported a CRC error."  He said:  "I could not explain why it was corrupt, but I assumed the nightly split process had crashed and left a corrupt file behind."  He says:  "I fixed the file's CRC manually, closed the ticket, and soon forgot about the problem."



He said:  "Months later, this happened again and yet again.  Every time, the file's contents looked correct.  Only the CRC at the end of the file was wrong.  Now," he said, "with several corrupt files, I was able to dig deeper and found a surprising kind of corruption.  A pattern emerged."



Okay.  So then Max goes on, as I said, at great length to explain how this kept nagging at him until he finally had to sit down and get to the bottom of it.  After a great deal of analysis and experimentation he finally figured out how to force the file corruption bug to occur at will.  That was the key.  Max wrote:  "All bugs become shallow once they can be reproduced."  And of course that is, you know, that's any software developer's dream is to be able to reproduce the problem.  I've referred to motherboards that I've been purchasing, old motherboards, off of eBay because one or several or the people testing SpinRite will all have a really weird problem.



For example, just to segue for a second, we've got one motherboard which will not run SpinRite from USB.  But if you copy it to the hard drive, it'll run it from there.  If you copy it to a RAM disk, it'll run it from there.  But it actually won't execute it from the USB drive.  Nobody else has this problem.  But this one particular gigabyte motherboard has a problem.  I actually found an instruction that would not execute from USB.  It's insane.  But anyway, as I said, if you can reproduce a problem, you can fix it.  I did reproduce it, and I did fix it, even though I don't understand it.  But now it works.



Anyway, Max then tracked it down to the bug, which he located in the Linux kernel.  He then checked Linux 5.10, which is the Debian Bullseye build, which had the bug.  But the bug was not present in the previous Linux 4.19, Debian Buster.  Those two releases were separated - I hope you're sitting down, Leo.  Those successive Linux releases were separated by 185,011 Git commits.



LEO:  Wow.



STEVE:  Oh, baby.  But using Git's various tracking tools he was able to locate the one commit, made nearly two years ago on May 20th of 2020, which introduced this very subtle bug into the Linux kernel.  That change refactored Linux's pipe buffer code for anonymous pipe buffers, changing the way the "mergeable" check is done for pipes, and in doing so introduced an extremely subtle flaw.  The vulnerability has been fixed in that one, in Linux 5.10.102, 5.15.25, and 5.16.1.



So that was what Max found, the first of the two problems.  Then two researchers at Huawei discovered a vulnerability in Linux's "control groups" feature which allows attackers to escape containment, to escalate privileges, and execute arbitrary commands on a host machine.  Now, Linux control groups, or "cgroups," as they're known, allow sysadmins to allocate computing resources such as memory, bandwidth, and such, among whatever processes might run on a system.  These cgroups provide fine-grained control over allocating, prioritizing, denying, managing, and monitoring system resources.  This means that cgroups can be a powerful tool for control and security within a system.  And in fact they're used by containers like Kubernetes.



A feature known as the "release_agent" file allows administrators to configure a release agent program that would run upon the termination of a process in the cgroup.  That meant that attackers who are capable of writing to the release_agent file could exploit it to gain full admin privileges.  And Linux wasn't checking that the process setting the release_agent file had admin privileges.  So basically a very simple problem which through a chain of complex features ended up being exploitable.  And all of this amounts in this case to a container escape, as I mentioned, for example, within a Kubernetes environment, which would provide attackers with access to other users' containers in public cloud environments.



And we're not finished with Linux yet.  Yesterday another just-disclosed security flaw came to light.  This one can be leveraged by a local adversary to gain elevated privileges on vulnerable systems to execute arbitrary code, escape containers, or induce a kernel panic.  It's CVE-2022-25636 with a CVSS of 7.8.  It impacts Linux kernel versions 5.4 through 5.6.10.  It's the result of an out-of-bounds write, in other words, a buffer overrun, in the heap of the kernel's netfilter component.  The flaw was discovered by a guy named Nick Gregory, who's a researcher with Capsule8.



Red Hat explained that:  "This flaw allows a local attacker with a user account on the system to gain access to out-of-bounds memory, leading to a system crash or a privilege escalation threat."  In addition to Red Hat, similar alerts have been released by the maintainers of Debian, Oracle Linux, SUSE, and Ubuntu.  Linux's Netfilter is a framework provided by the Linux kernel that enables various networking-related operations like  packet filtering, NAT, and port translation.  The problem results from incorrect handling of hardware offloading that could be weaponized by a local attacker to cause a denial-of-service or possibly execute arbitrary code.



The idea of hardware offloading is that there are a number of things associated with sending and receiving network packets that are very simple to do, such as calculating a packet payload's checksum, and so they're a waste of the processor's time.  They're time-consuming, like to do a checksum you've got to do math on every single byte that's going down the pipe and then just dump the sum of them there.  What happens is when you have a NIC which indicates that it's able to automatically do checksuming, then the software just skips that completely, and the NIC hardware itself plugs the proper checksum into the packet payload on its way out.



So another example of things coming on the way in is packet coalescing can be done in hardware to reduce the packet count, thus reducing the per-packet processing that needs to be done.  So what's happened is over time, as NIC hardware, Network Interface Controller hardware, has become increasingly capable, the more and more duties have been pushed down to the hardware level to free the software from doing things.  It's a waste of its time.



Anyway, Nick Gregory, who discovered this problem, explained.  He said:  "Despite being in code dealing with hardware offload, the flaw in netfilter is reachable when targeting network devices that don't have offload functionality, for example, the local virtual interface.  This can be turned into kernel Return-Oriented Programming (ROP), local privilege escalation, and other things without too much difficulty, as one of the values that's written out of bounds is conveniently a pointer to a net_device structure."



Okay.  So what does this mean?  Does the spate of flaws suggest that Linux is becoming rickety?  No.  I think it is an indication of it becoming increasingly complex.  As we know, security and complexity are perpetually at odds with one another.  For security, scrutiny is always a good thing.  At this point it's the only thing I think that's saving us is like people digging into UEFI and finding problems and surfacing them.  And all of the bug bounties that we have encouraging people to look closely at code.  We've seen that just making something open source doesn't automagically bring more security.  But I really do think in the long run it feels like the right solution, and that it's going to be the approach that wins.



Okay.  Our web browsers and operating systems all collectively trust any security certificate, such as a brand new web server TLS certificate that's never been seen before, because it's been signed by a Certificate Authority that is trusted globally by collectively all of our OSes and web browsers.  But the reason they trust a never-before-seen certificate is specifically and only because the signer of the certificate has promised to, and demonstrated its ability to, restrict its issuance of certificates or signatures on certificates to entities whose identity it has confirmed within the guidelines set up by the CA Browser forum.



Several times during this podcast's life we've discussed instances of apparent mistakes being innocently made, flaws in certificate issuing systems which were being exploited, and also clear instances of deliberate certificate signing abuse.  So everyone's a bit concerned over Russia's recent announcement of its creation of a new, unvetted, untested, and very likely rogue Russian Certificate Authority.  However, its creation was probably inevitable because the sanctions imposed by Western companies and governments are preventing Russian web services from renewing their expiring TLS certificates in absolute good faith.  They would like to.



But Western certificate authorities are banned through sanctions to do business with entities inside Russia.  So what are they going to do?  We all know that browsers take a very dim view of websites which offer them an expired certificate as proof of their identity.  So that won't work.  So if web service providers whose certificates are expiring are unable to purchase certificates from traditional certificate authorities, what choice do they have?  No problem.  Just pick up a certificate issued by your local authoritarian regime.



LEO:  Of which there are many choices.



STEVE:  Exactly.  None of whom are trusted.  Now, there's only one problem with doing that, which is that no mainstream web browser will accept any certificate randomly signed by Russia's newly minted CA.  That's just not going to happen in this lifetime.  Okay, well, there is an exception.  Russia does have the Yandex browser, based in Russia, and it will now, no surprise, work with the Russian Ministry of Digital Development-issued certificates.  So Russian sites are instructing their visitors to please switch over to Yandex.



But of course there is an alternative.  If someone wished to continue using Firefox, or one of the several Chromium-based browsers, in theory they could do so by adding the Russian-trusted root CA certificate to their browser's certificate root store.  But just to be clear, that's the last thing that anyone outside of Russia in the West should consider doing.  Nothing would or will prevent Russian Kremlin Gremlins from creating TLS certificates for any Western websites, thus allowing them to intercept, spoof, and alter such websites' network traffic.



What will likely happen, and it's probably already in the works, if it's not already got the seal of approval stamped on it, is that Chromium and Firefox, maybe even iOS if you're able to install certificates into iOS, I didn't look, will specifically blacklist Russia's rogue root certificate so that no one using Firefox or Chromium browsers or a Chromium browser could be hurt after mistakenly installing it into their browser's root stores.



So for those using the Yandex browser in Russia, it's a workable solution for providing continuity of services during this time.  But there's no conceivable way that any of our mainstream browsers are going to trust any certificates signed by Russia's new Certificate Authority.



Okay.  Now, one last thing.  You may have thought that you were going to escape this week without having to listen to me harp  on WordPress.



LEO:  Oh, well.  We tried.



STEVE:  No, no.  Almost made it.



LEO:  Almost.



STEVE:  I'm about to ask you about Bobiverse, Leo.  But first, I do promise to at least keep this brief.  And so as not to introduce any of my own well-known bias, I'm going to simply quote verbatim from just the top of Bleeping Computer's coverage of a recently released report on WordPress security.



Bleeping Computer wrote, this is not me, this is them:  "Patchstack, a leader in WordPress security and threat intelligence, has released a whitepaper to present the state of WordPress security in 2021, and the report paints a dire picture.  More specifically, 2021 has seen a growth of 150% in the reported vulnerabilities compared to the previous year, while 29% of the critical flaws in WordPress plugins never received a security update.  This is alarming considering that WordPress is the world's most popular content management system, used in 43.2% of all websites out there.



"Of all the reported flaws in 2021, only 0.58% were in WordPress core - 0.58%, that's probably one - with the rest being in themes and plugins for the platform, coming from various sources and different developers.  Notably, 91.38% of these flaws are found in free plugins, whereas paid/premium WordPress add-ons only accounted for 8.62% of the total, reflecting better code vetting and testing procedures."  And, I would argue, professional coders, rather than someone saying, "Hey, I just created an add-on.  It's free.  Here you go."  And I say, good luck to you.



LEO:  Yeah.  Well, and again, it's not in the core, it's in these plugins.  And just be very, very careful, I guess, about what plugins you install.



STEVE:  Yeah.  And I would say, given that the vast majority are in the free plugins, if you want something...



LEO:  Pay for it.



STEVE:  Find something that looks - yeah, pay for it, exactly.



LEO:  Yeah.



STEVE:  Find something from a really reputable plugin provider.



LEO:  We actually had Matt Mullenweg, the creator of WordPress, on FLOSS Weekly last week.



STEVE:  Oh, cool.



LEO:  I didn't mention anything to him about this.



STEVE:  So I've been meaning for weeks, Leo, to ask you if you had continued listening to the Bobiverse.



LEO:  I haven't even started.  I have it.  I bought it, and I haven't even, you know, I have such a backlog of books that I haven't gotten to it.  And I keep thinking, oh, I've got to press play, press play.  So how many books in are you?



STEVE:  Well, the initial three were the trilogy.  And so we have the fourth book.  And, you know, I was a little skeptical because the reviews that I read were a little mixed about it.  Now I understand what's going on.  I mean, it was a grumpy person who didn't want anything to change. 



LEO:  Ah.



STEVE:  Basically the first three books are a particular set of things, a style where we're looking at like this explosion of these - and I think I mentioned before, and this is not giving anything away because this is all - it's like on the jacket cover.  It's that a Bob is a human intellect that was transferred to a computer to create what's known as a von Neumann probe which by definition is a probe which goes out into space, finds a new star system, harvests the raw materials with which to build more of itself.



LEO:  Oh, boy.



STEVE:  So replicates and then sends copies of itself off in all directions, each with the same purpose.



LEO:  What could possibly go wrong?



STEVE:  So thus the Bobiverse.  We end up with a lot of them.



LEO:  Yeah.



STEVE:  And so the original plotline of the first three jumps around among all of these different Bobs.  And I think I mentioned when I talked about it before that I had a hard time keeping track of, okay, which Bob is this?  And so I just gave up and just sort of kept reading into a new chapter.  And I'd go, okay, it's that Bob.  So what's happened in Book 4 is we've focused on one particular interesting thing which has been found.  Now, there's this notion in sci-fi of megastructures.  An example, a well-known megastructure is the Dyson sphere.  Right?



LEO:  Right.



STEVE:  Where a society gets so advanced, and in fact people who've like bothered to chart all this, they have like a nomenclature for different stages of advanced spacefaring civilizations.  Well, at some particular stage, you become able to englobe your entire solar system in a Dyson sphere for the purpose of capturing the entire energy output of the sun, of your local star.  And of course this actually was the plot for one of our very favorite pair of books that Peter Hamilton wrote, "Pandora's Star."



At the very beginning of the book - and again, this doesn't give anything away - Dudley, an astronomer, happens to be looking in a particular direction when a star winks off.  And he's like, what?  Because stars don't wink off.  They can have all, you know, you can have a well-known star life cycle that cosmologists understand.  But turning off is not something that stars do.  So anyway, that was the beginning of the story is what could cause a star to wink off?  Well, its Dyson sphere was turned on is what happened.  So anyway, we need to hear about our last sponsor, and then we're going to go into WACs On or WACs Off.



LEO:  Another wacky episode of Security Now!.  All right, let's talk WACs On WACs Off.



STEVE:  Oh, my lord.  Okay.  So QWAC stands for Qualified Website Authentication Certificates.



LEO:  Oh, of course.



STEVE:  Uh-huh.  Let's have a good acronym; shall we?



LEO:  Yup.



STEVE:  And it's just something that the European Union has proposed to adopt in an amendment to Article 45 of their Framework for Digital Identity, in this case the Digital Identity of websites, not people.  What makes this way more interesting than watching paint dry, or watching the slowly grinding wheels of bureaucracy, is that the EU has been attempting to make this happen for the last six years, since 2016, and they still haven't given up.  And after the most recent proposal by the EU, a group of 38 well-informed security and IT academics and professionals, including the EFF, cosigned an open letter to the EU regulators warning that the enactment of this protocol could expose Internet users to cybercrime.



The open letter is titled "Global website security ecosystem at risk from EU Digital Identity framework's new website authentication provisions."  And the group starts off by saying:  "Dear Honourable Member of the European Parliament, Dear Member of TELE Working Party.  We the undersigned are cybersecurity researchers, advocates, and practitioners.  We write to you in our individual capacities to raise grave concerns regarding certain provisions of the legislative proposal for a European Digital Identity framework, the eIDAS revision, and their impact on security on the web."



Now, the EU doesn't appear to be backing down about what they want, and we've seen what a mess can be created when something like the GDPR and its cookie regulations are imposed upon the world.  So I was very interested to understand what the EU has gotten themselves up to now.  They've intruded into our lives by making us agree to every website's cookie usage.  So what's coming next?



I spent hours working to wrap my head around this.  I've read the arguments being made by both sides and all the references I could find.  I did find a very strong and beautifully written explanation of the fundamental problem with what the EU wants to do, which I believe was probably, well, I know it written at least by last July.  I think it was written by Google's Ryan Sleevi.  At least he had a tweet which pointed to it for more information.  But again, it was written last July, and the EU still doesn't appear to get it.



What the EU wants is something more than TLS certificates.  They want to have their own website authentication, essentially the ability to bind their own array of government validation and authentication assertions to websites, so that when an EU citizen is browsing the web and goes to a website, they want the user's browser to display a top-of-page banner stating that "This website uses a QWAC."  I'm not kidding you.  And then because they realize - I saw an example of this in one of their slides.  Actually I have the slide, although it's so tiny in the show notes it's difficult to see.  But it comes up in a banner at the top of the page, and then it says in parens, (Qualified Website Authentication Certificate).  Wow.  And then a list of a few privacy and security assertions in this banner which will have three tabs:  a Summary tab, an ntQWAC Validation tab, and a TLS validation.  And there will be an "I got it" button to dismiss the banner notification.  Please, please don't make us do this.



Anyway, so what's going on here?  How is this done?  The website would serve some JavaScript.  The website that the user is visiting would serve some JavaScript, who knows, perhaps the same JavaScript that now makes us all agree to be cookied.  This JavaScript makes a call to an external web service which is tasked with producing a report which will be returned for display to the user.



The web service first retrieves another blob, known as the ntQWAC, from a well-known default path on the website that the user is visiting.  And we've talked about well-known paths before; right?  Where it's domain/well-known/something, where a client knows to ask for something from the server.  So that blob contains a number of government-provided security and other assertions about the site and a hash of the website's TLS certificate.



Now, this is just a blob on a website.  The web service needs to have the blob validated.  So it queries an online validator in the next step to verify the authenticity of this particular ntQWAC blob.  It then hashes the original website's TLS certificate and compares it to the hash it expects.  So that binds the TLS certificate to it.  Presumably that's to prevent certificate forgery, such as we have mentioned earlier in this show and also in that BGP routing hack the other day.  Okay.  So now it knows that it's associated with the proper TLS certificate.  Next, it processes this ntQWAC attributes to produce a structured ntQWAC validation report, and it then performs an on-the-fly validation against additional authoritative sources, which are not specified, but they're shown in the diagram of how this all works.



And when all that's done, the web service returns the validation report to the waiting JavaScript, which then presents the banner to the EU citizen, who will have become quickly trained to click "Okay, I got it," just to get rid of the thing.



LEO:  Exactly the problem.



STEVE:  Yes.  It's just - it's ridiculous.  Maybe they could combine them with the "Okay to Cookies" button so you don't now have to press two things, one thing to get rid of the authentication from the government and another button to get rid of the cookies which the government has made be posted.



So, what do we get in return for all this industry?  We get what the EU apparently wants, which is its own governmentally controlled, separate from the TLS environment and community, governmentally controlled facility for determining, controlling and displaying various privacy and security assertions about individual websites.  The EU essentially wants to add a layer of their own input - much as they have with cookies, thank you very much  into the browsing experience of all of their citizenry.  There are a number of problems with this.  But as I said, these problems have been patiently, carefully, articulately, and repeatedly laid out and explained for the last six years, so far to no avail.



Now, okay.  One biggie should stand out to everyone right off the bat.  The web has been deliberately designed as a two-party system.  The user with their browser and whatever website they are visiting are the two parties.  It's me and you.  Now, while it's true that in today's world this pure two-party system immediately collapses due to a website's voluntary and deliberate inclusion of third parties which it brings in, that's the site's choice to make.  And we have been carefully chronicling all web browsers moving toward ever greater and stronger isolation of any and all third parties.  They're being siloed.  They're being given their own caches.  This whole, you know, we're all together, one big happy cookie farm, that's all going away.  So that controls tracking and profiling.



The larger problem to me is that - so there's that.  There's the fact that this essentially codifies a formal third-party presence involved in presenting all of this.  This web service, whatever it is, is like central headquarters for tracking of every user that goes to any of these sites.  But the even bigger problem that occurred to me is what prevents a malicious website from simply displaying the same happy news QWAC banner and giving its intended victim the all clear?



LEO:  Oh, man.



STEVE:  You know, if the system is that easily spoofed, then what's the point?  I mean, the whole thing seems absolutely nuts.  The only way to prevent that would be to force it into the Chrome, the browser UI Chrome, much as the certificate can be - you're able to view the certificate.  And that's not the web page showing you that assertion, that's the browser's unalterable Chrome, its UI that lets you see the certificate that it received.  So the only way you could prevent casual spoofing is if the EU forced the design of the browsers to somehow display this crap.



And, I mean, this is very - in much of what I read it's very much compared to the EV certs, where the idea was to show some additional information that would be useful to the user.  And we know what happened to that.  First they weren't green any longer, and then they were just gone completely from the user interface.  So the EFF in their never boring and always over-the-top, sky-is-falling style, issued a press release on March 3rd.  I have the thing in the show notes, but I'm not going to bore everybody with it.  As I imagine reading this, I'm just thinking, okay, everyone knows.  It's all, oh my god, this is going to completely destroy all of the website and all of our security, and experts are urging the EU to amend the revised Article 45 and so forth.



But what they talk about actually in this letter is different because they're talking about the enforcing of other certificate authorities, which is not the system that I just read.  So I thought, what?  Well, it turns out that that wasn't explained in the slide deck that I found, of which one of those slides is in the show notes.  There is an entirely different system.  The reason is there are actually three different proposed ways that this system could work.  And the one the EFF is all steamed up about is the worst of the three, which may be why it wasn't put in the slides.  It is a proposed single certificate solution.  In that solution, various governments - various governments - would each have their own root certificates added to our operating systems and browser root stores.  And these governments would be able to issue TLS website certificates containing all of that additional authentication information they desire.  And this is the point in the podcast where in unison we all ask:  What could possibly go wrong?



LEO:  What could possibly go wrong?



STEVE:  Yes.  So the Internet Society has published an Internet Impact Brief titled "Mandated Browser Root Certificates in the European Union's eIDAS Regulation on the Internet."  And skipping past a bunch of the intro, I'll just share three paragraphs.  They said:  "The European Commission is currently" - and this is the Internet Society, like InternetSociety.org, so toned down from the EFF.  They said:  "The European Commission is currently evaluating this regulatory framework.  It organized a public consultation in 2020 to collect feedback on drivers and barriers to the development and uptake of trust services and eID (electronic ID) in Europe.  On this basis, the Commission has proposed an amendment to the 2014 regulation to try to stimulate adoption of the framework and the availability of secure eID schemes across the internal market.



"Among the changes in the new proposal are provisions around QWACs that could have the effect of forcing browsers to include TSPs" - those are Trusted Service Providers in the EU's parlance, we know them as Certificate Authorities - "authorized by national governments into their root stores, without guaranteeing security parity with current best practices.  In other words, it would require browsers to add these CAs to the list of trusted middlemen, even if they do not meet industry standards or are recognized as a trusted party."



They said:  "Although the text of the proposal may seem uncontentious, the high-level goals have unstated technical consequences which we believe are unpalatable, regardless of which option is put into practice."  Then they said:  "For more information about our interpretation of the proposed revisions, see Annex A."  And I included Annex A in the show notes, but I'm not going to go over it because basically it's what we talked about.



So this ETSI's core assumption is that browsers will validate, in other words recognize, a QWAC by binding it to a TLS certificate for the domain it is intended to represent.  Since TLS certificates themselves do not provide the level of authenticity eIDAS wants to achieve, for example, they might just be a domain validation, a DV certificate like Let's Encrypt produces, or like a non-EV, or might not even be organizational, OV, they say this means that QWACs must be issued according to a separate set of processes and criteria for eIDAS-compliant Trust Service Providers.



So at this point it is very unclear what's going to happen.  And I wouldn't worry about it except that we're all now having to click on, yes, I agree about your g-d cookies for every website we visit.



LEO:  So out of control.



STEVE:  It is.



LEO:  Does QWACs solve a problem that exists?  I mean, isn't TLS sufficient?



STEVE:  Yeah, they don't think so.  They want...



LEO:  What's the problem they want to solve?  I don't understand.



STEVE:  They want browser certificates to contain additional verified like identity-ish things.  Like an actual name and an actual street address and an actual, I mean, they want to tie it to identity.  And so even that raises the hackles of a lot of people who are saying, wait a minute, we want the web to continue to be able to offer some level of anonymity to protect people.  And EU is saying, well, digital identity is important, and we want to push this down into the web browser so people know more about the sites they're visiting.



LEO:  Yeah, boy, I don't like this idea at all.



STEVE:  I know.  And they're not giving up on it.



LEO:  Wow.  It doesn't solve anything, and it introduces all sorts of security problems.  It breaks things.  And the worst thing I can imagine, and now more than ever this should be obvious, is to have countries have their own certificate authorities that you have to adhere to.  I mean, sorry, that's not - I'm sorry, that's terrible.  Plus another banner.  Just what we want.  I already - there are many pages now that have such long cookie banners I literally can't ignore the banner.  I have to click past it to get to the site.



STEVE:  Comes up, like half the page is there blocking you from doing anything.



LEO:  Yeah.  And by the way, Steve, correct me if I'm wrong, but  don't they have to save a cookie to say that you've seen this banner and not show it again?  Am I wrong?  That's a cookie.  So you can say I don't want any cookies, you're going to see that banner forever.  The whole thing is just so broken and stupid.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Yeah.  And, you know, on the coattails of GDPR, which has been a mess for everyone.



LEO:  Well, I'm in favor of some privacy protections, for sure.  And I think GDPR, if it weren't for GDPR, there would be none.  So okay.  But the cookie thing is ridiculous.  And this sounds not merely ridiculous, but actively dangerous, breaking.  It's why you don't want politicians making security or technology decisions.



STEVE:  Well, okay.  So if they wanted to do some legislation, why not require that the Do Not Track header is honored?



LEO:  There you go.  Adhere to that.



STEVE:  Exactly.  Just do some legislation and let people say they don't want to be tracked.  And if you see that, you cannot track them, and that's the law.  Thank you very much.  But no.



LEO:  That would be easy.  That would be simple.  Instead we get this.  I don't even - this is crazy.  This is just - this is like somebody's fever dream.



STEVE:  And, exactly, what prevents other sites from spoofing it?



LEO:  Yeah, that's easy to put that up.



STEVE:  Yeah.  Just do a screenshot of one that you like and just show it.



LEO:  Silly.  Silly, silly, silly.



STEVE:  I wish it were not like such a potential problem.  I mean, like it could actually happen.



LEO:  Somebody in the chatroom is saying, and maybe this makes sense, it's about taxation.  Maybe that's possible, you know, they don't want anybody evading their taxation.



STEVE:  Well, on that screen there was something about VAT.  I think it was one of those authentication providers.  There was some VAT thing.



LEO:  Interesting.



STEVE:  So who knows.  Anyway...



LEO:  Once again, you know, if you didn't listen to this show, you wouldn't even know about this stuff.  That's why you've got to listen.



STEVE:  Yeah, you could actually sleep at night.



LEO:  Yeah.  You'd dream the dream of babies.  You'd be so happy.



STEVE:  The world has no problems.  My computer is secure.  I can go anywhere I want.  



LEO:  Everything is fine.



STEVE:  I can go anywhere I want to.  I can click any link that I see.  What could possibly go wrong?  Nothing.



LEO:  We do this show every Tuesday, right after MacBreak Weekly, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You could stop by and watch us do it live at TWiT.tv/live.  The IRC channel is always going, but of course it's most active when there's a live show on, and that's irc.twit.tv.  Members of Club TWiT get their very own little Clubhouse, our Discord server, which isn't just for show conversations, conversations about everything.  Stacey's Book Club is coming up; a Paul Thurrott Fireside Chat.  We're going to talk to Patrick Delahanty on Thursday.  He's our engineer.  If you're curious about the back end, he's got a great back end.



Steve's got a copy of this show, actually two unique versions of the show.  He's got a 16Kb audio file for the very bandwidth-impaired folks, plus Elaine Farris's amazing transcription so you can read along as you listen at GRC.com.  While you're there, pick up a copy of SpinRite.  That's Steve's bread and butter.  That's what keeps the boat afloat.  It's the world's best mass storage maintenance and recovery utility.  SpinRite 6 is the current version; 6.1 is in progress, in process.  And you can participate in the development as well as get a free copy if you buy right now:  GRC.com.



The website for us, for the show, we have 64Kb audio, and we have video, is TWiT.tv/sn, TWiT.tv/sn.  There's a YouTube channel dedicated to Security Now!.  That makes it easy to share if you want your boss to see something or something like that, or your coworkers or colleagues or buddies.  Just send a share link from YouTube, that's a good way to do it.  You can also subscribe on your favorite podcast player and get it automatically every Tuesday evening, as soon as it's available.  If you do that, please leave us a five-star review.  Let the world know about this, the best, the one, the only Security Now!.



STEVE:  The original.



LEO:  The original.  Since 2005, baby.  Now in its 18th, 17th year of edumacation.  Thank you, my friend.  Have a great week. We'll see you next time on Security Now!.



STEVE:  Right-o.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#863

DATE:		March 22, 2022

TITLE:		Use After Free

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-863.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at the U.S.'s new cybercrime reporting law that was just passed.  We examine a worrisome software supply chain sabotage and the trend it represents.  We look at "Browser-in-the-Browser," a new way to spoof sign-in dialogs to capture authentication credentials, and we examine the way MikroTik routers are being used by the Trickbot botnet to obscure their command and control servers.  A very concerning infinite loop bug has been uncovered in OpenSSL - time to update! - and CISA walks us through their forensic analysis of a Russian attack on an NGO.  We then take a look at the Windows vulnerability that refuses to be resolved, and we'll finish by spending a bit more time than we have so far looking more closely at why Use After Free flaws continue to be so challenging.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's a new sheriff in town, a new U.S. law requiring cybercrime reporting.  We'll take a look at that.  A supply chain sabotage from one of the developers and maintainers of Node.js.  And a new way to spoof sign-in dialogs, it's called "Browser in Browser," and it's scary.  Plus Steve will take a deep dive into a common source of security flaws called the Use After Free bug.  It's all coming up next on Security Now!.  	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 863, recorded Tuesday, March 22nd, 2022:  Use After Free.



It's time for Security Now!, the show where we cover your security, your privacy, everything you need to know about staying safe online with this cat right here, Steve Gibson, GRC.  He's doing the - now I know why you love Bobiverse, by the way.  It's Star Trek-focused.  There's a lot of Star Trek jokes in there.



STEVE GIBSON:  There is actually.  In fact, you won't know it - I don't remember if it's in the third book or the fourth, but there's actually - there's this thing that he calls, how would I say it, replicative drift, where...



LEO:  Yes, because even though he's replicating himself, each one is slightly different.



STEVE:  Which, you know, it's like why is a computer going to have that, but it's a required, you know...



LEO:  It's an AI.  It's an AI.



STEVE:  Well, but...



LEO:  And you need it, that's right.  Otherwise they'd all be the same.



STEVE:  Exactly.  So it's a required plot vehicle for him in order to get different Bobs, as all these Bobs keep replicating.  So what happens is over time there are some - and of course you're right, Star Trek is a theme through the series.



LEO:  Well, one of the Bobs names himself Riker.



STEVE:  Yes, Will.



LEO:  Bob Number Two, yeah.



STEVE:  Yes, in fact, and he later drops Riker and just switches to Will.



LEO:  Will, that's funny.



STEVE:  But still it's that connection.  But then what happens is, as things go on, they start involving themselves with other cultures.  I mean, not in a bad way.  Like to help out these other intelligent sentient aliens who they find.



LEO:  They violate the Prime Directive?



STEVE:  Yes.  And there is a group that forms that are all up in arms about this, and they literally get labeled "Star Fleet."



LEO:  I'm just finishing the first book because you shamed me last week, saying have I read it yet, and I hadn't started it.  And I love it.  It reminds me so much of "The Martian" and "Project Hail Mary" in the attitude of the narrator.  Of course I'm listening to Ray Porter's narration, it's the same guy.  So he's got that same kind of funny attitude.  And in the science writing and the problem solving, it is another Andy Weir.  Andy Weir 2.  It's really good.  It's called "The Bobiverse."  First one's called "We Are Legion," by - I can't remember the guy's name.  Something E.  I know his middle name is E.



STEVE:  Well, in fact I was just thinking about this yesterday.  There's a - you know, there's no spoilers here.  But there's a moment where there's a problem that's being solved, and they're needing to rotate their passwords and update their keys.  And I'm like, I mean, that's what the guy is talking about.



LEO:  I know, I love it.



STEVE:  No wonder our listeners love this stuff, yeah.



LEO:  Dennis E. Taylor.



STEVE:  Dennis Taylor.



LEO:  And they're great, I have to say.  And I've only read the first one.  But so far I'm really, really enjoying it.



STEVE:  Yeah, you won't be let down.  So today we are at 3/22/22.  And there wasn't any big, I mean, there's like a lot going on, of course, because we still have all of this tension globally.  But a number of our listeners have written with questions about Use After Free.  We've talked about this vulnerability, how it continues to keep being a problem.  And when I saw that, after looking at everything that I wanted to talk about this week there wasn't anything that was clearly topical, I thought, well, I'm just going to spend some time digging a little bit deeper into this issue of memory management in our contemporary computer systems to answer sort of as a group a lot of these questions that we've had.  So I titled today's podcast "Use After Free," just because it's the way we're going to wrap things up.



But first we're going to take a look at the U.S.'s new cybercrime reporting law which finally did get passed and signed into law last week.  We're going to examine the most tweeted to me thing of the week, which was a very worrisome software supply chain sabotage.  And even more worrisome than one instance is that it turns out this is looking like a trend.  And it is concerning.



We're going to look at the browser-in-the-browser exploit, a new way to spoof sign-in dialogs to capture authentication credentials; and examine the way MikroTik, or as I originally called them "Microtic," routers are being used by the Trickbot botnet to obscure their command-and-control servers.  We've got  an infinite loop bug of great concern which has been uncovered by our friend Tavis Ormandy at Google in OpenSSL, which is what makes it a big concern - time to update - and CISA walks us through their forensic analysis of a Russian state-sponsored attack on an NGO, a non-governmental organization.  We then take a look at the Windows vulnerability that refuses to be resolved.  And we'll finish, as I said, by spending a bit more time than we have so far looking more closely at why Use After Free flaws continue to be so challenging.  And we also, for those who are coders, we have a terrific Picture of the Week.



LEO:  It's kind of relevant, too.  Hey, there's also a news story that I want to point you to and let you chew over it before we talk again.  Of course you spent many years working on SQRL, a better way of doing passwords.  I don't know if you saw this, but the FIDO Alliance has said, oh, we think we now know how to enter a passwordless world.  They say, I'll just quote a little bit of it:  "The passwordless FIDO standard already relies on device biometric scanners to authenticate you locally without any of your data traveling over the Internet to a web server for validation.



"The main concept FIDO believes will ultimately solve the new device issue," because of course switching and adding devices becomes the problem, "is a FIDO credential manager," this sounds a little bit familiar, "which is somewhat similar to a built-in password manager.  Instead of literally storing passwords, this mechanism will store cryptographic keys that can sync between devices and are guarded by your device's biometric or pass code lock."  I think yours is a little better, to be honest.



STEVE:  Yeah, mine only needs one, and it synthesizes them on the fly based on where you are, as we know.



LEO:  The problem is FIDO is Apple and Microsoft and Google, and you're just you.



STEVE:  Yup.  And you have not seen me any spend any more time.  After I spent seven years solving the problem, I dropped it like a hot rock and switched to SpinRite because I solved the problem.  If the world wants it, great.  Otherwise, fine.



LEO:  Somebody should write to FIDO and say it's been solved, take a look.  Because you address the exact thing they're talking about, the portability of these keys, and you address it in I think a better way than they're talking about.



STEVE:  Yeah.  And, you know, all kinds of little edge cases like users telling the website, oh, I lost my key, please let me use something else.  Well, that's an obvious exploit for spoofing.  And so with SQRL you can set a little checkbox that says please don't ever listen to those requests.  And so, I mean, and if you lose your credentials, how you can get them back, how you're able to obsolete lost credentials, I mean, I just - I did solve it once.  And there it is.  Meanwhile, I'm having a lot of fun making great progress with SpinRite.



LEO:  Good.  We'll hear more about that in just a second, and get our Picture of the Week, as well.  Steve Gibson, Security Now!.



STEVE:  So this is just a fun one.  It depicts the Six Stages of Debugging, which, as I said, most of us who have written code will understand.  The first stage is the "That can't happen" stage.  We would call that denial.  



LEO:  Denial, yeah.  Not in my house.



STEVE:  Then we have, exactly, then the second one is "That doesn't happen on my machine."  It's like, wait a minute, you know, it's something wrong with your computer.  It's all good here.



LEO:  I can't duplicate that, yeah.



STEVE:  That's right, exactly, can't duplicate it.  Then the next stage, as the coder moves through this painful process, is  "That shouldn't happen."  And so Stage 3.  Then when the coder has recognized, okay, well, I guess is it happening, we get to this fourth stage, "Why does that happen?"  And then when that gets realized and internalized, we get to the fifth stage of, "Oh, I see."



LEO:  In big letters.  OH, I SEE.



STEVE:  Exactly.  And then I love Stage 6, which is like I've felt it before.  It's like, "How did that ever work?"



LEO:  So true.  So true.



STEVE:  Like, my god, how did it take until now to find that?  But, oh, yeah, that's the nature of the process.  Okay.  So last Tuesday a $1.5 trillion government funding bill was signed into law.  I think that keeps us ticking until September.  President Biden signed it.  And we're talking about that because part of that, part of what was packaged into that legislation is something we've touched on a few times when it had periodically been discussed.  But what was under consideration until now, and it was only under consideration, became law.  That new law mandates that owners and operators of critical U.S. infrastructure, to be defined in a minute, must report - must report - when their organization has been hacked, and if a ransomware payment has been made.



So this so-called - it's called the "Strengthening American Cybersecurity Act" of 2022, and unfortunately no good acronym jumps out.  SACA, I don't know.  It's not very catchy.  Anyway, it was attached to the spending bill, which requires, as I said,  the critical infrastructure operators alert the Homeland Security Department's CISA, the Cybersecurity and Infrastructure Security Agency, within 72 hours, three days, of a breach; and within one day, 24 hours, if the organization makes a ransomware payment.  This new legislation also grants CISA subpoena power, allowing it to compel testimony from any entities which they believe have not reported a cyber incident or a ransomware payment.



And what's interesting is that this move is a reversal in recent policy because this legislation was removed, or as the legislators like to say, "stripped out of" the recent annual defense policy bill where it first tried to make it into law.  And that was signed a few months ago.  So, you know, given this change of heart, one has to wonder whether the potential for attacks against U.S. infrastructure, which would be secondary to the Ukraine/Russian conflict, might be behind this change of heart.  And my eyebrows rose when I was learning that this so-called "Strengthening American Cybersecurity Act" passed the Senate unanimously.  So no dissent at all.



LEO:  Wow.



STEVE:  Yes, let's do - I know.  Like when was the last time  something was unanimous in our U.S. Senate recently?  So again, it's no skin off anyone's teeth.  They're basically saying, you know, just notify us if something happens.  So now CISA will have up to two years to publish a notice in the Federal Register on proposed rulemaking to implement the reporting effort.  You know, nothing ever happens quickly.  However, reporting on this has suggested that CISA might choose to move faster, thanks to the current heightened threat climate.



The Senate's Homeland Security Committee Chair Gary Peters, who authored and championed the legislation along with Senator Rob Portman, issued a combined statement saying:  "This historic new law will make major updates to our cybersecurity policy to ensure that, for the first time ever, every single critical infrastructure owner and operator in America is reporting cyberattacks and ransomware payments to the federal government."



And additionally, Rob Portman, the panel's top Republican, said the legislation will "give the National Cyber Director, CISA, and other appropriate agencies broad visibility into the cyberattacks taking place across our nation on a daily basis to enable a whole-of-government response, mitigation, and warning to critical infrastructure and others of ongoing imminent attacks."



So as I was researching this I got to wondering, because it sort of seemed like a loophole to me maybe, what exactly constitutes critical U.S. infrastructure?  We hear the term all the time.  But who exactly would be required to report?  So to answer the question, I attempted to read, and that was a mistake, some of the actual legislation.  I don't recommend it because now I have even less idea how our government actually manages to work, and more amazement that for the most part it kind of seems to.



I did manage to track down Presidential Policy Directive 21, Section 2242 - that's right, Section 2242 - subsection 'b'.  And under the heading there, "Designated Critical Infrastructure Sectors and Sector-Specific Agencies," it explains:  "This directive identifies 16 critical infrastructure sectors and designates associated Federal SSAs."  Those are Sector-Specific Agencies of which DHS is almost always their reporting-to agency, the responsible agency.  Like DOE has the Department of Energy, DOD has the Department of Defense obviously, and so forth.  But in general,  DHS, the umbrella.



They said:  "In some cases co-SSAs are designated where those departments share the roles and responsibilities of the SSA. The Secretary of Homeland Security shall periodically evaluate the need for and approve changes to critical infrastructure sectors and shall consult with the Assistant to the President for Homeland Security and Counterterrorism before changing a critical infrastructure sector or a designated SSA for that sector.  The sectors and SSAs are as follows."  And as I said, this was after like a lot of distillation and whittling it down because, oh, my lord.



Anyway, those 16 are:  Chemical; Commercial Facilities; Communications; Critical Manufacturing; Dams; Defense Industrial Base; Emergency Services; Energy; Financial Services; Food and Agriculture; Government Facilities; Healthcare and Public Health; Information Technology; Nuclear Reactors, Materials, and Waste; Transportation Systems; Water and Wastewater Systems.



So anyway, I was curious.  And in case anybody else is, those are the 16.  So the bottom line is reporting attacks and extortion payments is no longer optional.  It will be the law.  Or it is now, although it looks like CISA has some - has to get like, since they are the reporting-to agency, they've got to figure out how to turn it into a sufficiently large bureaucracy.  Okay.



Winning this week's prize for the most listener-reported incident was a very worrisome software supply-chain event.  I don't think it qualifies for being called an "attack" since it was an inside job.  Consequently most of the tech press is using the term "sabotage."  NPM is the package manager we've discussed from time to time for today's number one most popular, most used, and most in demand programming language, JavaScript.  NPM is the default package manager for JavaScript's super-popular runtime environment Node.js. And a very prominent Node module known as Node-IPC, as in interprocess communication, which is commonly used for local and remote interprocess communication which has support for Linux, macOS, and Windows is the subject here.  Node-IPC has over 1.1 million downloads per week, which puts it up at the high end of popularity.



In the context of Java's, now, not JavaScript, Java's Log4j incident and vulnerability, we were talking about code dependency trees, the idea being that one software library relies upon several others, and they may rely upon others, et cetera, creating this rapidly branching tree which has the effect of creating a deep set of interdependencies.  Liran Tal at "snyk" - I don't know how you pronounce S-N-Y-K.



LEO:  Snike?



STEVE:  Snike?  Yeah.  Maybe.  I hope it's not Snike.  That'd be bad.  Where do you work?  I work at Snike.



LEO:  Snike.



STEVE:  Like, what, on purpose? Deliberately?  Okay, anyway, he was the first to report their discovery of this problem.  Liran wrote, he said:  "On March 15, 2022, users of the popular Vue.js frontend JavaScript framework started experiencing what can only be described as a supply chain attack impacting the npm ecosystem.  This was the result of the nested dependencies Node-IPC and 'peacenotwar' being sabotaged as an act of protest by the maintainer of the Node-IPC package."



He said:  "This security incident involves destructive acts of corrupting files on disk by one maintainer and their attempts to hide and restate that deliberate sabotage in different forms.  While this is an attack with protest-driven motivations, it highlights a larger issue facing the software supply chain, that the transitive dependencies in your code can have a huge impact on your security."



Okay.  So the story started back on March 8th, at which time npm maintainer for this package, "RIAEvangelist" is his handle, his real-world name is Brandon Nozaki Miller, he wrote some code and published an npm package named "peacenotwar," presumably referring to what's going on over Ukraine.  And that package describes itself:  "This code serves as a non-destructive example of why controlling your node modules is important.  It also serves as a non-violent protest against Russia's aggression that threatens the world right now.  This module will add a message of peace on your users' desktops, and it will only do it if it does not already exist, just to be polite."



Okay.  Now, first of all, it's interesting that Brandon writes that he'll only place his antiwar message on desktops that have not already received the message "just to be polite" because what Brandon then does is anything but polite.  His peacenotwar package sat around for a week with hardly any downloads.  Then he adds this peacenotwar module as a dependency to one of his other very popular modules.  Apparently he has 40 that he's maintaining.  In this case, he added it to the dependency of Node-IPC, which caused it to get sucked down 1.1 million times per week.



Then apparently still getting even more amped up over the Russia/Ukraine conflict, and feeling that he has the power to be more proactive, Brandon added some deliberately Base64-obfuscated sabotage code to his Node-IPC package.  This updated release added a function to check the IP address of the developers who were using Node-IPC in their own projects.  When an IP address geolocated to either Russia or Belarus is found, the new version wiped the file contents from the machine, replacing them with a heart emoji.



LEO:  Well, at least there's love involved.



STEVE:  There is obviously quite a lot that's wrong about this.  But on top of it just being plain wrong to take advantage of one's implicitly trusted position as a package maintainer, to have one's package do something that it was clearly not sanctioned to do, Brandon's targets were being very poorly chosen.  If we're to believe what we're told about the war sentiments of the Russian population, it's the younger coders and developers who are more likely to be using Russian propaganda bypassing measures and to have a much more Western view of what's actually happening over there.  And along comes some idiot in the West who wipes out a sympathetic developer's files only because of where they are coding.



And I should mention, it's not in the show notes, but the EFF just went ballistic over this.  I mean, whoa.  And sad as all that is, even more sadly, Brandon is not alone.  The count of similarly recently altered software has risen to 21 incidents, separately having been identified.  I noted as I looked through the list, it was in Russian, so I wasn't able to read the details, but I could see that the popular GUI toolkit QT was on the list, up near the top, actually; and another is es5-ext, which provides code for the ECMAScript 6 scripting language specification.  A new dependency named postinstall.js, which the developer added on March 7th, checks to see if the user's computer has a Russian IP address, in which case the code broadcasts a "call for peace."  Whatever that is.



So while not all of these changes are destructive, as Brandon's ended up evolving into, they are almost certainly not what those who are using them want or have every right to expect.  Now, Brandon quickly removed his malicious changes after the very predictable outcry arose from other enraged developers.  But, boy, you know, he has certainly not enhanced his credibility or reputation.  And as I said, it does also point out what is probably a larger problem with the fact that we've ended up putting ourselves in a position where we are trusting in the goodwill and ongoing goodwill and the unfaltering behavior of lots of random developers whose software we're using, and we link it into a dependency tree.  And our code grabs it, no matter what it is, assuming that it's going to stay what it was before.  And of course we have no guarantee of that being the case.  So, yikes.



As I start to describe this latest attack I have to conclude that the bad guys really do have an advantage.  They're able to sit back and steeple their fingers with a faraway look in their eyes.



LEO:  Bwa-ha-ha-ha-ha.



STEVE:  Exactly, while they endlessly cook up one sneaky way after another to bend the technology we've innocently given them to their dastardly purposes.



Today we have the latest attack concept that's been named Browser-in-the-Browser.  It could have been done before, but it just occurred to a pentesting guy.  It's a novel new form of phishing attack which was designed and documented by a pen tester who goes by the handle "mr.d0x."  It's obvious once you see it, but it would not be obvious to a user who were to encounter it when they're expecting to see it.  It's able to perfectly and convincingly simulate the pop-up browser window that we're all encountering more and more often with the spread of those third-party single sign-on options embedded on websites, you know, the Sign In With Google or Facebook or Apple or Microsoft, whatever.



I got two screenshots side by side which this guy produced, demonstrating that there's no way to tell them apart.  With page rendering which allows JavaScript, CSS, and HTML to perfectly recreate the appearance of a legitimate "Sign In With" whomever pop-up dialog, an innocent user has no idea.  And mr.d0x notes that one of the things that makes this attack so effective is that the spoofed dialog clearly and prominently displays the exactly correct domain name, the verification of which we've been drilling into our users for years, saying "Make sure you're at the right site.  Double-check the URL for any typos or lookalike characters.  Don't be fooled."



Well, then along comes this perfect replica of a third-party sign-in dialog that accepts their innocently entered username and password.  And because the JavaScript just printed a fake URL at the top of the dialog, which is where they're expecting...



LEO:  Oh, I see.



STEVE:  You know...



LEO:  It's easy, I mean, I understand it's easy to duplicate the login screen.  That's just it's all open.  It's JavaScript and HTML.  I get it.  So instead of where the browser puts the URL, you just put a graphic.



STEVE:  Yup.



LEO:  That looks the same.  And how would you know unless you clicked it.  I guess if you clicked it, you might not be able to click the padlock, for instance, I don't know.  Oh, that's really interesting.  Wow.



STEVE:  Isn't it?  I mean, and again...



LEO:  Of course, yeah, easy.



STEVE:  You click on Sign In With Facebook.  Up this thing pops.  And you think, oh, I mean, you're expecting it.  And so you may be doing due diligence.  You go, okay, wait, F-A-C-E-B-O-O-K dot com, yup, that's Facebook.  Nope.



LEO:  Wow.



STEVE:  Oh, and Leo, just because mr.d0x - I couldn't remember if it was dr.d0x or not.



LEO:  Dr.d0x would be better, but I guess he didn't get a degree.



STEVE:  Anyway, mr.d0x, yeah, to avoid any unnecessary duplication of effort, mr.d0x has thoughtfully provided attacker-ready, easy-to-use downloadable templates.



LEO:  He even has dark mode.



STEVE:  That's right.  You've got your dark mode, you've got your light mode, you've got your Chrome on Windows or macOS, whatever you need, just get it from GitHub.



LEO:  Yeah.  So you can't really just say look at the URL anymore.



STEVE:  Nope.



LEO:  I do wonder, though, what would happen - I guess you could write some JavaScript that if you clicked the padlock you've get something else.



STEVE:  Probably another little thing that says, yeah, it's all good.



LEO:  Oh, yeah. 



STEVE:  Yeah, because the attacker has control of the window, so it can do whatever you want.  I mean, it's a fake URL, so yeah, fake the click on the padlock.  Yeah.  And actually you'll be, since we were talking about SQRL, this already occurred to us back then.  And a lot of attention was given to designing a pop-up dialog that could not be spoofed in this fashion.  So again, we did a lot of work at the time.  But yeah, wow.  I just, you know, look what we've done to poor users.



LEO:  I'm sorry.  I'm sorry.



STEVE:  You know?  No wonder your mom says, "Leo, how do I, dot dot dot."



LEO:  What's going on?  By the way...



STEVE:  And of course...



LEO:  I know you're not reading the Bobiverse, you're not listening to it as an audiobook.  You're reading it.  But Ray Porter reading it does all the voices.  He does Homer.  Not a great Homer, but he does it.



STEVE:  Oh, no kidding.



LEO:  So he's doing a lot of good voices in there.  It's pretty funny.  He does - Colonel Butterworth he does perfectly.  It's very funny.  Anyway, sorry, didn't mean to distract.  Yeah, this is crazy.  What do we tell people now?  We always said make sure it's the right URL.  You have to manually enter it?  But that's not going to work for an authentication window.



STEVE:  And, well, look at the stuff that follows it.  It's like I used to joke, one of my best friends was a lot older than I.  He has since passed.  But his name was Gary.  I used to joke that the Windows, what is that key we type in, the five groups of five, I forgot what you call that.  Anyway, it was...



LEO:  The serial number, you mean, yeah, yeah.



STEVE:  Yeah.



LEO:  And you had to type it in.



STEVE:  Yes.



LEO:  The license key, yeah.



STEVE:  The license key.  I used to joke that it was the equivalent of copyright protection...



LEO:  Oh, it's terrible.



STEVE:  ...because he was never able to get it right.



LEO:  Remember?  We just typed that in.  That was crazy.



STEVE:  Oh, yeah.  He'd be looking at it going, okay, JQ5PP, yeah.  And I'd go, Gary, no, that's an 8.



LEO:  Oh, god.



STEVE:  But anyway, look at URLs.  URLs used to be - they were supposed to be meaningful.  Now they've got these GUIDs in the that's just gibberish.  So, yeah, you can't ask a user to type that.  They'd never get logged in.  They'd just go away.  So, wow.



Okay.  Last Wednesday, Microsoft blogged under the title:  "Uncovering Trickbot's use of IoT devices in command-and-control infrastructure."  Microsoft considers MikroTik consumer routers to be IoT devices.  That's how they lump them.  And I suppose since they lack a keyboard or display, and they are by definition connected to the Internet, they qualify as Internet of Things.  So Microsoft offered an interesting forensic write-up which I'll share the best bit of.  They begin with a bit of history.



They said:  "Trickbot, a sophisticated trojan that has evolved significantly since its discovery in 2016, has continually expanded its capabilities and, even with disruption efforts and news of its infrastructure going offline, it has managed to remain one of the most persistent threats in recent years.  The malware's modular nature has allowed it to be increasingly adaptable to different networks, environments, and devices.  In addition, it has grown to include numerous plugins, access-as-a-service backdoors for other malware like Ryuk ransomware, and mining capabilities.



A significant part of its evolution also includes making its attacks and infrastructure more durable" - and that's where we're headed here - "against detection, including continuously improving its persistence capabilities, evading researchers and reverse engineering, and finding new ways to maintain the stability of its command-and-control framework.



"This continuous evolution has seen Trickbot expand its reach from computers to Internet of Things devices such as routers, with the malware updating its command-and-control infrastructure to utilize MikroTik devices and modules.  MikroTik routers are widely used around the world across different industries.  By using MikroTik routers as proxy servers for its command-and-control servers and redirecting the traffic through nonstandard ports, Trickbot adds another persistence layer that helps malicious IPs evade detection by standard security systems."  Meaning that a security system might be banning connections to a known malicious IP, so no problem.  Let's have Trickbot bounce its traffic through some random MikroTik router that's not going to be banned, and then it will send its traffic on to the command-and-control server, thus serving as a proxy.



Okay, so when Microsoft says that MikroTik routers are being used as command-and-control proxies using non-standard ports, they mean that once they've managed to crawl inside - "they" the bad guys - inside a MikroTik router, and we'll discuss exactly how that happens in a minute, they set up a proxy server, or actually the equivalent, it's actually just persistent NAT, that's listening on one or more non-standard ports for incoming command-and-control queries from instances of Trickbot out in the field.  This provides the attackers with much more isolation and resilience than if all instances of Trickbot were phoning home directly.



We've all seen the movies where tracing the traffic is thwarted by having it bouncing around among various intermediaries in widely geographically spread jurisdictions. And also, assuming that there's a sufficient available inventory of vulnerable MikroTik devices, and we'll talk about that in a minute, apparently that's not a problem, having different instances of deployed Trickbots connecting to widely dispersed MikroTik proxies makes finding the actual command-and-control servers much more difficult, and it makes the Trickbot botnet much more difficult to take down.



Okay.  So Microsoft said:  "The Microsoft Defender for IoT research team has recently discovered the exact method through which MikroTik devices are used in Trickbot's command-and-control infrastructure.  In this blog, we'll share our analysis of the method and provide insights on how attackers gain access to MikroTik devices and use compromised IoT devices" - meaning these MikroTik boxes - "in Trickbot attacks.  This analysis has enabled us to develop a forensic tool to identify Trickbot-related compromise and other suspicious indicators on MikroTik devices.  We published this tool to help customers ensure these IoT devices are not susceptible to these attacks.  We're also sharing recommended steps for detecting and remediating compromise if found, as well as general prevention steps to protect against future attacks."



And I'm not going to go into all that because it's sort of down in the weeds and only applies to people who have these routers.  And if you've got a MikroTik router, just update it.  Make sure it's running the latest firmware and that you're not using default passwords because there are three ways Microsoft has detected attackers gaining an initial foothold on these MikroTik routers.  They will first attempt to gain access using default MikroTik passwords.  It turns out that the old lesson has still not been learned, and that MikroTik's design wrongly provides for remote access under default credentials.  Which is just hard to believe in this day and age.



As we know, default credentials, if any must exist, should never also be allowed to be used to enable remote authentication from the WAN interface.  Any user wishing to enable WAN-side remote admin - maybe after first passing a sanity test.  Why?  Why do you want to do this?  Are you really sure?  It's really not a good idea.  Anyway, should be required to create non-default credentials.  Not doing this, not requiring this, is incredibly poor security design in this day and age.  Well, really ever.  But now how do you excuse it?



Okay.  So Microsoft said these bad guys will first try using the device default credentials.  If that fails, then a brute force so-called "credential stuffing" attack is attempted.  Microsoft wrote that they have seen attackers using unique passwords that were probably harvested from other possibly related MikroTik devices.  Hey, it's worth a shot.  Why not?



And then, believe it or not - and, oh, yeah, I know we all believe it - by exploiting CVE-2018-14847, a well-known and long since patched four-year-old vulnerability which will succeed on devices still running RouterOS versions older than 6.42, there's another way in.  This vulnerability, which we covered back when it was news at the time four years ago, gives an attacker the ability to read arbitrary files from a remote MikroTik router, specifically a handy file such as user.dat, which contains the passwords.



And once in, they then change the router's password to retain control.  Since any such router is probably long forgotten, that is to say a router that hasn't had its firmware updated in four years, is long forgotten, probably tucked away in some dark corner or a closet with a sign on the door which reads "Don't mess with anything in here," it's unlikely that anyone will be inconvenienced by having their router's password unilaterally changed by some remote hacker, let alone having it commandeered for use as a botnet command-and-control proxy.



The attackers then use the handy-dandy MikroTik command that redirects traffic between two external ports on the router, thus enabling and establishing a line of communication between Trickbot-infected systems and Trickbot's command-and-control, or perhaps another indirect link in the command-and-control chain, if they want to be more fancy like in the movies.  MikroTik has a RouterOS containing very powerful SSH-accessible IP, firewall, and NAT stacks.  It's this unique remotely accessible static NAT capability that makes MikroTik routers so useful as unattended communications proxies.  Microsoft says that they monitored a Trickbot-infected device connecting to a MikroTik router over port 449, after which the MikroTik router connected to a Trickbot command-and-control server over port 80.



And as for today's inventory of MikroTik routers, a relatively recent report by Eclypsium published this last December found that "hundreds of thousands" of MikroTik routers are still vulnerable to remote exploitation despite patches having been available for them for years.  And because these devices feature unusually powerful hardware, which is why they're so popular, they're seen as high-value targets.



Back in early December of last year, 2021, when covering this report from Eclypsium, our friends over at BleepingComputer reached out to MikroTik for some comment about the sad state of their otherwise very nice and quite powerful router's updating.  MikroTik replied:  "The Eclypsium report deals with the same old vulnerabilities we have mentioned in our previous security blogs."



LEO:  Oh, exactly.  Yes.



STEVE:  Yup.



LEO:  This problem has been here since 2018.



STEVE:  "As far as we know," they said, "there are no new vulnerabilities in RouterOS.  Furthermore, RouterOS has been recently independently audited by several third parties.  They all arrived at the same conclusion.  Unfortunately," they wrote, "publishing updated firmware does not immediately protect the affected routers.  We don't have an illegal backdoor to change the user's password and check their firewall or their configuration.  These steps must be taken by the users themselves.



"We try our best to reach out to all users of RouterOS and remind them to do software updates, use secure passwords, check their firewall to restrict remote access to unfamiliar parties, and look for unusual scripts.  Unfortunately, many users have never been in contact with MikroTik and are not actively monitoring their devices.  We cooperate with various institutions worldwide to look for other solutions, as well.  Meanwhile," they finish, "we want to stress the importance of keeping your RouterOS installation up to date once again.  That is the essential step to avoid all kinds of vulnerabilities."



LEO:  Yes.  True.



STEVE:  Yes.  It is absolutely true.  And I suppose we might be more sympathetic if they didn't also apparently have a default password that allows users to login remotely.



LEO:  This has been covered before, again and again.  Why are you bringing this up, Steve?  We know that.



STEVE:  We don't know that it might have been changed in their recent later releases.  But again, so what?  What may have once started off as fiction in the movies is actually underway today. All of those hopping five times around the globe before finally connecting to the mothership, and it's like, oh, we can't - we don't have access into all those other geographies.  So, oh, well.  Yet there are hundreds of thousands of these things out there right now.  Unbelievable.



LEO:  Hundreds of thousands.  Amazing.



STEVE:  Okay.  This one's cool.  You're going to like this one, Leo.  A new and moderately severe problem has been discovered in all supported versions of OpenSSL.  Now, it's officially being called a high-severity problem; but since problems in OpenSSL can be so much worse than an infinite loop, I think it's more likely of moderate severity.  Nevertheless, especially in today's current climate with increased cyber hostilities, it does seem destined to be exploited because it is so powerful, so ubiquitous, and so easy to exploit.



The reason is that any Internet server or service running any Internet protocol whose security is being provided by OpenSSL, as most Unix, Linux, and a great many cloud-based services are, you know, OpenSSL is the default TLS connection provider for our Linuxes, for example, that any of those that has not just in the last week received update patches and been updated, as many Internet servers will not have been yet, which consumes a user-provided certificate such as a client certificate, can be taken down with a simple, deliberately crafted TLS handshake.



The trouble was discovered and reported by Google's Tavis Ormandy, who reported his findings to the OpenSSL team on the 24th of February, last month.  The trouble Tavis found and worked out with David Benjamin, Chrome's TLS guy, is that a modular square root function named "BN_mod_sqrt()," if provided with a maliciously crafted certificate to parse, will enter an infinite loop, thus pegging the processor at 100% and permanently hanging that thread.  The CVE write-up of this problem is interesting.  It explains.



It says:  "The BN_mod_sqrt() function, which computes a modular square root, contains a bug that can cause it to loop forever for non-prime moduli.  Internally this function is used when parsing certificates that contain elliptic curve public keys in compressed form or elliptic curve parameters with a base point encoded in compressed form.  It is possible to trigger the infinite loop by crafting a certificate that has invalid explicit curve parameters.  Since certificate parsing happens prior to verification of the certificate's signature, any process that parses an externally supplied certificate may thus be subject to a denial of service attack."



Okay.  In other words, since the certificate's signature is checked for validity after the exploitable parameters are processed, anyone can trivially create a certificate that will completely lock up anything that's using OpenSSL.  Their description continues:  "The infinite loop can also be reached when parsing crafted private keys as they can contain explicit elliptic curve parameters.  Thus the vulnerable situations include" - and this is from the CVE write-up - "TLS clients consuming server certificates, TLS servers consuming client certificates, hosting providers taking certificates or private keys from customers, certificate authorities parsing certificate requests from subscribers, anything else which parses ASN.1 elliptic curve parameters, and any other applications that use the BN_mod_sqrt() where the attacker can control the parameter values that are vulnerable to this DoS issue."



Okay.  In the earliest OpenSSL 1.0.2 version, the public key is not parsed during the initial parsing of the certificate, which makes it slightly harder to trigger the infinite loop.  However, any operation which requires the public key from the certificate will trigger the infinite loop.  In particular, the attacker can use a self-signed certificate, for example, to trigger the loop during verification of the certificate signature.  So you've got to sign the certificate rather than not care about the signature.  Big deal.  This issue affects OpenSSL versions 1.0.2, 1.1.1, and 3.0.  It was addressed in the release of 1.1.1n and 3.0.2 on the 15th of March, so last week.  That was exactly a week ago, last Tuesday.  So it's fixed in OpenSSL 3.0.2, which fixed the affected versions 3.0.0 and 3.0.1, and was fixed in OpenSSL 1.1.1n, which affected everything, 1.1.1 up through 1.1.1m.



So again, note that OpenSSL 1.0.2, which is vulnerable, has reached end-of-life and is not being actively supported, so users are advised, if anybody still has 1.0.2, to upgrade to a new release branch as soon as possible because this one is not going to be fixed publicly.



And note also that many critical infrastructure-ish things routinely make use of certificate-based mutual authentication, where each of the two endpoints provides an identity-asserting and assuring certificate as a means for securely establishing their identities to each other.  Not just IP addresses, but let's swap certs.  And Leo, you and I are always talking about a cert being the right way to authenticate an SSH connection.  So it's like that.  Therefore any such system with listening ports exposed to the public Internet, despite its administrators being sure that it will only connect to other endpoints which provide the proper matching certificate or expected certificate, are immediately vulnerable to being taken down by anyone else on the public Internet.



Not surprisingly, this has all generated a great deal of interest, and GitHub has been buzzing.  And all the work has been done, with someone else using the handle "catbro666" providing a working proof-of-concept certificate which will take down any unpatched instance of OpenSSL that makes the mistake of accepting any such certificate.  The hosting processor is pinned, as I said, at 100% utilization, and nothing else gets done.



So far there are no confirmed indications of this thing being exploited, but for this one it's more a matter of when, not whether.  There was one report of the Italian cert posting that this had been seen in exploitation.  But my guess is that was a mistake since it was only a one-off instance of that being alerted.  So probably nothing to worry about.  But again, if you're using OpenSSL, and your system will accept a cert from somebody, you want to make sure that you update it.  And this is only a week old.



Okay, this is really cool, CISA Alert AA22-074A.  I want to share this interesting CISA alert which details a Russian state-supported attack upon an NGO, a non-governmental organization, because it contains some interesting and quite specific forensic  detail from their investigation and may have some useful takeaways for some of our listeners.  CISA's alert is titled "Russian State-Sponsored Cyber Actors Gain Network Access by Exploiting Default Multifactor Authentication Protocols and 'PrintNightmare' Vulnerability."



So CISA's report opens with a summary.  They said:  "The Federal Bureau of Investigation and Cybersecurity and Infrastructure Security Agency (CISA) are releasing this joint Cybersecurity Advisory to warn organizations that Russian state-supported cyber actors have gained network access through exploitation of default MFA protocols and a known vulnerability.



"As early as May 2021, Russian state-sponsored cyber actors took advantage of a misconfigured account set to default MFA protocols at a non-governmental organization, allowing them to enroll a new device for MFA and access the victim network.  The actors then exploited a critical Windows Print Spooler vulnerability" - imagine that - "PrintNightmare to run arbitrary code with system privileges.  Russian state-sponsored cyber actors successfully exploited the vulnerability while targeting an NGO using Cisco's Duo MFA, enabling access to cloud and email accounts for document exfiltration.



"This advisory provides observed tactics, techniques, and procedures, Indicators of Compromise (IOCs), and recommendations to protect against Russian state-sponsored malicious cyber activity.  FBI and CISA urge all organizations to apply the recommendations in the Mitigations section of the advisory."  



Okay.  So here are some of the interesting details of this attack which were uncovered during its investigation.  They said:  "As early as May 2021, the FBI observed Russian state-sponsored cyber actors gain access to an NGO, exploit a flaw in default MFA protocols, and move laterally to the NGO's cloud environment."  They said:  "Russian state-supported cyber actors gained initial access to the victim organization via compromised credentials and enrolling a new device" - meaning computer, or I guess it could have been handheld - "in the organization's Duo MFA.  The actors gained the credentials via brute-force password guessing attack, allowing them access to a victim account with a simple, predictable password."  Okay.  So they're calling it brute force, but apparently not much brute was needed, or force, for that matter, if it was a predictable password.



Okay.  So now we're in.  "The victim account had been un-enrolled from Duo due to a long period of inactivity, but it was not disabled in Active Directory.  As Duo's default configuration settings allow for re-enrollment of a new device for dormant accounts, the actors were able to enroll a new device for this account, complete the authentication requirements, and obtain access to the victim network."



Okay, now, first of all, I'll just mention, because I don't mention it later, that's a bad default; right?  But it's not the worst default, as we'll see in a minute.  But this says that if an account is dormant and stops being supported, just using it again without needing to do anything else brings Duo's support for multifactor authentication on that dormant account back to life.  Doesn't have to be that way, but it's the default.



They said:  "Using the compromised account, Russian state-sponsored cyber actors performed privilege escalation via exploitation of the PrintNightmare vulnerability to obtain administrator privileges."  And again, this is another reminder.  It's easy to think of remote code execution attacks or remote code execution vulnerabilities as being really bad.  But we keep seeing that the bad guys are able to get in as an unprivileged user.  You do not want them to be able to elevate that privilege to admin.  Anyway, PrintNightmare made it possible.



"The actors also modified a domain controller file."  Get this:  c:\windows\system32\drivers\etc\hosts.  Anyone who's familiar with the Windows directory layout knows that that's where Windows hosts file lives, and it is still there and still effective to this day.  And we may remember a little anecdote that I got caught out when the credit card processing service backend that we use changed their IP address, and I had stuck their IP address in GRC server's host file, and I couldn't figure out why isn't DNS working?  Well, I overwrote it in the hosts file, as one can.  Anyway, these guys modified a domain controller file, the domain controller's host file, thus redirecting Duo multifactor authentication calls to localhost instead of where it would normally go, the Duo server.  This change prevented the multifactor authentication service from contacting its server to validate MFA login.



LEO:  So it failed, of course; right?  And locked you out.



STEVE:  It failed, of course.  This effectively disabled multifactor authentication for active domain accounts because the default policy of Duo for Windows is to fail open.



LEO:  To fail open, aghhhhh.  I remember Bruce Schneier talking about how many enterprise tools fail open, including firewalls.



STEVE:  Leo, we wouldn't want to inconvenience anybody, especially not a Russian attacker.



LEO:  Well, yes, exactly.



STEVE:  That would be an inconvenience.



LEO:  Who wants to close them down?  That's why I'm putting on my hoodie.  I'd getting ready.  Oh, wow.



STEVE:  I know.



LEO:  This is a real problem because a lot of supposedly very secure places use Duo.



STEVE:  Yes, exactly.  So, and then as if to apologize for mentioning this, CISA added:  "Note:  Fail open can happen to any MFA implementation and is not exclusive to Duo."  Okay.  But since the "tyranny of the default" is one of this podcast's core observations, I was curious about Cisco's default for their Duo MFA.  And sure enough, in an FAQ over at Duo.com, in answer to the question, "How can I configure the fail mode?" Duo replies:  "By default, Duo Authentication for Windows Login will 'fail open' and permit the Windows logon to continue if it is unable to contact the Duo service.  You can set the fail mode during installation to 'fail closed'..."



LEO:  Oh.



STEVE:  Yes.  During installation...



LEO:  But it's not the default.



STEVE:  No, "...by deselecting the 'Bypass Duo authentication when offline' box during installation."  Okay.  But come on.



LEO:  That's as if your two-factor authenticator, "You don't have the code?  That's fine.  Go on in."  It's so dumb.



STEVE:  Yeah, exactly.  And if an admin is installing this for the first time, and you don't yet know it all well, so you're going to leave all of the presumably recommended defaults alone, at least for the time being.



LEO:  Sure, right, yeah.



STEVE:  We've all been there; right?  So is there any reminder in the UI that that's the way it was set?  No.  Okay?  But it's easy to change that later; right?  No.  Since they're answering the question, "How can I configure the fail mode?," meaning that presumably it's not obvious, thus needing to be answered in the product's FAQ, Duo then proceeds to explain:  "To change the fail mode after installation, use the Registry Editor, regedit.exe..."



LEO:  Oh.  What?  You can set it.



STEVE:  Yeah.  We provide a key for that.  Just dig down 12 layers in the registry.



LEO:  What?  Oh, my god.



STEVE:  ...with admin privileges, of course, to create or update, they say, the following registry value.  Okay.  Wow. So not really the "secure by default" operation that you'd expect from an add-on multifactor authentication system whose entire purpose is to meaningfully increase the system's security.



LEO:  Yes.



STEVE:  After all, as you said, Leo, this system is now inconveniencing everyone all the time by having it, except the Russians, who simply bypassed it by taking strategic advantage of Duo's well-known default fail-open policy.  So I wanted to bring this up in case any of our listeners work with companies using Cisco's Duo MFA.



LEO:  I do, yeah.



STEVE:  It might be worth having your IT admin check out the current setting of that registry key.  Anyway, I've included a link to the FAQ item in question in the show notes.  Anyway, CISA continues:  "After effectively disabling multifactor authentication, Russian state-sponsored cyber actors were able to successfully authenticate to the victim's virtual private network as a non-administrator user and make Remote Desktop Protocol connections to Windows domain controllers.  The actors ran commands to obtain credentials for additional domain accounts, then used the method described in the previous paragraph, changed the MFA configuration file, and bypassed multifactor authentication for these newly created and compromised accounts.



"The actors leveraged mostly internal Windows utilities already present" - remember Living Off the Land - "within the victim network to perform this activity.  Using these compromised accounts without MFA enforced, Russian state-sponsored cyber actors were able to move laterally to the victim's cloud storage and email accounts and access desired content."



And for those who aren't clear about the nature of NGOs, the target of this attack, they're typically benign, non-profit, explicitly unaffiliated with any particular government.  A few examples of NGOs would be Greenpeace International, CARE, the World Wildlife Fund, and Doctors without Borders.  CISA doesn't tell us which NGO was targeted in this instance.  But Russia knows.  Wow.



Then we have the Windows local privilege escalation, speaking of local privilege escalations, that Microsoft seems unable to fix.  This is the, believe it or not, ongoing saga of a Windows flaw that Microsoft, paradoxically, appears to be unable to fix.  Since there's no obvious reason why this particular problem should be beyond them, one must conclude, again, as we have been with increasing frequency lately, that they just really don't care, or that something has gone very wrong somewhere deep inside.



This all begins way back last August of 2021 with an elevation of privilege vulnerability in the Windows user profile service.  Back then, it was tracked as CVE-2021-34484 after having been discovered by a security researcher named Abdelhamid Naceri.  And if his name seems familiar it's because the problem he found just won't go away, and we've been following.  And it's not a small problem.  It's, as I said, a local privilege escalation which allows unprivileged users, like those pesky Russkies who manage to gain access, to then acquire valuable administrative privileges in Windows 10, 11, and the latest version of Windows Server.  It has a CVSS of 7.8.  And although exploits have been publicly disclosed, thus by Microsoft's own definition it's a zero-day for them, no evidence has yet been found of its exploitation in the wild or, for that matter, any sincere attempt on Microsoft's part to actually fix it.  No discovery.



It was originally said to be fixed, as I noted, as part of August 2021's Patch Tuesday.  But Naceri became curious after those patches were published and pushed out, to see how Microsoft had fixed what he had found.  What he now found back then was that Microsoft had not fixed the problem.  Instead, it was another one of those "address the symptom not the problem" pseudo-fixes.  So Naceri presented another proof of concept which bypassed Microsoft's non-fix across all Windows desktop and server versions.  And spoiler alert, that remains true today.



It was at this point that our friends over at 0patch, the guys who create those wonderful little itty-bitty micropatches which fix problems that Microsoft hasn't yet, or can't, or won't, decided to release one of their own unofficial patches for all Windows versions, making it free for all their registered users.



Apparently not being in any huge hurry to fix this after their August fail, the first time they checked this off their "fixed" list, Microsoft finally responded to this second bypass in their January of this year, 2022, Patch Tuesday.  And they updated its CVE for some reason to 2022-21919, marking it again as fixed.  And again, believe it or not, it wasn't, and today still isn't.



Once again, Naceri found a way to bypass Microsoft's second attempt, and noted that this one was even worse than the first.  Happily, that original micropatch still worked.  But March's, this just two weeks ago, update two weeks ago changed the DLL in question, which is prof as in profile, P-R-O-F-E-X-T dot dll, which broke 0patch's micropatch.  So not only did Microsoft's March update still not resolve the problem, but it also removed the third-party protection that 0patch users had been enjoying until then.  Undaunted, 0patch has updated its micropatch so that its users are once again protected while we presumably wait for Microsoft to yet again try to actually fix the problem.



And in one last bit of happy news, users of Windows 10 editions 1803, 1809, and 2004 have remained safely protected by 0patch's original patch since those Windows editions have reached the end of their support life and were therefore spared from receiving Microsoft's update, which reexposed everyone else to the problem.  Yes, that's what makes the security industry and the computer industry so interesting [crosstalk] nonsense.



LEO:  I can fix that.  Again and again.



STEVE:  Wow, Leo.



LEO:  All right.  Let's talk about garbage collection and Use After Free, which sounds kind of sexy, actually.  I don't know, what is Use After Free?



STEVE:  So today's nominal podcast topic was inspired by several pieces, as I mentioned at the top, of recent listener feedback.  And it was a tweet from Stephen Bellchester that finally caused me to decide to give this a bit more time.  Stephen tweeted:  "It would be good if you could clarify your comments about use after free vulnerabilities in Firefox," he said, "(and similar ones in previous weeks)."  He said:  "You've said a few times that use after free issues are being caused by memory management being done automatically by the language.  That implied, to me at least, that the language that Firefox is written in is memory managed and causing the issues, which seemed pretty unlikely to me.  After some further digging it seems like the issue was actually with Firefox's automatic memory management that runs underneath the HTML/CSS/JavaScript stack."



Okay.  So first of all, if I didn't say it, I would have meant that memory management is being performed by the language's runtime.



LEO:  But you could have a Use After Free if you malloc something and deallocate it and then call it.  You could do it to yourself.  It doesn't have to be an automatic tool.



STEVE:  That is exactly right.  And in fact I will get to that sort of non-automatic issue in a second.  But the attacks that we see being leveraged are generally the implementation of JavaScript.  And JavaScript is famously a language with automatic background garbage collection.  And although unfortunately we have a name collision here, we made it very clear a long time ago that Java, the language, and JavaScript have nothing in common; right?  But Java, the language, is a perfect example, both because being a virtual machine-based language, you know, there's the Java JVM, Java the language has a clear run-time environment, and because it's famous for having a very powerful garbage collector.



In looking around for some examples, I found a web page titled "How Java Garbage Collection Works."  And it starts off saying:  "Java Memory Management, with its built-in garbage collection, is one of the language's finest achievements.  It allows developers to create new objects without worrying explicitly about memory allocation and deallocation, because the garbage collector automatically reclaims memory for reuse.  This enables faster development with less boilerplate code, while eliminating memory leaks and other memory-related problems."



Then it says:  "At least in theory.  Ironically, Java garbage collection seems to work too well, creating and removing too many objects.  Most memory-management issues are solved, but often at the cost of creating serious performance problems.  Making garbage collection adaptable to all kinds of situations has led to a complex and hard-to-optimize system.  To wrap your head around garbage collection, you need first to understand how memory management works in a Java Virtual Machine."  And we're not going to get into that because the rest of that page goes into a lengthy treatise on Java's memory management.  The task of sweeping out the memory garbage has been the topic of a great many academic papers.



And I found to my, I guess not surprise, but I thought it was interesting, that Amazon lists two books dedicated solely to the subject.  There's "The Garbage Collection Handbook: The Art of Automatic Memory Management," and then it says "(Applied Algorithms and Data Structures)."  And then we have "Garbage Collection: Algorithms for Automatic Dynamic Memory Management."  And I thought the back flap of that one was interesting.



It says:  "The memory storage requirements of complex programs are extremely difficult to manage correctly by hand.  A single error" - like what you were talking about, Leo - "may lead to indeterminate and inexplicable program crashes."



LEO:  Often does.  Very common.



STEVE:  Worse still, and in fact that's what you want, actually, is a crash.



LEO:  Yeah, you wish it would crash.



STEVE:  Exactly.  Then you can begin to track it down.  And the flap says:  "Worse still, failures are often unrepeatable [oops] and may surface only long after the program has been delivered to the customer."



LEO:  Yeah.



STEVE:  "The eradication of memory errors typically consumes a substantial amount of development time.  And yet the answer is relatively easy, garbage collection, removing the clutter of memory management from module interfaces, which then frees the programmer to concentrate on the problem at hand rather than low-level bookkeeping details.  For this reason, most modern object-oriented languages such as Smalltalk, Eiffel, Java, and Dylan" - and of course JavaScript - "are supported by garbage collection."



LEO:  You can tell how old this book is by those choices of languages.



STEVE:  Yes.  Dylan?  Hello, Dylan.



LEO:  Smalltalk and Eiffel?  Okay.



STEVE:  Smalltalk, right.



LEO:  They are garbage collected, that's true, yeah.



STEVE:  Yeah, in fact I think it was 1991, maybe.



LEO:  The '90s, yeah, yeah, I think so.



STEVE:  '91, maybe, yeah.  It says:  "Garbage collecting libraries are even available for such uncooperative languages as C and C++.  This book considers how dynamic memory can be recycled automatically to guarantee error-free memory management.  There is an abundant, but disparate, literature on the subject, largely confined to research papers.  This book sets out to pool this experience in a single accessible and unified framework.  Visit this book's companion website" - which I'll be surprised if it's still around - "for updates, revisions, online GC resources, bibliography, and links to more GC sites."  And speaking of the devil, Dr. Dobb's Journal reviewed the book.



LEO:  Oh, yeah.  Oh, yeah.  "Running Light Without Overbyte."  Oh, yeah.



STEVE:  That's right.  They said:  "Whatever else Java has accomplished, it has finally brought garbage collection into the mainstream.  The efficiency and correctness of garbage collection algorithms is henceforth going to be of concern to hundreds of thousands of programmers.  Those who really care about this could do no better than to start with 'Garbage Collection: Algorithms for Automatic Dynamic Memory Management,' the sort of comprehensive engineering manual that is so rare in computing."



LEO:  Just before we go too much farther, just so people know why this important, almost always when you're doing a computer, let's say you're creating an array, or you've got a database.  You're going to allocate some memory to store the stuff in, operate on it, and then when you're done, deallocate it.  If you don't deallocate it, eventually memory fills up, it's called a "memory leak," and you crash.  So you've got to deallocate it.  And there are really only three different ways to do it.  One, the way Steve does it, which is he keeps track of it, and when he's done with it, he deallocates it; right?  You do it by hand because you're in assembly language. 



STEVE:  Yeah.



LEO:  There is garbage collection, which is an automated process and can be done well or poorly, that periodically on a timer goes to the system and says, are you using that?  Do you need that?  Okay, I'm going to let it go.  There's also what Rust does, which I think some people think is more effective, just another kind of garbage collection, which is counting by reference.  So it just counts up how many times something uses it, counts down.  When it's not being used, it releases it.  All three have problems.  There's no perfect way to do this because if you deallocate something and then use it, that's a Use After Free bug.  So it's not unique to garbage collection.  I think people would argue letting programmers do it is even worse.



STEVE:  So, in fact, yes.  I mentioned in my notes, I said it could certainly be that in a non-automatic language like C, one thread or process manually frees some memory that another thread or process later attempts to use.  In that case, the culprit is clearly a coding error.  Either the memory should not have been freed when it was, or the code attempting to still refer to it later should have known not to attempt to do so.  Or, as in an instance we referred to recently, there was no actual error in the code at all.  But there was a dangling pointer left around which pointed to some previously freed memory.  The original code was not going to refer to that pointer, but a clever hacker figured out how to exploit it to their advantage.



LEO:  Oh.  So you could have written a perfect code.



STEVE:  Yes.



LEO:  Released some memory.  And then the bad guy modifies your code and attacks you.  Oh, that's devious.



STEVE:  Yes.  Yes.  And so what we see is that the term "Use After Free" is a bit of a catchall, more than we'd like it to be.  It's a bit like when Microsoft describes a vulnerability by saying that it allowed a security restriction to be bypassed.  Like, what?



LEO:  Huh?  Well, we, you know...



STEVE:  Yeah, yeah, we're not really sure, but yeah.  It seemed bad.  So probably because it allowed a security restriction to be bypassed.  But for what it's worth, this issue is a big deal.  You know, it's been the sole topic of many computer science Ph.D. dissertations.  There are people with "Dr." in their name because they wrote something, they invented some new twist on garbage collection.  I mean, you know, in scrolling down through a lot of this stuff, Leo, I saw simple examples, right, where many languages allow you to use a block structure where you'll open curly braces and do a bunch of stuff and then close curly braces.



And the presumption is that block is self-contained.  So if inside those curly braces you were to just say, you know, A equals something or other, then the presumption is when you close the curly brace, because you implicitly declared and first used the variable A within the curly brace, that leaving the curly brace, that A is so-called it's "out of scope."  



LEO:  Scope, yeah.  Scope variables.  Almost all modern languages do that.



STEVE:  Yes.  Yeah.  And again, super handy, super convenient.  But you just need the language to know that, oh, we're no longer responsible for talking about A.  And if you do, it's the programmer.



LEO:  It's an error.  Well, and the compiler or the interpreter's going to go, hey, A's undefined.  What are you doing?



STEVE:  Yeah, we don't know what that is now.



LEO:  Yeah.  The reason I'm defensive of GC is because every language I use has garbage collection.  It's hard to find a language these days except for C and Rust and Assembly.



STEVE:  Well, and I'm not sure where we are today, but there was a time when languages, like the operation program would suddenly stall for no apparent reason.  It's because the garbage collector had kicked in.



LEO:  That's the problem.



STEVE:  It was, like, busy.  Yes.



LEO:  Yeah, you get these periodic pauses while going through the - little robots going around looking, got any garbage for me?



STEVE:  It's that guy with the little white moustache and...



LEO:  On a wheel, he's got one wheel.



STEVE:  And he's got a little broom, and he's like, you know, sweeping along.



LEO:  Yeah.  I love this subject.  Keep going.  This is good stuff.  This is also in some ways the history of computing, really.



STEVE:  Yeah, you know, as our old-time listeners know, we had a lot of fun back in the early days of the podcast talking about how this all happened and the whole history of computing.  So anyway, all I wanted was that it is one of the reasons that our listeners are sometimes confused is that Use After Free is not just one thing.  It doesn't have one cause.  It doesn't have one environment.  I've had people say, hey, static languages don't have the problem.  Yes, they do.  Dynamic languages don't have the problem.  Yes, they do.  Again, it's like saying, oh, you know, this vulnerability is a security bypass.  Okay, well.  And so Use After Free just sort of is more generic than we would like, but so it serves as a catchall for a large set of environments where a hacker was somehow able to reference memory in a way that wasn't intended.  And so we just kind of call that Use After Free.



LEO:  And don't.



STEVE:  Yeah.  And more specific details would be nice, if we had them.



LEO:  Yeah.  I never even thought about that.  But of course if a bad guy could write code, try to access memory that had been deallocated, your code could be perfect, but the bad guy was able to modify that code.  That's bad.



STEVE:  Yes.  And what can happen is that if you've deallocated some code, and then it's been reused, then the old pointer used to point to something in a different context.  Now it points to its new contents, which the attacker may have just managed to load.



LEO:  Right. 



STEVE:  So software is hard.



LEO:  I used to play with and love a language called Forth.  It was all on the stack.



STEVE:  The write-only language.



LEO:  Yes.



STEVE:  You cannot read that language.



LEO:  But you didn't really, I guess you could, as I remember, allocate memory.  But the idea was it's a stack-based language.  Everything's on the stack.  But you could still overrun the stack.  So even there.



STEVE:  Sure.  And, for example, in C, local variables are on the stack because our computer instructions provide a very nice stack pointer, actually it's a frame pointer reference because the stack might still be moving around.



LEO:  It's this part of the stack here, yeah, yeah.



STEVE:  Exactly, exactly.  And so when you exit from the routine, you so-call "pop the stack," which just disposes of all of those temporary references.



LEO:  This stuff is fascinating.  People who end up getting into programming often end up into writing their own interpreters, compilers, languages because it is so interesting.  



STEVE:  And we've also seen the phenomenon, if you give somebody am object-oriented language, which inherently allows you to create meta language to describe the problem domain, you can end up spending all your time coming up with the coolest way to start getting ready to actually write some code.



LEO:  That's kind of like Forth, too.  That's why it's read-only, because you write your own vocabulary.  So you're writing in a language you understand, but nobody else does.  And absolutely that's the big thing of Lisp and Scheme, the languages I love, is writing a domain-specific language.  And you're right, you could spend your entire day doing that and not solving the problem.



STEVE:  Getting really ready to actually start.



LEO:  I am so set.



STEVE:  I've got this cool vocabulary.



LEO:  Steve Gibson.  So much fun.  It's really great to have somebody who has roots in the past, understands the present, and can contextualize what's going on.  That's what this guy does so well, every Tuesday, right here, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you want to watch it live, live.twit.tv.  People who are watching live often head over to our IRC, yes, we run an IRC server, well, our community does, at irc.twit.tv.  You could chat live there.  People who are members of Club TWiT also have their own place to chat.  Our Club TWiT also have their own place to chat, our Club TWiT Discord, which is a place going 24/7.  In fact, we have a great coding section in there.  We're talking about this kind of stuff all the time.



If you want to get copies of the show, there's two places you can go.  Of course, our site, TWiT.tv/sn.  Steve's got them, too.  In fact, he has two unique formats.  He's got the 64Kb audio, just like we do.  But he also has a 16Kb version for people who just don't want a big file, for the bandwidth-impaired.  He also has transcripts written by the great Elaine Farris.  She writes it all down so you can read along as you're listening, or even just read instead of listen.  Or, and the most useful thing, search the transcripts to find the part of the show that you're interested in.  And that goes - you don't have transcripts for all the shows.  You just have it for...



STEVE:  Yeah, we went back and did it from day one.



LEO:  Wow.  Wow.  That's really nice.  Thank you.  That's all at GRC.com.  While you're there, by the way, you might want to check out SpinRite.  That's Steve's day job, his bread and butter, the world's best mass storage recovery and maintenance utility.  You can pick up a copy of 6.0 right now.  You'll get a free copy of 6.1 when it comes out if you buy today.  You'll also get to participate in the development of 6.1.  Steve's working very hard on that.  It's imminent.  GRC.com.  Plus lots of free stuff like ShieldsUP! and so forth.



And InControl, which I plugged on the radio show this weekend because somebody said, "I don't want it.  I never want to use Windows 11."  I said, "Well, I've got a perfect program for you.  Be in control."  I guess that's all I need to say except thank you again, Steve, for doing such a great job, and we'll see you next time on Security Now!.



STEVE:  Always.  Right-o.  What is it, one more, we've got one more episode this month because we got normal-size month, and we started on the first of the month.  So on the 29th.



LEO:  Spring is here.  The spring edition.



STEVE:  Right-o.  Bye.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#864

DATE:		March 29, 2022

TITLE:		Targeted Exploitation

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-864.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we start by looking at Chrome's second zero-day vulnerability of the year.  We then spend some time with an interview of the Chief Technical Officer of one of Ukraine's largest ISPs learning of the challenges they're currently facing.  JavaScript's most popular package manager npm is under attack again, and Honda tells worried reporters that they have no plans to address the consequences of a new glaring security vulnerability affecting five recent years of their Honda Civic design.  The FCC classifies Kaspersky Lab as a national security threat and adds a bunch of Chinese Telecom companies and services, as well.  Then, after addressing a piece of use-after-free listener feedback, we take a detailed look at the consequences of Chrome's first zero-day of the year and at the attacks launched by North Korea which leveraged that flaw. 



SHOW TEASE:  It's time for Security Now!.  Birthday boy Steve Gibson is here, and we have lots to talk about, as always.  The second Chrome zero-day vulnerability of 2022.  We'll talk a little bit about the challenges presented to ISPs in Ukraine and the war there.  And Kaspersky Labs banned by the FCC, but should you stop using it?  It's all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 864, recorded Tuesday, March 29th, 2022:  Targeted Exploitation.



It's time for Security Now! with the star of the show, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hello, my friend.  Well, we're wrapping up the first quarter of March.  This is Security Now! Episode 864 for March 29th.  And I am now 67 years old.  A big thank you for all of the very thoughtful birthday wishes last week and over the weekend from our listeners.



LEO:  Congratulations.  I forgot to say Happy Birthday, yeah.



STEVE:  All were appreciated.  I don't feel any different.  I think maybe when I hit seven zero, that may have an effect.  But still, everything still works, and I've got lots of energy, so that's good.



We're going to talk about targeted exploitation because Google has just suffered its second-of-the-year zero-day vulnerability, which is what we'll start talking about here in a minute, and issued an out-of-cycle, you know, everyone wants to call it an emergency patch.  But, you know, browser update, okay, and better sooner than later.  But the first zero-day vulnerability we now have thanks to a report published by their TAG, their Threat Analysis Group, a complete readout on what was going on with the first zero-day and its use in targeted exploitations.  So we've talked about it.  We've talked about how, eh, it's not really that big a deal.  But the opportunity to really take a look at exactly what one of these is, I thought, was too good to pass up.  So we're going to start off looking at Chrome's second zero-day vulnerability of the year, which, by the way, makes it in better shape than it was this time last year.



We then spend some time with an interview of the Chief Technical Officer of one of Ukraine's largest ISPs.  And we're going to learn about the challenges that he's facing as he works to keep the Internet available, but for only some of the people who are in-country.  We also have JavaScript's most popular package manager, npm, that we were just talking about, once again under attack.  Oh, and Honda tells worried reporters that they have no plans to address the consequences of a new glaring security vulnerability affecting five recent years of their Honda Civic's design.  We can judge for ourselves.



The FCC has classified Kaspersky Lab as a national security threat, which we're going to talk about, and also adds a bunch of Chinese telecom companies and services to that list, as well.  Then after addressing one piece of use-after-free listener feedback, as I said, we're going to take a detailed look at the consequences of Chrome's first zero-day of the year, which they patched mid-February, and the attacks which North Korean-backed groups launched using it, in some detail.  So I think a great podcast for our listeners.  We have a fun Picture of the Week, and then we'll get into it.



LEO:  Nice.  Nice.  Picture of the Week?



STEVE:  So I got a kick out of this one.  We have some techie guy, he doesn't really have a pocket protector, but he's at least got a pen stuck in his pocket.  And he's standing in front of his boss's desk, and he's reporting.  He says:  "Our devices are now 100% secure."  The boss looks up from his notes and says, "How did you do that?"  And the employee says, "I turned them all off."



LEO:  Yeah, air-gap them.  That's the best way.



STEVE:  That's it, or just gap-gap them.



LEO:  Gap them.  Gap-gap them.



STEVE:  Just pull out the cord, and just weather the storm.  So as I mentioned, we have a high-severity zero-day vulnerability update for Chrome.  Last Friday Google pushed an out-of-cycle security update to address what they considered to be a high-severity vulnerability.  And I'm glad they do because it could hurt people.  It was being, you know, they're using the traditional definition of zero-day, not Microsoft's weird one, this meaning it is actually being exploited in the wild.  That is, they learned of it that way.  And in fact we'll find out a little bit more about the way this happens at the end of this podcast.



But when I was working with Chrome last night, it was at 99.0.4844.82.  But when I went to go look, it goes, ooh, I got something, and so it went from .82 to .84.  That's what this update required.  And again, I'm always a little bemused by the fact that this was announced and made available on Friday.  I used Chrome on Saturday and on Sunday and earlier on Monday, and it wasn't until I went to look that it said, oh, I guess since you're looking maybe I should give you the latest.  So, yeah.



On March 23rd, an anonymous researcher reported the bug and, to their credit, to Google's credit, 48 hours later they were pushing out a fix.  The trouble was a type confusion vulnerability in Chrome's V8 JavaScript engine being tracked as CVE-2022-1096.  And as we know, the so-called "type confusion errors" arise when some language atom, you know, a variable or some other object, is accessed using a type that's incompatible with what it was originally initialized to be.  And in languages such as C and C++, which allow for powerful implicit use and explicit type overriding, or casting, as it's often called, this can have serious consequences.



I actually do it on purpose when I know what I'm doing.  In assembly language I'll declare something as a quad word and then deliberately address it in two pieces as a high and a low double word.  But so there are uses for this kind of cast, variable casting, variable typecasting override; but you just have to know what you're doing.  Certainly in these cases these type confusion errors are mistakes.  So as I said, it's possible for that to be used benignly and deliberately, but it can also be exploited by a malicious actor to perform, as was the case here, out-of-bounds memory access.



And now whenever I hear about something happening in Chrome's V8 engine, I'm wondering whether that cool idea that Microsoft has in Edge of disabling like the degree to which Chrome uses its V8 engine would have protected people from that.  That would be interesting to know.  Anyway, Google's acknowledged that it was, as they said, "aware that an exploit for CVE-2022-1096 exists in the wild," but as usual they offered no additional specifics because, they figure, why should they?  And as I mentioned, so far this year things are going better for Chrome.  We're nearing the end of the third month of 2022, and this is the last podcast of the first quarter, and Chrome has only been visited by two zero-days this year.  There's this one, which is this type confusion error, and then the first one was, not surprisingly, this may have been what kind of has had this term on our radar, yes, a use-after-free that was being exploited in Chrome's animation component, and which they patched in the middle of February.



So my sense is, because these things generally are only being used in targeted attacks even, it appears, after patches are made available, that is, we've thought, well, maybe once the world knows about it, they'll start, like, using them on a wider scale.  I just think that the window of opportunity closes very quickly.  Not as quickly as I wish it would close because it  would be great if all Chrome browsers immediately updated themselves when they got the news.  But this one affects Windows, Mac, and Linux.



And of course if you're using any of the Chromium-based browsers - Edge, Opera, or Vivaldi - you'll want to apply those fixes similarly as soon as those other Chromium users offer one.  And we'll get back to this at the end of the podcast, taking a look at what Google's TAG team discovered about North Korea's successful use of Chrome's first zero-day of the year, which was used from at least January 4th until it was spotted and then closed on the 14th, which says 42 days of active targeted exploitation of that first problem.  So, and I'm also going to talk a little bit about the benefit of raising Chrome's protection to its maximum setting, even though it comes at some cost of privacy.  I imagine for many users it'll be worth doing.



Okay.  Prior to Russia's invasion of Ukraine, local officials warned that Russia might try to cut Ukraine off from the Internet.  But even as Russian tanks were rolling into the country on February 24th, subsequent attacks, to many people's surprise, didn't have a significant effect on the country's Internet.  Most of Ukraine's citizens were able to remain online.



And following up on our recent report of Elon Musk activating Starlink's Internet service over and throughout Ukraine and then providing what was described as a "truckload of satellite uplink stations," some were questioning whether that would be it.  Turns out he has followed through to deliver, not only thousands more uplink stations, but now also solar battery systems, I guess an offshoot from his work with Tesla stuff.  So the Starlink systems were financed by unnamed private sources, but we know that also both France and Poland contributed to the financing of Starlink to help Ukraine's connectivity.



But as for Ukraine's traditional land-based infrastructure, analysts argue that Russian military may not be attacking that infrastructure because it needs the Ukrainian Internet itself to stay connected to gather intelligence.  Others say that Ukraine has managed to build a resilient infrastructure maintained by local ISPs.  And that's also what the CTO of Ukrtelecom, which is a major provider of both mobile and broadband Internet in the country, said shortly before Ukrtelecom suffered a massive cyberattack that dramatically curtailed its service.  It's just coming out of it a day or two ago.  It was a DDoS attack that grew in strength over time.



And I thought that the timing was interesting, that he was boasting about how they're not having any problems, and then wham.  And even I learned the hard way that there's no upside whatsoever to bragging about not being DDoSed.



LEO:  Don't mention it.  You were that way.  You didn't want to talk about it.  You didn't do anything about it.  You just kind of kept it quiet; right?



STEVE:  Right.  Well, and what I realized was, you know, you and I did a number of podcasts where I'd look up and go, oh, well, GRC's off the 'Net again.



LEO:  Right.



STEVE:  Oops.  It wasn't until I took down those pages which I originally had up that were detailing my own adventures with DDoS attacks, even though I wasn't bragging in them really, I mean, I was just like talking about this is what they are, when I removed those from my site, the attacks finally stopped.  Like I said, no upside to saying, oh, you know, come get me, because they can.



LEO:  Don't feed the trolls is the rule.



STEVE:  Right.  So anyway, Ukrtelecom was originally a state-owned company which controlled the country's telecommunications market.  And at that time it was maybe a little employee-heavy, 24,000 employees, which relied on old and obsolete telecom technology.  Nine years ago, in 2013, the company was acquired by Ukraine's richest person, looks like I would pronounce his name Rinat Akhmetov.  By 2021, the company had cut its bureaucratically oversized employee count in half and had by 2021 203,000 Internet broadband users, bringing in a total of $33 million in annual revenue.  So a nice size ISP operating in Ukraine.



During the invasion, Ukraine has worked to cut off Russian invaders from their use of their infrastructure for communication, while at the same time keeping stable Internet available for those who hide in bomb shelters, study online in their basements, or want to ask their friends and relatives in the occupied territories probably the most important question, which looks like you'd pronounce it "Yak ty," which is Ukrainian for "How are you?"



Anyway, the company's CTO noted that this is the second war since Russia's 2014 invasion of Eastern Ukraine.  He said:  "We learned a lot at the time, but this war is different.  People are more united."  In an online interview that he conducted with The Record, which that CTO joined via Starlink, he explained how his workers are repairing Internet infrastructure in the occupied territories and keeping Ukraine online even amidst ongoing assaults by the Russian military.  So The Record posted a Q&A, from which I've excerpted the most interesting pieces for our audience.



The Record asked:  "Can Russia cut Ukraine off from the Internet?"  And the CTO said:  "No.  First of all, Ukraine has a dispersed Internet infrastructure, which means that key national providers, including Ukrtelecom, can use various routes to provide Internet access."  And in other words, as our listeners would know, got to love autonomous packet routing with redundant and richly interconnected links.  Anyway, he continued:  "Ukraine has a variety of Internet service providers across the country that manage their infrastructure independently or in collaboration with others.  We also have the resources and people to repair damaged infrastructure and protect the working of our networks from enemies."



He said:  "Ukrtelecom, for example, employs 12,000 Ukrainians, of whom nearly 6,500" - so a little over half - "are doing technical work."  He said:  "Our external channels to the global Internet cross Ukraine's western border, so we're not connected with Russia, which is in the east."  He said:  "In order to completely cut Ukraine off from the Internet, Russia must destroy all of the infrastructure in Ukraine, both civilian and telecommunications.  The Russian military has neither the resources nor the skills," he claims, "to do so."  And now of course, as we know, thanks to Starlink, it might be even a little more tricky to cut people off because there's a lot of uplinks now to the Starlink network.



The Record asked:  "Can Russian troops use the Ukrainian Internet?"  He answered:  "They can if they steal mobile phones from Ukrainian citizens and connect to Ukrainian telecommunication networks."  He said:  "We know about these cases, but they are hard to track."  And of course he's putting the best face on it possible.  In other words, yes, of course, Russians are taking civilian handsets and using them for their own purposes.  So anyway, he said:  "If the Russians manage to seize our fixed Internet infrastructure, we block the equipment so they can't use it.  During the war, all Ukrainian Internet providers are working closely with our military and intelligence services to avoid such incidents."



He said:  "Note that there's also a very reasonable theory that Russians need Ukrainian Internet services for their own purposes, either communication or intelligence gathering like eavesdropping on phone calls."  And, interestingly, to secure his own conversation with Ukraine's allies, the Ukrainian President Zelensky uses a secure sat phone that the U.S. gave the Ukrainian government a month before the invasion.  So they were ready, and he has a secure means of communicating with allies.  So Ukrainian officials also said that Russia's own cellular handsets and networking equipment do not work properly in Ukraine, encouraging its soldiers to therefore steal mobile phones from ordinary Ukrainians.



It's also possible that Russia is trying to avoid ruining the telecommunications infrastructure that it hopes it would need and be able to use if it manages to take the country, although anyone who's been keeping up with the news knows that the possibility of that happening is dwindling now by the day.  It was noted that when Russian troops destroyed several 3G cell towers in Kharkiv, they could no longer use their own encrypted phones that communicate via that network.  Whoops.  And it turns out that rebuilding infrastructure from scratch is difficult.  When Russia had illegally annexed the Crimean Peninsula in 2014, the Kremlin needed about three years to regain full control of that region's mobile infrastructure.  So it makes sense that they would be preserving it if they think that they'll be needing it shortly.  They probably learned some hard lessons from Crimea.



The Record asks:  "How accessible is Ukrtelecom's Internet in Ukraine now?"  And the CTO said:  "As of March 25th, Ukrtelecom's Internet coverage stayed up to 84% of pre-war levels."  He said:  "Major disruptions happen in the occupied territories, where there's no electricity or where the Internet infrastructure, including fiber-optic underground cables, were damaged during the attacks."  He said:  "Our workers make heroic efforts to provide Internet access even in besieged cities.  They go to the frontline a few times a day, while some of them live in their cars because they have to work around the clock.  We know what it's like to provide Internet during a war."  He said:  "We learned it in 2014.  So we try to protect our workers from unnecessary danger."



Oh, and he said also:  "During the COVID-19 pandemic, we learned to control and manage our networks remotely, even from our home offices.  We have network monitoring centers throughout Ukraine, which provide real-time data on the work of each node station, equipment, communications channels, and quality of service.  It's almost impossible," he said, "to find these centers because they're distributed across the country and work through the cloud."



And the last question:  "How do competing Internet providers work during the war?"  And he said:  "Before the war, competition in this market was fierce.  But now Ukrainian operators work as a team, not as rivals.  We exchange information and resources and help each other repair damaged infrastructure."



And the New York Times reported that some Ukrainian providers had been preparing ahead of the crisis by deliberately establishing fail-safe links with each other and setting up new backup network centers.  The work of all operators is coordinated by a special department of the state communication and information protection service.  Strong cooperation with foreign telecom operators also helps Ukraine to remain connected with the outside world.  In the first days of the war, Ukrtelecom reported that it lost about 30% of their external Internet channels due to damaged infrastructure, but they now have 130% of pre-war capacity.  So they've built up to even more than they had before.



So anyway, that was, I thought, some interesting feedback from the CTO of one of Ukraine's primary ISPs who's working hard to keep Ukrainian citizens connected, while at the same time doing that they can to keep Russia and their attacking and occupying forces from using the same Internet connectivity to further the Kremlin's goals.



Once again, npm is under attack.  Analysts at the DevOps security firm JFrog, whom we've written about and talked about a number of times, recently blogged about their discovery of 218 malicious packages targeting the Microsoft Azure npm scope.  Npm, as we noted last week, is the Node.js package manager for JavaScript.  JFrog's analysts immediately notified the npm maintainers, who removed the offending packages, 218 that they had found.



JFrog's automated analyzers began alerting them to a set of packages that grew from an initial count of 50 to over 200.  The unknown threat actors used typosquatting - and I'm not sure that I would use that term, that's what the tech press was reporting,  I'll explain in a minute - but essentially a naming duplication attack by attempting to trick victims into using packages that have the same name as legitimate ones.  Packages are able to reuse the same names due to npm scoping, which I'll explain in a second.



JFrog said:  "After manually inspecting some of these packages, it became apparent that this was a targeted attack against the entire @azure npm scope by an attacker who employed an automatic script to create accounts and upload malicious packages that cover the entirety of the @azure scope.  Currently," they wrote, "the observed malicious payload being carried by these packages are Personally Identifiable Information (PII) stealers.  The attacker seemed to target all npm developers that use any of the packages under the @azure scope, with a typosquatting attack.  In addition to the @azure scope, a few packages from the @azure-rest, @azure-tests, @azure-tools, and @cadl-lang [C-A-D-L] scopes were also targeted.  Since this set of legitimate packages is downloaded tens of millions of times per week, there's a high chance that some developers will be successfully fooled by the typosquatting attack."



Okay.  So the npm documentation explains scoping as follows.  They said:  "When you sign up for an npm user account or create an organization, you are granted a scope that matches your user or organization name."  So @azure, for example.  "You can use this scope as a namespace for related packages.  A scope allows you to create a package with the same name as a package created by another user or organization without conflict."  So, for example, where you might have a package with a common name like loader, if it was named @azure.loader, that would explicitly make the @azure instance of loader separate from all other npm packages that might have the name of loader.  "So when listed as a dependent in a package.json file, scoped packages are preceded by their scope name.  The scope name is everything between the @ sign and the slash."  Oh, so it's a slash, not a dot.  So it'd be @azure/loader, for example.



Okay.  So "The individuals behind the attack sought to obscure the fact that the packages all came from the same author by using randomly generated names to create unique users for each uploaded malicious package.  JFrog also noted that the attacker sought to specifically go after machines and developers running from internal Microsoft/Azure networks."  They wrote:  "We suspect that this malicious payload was either intended for initial reconnaissance on vulnerable targets, for example, before sending a more substantial payload, or as a bug bounty hunting attempt against Azure users and possibly Microsoft developers."



JFrog also suggesting that developers make sure their installed packages are the legitimate ones by checking that their name starts with the @azure scope.  In other words, don't be lazy about that.  Any results that don't start with an "@azure" scope may have been affected successfully by this attack.



In other words, from a developer's standpoint, any developer who  may have not been explicitly scoping their package dependencies with the @azure in order to specify reusing my example, the @azure loader package, may have inadvertently picked up one of these 218 mildly malicious non-scoped same-named packages instead.  So the attack was the use of same-named non-scoped packages.  And JFrog found that there were about 50 downloads per package, that is, per 218 of these packages, meaning that while none were downloaded in huge numbers, the non-scoped attack was somewhat effective.  And it would have been much worse if JFrog hadn't picked up on it so quickly.  So, cool that they've got some automated analyzers that are apparently looking at everything that is added to the npm repository and immediately said, hey, wait a minute, these things have the same name as some @azure things.  That could be a problem.



As for how to avoid this sort of supply-chain dependency attack, the JFrog researchers said that developers should use automatic package filtering as part of a secure software curation process.  They said:  "What's alarming about this attack is this is a package that is downloaded tens of millions of times each week,  meaning there is a high chance many developers will be fooled."  As we know from having talked about it before, typosquatting was originally coined, the term "typosquatting" originally coined due to its use in domain name space to make a website or email look like it's from a trustworthy source.



The fact that we're now seeing a related attack seeping into the supply-chain suggests how dependent software is today on third-party packages - and of course think no further than Log4j - and that this has become so widespread today that threat actors are now seeing this kind of attack as a viable vector.  It's long been common practice to sanitize user inputs being accepted by an application.  What's becoming clear is that in today's composite application assembly environment, it's also becoming every bit as necessary to sanitize the backend build process, as well.



Those in the industry have noted that the likelihood of a successful attack varies depending upon how much control the maintainers of a repository have.  In many cases, packages are signed, and only known members of a development team are able to perform such maintenance functions.  In npm's case, and many others, end users are able to offer up modules, and think no further than WordPress.  What could possibly go wrong there?  And the vetting of these modules from a security perspective will vary by the package manager.



In this particular case, due to sheer volume of npm users, it's likely the attack was successful across many machines.  Based on the nature of the attack it's more likely to affect new users of npm, but even experienced developers could be affected if they fail to pay close attention to the name of a specific package they're installing.  Given how quickly the maintainers took down the malicious content, the overall impact to the community, as we know, was limited - this time.



So my feeling is all of this should be sending up bright red warning flares about just how brittle our community-sourced supply chain environment has become.  We've got many years of sort of implicit trust here.  And we know things are slow to change.  Well, the bad guys have figured out supply chain is a big chunk of low-hanging fruit.  So it's becoming clear that insufficient attention has been given to true security here, or at least that security has taken a backseat to convenience.  And that's not a safe place to be moving forward.



LEO:  Fully hydrated, he's gone to Morocco seeking the waters.  Or Casablanca.  He's in Casablanca.  What's next?



STEVE:  So it's been a while since we've talked about automotive vehicle security or lack thereof.  So first, our long-time listeners will recall our coverage of Samy Kamkar and his development of what he named the "RollJam" attack.  Back then, and I think it was 2015, Samy demonstrated a $30 device that was capable of sniffing and jamming a car's constantly changing "rolling codes," and we're talking about like for the remote door open and car starts and like trunk open stuff, right, which as we know many car manufacturers use to unlock and start their vehicles and also to open garage doors.



Samy's RollJam system was effective against both Microchip Technology's named KeeLoq, K-E-E-L-O-Q, KeeLoq access control system, and also the so-called "High Security Rolling Code" generator which was being produced by National Semi.  And since it was able to subvert either of those two most popular technologies, it gave its perpetrator access to vehicles made by Chrysler, Daewoo, Fiat, GM, Honda, Toyota, Volvo, Volkswagen, Jaguar, and those using third-party add-on security systems from Clifford and Shurlok.  And it was also effective against a wide variety of garage-door openers which all used the same chips made by Microchip and National Semi.



Okay, now, as we know, rolling codes are similar to the pseudorandom numbers produced by today's six-digit one-time password, right, or TOTP time-based authentication systems.  But those systems encrypt a sequential counter, that is, the one-time, like the rolling codes, encrypt a sequential counter rather than a time-of-day clock.  Both sequential counter and time-based systems are designed specifically, what they're used for is to thwart replay attacks, unlike a password, which is inherently susceptible to replay attack.  You get someone's password, you just use it; right?



Not so with these one-time tokens.  In the case of a counter-based scheme, a legitimate rolling code is valid only until it is received by the lock, which then advances its own internal counter to expire the code which it just received, which also prepares it to receive the next expected code.



What Samy demonstrated to the world back in 2015 was that the system was "cute," but that it was not strong enough to stand up to an active attacker.  And what Samy came up with was kind of brilliant.  His little $30 device would be placed somewhere near a locked car or garage.  When its owner attempted to unlock their car, Samy's gadget would receive and store the first code, while also jamming its reception to block it from working.  So the car's owner would think, huh, and push the unlock button to try again.  The second time, Samy's device would again receive and block the reception of the second code, but then it would immediately forward the first code it had intercepted the first time, thus unlocking the car or opening the garage as its owner expected.



The first code would work because the car or the garage door opener had been blocked from receiving it the first time; right?  Which would leave Samy's device holding the second and still unused code which the receiving device would now be primed to expect, and which could then be used in place of its owner's unlocking key.  Which I just think is so clever.  And Samy's been around a long time.  He's done all kinds of cool hacks like this.



Okay.  So that was then, way back in 2015.  What lessons have been learned since?  As it turns out, sadly, not nearly as much as we would hope.  Some student researchers and their associates at the University of Massachusetts Dartmouth used a small assortment of off-the-shelf, widely available components to examine the signal being produced by Honda's vehicle unlock and remote start technology.  They used the HackRF One SDR  software defined radio  with a laptop, an account on FCCID.io, and Gqrx's SDR receiver software with a GNU Radio development toolkit.  Just basically just assemble the off-the-shelf pieces.



What they discovered stunned them.  The Remote Keyless System in Honda Civics made for five years, from 2016 through 2020, encompassing the Honda Civic models LX, EX, EX-L, Touring, Si and Type R, all share the same Remote Keyless entry and engine start system, and the keys for all of those cars produced for five years emit an unchanging fixed-for-that-car signal, encoded using frequency shift keying (FSK) over a carrier at 433.215 MHz, standard 433 MHz frequency.  The key's various functions such as door unlock, trunk unlock, and engine start each emit different codes, but the codes for each function - it's hard for me even to believe it - never change.



Okay.  This means  and there are now multiple GitHub videos showing this  successful unlocking and starting of these Honda autos can be achieved simply by replaying the same signals that were earlier produced by, and recorded from, the key.  So what we've had during the intervening years appears to be a significant drop in deployed vehicle security, presumably because nobody was looking, and there was no one to hold vehicle manufacturers accountable.



The multiple implications of this are obvious, I'm sure, to our listeners.  If someone's key was found while they were at a party or at the gym or wherever, its various signals could be recorded kind of offline, as it were, and then replayed at any later time, forever, to gain access to the vehicle and even to remotely start its engine.  Well, "remotely" meaning outside of the car.  Or the key's signal could be captured and recorded near the vehicle when the key is being used in, for example, an employee parking lot.  And then the next day, when the vehicle is locked and unattended, it could be readily unlocked and entered.  Unlike, for example, the use of a "Slim Jim," one of the super thin aluminum strips with a hook at the end used to unlock a car, which would make any thief quite conspicuous, anyone watching a remote radio attack, even a police officer,  would see the car unlock itself as the thief casually approached and entered the vehicle, reasonably assuming that the car's owner had just approached and entered.



LEO:  It would help if the thief goes "uh uh" as he gets in, just to simulate that.



STEVE:  Actually, I think the car probably makes that sound.



LEO:  It might, oh, right, it would because it doesn't know, does it.



STEVE:  Yeah, it has no idea.  It's the same signal that the key emits.  Okay.  Despite this, a spokesman for Honda said they had no plans to update their older vehicles, even after researchers released their comparatively trivial proof of concept for this glaring design failure, also now known as CVE-2022-27254.  When contacted and asked about this issue, Honda spokesperson Chris Martin said:  "It is not a new discovery and doesn't merit any further reporting."  Yeah, and please stop talking about it.  Martin confirmed that "legacy technology utilized by multiple automakers" may be vulnerable to "determined and very technologically sophisticated thieves."



The trouble is, as we've so often seen, what may have required sophistication at the turn of the century is now available using plug-it-together, off-the-shelf technology.  A precocious student in elementary school could pull this off.



LEO:  Wow.



STEVE:  Amazing.  Honda's Chris Martin added that:  "Honda has not verified the information reported by researchers" - yeah, and they don't want to - and continuing the quote, "and cannot confirm if its vehicles are vulnerable to this type of attack.  Honda has no plan to update older vehicles at this time, and there is no indication that the type of device in question is widely used."



Widely used?  Okay.  How would there be an indication?  You want thieves to voluntarily confess?  "Yeah, I did that.  Worked great."  Okay.  One has to wonder whether Honda owners might get together in a large, dare I say "class," and take some "action" about the fact that the security of the vehicles Honda has sold them as recently as two years ago is so readily compromised.  Amazing.  Like, you know, it's magic; right?  You press the button, and your car unlocks.



LEO:  How is this different from what we've talked about before?  I mean...



STEVE:  It's like way lower tech, Leo.  Like before you had to be like the car sent your key fob a signal, and then the key fob had to echo it.  This is just a blind transmission of a fixed code.  It's like the remote control on your TV.



LEO:  Oh, that's terrible.



STEVE:  I mean, it is awful.



LEO:  Is it only Honda?  Does anybody else use this?



STEVE:  That's a really good question.



LEO:  Because I doubt Honda is the only one; right?  It's probably...



STEVE:  Oh, boy.



LEO:  Yeah.



STEVE:  I mean, it's just, I mean, now, and the problem now is it's now public knowledge. It's getting a lot of press.  Everybody knows it.  We know that Hondas for the year 2016 through 2020, five years of Honda Civics, all have keys with a static code.  You simply record the code once, and then you play it back anytime you want.  It's just...



LEO:  That's terrible.



STEVE:  It's unconscionable.  I mean, yeah, sure, maybe it's true that any of these things can be defeated.  And we've talked about them.  But doing it is typically so burdensome that bad guys don't try.  It matters how high the bar is.  And I didn't put it in the show notes, but Martin also said, yeah, you know, there's like Slim Jims that can be used to unlock doors.  It's like, hello.  Yes.  But the thief has, you know, is exposed while they're doing it.  And so they don't do it.  This makes it just falling-off-a-log easy.



LEO:  Yeah.



STEVE:  Wow.  Well, the U.S., the FCC, Kaspersky Labs, and Chinese Telecoms are all mixed up.  Last Friday, in an announcement titled "FCC Expands List of Equipment and Services That Pose Security Threat," the U.S. Federal Communications Commission added the well-known-to-us Russian cybersecurity firm Kaspersky to its "Covered List," believing that the use of Kaspersky Lab products poses unacceptable risks to U.S. national security.  The coverage includes Kaspersky's information security products, solutions, and services supplied by Kaspersky or any linked companies, including subsidiaries or affiliates.



And the same day, last Friday, the HackerOne bug bounty program also terminated their relationship with Kaspersky.  HackerOne's decision to disable Kaspersky's bug bounty program follows the news that Germany's Federal Office for Information Security, known as BSI, had warned companies against using Kaspersky products.  The German regulator indicated that Russian authorities could force the AV provider into allowing Russian intelligence to launch cyberattacks against its customers or have its products used for cyberespionage campaigns.  Just to be clear, this is all entirely without any precipitating evidence and only out of an abundance of caution.



Kaspersky responded by writing:  "Kaspersky is disappointed with the decision by the Federal Communications Commission to prohibit certain telecommunications-related federal subsidies from being used to purchase Kaspersky products and services.  This decision is not based on any technical assessment of Kaspersky products that the company continuously advocates for, but instead is being made on political grounds.  Kaspersky maintains that the U.S. government's 2017 prohibitions on federal entities and federal contractors from using Kaspersky products and services were unconstitutional, based on unsubstantiated allegations, and lacked any public evidence of wrongdoing by the company.



"As there has been no public evidence to otherwise justify those actions since 2017, and the FCC announcement specifically refers to the Department of Homeland Security's 2017 determination as the basis for today's decision, Kaspersky believes today's expansion of such prohibition on entities that receive FCC communications-related subsidies is similarly unsubstantiated and is a response to the geopolitical climate rather than a comprehensive evaluation of the integrity of Kaspersky's products and services.  Kaspersky will continue to assure its partners and customers on the quality and integrity of its products and remains ready to cooperate with U.S. government agencies to address the FCC's and any other regulatory agency's concerns.



"Kaspersky provides industry leading products and services to customers around the world to protect them from all types of cyberthreats, and it has stated clearly that it doesn't have any ties with any government, including Russia's.  The company believes that transparency and the continued implementation of concrete measures to demonstrate its enduring commitment to integrity and trustworthiness to its customers is paramount."



Now, I completely agree that Kaspersky has never given us any cause to mistrust them.  But that's not the question or the problem.  That's a misdirection, I think, that misses the point.  And they know what the point is.  Where they are is the point.  So I'm not sympathetic to Kaspersky's plight.  None of this should have been a surprise to them.  It's been their conscious choice to remain operating in Russia for the past eight years since 2014, after their president and country illegally invaded Ukraine and annexed its Crimean Peninsula.  And being in Russia, they know far more than we do how their country is being run and has been acting.



We know that not everyone in Russia agrees with Putin.  And I don't doubt that Kaspersky would resist and fight any subversion of their integrity.  That's all they have, and that's a lot to lose.  But given everything we've seen recently, it might not be their choice.  And that's the point.  Given the awesome networking power that a deeply trusted and embedded company such as Kaspersky wields, and in the context of an authoritarian regime which is increasingly acting as if it has nothing left to lose, there's every reason to worry that Kaspersky's employees could be forced to act against their will.  So it's not Kaspersky for a moment that I don't trust, it's their ruthless and immoral government that ultimately controls them which we cannot afford to trust in this instance.



LEO:  And there are plenty of good, maybe even better, choices in the world.  It's not like they have a...



STEVE:  Exactly.



LEO:  Now, I have to point out that Kaspersky got his technical education from the KGB Higher School, which prepares intelligence officers for the Russian military and KGB.  He has a degree from there in mathematical engineering and computer technology.  He served in the Soviet military intelligence service as a software engineer.  And he met his wife at a KGB vacation resort two years before he founded Kaspersky Antivirus.  I'm not saying, I mean, here's part of the problem is everybody loves Eugene because he goes - he's a very good salesman.  And he goes around, and he goes to conferences and stuff, and he buys people drinks.  Dvorak used to swear by Kaspersky, probably because he used to hang with Eugene. 



STEVE:  Yup.



LEO:  I don't know.  I think there's no evidence, but there's enough smoke.



STEVE:  Yes.  And your point, Leo, is why take the risk?



LEO:  You don't have to, so why?  And all this, by the way, is saying you can't use government subsidies to buy Kaspersky.



STEVE:  Right.  Right.



LEO:  And by the way, you can't buy a lot of Russian stuff right now, not because they're inherently insecure, but because it's money to Russia.  So I don't think this is a bridge too far.



STEVE:  Yeah.  And from my standpoint there's no way I would feel completely comfortable right now if my computer was running software that was routinely phoning home to Russia.  That just, you know...



LEO:  Yeah.  Seems a bad idea.



STEVE:  We're waiting for the big cyberattack.



LEO:  And they were implicated in the leak of the NSA hacking tools.



STEVE:  Yes, they were.



LEO:  Whether intentionally or not, they were involved.



STEVE:  Yeah.  Which is not to say that other AV might not have also been doing the same thing.  But theirs went to Russia.



LEO:  So anyway.



STEVE:  And, you know, for what it's worth, Kaspersky has not been singled out for this treatment, at least not globally.  Last week's decision to designate Kaspersky as a national security threat follows previous decisions to ban and revoke China Unicom Americas' license over serious national security concerns in January of this year.  And two and a half weeks ago the FCC added the Chinese telecommunications companies Huawei, ZTE, Hytera Communications, Hikvision, and Dahua to its ban list.



Back in June of 2020, Huawei and ZTE were designated national security threats to the integrity of the U.S. communications networks or the communications supply chain, and now the Chinese state-owned mobile service providers China Mobile International USA and China Telecom Americas have been added, as well.  So tensions are running high.  And Leo, we're in this weird world of deep economic co-dependency with those we do not trust.  It's freaky.  I mean, I don't think I have anything.  I don't think I own anything that didn't come from China.



LEO:  It's all made in China, baby, yeah.



STEVE:  Yet here we are, and how many times have I talked about our IoT stuff.  All my lights and plugs and things turn on and off because they're connecting to Chinese cloud services.



LEO:  I actually think that's a good thing, not from a security point of view but from a global economic perspective.



STEVE:  Yes, I do, too.



LEO:  Interdependence is good for peace.



STEVE:  Yes.



LEO:  And if we weren't so interdependent, we couldn't sanction Russia to the degree we have. 



STEVE:  Yes.  And in fact I...



LEO:  It obviously is not enough to stop them.



STEVE:  Well, it's not enough to stop one man.  And I think...



LEO:  Right, that's the problem.



STEVE:  ...that's the problem is that this guy is believed to be the richest person in the world.  He doesn't care at this point.



LEO:  Doesn't care.  Doesn't care.



STEVE:  And there are no handles on him.



LEO:  Right.



STEVE:  There's nothing we can do.  And so we'll see what happens.  Yikes.  Austin Wise tweeted me from @AustinWise.  He said:  "On the most recent Security Now! episode, the phrase 'use-after-free' was overloaded" - and he means overloaded in the object-oriented sense - "to mean both 'using a pointer after calling free()' and 'a language runtime gets confused about the lifetime of garbage collected memory.'  For the second case," he says, "the .NET developers call it a 'GC hole'" - GC as in garbage collection - "a GC hole.  The runtime normally keeps track of memory.  But if it falls into a GC hole it can get lost and freed too early."  He says:  "Section 2.1 of this guide has more details about GC holes in .NET."  And then I have a link in the show notes which he provided.



He said:  "I think a phrase like 'GC hole' could make it more clear when talking about these sorts of use-after-free problems.  Anyways, I love the show.  Thanks for helping everyone know how computers work and how they break."  And so that's what he wrote.  Austin, thank you.  And he's correct that the phrase "use-after-free" was overloaded, and that was the point I was hoping to make.  Much like someone saying that the security vulnerability allowed for security protections to be bypassed.  Gee, thanks for the clarification.  We've seen that in vulnerability reports the term "use-after-free" has similarly become a catchall for any use of memory by any means whatsoever when that memory is no longer allocated.



Last week we talked about how one obvious source of such error is a programmer whose code on their behalf makes that mistake by explicitly freeing something that can later be referenced.  The link Austin provided to the GitHub page regarding Microsoft's .NET common language runtime offered a nice bit of detail about the challenges on the automatic garbage collection side.  And so I did want to share a couple paragraphs from that.



And so this is written to .NET coders asking, "Is your code GC-safe?"  And then it says "How GC holes are created."  And it says:  "The term 'GC hole' refers to a special class of bugs that bedevils the CLR," meaning the Common Language Runtime.  That's like the Java VM; right?  It's the thing that reads the intermediate language.  They say:  "The GC hole is a pernicious bug because it is easy to introduce by accident, repros rarely [meaning reproduces rarely], and is very tedious to debug.  A simple GC hole can suck up weeks of dev and test time.



"One of the major features of the CLR is the Garbage Collection system.  That means that allocated objects, as seen by a managed application, are never freed explicitly by the programmer.  Instead, the CLR periodically runs a Garbage Collector.  The GC discards objects that are no longer in use.  Also, the GC compacts the heap to avoid unused holes in memory, known as heat fragmentation.  Therefore, a managed object does not have a fixed address.  Objects move around according to the whims of the Garbage Collector.  To do its job, the GC must be told about every reference to every GC object. The GC must know about every stack location, every register, and every non-GC data structure that holds a pointer to a GC object.  These external pointers are called 'root references.'



"Armed with this information, the GC can find all objects directly referenced from outside the GC heap.  These objects may in turn reference other objects, which in turn reference other objects and so on.  By following these references, the GC finds all reachable 'live' objects.  All other objects are, by definition, unreachable, and therefore discarded.  After that, the GC may move the surviving objects to reduce memory fragmentation.  If it does this, it must, of course, update all existing references to the moved object.



"Any time a new object is allocated, a GC may occur.  GC can also be explicitly requested by calling the GarbageCollect function directly.  Garbage Collections do not happen asynchronously outside these events; but since other running threads can trigger Garbage Collections, your thread must act as if Garbage Collections are asynchronous, unless you take specific steps to synchronize with the Garbage Collection.  More on that later."  And don't worry, we won't get there.



"Finally, a Garbage Collection hole occurs when code inside the Common Language Runtime creates a reference to a Garbage Collection object, neglects to tell the Garbage Collector about that reference, performs some operation that directly or indirectly triggers a Garbage Collection, then tries to use the original reference.  At this point the reference points to garbage memory, and the Common Language Runtime will either read out a wrong value or corrupt whatever that reference is pointing to."  Whew.



So I liked that just because it should give everybody a sense for how much machination is going on invisibly behind the scenes, and how excruciatingly easy it is for the automatic runtime Garbage Collector to get out of sync with the program's use of variables.  I mean, the fact that people even tried to do this, to me, is mindboggling.  And yes, I am staying with assembly language, which has no garbage to collect. 



LEO:  Hey, you said something earlier about typecasting in assembly language.  You don't even have types, really.



STEVE:  Yeah.  No, no, we do.



LEO:  Does assembler respect types, and you can say is this...



STEVE:  Yes.  Yes.



LEO:  But it's really how big a register it needs or like that; right?  How much of the register it's going to use?



STEVE:  And structures are types.



LEO:  Oh, yeah.  Okay.



STEVE:  So it floats.



LEO:  You have to represent differently than an integer.  And a 32-bit integer can be represented in a 32-bit - 32-bit integer in a 32-bit register.  So sort of types.



STEVE:  So for example - okay.  So for example, say that there's something that I want to refer to, sometimes as a word, and sometimes as a double word.



LEO:  Right.



STEVE:  Just for like convenience because, for example, so a word is 16 bits.



LEO:  And that's hardware-dependent.



STEVE:  Yes.  A word is 16 bits.  A double word is 32 bits.



LEO:  On x86.



STEVE:  On x86.  So I really only want to use 16 bits, but I may want to load this thing into a 32-bit register.  So what I'll declare, and I do this in SpinRite, is I'll declare something as a word, using DW, Declare Word. 



LEO:  Yeah.



STEVE:  And then I'll have like a - then I'll say "zero comma zero."  So I'm actually declaring two words.  



LEO:  Two, yeah.



STEVE:  So I've reserved 32 bits.  But the Intel instruction set is a little-endian instruction set, so the least significant bytes come first.  Which means that I can refer to that location as a word, but I can also load it into a 32-bit register because that second word will be the high half of the 32 bits.



LEO:  Doesn't matter, yeah.



STEVE:  Exactly.  But in assembler, if I try to use a move instruction, I'll say move, you know, EAX comma, and then the name of that variable, it'll complain because I've declared that as a word, and I'm trying to load it into EAX.



LEO:  A double word register.



STEVE:  Yes.



LEO:  Ah.



STEVE:  So what I can do is, well, actually there is an instruction for zero extending a 16-bit into 32.  So I could do that.  But I could also say "move EAX comma D word pointer" and then the name.  So what I've done is I've cast that, I've overridden the word-size-ness of that and said to the assembler, "Trust me, baby, I got this.  This is a D word."  And so the assembler doesn't complain.  It says, okay, and it loads the 32 bits that are there. 



LEO:  So it's really because the assembler is doing some type checking.



STEVE:  Yeah.  It is a strongly typed assembler.



LEO:  I would never have thought of that.  Are you using MASM still?  What are you using?



STEVE:  Yeah, MASM.  I'm still...



LEO:  So that's a feature of MASM.  But, I mean, if you're really hand-coding assembly, you can do whatever the hell you want; right?  I mean...



STEVE:  Oh, and Leo...



LEO:  The microcode isn't doing type checking.  Or is it?



STEVE:  No, no, the microcode does not.  It's all in the...



LEO:  You tell me to put that in the EAX register, I'm going to put it there.



STEVE:  Yes.  It's just an aide to the programmer.  And I have to tell you, it's helped, it's caught me.



LEO:  Oh, sure.



STEVE:  A number of times where I'll just like do something, and then they'll go, eh.  I go, oh, that's right, that's a word.  And so it's very useful to have that.



LEO:  It's interesting.  Hmm.  Yeah, I heard you say that, and I thought, wait a minute.  How would you typecast in assembly?  Well, you can.



STEVE:  So I thought that the term "targeted exploitation" was a little more catchy than calling the podcast "The Exploitation of CVE-2022-0609."



LEO:  Now, I bet Vulnonym has a good name for it, I'm sure.  Let me look it up.



STEVE:  I'll bet they do, yes.  You know, Itchy Camel or something.



LEO:  I'm going to look it up while you go on.



STEVE:  Okay.  So let's talk about last month's Chrome use-after-free exploitation.  Last week on Thursday, the day before Google pushed their second urgent update to Chrome, their TAG team, the Threat Analysis Group, provided some very interesting information about their detection of attacks which were leveraging that first zero-day that was closed mid-February by their first urgent update of the year.  Since except in the case of Microsoft we're almost always talking about vulnerabilities in the past tense, I thought it would be interesting to take a closer look into a case study of targeting.  We're often taking an appropriate, what I think is an appropriate relaxed stance toward the need to update when something will almost certainly only be used against specific targets, since what are the chances that that's us?  But I think this case history should serve to provide a useful reminder of what can actually happen, and also what targeting really looks like.



On February 10th, the Threat Analysis Group discovered two distinct North Korean government-backed attacker groups exploiting a remote code execution vulnerability in Chrome, which was given CVE-2022-0609.  These groups' activity has been publicly tracked as "Operation Dream Job" and "Operation AppleJeus," J-E-U-S.



LEO:  Oh, lord.



STEVE:  I think they are taking some hints from the Vulnonym people.



LEO:  Yeah.  I can't find it.  There are so many vulnonyms.



STEVE:  Oh, god, they're so bad.



LEO:  They need a search engine in vulnonyms.



STEVE:  So we observed the campaigns targeting U.S.-based organizations, says Google, speaking of themselves.  Google says:  "We observed the campaigns targeted U.S.-based organizations spanning news media..."



LEO:  Oh, here it is, "Trollopian Bomd."  That's not any better.



STEVE:  Trollopian Bomd.



LEO:  Oh, wait a minute.  That's 2020-0609.  All right.  Sorry.  I've got to get 2022-0609.



STEVE:  Okay.



LEO:  Very confusing, I tell you.  I'll keep looking.



STEVE:  Okay.  So Google observed the campaigns targeting U.S.-based organizations spanning news media, IT, cryptocurrency, and financial technology industries.  They said:  "However, other organizations and countries may have been targeted, as well.  One of the campaigns has direct infrastructure overlap" - no?



LEO:  They have no name for it.  No name for it.



STEVE:  I'm not...



LEO:  Get going, Vulnonym.



STEVE:  And you know, I'm still following that ridiculous thing.



LEO:  Oh, I do, too.



STEVE:  Every so often I look at what they're coming up with.



LEO:  Hysterical.



STEVE:  It's just nuts.  I'm like, turn that off.



LEO:  They do, it turns out, have a search engine at Vulnonym.org.  But they haven't named that.  So, you know, come on.  This is a big CVE, dudes.



STEVE:  Yeah.



LEO:  Shocking.



STEVE:  That's why we're talking about it.  So Google found out  about this on February 10th, patched it on February 14th, obviously of this year, so four days later.  Again, props to Google for responding so quickly.  They said the earliest evidence we have of this exploit kit being actively deployed is January 4th, 2022.  So of course what happened was, once they knew, once they saw it and understood what was happening, they were able to look back in logs and realize, ooh, we didn't know what that was, but that was happening as far back as January 4th.  So in other words, what, 42 days that this was going on.



They said:  "We suspect that these groups work for the same entity with a shared supply chain" - meaning malicious supply chain - "hence the use of the same exploit kit.  But each operate with a different mission set and deploy different techniques."  They said it's possible that other North Korean government-backed attackers have access to the same exploit kit, as well.



"In this blog," they said, "we will walk through the observed tactics, techniques, and procedures" - and we now have an abbreviation for that, right, TTPs, Tactics, Techniques, and Procedures - "share relevant IOCs" - and that's Indications of Compromise - "and analyze the exploit kit used by the attackers.  In line with our current disclosure policy, we are providing these details 30 days after the patch release."  In other words, we're sure by now this update has leaked out to all instances of Chrome so we can talk about it.



"The campaign, consistent with Operation Dream Job, targeted over 250 individuals working for 10 different news media, domain registrars, web hosting providers, and software vendors."  In other words, they know who these people are; right?  And we've talked in the past that Google will reach out and contact these entities when they identify them to say, uh, you should know that maybe you clicked a bum link, and think about that.  They said:  "The targets received emails claiming to come from recruiters at Disney, Google, and Oracle with fake potential job opportunities.  The emails contained links spoofing legitimate job hunting websites like Indeed and ZipRecruiter."



LEO:  Our sponsor.  Wow.



STEVE:  Yeah.  And note I've got a screenshot of one of these emails.  And so it's got the padlock, https, and the URL reads Indeedus[.]org, which is not the legitimate domain for Indeed.



LEO:  But you might not know that.



STEVE:  Exactly.



LEO:  Close enough, yeah.



STEVE:  Exactly.  Exactly.  Then it says /viewjob and then a big scrambly bunch of characters which all URLs, as I was groaning about last week, have.  So you get this, and it looks legitimate.  I mean, Indeedus[.]org.  It's not Indeed.com.  But again, who would know?  The email solicitation appears to be totally legitimate.  I didn't actually read it, but I hope they have an English speaker in North Korea who's writing these.  I'm sure they do.  Anyway, victims who clicked on the links received by email would be served a hidden iframe that would trigger the exploit kit.



And remember, normally you can click on things that are, I mean, it's because our browsers are not designed to be vulnerable.  So it's when there's a mistake which the bad guys have leveraged that clicking on something gets you in trouble.  But clicking on something is required almost all the time in order to make something happen.  So victims who clicked on the email links would be served a hidden iframe that would trigger the exploit kit.  Attacker-owned fake job domains were disneycareers[.]net, find-dreamjob[.]com, indeedus[.]org, varietyjob[.]com, and ziprecruiters[.]org.



LEO:  So it's really close.



STEVE:  It's really close.



LEO:  Yeah.



STEVE:  And, you know, ziprecruiters[.]org.



LEO:  Plural, though, not singular, so that's the...



STEVE:  Exactly.  And once again it's like it's enough that you even...



LEO:  That's typosquatting right there.



STEVE:  Yes.  Exactly.  Even if you were to scrutinize the URL, you know, you go, okay, yeah, ziprecruiters[.]org, it looks good.  So they said:  "Another North Korean group, whose activity has been publicly tracked as" - this is the other one - "Operation AppleJeus, targeted over 85 users in the cryptocurrency and financial tech industries leveraging the same exploit kit.  This included compromising at least two legitimate financial tech company websites and hosting hidden iframes to serve the exploit kit to visitors."  So you could go to a legitimate fintech website and get hit by this.  "In other cases, fake websites were observed, already set up to distribute trojanized cryptocurrency applications, hosting iframes, and pointing their visitors to the exploit kit."



And those sites were blockchainnews[.]vip, chainnews-star[.]com, financialtimes365[.]com, fireblocks[.]vip, gatexpiring[.]com, gbclabs[.]com, giantblock[.]org, humingbot[.]io, onlynova[.]org, and teenbeanjs[.]com.  Those were the typosquatting sites.  The two legitimate compromised websites between February 7th and 9th were www.options-it[.]com, a real site, and www.tradingtechnologies[.]com, another real site.



"The attackers made use of an exploit kit that contained multiple stages and components in order to exploit targeted users.  The attackers placed links to the exploit kit within hidden iframes, which they embedded on both websites they owned, as well as those two websites they compromised.



"The kit initially serves some heavily obfuscated JavaScript used to fingerprint the target system.  This script," they wrote, "collected all available client information such as the user-agent, the screen resolution [and all the other stuff that's available] and then sent it back to the exploitation server.  If a set of unknown [to them because they had no way to know] requirements were met" - meaning if that fingerprinting said, oh, yes, this is a go - "the client would be served a Chrome remote code execution exploit and some additional JavaScript.  If the RCE was successful" - and that's this first zero-day of the year.  That's the Chrome RCE exploit.  "If that RCE was successful, the JavaScript would request the next stage referenced within the script as 'SBX,' which is a common acronym for Sandbox Escape."  Because of course you've got to get out of the browser sandbox.  And they said:  "We unfortunately were unable to recover any of the stages that followed the initial RCE."



Okay.  And this is really interesting, the way the bad guys hide and protect themselves.  "Being careful to protect their exploits, the attackers deployed multiple safeguards to make it difficult for security teams to recover any of the stages."  Which is why Google's team with absolute access to Chrome were unable to get any more than to learn of the first RCE.  The safeguards include only serving the iframe at specific times, presumably when they knew an intended target would be visiting the site.  Which, like, think about that.  So like if the site is clean and doesn't contain the iframe, except right at the time that they expect that their email will have been received and will likely be getting the click if it's going to, that would bring the target back to the site.  So a real narrow window during which that vulnerability is being sent to people who are entering the site.



"In some email campaigns the targets received links with unique IDs.  This was potentially used," they wrote, "to enforce a one-time-click policy for each link and allow the exploit kit only to be served once per targeted visitor."  So again, even if you grab the email from somebody who received it, you cannot obtain the RCE by reusing the link that was already used once.



"The exploit kit would AES encrypt each stage, including the clients' responses, with a session-specific key."  So traffic sniffing won't work.  And "Additional stages were not served if the previous stage failed."  So all of that being done to tightly constrain the disclosure of the, for example, the sandbox escape.  They know because they saw it in the script that there was an SBX acronym.  Presumably the RCE was then followed by a sandbox escape.  Google doesn't know what it is because they were unable to get it, because the bad guys are being so careful to tightly control and constrain the exploit chain step by step.  And the second it's broken, it doesn't go any further.  And as we said, using unique one-time-only links, you're never able to begin to explore down that chain again.



They said:  "Although we recovered a new Chrome RCE, we also found evidence where the attackers specifically checked for visitors using Safari on macOS or Firefox on any OS, and directed them to specific links on known exploitation servers."  In other words, this set of exploits was for Chrome.  But if somebody was coming in on Safari on a Mac or using Firefox on any platform, they got their own exploits tuned to the environment that they were using.  They said:  "We did not recover any responses from those URLs."



So Google's TAG team has extreme visibility via instrumentation into their own Chrome browser, but obviously not into other non-Chrome browsers.  But this evidence strongly suggests that users of Safari on Mac and Firefox anywhere may have been served their own different browser-specific exploits.



I have a link in the show notes, Leo, if you're curious.  It's what VirusTotal thinks of the exploit kit that Google discovered.  I told a friend of mine about VirusTotal because he had a questionable file.  And like one obscure AV engine out of 70 thought there was a problem, and he got all freaked out.  I said, no, Mark, it's okay, that means nothing.  But this thing lights up like a Christmas tree with how many?



LEO:  They said 25 out of 59.



STEVE:  Yeah, if you ever see 25 AV engines thinking that there's something wrong, don't proceed.



LEO:  Kaspersky got it.  So did Microsoft.



STEVE:  Yeah.  The attackers made multiple attempts to use the exploit - now, here, this is interesting, too.  The attackers made multiple attempts to use the exploit days after the vulnerability was patched on February 14th, and Google says "which stresses the importance of applying security updates as they become available."  Well, I'm here to tell you, I keep complaining about this.  I was using Chrome days after this thing was patched, and my Chrome wasn't patched.  It didn't patch itself until I went to look.



LEO:  At some point, with Firefox, whenever Firefox gets updated, it says you can't use this.  Restart.



STEVE:  Yeah.



LEO:  But Chrome doesn't do that; huh?  That's weird.



STEVE:  Well, now, my Chrome usage, you know, I have Firefox open statically.  I generally start and stop Chrome multiple times through the day.



LEO:  So it should update.



STEVE:  So you'd really think it should.



LEO:  Yeah.



STEVE:  But it always is the case when I look it goes, oh, yeah, we've got something.  It's like, well, it would have been nice if you just did that for me.



LEO:  Yeah.



STEVE:  So under "Protecting Our Users" they said:  "As part of our efforts" - and this is something I wanted to share.  "As part of our efforts to combat serious threat actors, we use results from our research to improve the safety and security of our products.  Upon discovery, all identified websites and domains were added to Safe Browsing to protect users from further exploitation."  So that's one good thing that was done.  So that means even if my Chrome hadn't updated itself, safe browsing would have protected me if my unsafe Chrome had tried to go to any of those places.  But of course that also presumes that they have full visibility into all of the sites that were doing the exploiting, and they can never know that.



They said:  "We also sent all targeted Gmail and Workspace users government-backed attacker alerts notifying them of the activity."  As I said before, Google is good about notifying people who they can that they may have been compromised.  They said:  "We encourage any potential targets to enable Enhanced Safe Browsing for Chrome and ensure that all devices are updated."  They said:  "TAG is committed to sharing our findings as a way of raising awareness with the security community, and with companies and individuals that might have been targeted or suffered from these activities.  We hope that improving understanding of the tactics and techniques will enhance threat hunting capability and lead to stronger user protections across industry."



So let's talk about Enhanced Safe Browsing.  We've mentioned it before in passing, but I want to take this opportunity to talk about Google's, you know, what they're doing.  They said:  "In 2020 we launched Enhanced Safe Browsing, which you can turn on in your Chrome security settings, with the goal of substantially increasing safety on the web.  These improvements are being built on top of existing security mechanisms that already protect billions of devices.  Since the initial launch, we have continuously worked behind the scenes to improve our real-time URL checks and apply machine learning models to warn on previously unknown attacks.  As a result, Enhanced Safe Browsing users are successfully phished 35% less than other users.  Starting with Chrome 91, we will roll out new features to help Enhanced Safe Browsing users better choose their extensions, as well as offer additional protections against downloading malicious files on the web."



Let's see.  Is there anything good here?  Yeah.  "Every day millions of people rely on Chrome extensions to help them be more productive, save money, shop, or simply improve their browser experience.  This is why it's important for us to continuously improve the safety of extensions published in the Chrome Web Store.  For instance, through our integration with Google Safe Browsing in 2020, the number of malicious extensions that Chrome disabled to protect users grew by 81%.  This comes on top of a number of improvements for more peace of mind when it comes to privacy and security.



"Any extensions built by a developer who follows the Chrome Web Store Developer Platform Policies will be considered trusted by Enhanced Safe Browsing.  For new developers, it will take at least a few months of respecting these conditions before becoming trusted.  Eventually, we strive for all developers with compliant extensions to reach the status before meeting these criteria.  Today, this represents nearly 75% of all extensions at the Chrome Web Store."  But that means, and they said "nearly 75."  That means more than 25% don't.



Let's see.  And finally, under Improved Download Protection:  "When you download a file, Chrome first performs a first-level check with Google Safe Browsing using metadata about the downloaded file, such as the digest of the contents and the source of the file, to determine whether it's potentially suspicious.  For any downloads that Safe Browsing deems risky, but not clearly unsafe, Enhanced Safe Browsing users will be presented with a warning and the ability to send the file to be scanned for a more in-depth analysis.



"If you choose to send the file, Chrome will upload it to Google Safe Browsing, which will scan it using its static and dynamic analysis classifiers in real time.  After a short wait, if Safe Browsing determines the file is unsafe, Chrome will display a warning.  As always, you can bypass the warning and open the file without scanning.  Uploaded files are deleted from Safe Browsing a short time after scanning."



Okay.  So under Chrome's three-dot menu, you go to Settings in the dropdown menu.  Then on the left of the page you'll see Security and Privacy.  Click on that.  Then click on Security in the middle.  You're now looking at the Safe Browsing choice.  If you want it, as I would and do, simply click the Enhanced Protection button to enable Google's useful, and I named it here "Big Brother Overwatch," feature set.  It is that.  If you do this, then some of your surfing is being sent back in real-time to Google for their verification.



Well, the way I use Chrome, there's nowhere I'm going that I'm embarrassed for Google to know about.  And, I mean, as the host of this podcast for so many years, I'm becoming increasingly circumspect about the Internet and about what's going on out there.  And so sometimes I'm like, trying, I'm looking for like a DOS Ethernet driver for some motherboard's onboard Ethernet for working on SpinRite in order to do network debugging.  And so I'm going to some websites that look a little sketchy, that are trying to get me to download their "We'll check all your drivers for free."  You know, it's like no, no, no, no.



LEO:  Unh-unh.



STEVE:  And so I'm happy to have Google watching where I go and making sure I don't step in something that I'll regret later.  So I think our listeners probably fall into two categories.  One is the I don't want anybody watching me feeding anything back to the mothership.  There are other users who probably think, yeah, I need to help.  So I just wanted to make sure everybody knew that this Enhanced Browser Protection is available.  You can turn it on.  I run with it on, and I'm glad.



LEO:  Yeah.  Yeah.  No, I think it's - and a lot of browsers use Google's protection.  They actually use that service.  It's a service you can use in your browser.



STEVE:  Yup.  Yup.



LEO:  So I think that's a good public service, absolutely.  Speaking of public services, you're the man.  Thank you so much, Steve Gibson.  Yet another fabulous Security Now!.  This is a show we do every Tuesday.  It's kind of a must-listen for a lot of people every week, about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you wanted to watch or listen live you could at live.twit.tv.  If you're watching live, chat live at irc.twit.tv or in our Club TWiT Discord.  There's a lot of conversation going on in both places.



After the fact, on-demand versions of the show are available from Steve directly at his site, GRC.com.  He has 16Kb audio versions for the bandwidth-impaired, the normal 64Kb audio, and transcripts carefully crafted by Elaine Farris.  All that at GRC.com.  While you're there pick up SpinRite 6.0, the world's finest mass storage maintenance and recovery utility, soon to be 6.1.  Steve's working hard on that.  You'll get a free upgrade if you buy now, GRC.com.



We have the show at our website, TWiT.tv/sn.  There's a YouTube channel dedicated to it.  And of course you can subscribe free in any podcast client, get it automatically the minute it's available.  I think a lot of people collect all the episodes, all 864 of them.  Too bad you can't put that on your bookshelf.



STEVE:  I know they do because I watch it happening on my server's bandwidth.



LEO:  Yeah, the old ones.



STEVE:  I want them all.



LEO:  Yeah, yeah, yeah.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#865

DATE:		April 5, 2022

TITLE:		Port Knocking

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-865.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine a critical Java framework flaw that's been named "Spring4Shell" because it's mildly reminiscent of Java's recent "Log4j" problem.  We'll also take a look at the popular QNAP NAS devices and several recent security troubles there.  Sophos has got themselves an attention-grabbing, must-patch-now 9.8 CVSS vulnerability, and it didn't take long (10 days) for the theoretical Browser-in-the-Browser spoof to become non-theoretical.  There's more worrisome news on the NPM supply-chain package manager exploitation nightmare, the FinFisher spyware firm happily bites the dust, and some of the young hackers forming the Lapsus$ gang have been identified.



Squarely in the doghouse this week is Wyze, whose super-popular webcams have problems which are just as serious as those of the company itself.  And, oh, the authentication bypass details, which I'll share, are so wonderful!  Then, after a bit of closing-the-loop feedback with our listeners, I want to talk about and put the idea of "Strong Service Concealment" on everyone's radar.  "Port Knocking" is not a new idea by any means; but it is extremely clever, cool and useful.  In today's world, there's more reason than ever for ports and the services behind them that are not actively soliciting public traffic to be kept completely hidden.  There are a number of ways this can be done which are very cool.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a new Java framework flaw called Spring4Shell to talk about.  Yes, Steve's going to issue a slight spanking to Wyze, their three-year flaw in their cameras unpatched until January.  How did that happen?  And then a new way to log into servers.  It's a lot easier.  It's called "port knocking."  But is it safe?  Steve explains all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 865, recorded Tuesday, April 5th, 2022:  Port Knocking.



It's time for Security Now!, the show where we cover your security, privacy, and safety online with Steve Gibson of the Gibson Research Corporation.  Hello, Steve.  I'm coming up with some sort of "Let's get ready to secure" or something.  I don't know.



STEVE GIBSON:  Yes, just before we hit 999 we'll come up with a slogan.



LEO:  That's right.  Well, we have a slogan:  "What could possibly go wrong?"



STEVE:  Yes.



LEO:  That's a great slogan.



STEVE:  I think that's probably the best thing ever, yeah.  So we're at, oh, are we at 864 or 865?  I've got 864 in the show notes, at the top of the show notes.



LEO:  Oh, I don't know.  It says 865 in all of my stuff.



STEVE:  Good.  I think I just didn't update the title page or the title of the show notes.  So everyone, ignore that.  It is definitely April 5th, though.  That I'm sure about.  We're going to examine a critical Java framework flaw that's been named of course Spring4Shell because it's mildly reminiscent of Java's recent Log4j problem, or Log4Shell problem.  We'll also take a look at the popular QNAP NAS devices and several recent security troubles there.  Sophos has earned themselves an attention-grabbing, must-patch-now 9.8 CVSS vulnerability.  And it didn't take long, like about 10 days, for that theoretical browser-in-the-browser spoof that we talked about to become non-theoretical.  It's now in use.



There's more worrisome news on the NPM supply chain package manager exploitation front nightmare.  The FinFisher spyware firm happily bites the dust.  And some of the young hackers forming the Lapsus$ gang have been identified.  Squarely in the doghouse this week is Wyze, whose super-popular webcams have problems which are just as serious as those of the company itself, it seems, and oh my god.  The authentication bypass details which I will share are so wonderful.



Then, after a little bit of closing-the-loop feedback from our listeners, I want to talk about and put the idea of strong service concealment on everyone's radar, thus the title for today's podcast is "Port Knocking."  It's not a new idea by any means.  Articles and conversations about it typically have dates like 2008.  But the concept is really clever, I think.  I've always thought it was cool and useful.  And in today's world, there's I think more reason than ever for ports and the services behind them that are not actively soliciting public traffic, like a public web server does, things that are like you want to log onto your own NAS sort of thing.



If you don't need to publicly expose ports, there's just so much reason to keep them completely locked and hidden.  And it turns out there are a number of ways this can be done.  Linux now has a port knocking technology built into it, DD-WRT, OpenWRT.  There's a lot of ways to do this now.  So I just kind of wanted to talk about this, the idea that there is a way for you to be remote from your location, have no exposed open ports, yet by knocking in a certain way have the ports opened only to you at your IP, and only for as long as you want them to be.  So I think an overall really interesting podcast for our listeners.



LEO:  I hear you knocking, but you can't come in.  Actually, if it's me knocking, let me in; right?  It's okay.  Picture of the Week?



STEVE:  Yeah.  This was one that I had in the collection.  It's just sort of fun.  Anyone who has actually spent time programming will probably get a sense for this.  The picture just depicts your kind of typical programmer dude.  We've got some energy drink cans like on the desk, a coffee mug that looks like it's got some coffee dripping down the sides, some crumbled up pieces of paper.  Of course the ever - you have to have yellow Post-it notes like stuck on...



LEO:  Everywhere, yeah.



STEVE:  ...the margins of your monitor in order to qualify.



LEO:  And crumpled up paper, yeah, yeah.  And energy drinks, yeah, yeah.



STEVE:  Yeah.  So at this moment we catch him raising both fists in the air, celebrating.  He's like, got some success.  And he says:  "Wow, a different error message.  Finally some progress!"



LEO:  We made progress.



STEVE:  It's like, he's been like, what is wrong?  No matter what he tries, just keep getting the same error message.  Oh, look, it just changed.  Okay, we're getting close.



LEO:  I've got another one.  I don't know if you ever get this.  But I'm still working on the Advent of Code problems, you know, off and on.  They're really fun programming problems.



STEVE:  Oh, my goodness, yeah, yeah.



LEO:  Yeah, really, this one's - Day 19 was a challenge.  But...



STEVE:  Well, and Leo, I've always told people the best way to learn a language is to use it to solve problems.



LEO:  Yeah.  And these are hard problems.  So something worked by accident.  You ever have that happen?  Where I went, this shouldn't work, but it does.  And then I tried it with different problem sets and different - and it works.  I'm not sure why it works.



STEVE:  No, I've had that.  In fact there have been times when I've written a huge amount of new, like, SpinRite code, for example.  And it assembles correctly, no syntax errors, and then I launch it, and it goes.  And it's like, I'm not sure I trust it because...



LEO:  It's too easy.



STEVE:  I have to pay a bigger price in order to have it all work correctly.



LEO:  Absolutely.  Absolutely.



STEVE:  Well, last week we noted Chrome's second zero-day of the year and titled the podcast, last week's podcast, "Targeted Exploitation," with information thanks to Google's Threat Analysis Group research, which documented the details of the exploitation of Chrome's first zero-day of the year.  This week we switched to Apple, who last Thursday pushed out patches for its own fourth and fifth zero-days of the year.  Last year Google had a total of 16 for Chrome, and Apple was at 12 for the year.  So for Apple to be already at five by the end of the first quarter suggests a run rate that might break both of their 2021 totals.  We'll see how that goes.



Anyway, the fourth and fifth patches which, again, zero-days, cover iPhones, iPad, and Macs.  And these are true zero-day flaws since in their typically obscure way they said that the issues "may have been actively exploited."  Uh-huh.  May?  Okay, so why the emergency? I mean, they, like, this was a "Get this out now" level patch.  Anyway, in any event, we have an out-of-bounds write issue in the Intel graphics driver that allows apps to read kernel memory, and that's never what you want; and an out-of-bounds read issue in the Apple AVD, that's the Audio Video Media Decoder, that will enable apps to execute arbitrary code with kernel privileges.  Both bugs were reported by anonymous researchers and resulted in iOS and iPad OS both moving to 15.4.1, and macOS Monterey to 12.3.1.  The first flaw was fixed by improving input validation is all they said, and the second by improving bounds checking.  So okay.  We'll see how Apple goes with the rest of the year.



Another worrisome vulnerability in a Java framework has surfaced.  The cybersecurity firm Praetorian said that the flaw impacts the "Spring Core" on the Java Development Kit (JDK) versions 9 and later.  And this is odd, too.  It's a bypass for another much, and I mean much older vulnerability from way back in 2010.  That one was tracked as 2010-1622.  And yes, only four digits.  Those quaint days when we only needed four digits to number our CVEs.



LEO:  There are only 10,000 security flaws.



STEVE:  Yeah.  What happened to that?  If exploited, this bypass enables an unauthenticated attacker to execute arbitrary code on the target system, making it, of course, a remote code execution, RCE.  And unfortunately, a Chinese security researcher briefly posted a working proof-of-concept for this exploit to GitHub before deleting the account.  But as we know, it doesn't take long.  Nothing remains hidden on the Internet.  So indeed, the proof-of-concept code was quickly shared in other repositories and tested by security researchers, who confirmed it was a legitimate exploit for this new, potentially severe, and previously unknown Java vulnerability.  



LEO:  So it had been patched in 2010, but there was another way to get to it or something.



STEVE:  Yes, exactly.  So probably not fixed the way we would have wished.



LEO:  Right.



STEVE:  Which is to say it would stay fixed.  This was a bypass around the way it was fixed.  So Spring is a software framework for building Java applications, including web apps on top of the Java EE, the Enterprise Edition platform.  Researchers who've looked at it have said that:  "In certain configurations, exploitation of this issue is straightforward, as it only requires an attacker to send a crafted HTTP" - you know, a standard web query to a vulnerable system.  "However, exploitation of different configurations will require the attacker to do a little additional research to find payloads that will be effective."



So in that sense it does feel like the Log4j, which as we know turned out not to be like the end of the world because it wasn't just drop-dead simple for script kiddie weenies to massively exploit.  It really required more expertise.  The Spring framework's maintainers, Spring.io, which is a subsidiary of VMware, last Friday released emergency patches to fix this so-called, and it actually has been called "Spring4Shell."  It's a zero-day RCE.  Well, that's what they're calling it.  And I was trying to decide whether this would really be a zero-day if it wasn't being actively exploited in the wild against victims.  We know that Microsoft has their own definition for zero-day.  Normally we reserve the term, and we're trying not to overuse it by making it overly broad, to be like, whoops, we learned about this because we saw it being used.  That's clearly a zero-day.



So this is kind of a gray area because it's been - the exploit has been publicly disclosed.  And I imagine that in the next week or two I'll be saying, oh, yeah, it went from public disclosure to weaponized.  But anyway, I think that since a public proof-of-concept exploit exists before a patch is ready, and actually a patch just happened, but certainly the proof of concept has existed for some time, it probably qualifies as a zero-day.



There was also some confusion because two other related vulnerabilities in that same Spring framework were also disclosed last week.  There was a DoS vulnerability, meaning you can crash the thing, and the Spring Cloud expression resource access vulnerability, and I didn't dig into those because they've been patched, and it's like, okay.  And they're unrelated to this one.



So there' also been some questioning about just how bad this RCE really is.  The concern is that it is in use by enterprises for all kinds of their own custom server things, which means that it's - who knows what's wrong with any particular enterprise's implementation?  After an independent analysis, Flashpoint said:  "Current information suggests in order to exploit the vulnerability, attackers will have to locate and identify web app instances that actually use the" - and here it is - "the DeserializationUtils, something already known by developers to be dangerous."  Okay.  But that doesn't mean developers aren't using them because they're there; right?  It's an API.  Oh, look, this does what I want.



So again, it's not like developers are nearly as security focused as we are, and the group listening to this podcast is.  And we've talked a lot about the dangers of deserialization. Java is an object-oriented language, which means that an object is a complex, or at least can be, typically is a complex data structure.  So how do you store such a thing?  The way you store it is you serialize the object into a byte stream, you know, a blob, which you then store.  And in order to later reconstitute the object into a form that Java can use it, you need to deserialize the blob.  And a deserializer is inherently an interpreter of the byte stream.  And as we know, naive interpreters are written to assume that they will only ever receive a valid deserialization stream to deserialize.



In fact, in an interesting twist, we're going to see that this Wyze cam authentication flaw is sort of like that.  The guys that designed the handshake just assumed you'd be a valid handshaker.  But no.  Anyway, the security firm Rapid7 said that despite the public availability of proof-of-concept exploits, "it's currently unclear which real-world applications use the vulnerable functionality."  Which is really just to say we don't yet.  This just happened.  So we need exploits in order to - we need actually to have some problems before we know.  And they said:  "Configuration and JRE version may also be significant factors in exploitability and the likelihood of widespread exploitation."



However, CERT CC's oft-quoted vulnerability analyst Will Dormann tweeted, he said:  "The Spring4Shell exploit in the wild appears to work against the stock 'Handling Form Submission' sample code from Spring.io.  If the sample code," he says, "is vulnerable," he said, "then I suspect there are indeed real-world apps out there that are also vulnerable to remote code execution."  And I think is logic is exactly right.  The developers, again, they're just going to take the sample code, which is obviously using the deserialize utils, and like tweak it, change the name to their company and whatever.  So the flaw was assigned a CVE with a CVSS of 9.8.  So that's meant to grab everyone's attention.



And yesterday, so Monday, VMware published security updates to remove the flaw from their Spring.io subsidiary's code.  But as we also know, publishing the update is different from having it deployed on a server that's out in the field, and this is all just very fresh.  So as I said, I expect in a couple weeks, much as we'll be talking about the exploitation of the browser and the browser flaw that was theoretical two weeks ago, not anymore.  So I think the same thing will probably be happening here.



Between the initial discovery of the vulnerability and yesterday's patch publication, exploitation of the vulnerability where possible appears to have taken off at least as much for the CVE to get a 9.8.  So it's considered to be of critical importance to anyone using this Spring framework.  If you're responsible for it, or you know that your organization uses it, definitely go get VMware's update and fix this.  The framework is of the MVC style, the Model View Controller approach, and also Spring WebFlux apps running on JDK 9 and later are vulnerable.  So definitely worth doing.



Okay.  We recently talked about the denial-of-service bug that Tavis Ormandy and Chrome's TLS guy together worked to discover in the OpenSSL library.  Remember that that's the one which results in an infinite loop and essentially processor capture when processing client certs that have been deliberately manipulated to use specific elliptic curve crypto parameters.



Among the many companies whose products can likely be hung when they receive such a maliciously crafted client cert is the Taiwanese company QNAP, which last week revealed that a selected number of its network-attached storage appliances were in fact vulnerable to this OpenSSL problem.  Last Tuesday their advisory said:  "An infinite loop vulnerability in OpenSSL has been reported to affect certain QNAP NAS.  If exploited, the vulnerability allows attackers to conduct denial-of-service attacks."  So, you know, that's not the worst of all possible outcomes.  Yes, your NAS goes down.  But it doesn't go down in a way that lets any bad guys get in.  So okay.  It sort of sacrifices itself.



I have a list of the affected QTS and QuTS versions, but QNAP doesn't yet have any patches available anyway.  The good news is, as I said, the only thing an attacker can do is to hang your NAS, which you then reboot, and it's back up until they hang it again.  And they can only do that if TLS connections are being accepted from random IPs out on the public Internet.  And everyone knows that's never a good idea; right?



QNAP keeps being somewhat of a mixed blessing.  As I've looked through some of the back-and-forth conversations that surround its various problems, I see that its users generally love their devices, despite them having had more than their share of security troubles even just this year.  I mean, we've been talking about QNAP vulnerabilities for years.  QNAP is currently working to catch up to the OpenSSL DoS flaw which is only a couple weeks old, but they're also still working to patch that recent "Dirty Pipe" Linux kernel flaw from earlier in March, which also currently has no mitigation on QNAP's NAS devices.



The good news there is that at least it's only a local privilege escalation vulnerability.  On the other hand, if you're in a big enterprise, and you're not able to trust all the people on the inside of your network, the fact that it's only local doesn't provide much comfort.  And it's not QNAP's fault about the OpenSSL side, at least.  Well, actually even this Dirty Pipe because both of these problems arise from the Linux kernel that it's built on.  So everybody who was using the Linux kernel with those problems would have been subjected to these potential vulnerabilities.



But more than that, attackers have been pummeling QNAP devices all year with both ransomware and brute-force attacks to the point that the brute-force attacks prompted QNAP to urge its own customers to remove their Internet-exposed NAS devices from the Internet.  In late January, QNAP forced out, pushed an unexpected and not entirely welcome update to its customers' NAS devices after warning them that the DeadBolt ransomware was mounting an offensive aimed at QNAP's users.  So on top of everything else these users are targets.  And just two weeks ago reports surfaced that DeadBolt, the DeadBolt ransomware, was at it again in a new wave of attacks against QNAP.



And last August, two vulnerabilities that could result in remote code execution and denial-of-service respectively prompted emergency patches by QNAP.  Now, interestingly, this broader topic, and I think this is probably what made me think of it actually, of today's topic, the broader topic of the dangers of public port exposure, which QNAP perfectly evidences, serves as a perfect lead-in to today's discussion of Port Knocking, the idea being there is a way, and it's hosted on Linux, so QNAP could use it, of completely blinding the public Internet to the presence of open QNAP services, except to people who know the secret knock, which we'll be talking about later, both pros and cons, because port knocking has had some people saying, eh, it's just security through obscurity.  I'm not sure that I buy that.



Okay.  Sophos has a 9.8.  Last week the cybersecurity firm Sophos warned that a recently patched critical security vulnerability - and that's the way you want to start these notices about a 9.8, it's recently patched, good, better than we don't have any fix for it yet - recently patched critical security vulnerability in its firewall product was now being actively exploited in real-world attacks.  Now, that's not what you want to say; but that flaw CVE-2022-1040 sports, as I said, the attention-grabbing CVSS of 9.8 and impacts Sophos Firewall versions 18.5.3, also known as 18.5 MR3, and earlier.



And what you never want to hear is it's an authentication bypass vulnerability in the user portal and web admin interface.  You could argue that the user portal needs to be open and running on the public side.  Web admin, eh, I have to be convinced.  But once again, there's a way to protect that.  And still worse,  when it's exploited, this particular 9.8, thus the number, allows a remote attacker to execute code of their choosing.  And obviously, since it's under exploitation, the bad guys are aware of it.  Sophos's security advisory said:  "Sophos has observed this vulnerability being used to target a small set of specific organizations primarily in the South Asia region.  We have informed each of these organizations directly."  So clearly they've got some telemetry with their product which has allowed them to determine, whoops, this is happening, and nice of them to let their customers know.



And, you know, as for "doing it right," the flaw was addressed first in a hotfix that is automatically installed for customers who have the "Allow automatic installation of hotfixes" setting enabled.  And I recognize we've talked about this a lot, that the whole issue of automatically pushing updates of security vulnerabilities is a bit controversial.  I would argue it's becoming less controversial with time.  But at least in this instance, having Sophos taking responsibility for and maintaining their firewalls to me sure seems like a good idea.  Our operating systems are doing it now.  Our mobile device platforms are doing it now.  Pushing that out one level or layer to the firewall that is in front of the operating systems, that seems like a good idea, and especially when it's also in front of potentially a whole organization.



If it were mine, I'd be inclined to let Sophos autonomously maintain the firewall whose code they created in the first place.  And I'm sure it's signed and authenticated, and there's no way for it to be spoofed.  And yes, it's true.  If they suffered a break-in, bad guys could potentially poison the source of the updates and push that out.  So there's the downside of that.  But on balance, probably a good idea.  And not surprisingly, until the firewall is updated one way or the other, if a user goes and gets it themselves, they recommend that their users disable WAN access to the User Portal and the Web Admin interfaces.  And I would wonder why.  Certainly those should not be enabled unless they're absolutely needed.



And in a statement about both their integrity, their commitment to doing things right, and the severity of the issue, they have also provided updates to many earlier past-end-of-life versions of their firewalls and firmware.  Sophos said:  "Users of older versions of Sophos Firewall are required to upgrade to receive the latest protections and this fix."  These things were past end-of-life, and they thought, okay, this is bad enough, we're just going to fix those.  And as we'll be seeing, that's a choice that Wyze did not make with their cameras.



Okay.  So last Thursday the U.S. CISA ordered federal civilian agencies to patch, not only that critical Sophos firewall bug we were just talking about, presumably if they don't already have automated update or auto update enabled, but in addition seven other vulnerabilities.  And the federal civilian agencies have three weeks, or until April 21st, to get them all patched.  And CISA says all eight of these vulnerabilities are under active exploitation.  We know that the Sophos one is.



In addition to the Sophos problem, the CISA also ordered federal agencies to patch a high-severity arbitrary file upload - that doesn't sound good - vulnerability in the Trend Micro Apex Central product management console that can similarly be abused in remote code execution attacks.  Two days earlier, Trend Micro said that it had observed "at least one active attempt of potential exploitation," which sounds a little bit like they're hedging, but I'm sure they know that this is actually happening.  And the list of eight total "you must patch these" commandments include the need to patch a different QNAP NAS problem than the one we were just talking about, an improper authorization vulnerability which has been reported to affect QNAP NAS running HBS 3, which is their Hybrid Backup Sync.  When exploited, that vulnerability allows remote attackers to log into a device.  So I suppose it's no surprise that QNAP is being targeted as much as they are.



Anyway, when I was looking over this list, I did a double take, since two of the eight CVEs which CISA says are currently under active attack are dated 2018, and one is back from 2014.  The two from 2018 are for Dasan GPON Routers. And I'm sure we talked about this back in 2018 because I remember the GPON stands for Gigabit Passive Optical Network routers.



LEO:  Oh, GPON.



STEVE:  GPON.  And they suffer from, again, a long-since-patched command injection vulnerability and authentication bypass vulnerability, two different problems, currently under attack, that have long since patched.  But again, it's a router.  So you can kind of see it like in a forgotten closet somewhere, just sitting there, doing its job, hosting all kinds of crypto currency miners and who knows what else.



Okay.  The CVE from 2014 is a bit startling.  2014.  It's CVE-2014-6324.



LEO:  So the year gets assigned when it's discovered.  Right?  So it's been eight years.



STEVE:  Yes, eight years, this thing.  And it's in use now.  It's being, like...



LEO:  Sure it is.



STEVE:  They're seeing people scanning for this thing.



LEO:  It's an oldie but goodie, yeah.



STEVE:  Oh, boy.  Its description in the National Vulnerabilities Database says:  "The Kerberos Key Distribution Center (KDC) in Microsoft Windows Server 2003 SP2, Windows Vista SP2, Windows Server 2008 SP2 and 2008 R2 SP1, Windows 7 SP1, Windows 8, Windows 8.1, and Windows Server 2012 Gold and R2."  It's there.  It's in all those.  "Allows remote authenticated domain users to obtain domain admin privileges using a forged signature in a ticket."  And that sounds familiar to me.  I'm sure we talked about it back then.  And it's been exploited in the wild since November of 2014.  It's known as the Kerberos Checksum Vulnerability."



So here we are, eight years later, and the U.S. CISA feels the need to explicitly tell federal civilian agencies that they now - eight years, but no, no.  Now you have three weeks to get that patched.  I'm sure all they needed was a gentle reminder.  And oh, yeah, get right on that.  Meant to do it yesterday, but thanks for the reminder.  Wow.



Okay.  Two weeks ago, when we talked about the browser-in-the-browser attack during podcast #863, it was all just theoretical.  Remember that its penetration testing developer, "Mr.d0x," had simply produced a very convincing proof of concept.  We showed in the show notes side by side a real OAuth popup authentication and his faked one.  And they were the same.  The same domain name which was spoofed, everything.  So he demonstrated that using just HTML, CSS, and JavaScript, you could produce an actual lookalike multifactor authentication popup.  And in today's world it took less than 10 days for that "Hey, that's a great idea" concept to become fully weaponized.



Last Thursday, a Belarusian threat actor known as Ghostwriter, also known as UNC1151, had been spotted leveraging this recently disclosed browser-in-the-browser technique as part of their credential phishing campaigns which are simultaneously exploiting the ongoing Russian invasion of Ukraine.  As Mr.d0x demonstrated, this technique allows a legitimate domain and popup to be shown to an unsuspecting user.



Google's TAG team, their Threat Analysis Group, wrote in a posting last Wednesday that Ghostwriter was using Mr.d0x's browser-in-the-browser to siphon credentials entered by unsuspecting victims.



LEO:  Well, of course.  Who doesn't love Mr.d0x?



STEVE:  What could possibly go wrong?  Let's just show this to the world because maybe the Belarusians are out of fresh ideas for how to phish people.  So the TAG team said:  "In early March, Google's Threat Analysis Group published an update on the cyber activity it was tracking with regard to the war in Ukraine.  Since our last update, TAG has observed a continuously growing number of threat actors using the war as a lure in phishing and malware campaigns.



LEO:  Oh, of course.  God.



STEVE:  Of course.  It always happens.  Government-backed actors from China, Iran, North Korea, and Russia, as well as various unattributed groups, have used various Ukraine war-related themes in an effort to get targets to open malicious emails or click on malicious links.  Financially motivated and criminal actors are also using current events as the means for targeting users.  For example, they wrote:  "One actor is impersonating military personnel to extort money for rescuing relatives in Ukraine."  Wow.



LEO:  That's just low.  God.



STEVE:  Hey, would you like your mother to be rescued?  I'm a Russian, and I found your Mom, and send me some money, and I'll bring her home.  Wow.



LEO:  Ugh.



STEVE:  TAG has also continued to observe multiple ransomware brokers continuing to operate in a business-as-usual sense.  So it should be no surprise.  Anytime anything of importance happens anywhere, the scum surface in an attempt to leverage the event to their advantage, whatever it might be.



In this case Google's group wrote that:  "Ghostwriter actors have quickly adopted this new technique, combining it with a previously observed technique, hosting credential phishing landing pages on compromised sites."  So here we have an example of a purely theoretical proof of concept being picked up within days of its publication and quickly being leveraged to significantly increase the effectiveness of a traditional phishing and logon campaign.  And as we discussed at the time, Leo, two weeks ago, you look at it, and it's like, yeah, this says PayPal, very clear, P-A-Y-P-A-L.  No typo.  It's not Popal or anything else.  You know, looks great.  Click that link.  What could possibly go wrong?



LEO:  Yeah, yeah.



STEVE:  When we first talked about NPM supply chain attacks last week, the security firm JFrog had identified at the time a total of 218 malicious packages which were using a form of name collision to replace packages in the @azure namespace.  By naming their malicious packages without any namespace designation, their packages might be obtained if a developer had not explicitly specified the @azure namespace as their target for their dependency.



At the time, and it turned out that was true, it wasn't a massive effect, but it was worrisome.  And at the time JFrog had not identified the threat actor behind this NPM repository attack.  Now, a week later, we know more.  The threat actor is named "RED-LILI," R-E-D hyphen L-I-L-I.  They've been linked to this ongoing large-scale supply chain campaign targeting the NPM repository, and have published nearly 800 malicious modules.



The Israeli security company Checkmarx said:  "Customarily, attackers use an anonymous disposable NPM account from which they launch their attacks, as in one account.  But this time, they said, the attacker has fully automated the process of NPM account creation and has opened dedicated accounts, one new account per package, thus making this new malicious package much more difficult" - these new malicious packages - "much more difficult to spot."



Checkmarx's findings build upon recent reports from, as we know, JFrog, but also Sonatype, which detailed hundreds of NPM packages which leveraged the dependency confusion typosquatting-style package replacement to target not only Azure, but also Uber and Airbnb developers.



According to a detailed analysis of RED-LILI's modus operandi, earliest evidence of anomalous activity was found to have occurred on February 23, with the cluster of malicious packages being published in "bursts" over a span of a week.  Specifically, the automation process for uploading the rogue libraries to NPM, which Checkmarx described as now being a "factory," involves using a combination of custom Python code and web testing tools like Selenium to simulate user actions required for replicating the user creation process in the registry.



So in other words, an actual user goes through some processes to sign up for and acquire an NPM account.  Well, nothing prevents all of that from being automated.  This reminds me of the problems we were having initially over on our web forums, right, because you could have bots or actual users, and we have seen actual users creating accounts.  So it's true they're not a robot when they click "I'm Not a Robot."  They're telling the truth.



Bypassing the one-time password verification barrier put in place by NPM is no problem since NPM sends a one-time password to the email address the attacker's bot registers with.  And what I've seen firsthand from bots registering on our web forums, they just create Gmail email accounts like there is no tomorrow.  Typically a normal first name and then six or seven digits, which they just make up at random.  Probably doesn't exist.  Create the account, looks, I mean, it is a valid Gmail account.  They then register under that account.  The one-time verify your email goes there.  They pick it up from there, plunk it into the web page.  The whole thing is now automated.  So one malicious package per account, thanks to automation.



Checkmarx researchers said:  "As supply chain attackers improve their skills and make life harder for their defenders, this attack marks another milestone in their progress.  By distributing the packages across multiple usernames, the attacker makes it harder for defenders to correlate and take them all down with 'one stroke' as had traditionally been possible."



So I read this as sort of the chickens coming home to roost.  The NPM system never had super-tight security.  And that was fine for a long time.  But now it's not.  And this is sort of,  if we were to - if there was a recent theme, it would be things that were okay for a long time are no longer so.  The lowest of the low-hanging fruit has been picked.  Now attackers are looking around for other targets, and they're finding them.  They're finding things that were not really deeply secured and going after those.



On the NPM side, its lack of tight security is finally becoming a problem for it.  And the only way to combat this would be to impose much more stringent strictures on account creation and content publication.  And like making somebody be a registered user for some length of time.  The problem is I've seen that being bypassed.  I've had over on GRC prior to us locking things down to a much greater degree, which we finally have, when I was getting rid of old accounts, there were all these bogus accounts that had been created that had never posted anything, presumably waiting for a time when they would come back later.  And if there was some sort of a time, a minimal time somebody has to be a member before they're allowed to create content, they were just letting those clocks tick, waiting for the time that they would start posting spam under those accounts.  So I don't know how you solve this problem.  But it really is one.



FinFisher has been lurking around for years as one of the more successful and prevalent commercial spyware purveyors.  Their product is called FinSpy.  And the good news is that this Munich, Germany-based spyware company formally declared its insolvency last month amid an ongoing - not only "amid," but due to an ongoing and certainly unwelcome - to them, welcome to the rest of us - investigation into its business dealings.



They made the mistake, FinFisher did, of selling their premiere spyware product FinSpy to the Turkish government without having the legal documentation required to do so, after which their FinSpy system was used in a Turkish operation that preyed upon anti-government protestors.  We talked about this at the time.  Legal complaints filed by Reporters Without Borders, Netzpolitik, the Society for Civil Rights, and the European Center for Constitutional and Human Rights, all accused FinFisher of failing to abide by European export regulations including the requirement to obtain a permit granting trade to non-EU countries by the Federal Office of Economics and Export Control.  FinSpy was created back in 2016 and has been linked to customers including the governments of Egypt, Bahrain, Bangladesh, Ethiopia, Oman, Saudi Arabia, and Venezuela.



According to the NGO's investigation, they said:  "There are urgent indications that the Munich-based company conglomerate sold the spy software FinSpy to the Turkish government without the approval of the federal government, and thus contributed to the surveillance of opposition figures and journalists in Turkey."  So about a year and a half ago, in October 2020, German authorities raided FinFisher's corporate offices, two associated businesses, and the residences of directors and executives, leading to the recent announcement that FinFisher accounts were seized and operations halted.  So it's very likely the end of that operation.  Not the end of all mobile spyware sales, unfortunately, but at least one fewer.



Recall that three weeks ago, during our QWACs On, QWACs Off episode, we mentioned the attack on Nvidia's networks and that the attackers subsequently exfiltrated about a terabyte of Nvidia's data, which paradoxically included some expired Nvidia driver signing certificates.  Those certificates were then immediately used to sign malware, and I was puzzled at the time over how and why Windows would choose to honor drivers signed by certificates that were expired at the time of their signing.  That still remains a mystery.  The mystery that no longer remains regards a couple of the perpetrators behind that and apparently many other recent very high-profile attacks including the likes of Microsoft, Nvidia, Samsung, Okta, and Ubisoft, with many of them resulting in massive data leaks.



The group calls itself Lapsus$  spelled L-A-P-S-U-S with an additional trailing dollar sign appended.  And despite the trailing dollar sign and their high-profile victim list, most Lapsus$ members are believed to be teenagers driven mainly by their goal - actually I don't think we can say "most" because we don't know how big the group is.  But we've found...



LEO:  Seven teenagers were arrested, yeah.



STEVE:  Yes, in addition to these most recent two, who were aged 16 and 17.  They made the news last week when they appeared at the Highbury Corner youth court in London, charged with a number of cyber offenses.  The names of both men, being minors, are being kept private, and both were released on bail.  They've both been charged with three counts of unauthorized access with intent to impair operation of, or hinder access to, a computer; and two counts of fraud by false representation.  Additionally, the 16 year old has also been charged with one count of causing a computer to perform a function to secure unauthorized access to a program.  Which, you know, is gobbledy-gook for they're hackers, or legalese for they're hackers.



And the pair appears to be part of a larger group because also last week, as you said, Leo, the City of London police, which is leading the international investigation into Lapsus$, announced that it had arrested seven people, all between the ages of 16 and 21, in the U.K. alone.



In the U.S., our FBI is looking into the group's illegal activities and is seeking information concerning the Lapsus$ members involved in the compromise of computer networks belonging to multiple U.S.-based companies.  The FBI said:  "These unidentified individuals took credit for both the theft and dissemination of proprietary data that they claim to have illegally obtained.  The FBI is seeking information regarding the identities of the individuals responsible for these cyber intrusions."  So we've got an interagency issue.  In the U.K., they're not disclosing the names of these individuals.  The FBI is saying, well, thank you, we understand that, but we need to know.  So I imagine that'll happen.



While it's still unclear how many active members the gang has and what roles each of them play, based on Telegram chats it's believed that they at least have affiliates, if not core members, located all over the world, speaking multiple languages including English, Russian, Turkish, German, and Portuguese.



Their bail and release was said to have "conditions."  And I would bet that one of those conditions is an utter and total parental-enforced ban from any use of any Internet-connected devices while this case moves through the courts.  If so, that might explain why around the time of the news of the arrests, Lapsus$ told its nearly 58,000 Telegram followers, among whom I'm sure is the FBI, that some of its members would be "taking a vacation." 



But those recent arrests haven't put a damper on the larger group's activities because last week 70, seven zero, gigabytes of data belonging to the software services giant Globant were leaked on March 30th.  Globant, whose headquarters are in Luxembourg, said they're currently conducting an exhaustive investigation and that it's "taking strict measures to prevent further incidents."  I bet they are.  Too bad they didn't do that beforehand.



Okay.  I titled this "Not So Wyze."  One week ago, last Tuesday, Bitdefender published the results of their close examination of the very popular Wyze family of security and surveillance-oriented Internet-connected webcams.  And it will surprise no one to learn that they found problems, nor that the problems were extremely critical given the application these webcams are typically deployed for.  Right?  I mean, they're being sold as let's use this for security.  And as I said at the top of the show, I utterly love the details, and our listeners will, too, and you will, Leo, of the authentication bypass that Bitdefender found, which I'll describe in a minute.



The most distressing part of the story, well, the equally distressing part of the story, is the fact that the Bitdefender group has been working with Wyze, or perhaps better stated "attempting to work with Wyze" for three years to get these three critical problems which they uncovered resolved.  Back on March 6th of 2019, Bitdefender made first contact with Wyze and asked for a PGP key via their support form.  You know, and as we know, that's standard practice now.  You ask a vendor for a PGP key which will allow you to securely communicate with them, which involves the disclosure of potentially extremely sensitive details that they don't want exposed any more than the discoverer wants them exposed.  No response.



They waited a week.  On March 15th, 2019, three years ago, a little more now, Bitdefender made a second attempt at getting in touch with the vendor, still no response.  Apparently unrelated, on April 22nd, Wyze released an update for Wyze Cam v2 to v4.9.4.37, which reduced the risk for unauthenticated access to the contents of the SD card that the camera might have.  But still no contact with Bitdefender's research team.  So this looks like it was just coincidental.



The next day, 4.10.3.50 was released for Wyze Cam Pan v1 with the same risk reduction for unauthenticated access to the contents of the SD card.  So that looked like they did the same firmware update to a different product.  That was April 23rd.  A month goes by, and Bitdefender thinks, well, okay, let's reserve some CVE numbers for what we will eventually be publishing.  So they did that.  So that's May.



June, July, August, September, four months.  And Wyze released Wyze Cam v2 that happened to fix one of the three CVEs that had been issued, but not the most critical one.  So that was September 24th, 2019.  Now we move to November 9th, 2020.  And the vendor fixed a different one of the CVEs through an app update.  The next day, finally, Wyze acknowledges the reception from a year and a half before and assigns an internal contact at Wyze to deal with Bitdefender.  Two days later, Bitdefender sends the advisory to them and a proof of concept.  Nine months pass.  Silence.



On August 31st, 2021, Bitdefender follows up on patch progress.  Hello.  Is anybody there?  September 13th, 2021, so two weeks from August 31st to September 13th.  Bitdefender notifies the vendor.  Oh, it actually probably was exactly two weeks.  They waited.  Nothing happened.  So they said, okay, we're going to publish.  Four and a half months pass, which brings us to January 29th, 2022.  Wyze released firmware to fix the unauthenticated access to the contents of the SD card issue, which is one of the biggest problems.  Okay.  So that was on January 29th.  Being again ridiculously responsible, Bitdefender waited 60 days from January 29th to March 29th.  On March 29th, they published their report.



I've said it before, and I'm sure this won't be the last time I say it again:  There is something fundamentally wrong with the idea, the way we have everything set up today, that an independent security research group must expend this level of effort to not only first reverse engineer and examine a product whose security is critically important to its users, but to then face an utterly unresponsive product publisher and attempt for three years to get them to fix critical flaws in the operation of their surveillance interconnected webcams.



And look at the Catch-22 that Bitdefender is then in.  The only way to leverage responsibility from Wyze, to get Wyze to get off the dime, would be to go public with the news and the details of the flaws.  But doing so would immediately place all of Wyze's gazillion webcam users at significant risk.  And even if details were withheld, from like a partial disclosure by Bitdefender, we've all seen many instances where just telling the bad guys where to look for vulnerabilities is all that's necessary.  Those wearing black hats could certainly follow in Bitdefender's footsteps.  So Bitdefender had little true choice other than to wait and push and poke and prod and hope that Wyze would eventually open a responsible dialog.  Again, they couldn't risk drawing any attention to the Wyze cams because other people could figure out how to exploit them.  And the problems were really bad.



And what I loved, it's just rich that Wyze's cybersecurity team, like they have one, finally said they appreciated the responsible disclosure provided by Bitdefender on the vulnerabilities.  Yeah, I bet they did.  Three years Bitdefender patiently waited because of Bitdefender's ethics.  Essentially Wyze had Bitdefender over a barrel. 



Okay. So get a load of this truly amazing classic remote connection authentication bypass.  It's just the best thing ever.  When connecting remotely, a client is required to log onto the camera; right?  The camera running a service, so we'll consider it to be the server.  The client being a user on a web page or whatever.  A client is required to log onto the device.  Of course, because you don't want everyone to have access to your webcam, by definition.  The client and the webcam share a 128-bit secret key.  Okay, that's good security.  Webcam has a 128-bit secret key burned into it.  The client is required to know it, a pre-shared key.  Good security.  No problem there.



So the client initiates its connection by sending an IOCTL, an IO Control command, with the ID of its hex, 2710.  Upon receiving, the cam will accept a TCP connection.  Then the Wyze cam receives this packet with the ID 2710, which induces it to generate a random nonce value which it encrypts with its 128-bit shared secret key.  Okay, that's great.  It sends the encrypted blob to the client.  By the design of this simple protocol, the client must have that same 128-bit shared secret key, which it uses to decrypt the camera's randomly chosen nonce value, which it had then encrypted, to authenticate itself to the camera, which it does by returning the properly decrypted camera nonce using an IOCTL command with the ID 2712 instead of 2710.



So 2710 initiates the handshake, asks the camera to generate a nonce, which encrypts the 128-bit shared secret, sends the encrypted blob back to the client.  Client that has the same 128-bit shared secret key decrypts it and then returns it to the camera under the command 2712.  The camera receiving the 2712 IOCTL compares the nonce that was hopefully decrypted by the connecting client to the value that it stored locally.  And only if they match will the authentication succeed and the connection be accepted.  And after that the client is free to do whatever it wishes with the camera.  Right?  No problem.  Simple.  Shared secret.  Workable protocol.



Here's what the Bitdefender guys found.  The way the Wyze firmware works is that upon receiving that initial 2710 command, it generates and stores the nonce for subsequent comparison.  And it then encrypts it and sends it to the client.  But if the client never sends the 2710 command in the first place, the nonce's value stored in RAM remains set to all zeroes.  I just love this.



So all any attacker needs to do to gain full access to any original or only just patched just, what, earlier last month, or any unpatched cam, is to connect and skip issuing the first 2710 command which asks the camera to begin the authentication handshake.  Instead, an attacker simply first sends the second 2712 command with an all-zeroes authentication.  Since that will always match the camera's default null nonce, anyone can log into anyone's Wyze cam.  You can see why Bitdefender said "Holy crap" three years ago.



LEO:  Anyone would.  But you do have to physical access.  You have to be on the WiFi; right?  You can't do this from the Internet.  Or can you?



STEVE:  No.  No no no no.  It is a network attack.



LEO:  Okay.



STEVE:  So the camera needs to be exposed.  You have to be able to connect to the camera.



LEO:  Right.



STEVE:  So if it's behind a NAT router, you wouldn't be able to.  But if there were some reason that somebody had put their Wyze cam on the Internet, then anybody can access it.



LEO:  Wow.  Okay.



STEVE:  Bitdefender wrote:  "After authentication, we can fully control the device, including motion control, pan and tilt; disabling recording to SD; turning the camera on or off, among other things.  We cannot view the live audio and video feed, though" - get this - "because it is encrypted under that same still unknown to a remote attacker shared private key.  However," they wrote, "we can bypass this restriction by daisy-chaining a stack buffer overflow which leads to remote code execution as detailed in Part 2."



They said:  "For the stack buffer overflow, when processing IOCTL with ID 2776, the device does not check whether the" - you're not going to believe this, Leo - "whether the destination buffer is long enough before copying the contents onto the stack."



LEO:  Well, there you go.



STEVE:  Uh-huh.  "Exploiting this vulnerability is straight-forward.  Through the IOCTL with ID 2776, we can set which servers to use to connect to the cloud.  This seems to be a debugging function that allows the selection of production, beta, or internal API servers.  When sending a request, we specify the length of the buffer in the first byte, then the buffer itself."  They said:  "This content is then copied onto the stack into a fixed 40 hex, which is 64 bytes, length buffer.  Even though the specified size in the first byte is taken as a signed INT" - okay, now a signed INT that is a byte will have a maximum size of 7F because the signed bit that we were talking about a few weeks ago, that's got to be off for the signed INT to have a positive value.  Still, 7F is 127, so that's enough to overwrite a 64-byte buffer and allow the stack to be overwritten and run the attacker-provided code.



The third and final flaw they found is unauthenticated access to the contents of the SD card:  "When inserting an SD card into the camera," they said, "the contents of the SD card, including the recordings, can be accessed via the web server listening on port 80 without authentication.  This is enabled by the fact that, after an SD card is inserted, a symlink to the card mount directory is automatically created in the www directory, which is served by the web server.  The card contents can be viewed through the hello.cgi functionality located at /cgi-bin/hello.cgi; then the files can be downloaded through the /SDPath/path.



The SD card also holds the camera's log files.  Before writing them to the card, the device XORs the content with a hex 90" - like why? - "not very strong protection.  These log files can contain sensitive info such as the unique ID and the shared private key, which can then be used to connect remotely," and view the stream in real-time because now we're able to decrypt the stream, having obtained the shared private key.



The good news here, such as it is, and it's not much, is that the second- and third-generation Wyze cams can be updated to cure these various problems.  The bad news is that the first-generation cameras have been abandoned by Wyze, and Wyze has said that they do not plan to support or update them in the future.  The only thing we can hope for, for anyone who has early first-gen Wyze cameras, is that maybe the press that this is now, finally, after three years, generating with the negative publicity of having critically broken and trivial-to-hack first-generation webcams might cause Wyze to change their minds.  I mean, unless it's burned into actual ROM, there is no reason, or maybe they have no - they didn't provide a means for updating the firmware.  I can't explain.  They're just...



LEO:  They say there's not enough RAM to update the firmware, not enough memory.  That seems hard to believe.



STEVE:  Okay.  It does.  It seems more likely that they're just like, well, those are too old.



LEO:  Yeah.  They were only 20 bucks; right?  I mean, you know...



STEVE:  It was an amazing little camera for the price.



LEO:  Right.



STEVE:  But Wyze, you know, and this is what we see, right, with bottom of the barrel IoT vendors that just want to sell their stuff and not be bothered with security.  Unfortunately, they tried to have it secure, but nobody audited their stuff; right?  It's all proprietary.  It's just trust us, you know, we got this.  It's, oh, 128-bit encryption.  Military grade.  Can't get in there.  Yes.  Just drop the first half of the handshake and do the latter half, and handshake with all zeroes, and you're in.



LEO:  Wow.



STEVE:  Yeah.  Two tweets from listeners, from someone whose name is The Nargles.  His Twitter handle is @WithTheNargles.  He said:  "Thanks for your recommendation.  I just finished the first Bobiverse book.  All I can say is, thanks for the recommendation.  It was a lot of fun."  He said:  "And Ray Porter's narration," he said, "particularly of GUPPI, is spot-on."



LEO:  Yeah, you're missing out on that.  It's pretty good.  He does GUPPI as Admiral Akbar.  So he's always talking like this.  It's great.  It's really funny.



STEVE:  That would be great.



LEO:  Yeah, Ray Porter's really a talent, and he kind of brings a Martian, Andy Weir's "The Martian" style to it, which is great.



STEVE:  Nice.  Scott Cleveland tweeted:  "@SGgrc A few weeks ago you and Leo were talking about the Bobiverse 'We Are Legion (We Are Bob)' books."  And he said:  "Thank you!"  He said:  "It's so hard to narrow down when scrolling through Audible what I will like.  Your suggestions are pretty much a spot-on automatic win for me."  So thank you, Scott.  And then it occurred to me, if the quality of my recommendations is to hold, I should probably note that Book No. 4 is noticeably dragging.



LEO:  Yeah.



STEVE:  It's still okay, but Dennis appears to be running out of new ideas for his Bobs.  He's reusing the ideas he already has, and I have to say he's built an extremely interesting and clever Bobiverse using subspace com links and virtual reality in clever and, given a suspension of disbelief, feasible ways within this universe.  But perhaps it should have been left at a trilogy.



LEO:  Yeah.  I'm halfway through Book 2.  Maybe I'll just stop at 3.



STEVE:  Yeah, I think you should.  But, you know, still there is more fun stuff happening.  Oh, have you run across the Others yet?



LEO:  Yes, yes.  The Others exist.  In fact he's just now encountering them for real.  So I'm excited.  It's getting exciting.



STEVE:  Yes.  They are really, really, really, I mean, they're, I mean, bad.



LEO:  Yeah.  Yeah.



STEVE:  And seeing how they get taken care of is worth finishing the trilogy.



LEO:  Peter F. Hamilton's aliens from, is it "Fallen Dragon," I can't remember which one.



STEVE:  "Pandora's Star."



LEO:  Maybe it's "Pandora's Star," the ones that just kind of, yeah, they're great.  They look like little...



STEVE:  The motiles.



LEO:  The motiles, yeah, yeah.  That's my favorite evil alien.



STEVE:  Oh, and they were non-biological.  And they were quadrilaterally symmetrical; right?  So they had like four feet, and they were sort of - kind of like Daleks except bad.



LEO:  Quick follow-up from the chat room.  Would it be safe to use a Wyze cam v1 behind a firewall?



STEVE:  I think so, yeah.



LEO:  Yeah, because it needs access from the outside world. 



STEVE:  Yes.  The threat model is that you might have mapped a port through it so that you had access to the camera directly, remotely.  And unfortunately that means other people could, too.



LEO:  Thanks.



STEVE:  And of course that's another reason why our topic for the day, port knocking, will be of I think great interest to some of our listeners.



LEO:  Yes, indeed.



STEVE:  David Lemire, he said:  "Hi, Steve.  I recently bumped into the author of NoScript on Twitter," he said, "when I mentioned I'd abandoned it long ago.  He encouraged me to take a fresh look.  So I found the website."  He said:  "Looking at the usage page, it does appear that the program has been updated/adjusted to the realities of the modern web."  And he said:  "I know you also gave up on it long ago, but this was interesting enough I thought maybe it was time for a revisit of NoScript."  He said:  "And no, I haven't actually tried playing with it myself yet, or I'd include my experience here."



Okay.  So David, thank you.  And I wanted to share that little tidbit with our listeners.  I appreciate knowing that NoScript hasn't thrown in the towel yet, despite its name.  And it occurred to me that the author is probably suffering the same dilemma I am with SpinRite.  Renaming his program "SomeScript" really doesn't pack the same punch.



LEO:  No.  Well, it's some scripts.



STEVE:  It's SomeScript because you need some script.  But you can't do NoScript.  Our development group's discoveries with SpinRite strongly indicate that SpinRite's future with solid-state mass storage is guaranteed, perhaps maybe even more so than it ever was with spinning media.  But I can't change the name, even though someday nothing will be spinning anymore.  It's still going to be SpinRite.



LEO:  On we go with a little knocking, with Steve Gibson.



STEVE:  Don't knock it.  Don't knock it.  Okay.  So our listeners have heard me over and over and over lament the dangers of having exposed ports on the public Internet.  There's the problem of a recognizable server or service that is in some way protected, but with a password which can be brute-forced in the background over time.  Or, I mean, equally problematic is a service that has strong authentication, but a bug in the service itself.  A perfect example is this OpenSSL bug.  There's nothing that you need to authenticate about establishing a TLS connection.  The problem there is a bug in the underlying service itself.  And if the access to the port is completely unrestricted, then that means an incoming packet from any of 4.3 billion IPs is treated just like any other.



So the solution to this is firewall rules.  And I have three locations, and I've got static links running in a triangle configuration between all three using strong firewall rules.  I have the advantage that GRC is a set of fixed IPs.  They will never change.  But a cable modem rarely changes.  I mean, you have to be offline or unplugged for day.  And then when you reconnect you may get a different IP.  You probably will.  But, I mean, I'll go years often with no IP change.  And so the key is that every endpoint knows its IP and knows the IPs of the other endpoints it trusts and selectively allows packets on specific ports only from those IPs.  And when they're TCP connections, because of the need for a three-way handshake, IPs cannot be spoofed, as we know.  UDP spoofable; TCP not.



And of course we've talked about being stealth, you know, GRC's ShieldsUP! service likes the idea that your firewall, your router is not even saying to a requested connection no, thank you, there's no port here, go away.  Instead, it just drops the packet.  In this day and age, technically by the original formal protocol rules of the Internet, you should respond by saying hi, I got your packet, but you should know there's no service running here.  Well, unfortunately that provides information out of a sea of IPs that there may not be a service there, that service.  But there's something there, maybe at a different port.  So better just to let the packet die in a modern Internet.



So imagine that you want to make a service available or services available to an IP which is not static and not previously knowable, but could be anything, yet you simultaneously want all other services or that service at all other IPs not to be available.  Well, since the way you enforce allowing a specific IP to connect to a specific port is with a firewall rule that permits packets in identifying themselves with a source IP and a destination port which is specified, what you want is essentially a means of on-the-fly changing a firewall rule to permit a specific client anywhere on the public Internet to get in.



And there are several ways to do this.  Generically those are known as port knocking.  And the original old-school port knocking was very clever.  The idea is that in the machine with the firewall which is publicly exposed to the Internet, you run a service.  And in Linux it's known as "knockd," K-N-O-C-K-D.  And it's available.  Linuxes have it.  It's not that widely used, which is one of the reasons I wanted to talk about it today.  It's there, and it is cool.  It runs monitoring the interface itself below the level of the TCP/IP stack.  And you have to have libpcap installed in order to allow it to open a connection to the raw interface.  This service is script driven.  It takes a config file that tells it what to do when it sees different things.



So the idea is that when packets come down the wire to your IP and hit the machine, if they're unsolicited from some random IP on some port that you don't have open, they just die.  They hit the machine, they die.  But the point is they cross the NIC to the guts of the computer where libpcap and this knockd daemon are able to see them.  So imagine if this firewall, this machine, had a secret knock sequence, which is to say send a packet to port 10192.  Then send a packet to 10234.  Then send a packet to 32769.  Then send a packet to 50743.



The point is you can create an arbitrary long sequence which has to be specified in the proper sequence to create an unlocking sequence which the knockd daemon will recognize because it's watching all the incoming traffic to your IP.  And if it sees a sequence of the proper packets all coming from one specific public IP, it then, using its config file, emits a command to your firewall, IP tables or whatever, it supports all of the different firewalls, to selectively open a port to the IP that generated this correct knocking sequence.



And what you now have is a means of having services which are publicly available, but absolutely non-existent.  It's also the case that your use of port knocking is invisible, unseen, unknowable.  Unless you tell people, there's no way for anyone to know that you're running a port knocker on your side, and that by sending a specific sequence of packets would have any effect.  And the good news is it's not very common.  So people aren't expecting you to do it.



The other piece of good news is this is actually pretty strong security.  There are some problems.  We'll talk about that.  But on the pro side of this, we know that port numbers are 16 bits.  So that means that a randomly chosen port carries 16 bits of entropy, essentially.  Think of it as 16 bits of password.  That means that four randomly selected ports, each carrying 16 bits, will give you 64.  Or eight randomly selected ports gives you 128 bits.  Okay.  Eight, and 128 bits.  There are people on the Internet who say, well, port knocking is security through obscurity.  I would disagree.  So is a password.  Nobody knows what the password is.  It's obscure.  Right, it's a secret.  Well, so is the proper port knocking sequence a secret.  The biggest problem with it has been solved in its evolution.



But I like this just for its clarity and its simplicity.  The biggest problem is, if somebody were able to somehow arrange to sniff the traffic between the client sending the packets and the server receiving them, so at either end or somewhere in between, there is no prevention for a replay attack.  So standard old-school port knocking is not safe against replay attacks.  On the other hand, I'm not suggesting that this be the only security that your system would have.  For example, I'm not suggesting that after providing the knocking sequence, the server you connect to doesn't have still its own security.  Again, multi-layers of security are good.



This is another really intriguing and useful layer because it is able to hide the fact that there is a server accepting TCP connections.  Without port knocking, that server will accept a TCP connection from anyone because it might have to accept a connection from anyone.  That tells the bad guys there's a server there listening on that port, and they can go to town.  If you put up another layer around your system, a port knocker, you look like every single port on your machine is stealth.  You know different.



The other kind of cool thing about port knocking is that it doesn't take a sophisticated client to be able to generate these.  I've seen an example, for example, where telnet, trying to initiate a connection, will send three SYN packets to an IP that doesn't respond.  So you could set up the knocker daemon to look for three SYN packets on the first port, then three SYN packets on a second port, and so on.  That would mean that you could just use a brain-dead telnet which you ask to connect to eight successive ports which it will ultimately fail.  You know that each of those generated three connection attempts.  So you've ended up sending a total of 24 packets.



And now the knocker daemon is satisfied.  It sends a command to IP tables, opens up a rule, adds a rule to only accept incoming connections to the destination that you specified for that knocking sequence from whatever IP you're at.  So even with the knock completed, bad guys still can't see that you have anything open because it is only open for the IP which was the source of the knock packets.  Anyway, my point is it is such a clever and cool idea that I wanted to share the concept with everyone.  And there are knock generators, knocking clients which will do a much more clean job of establishing connections, depending upon what kind of client you have.  Okay.  But I said that the problem was replay attacks.



LEO:  Right.



STEVE:  The evolution of this which has occurred is known as single-packet authorization.  And what we're missing from port knocking is that all we're taking advantage of is the fact of a packet hitting the firewall, not its contents.  Which means we're missing a huge opportunity.  The cleverness of it is that it uses just the fact of the packet's arrival.  But if we want to step up our game, we do it with what's known as single-packet authorization.  There is a tool, FWKNOP.  And that stands for Fire Wall Knock Operator.  The guy behind it took this to the next level.  And again, it's on GitHub.  All of this is free.  All of it's open source.  And it has had, over time, it's been scrutinized to death.



Single packet authorization takes the IP of the source, that is, the IP of the client.  It encrypts it with a public key which the user knows, or private key, or both, or symmetric key, and uses an HMAC in order to authenticate the result.  And it sends one packet to a pre-determined closed port on the destination.  The agent which is listening there gets the packet, uses its matching secret or its private key.  If you want to use asymmetric encryption, public key encryption is also supported.  It authenticates the packet.  It decrypts the packet.  It verifies that the IP that was contained in that envelope is the source IP from which the packet came.  And only if all of that works, it then does whatever its been configured to do, which could be anything.  So now we have fully stealth cryptographically secure single-packet authentication which can be used to do lots of things.



Oh, I forgot to mention that some of the cool things that the behind-the-scenes scripts can do is, for example, it could open a port, and also send a Wake-on-LAN packet to a server on the LAN, causing that machine to power up like by command in order to then provide services for whatever the port was that was opened.  So it's all configurable.



This FWKNOP, I've got links in the show notes.  Its founder is at Cipherdyne, C-I-P-H-E-R-D-Y-N-E, dot org, Cipherdyne.org.  And to go directly to the page is /fwknop.  There are clients for Fedora, Red Hat Linux, CentOS, Debian, Ubuntu, OpenWRT, FreeBSD, macOS, OpenBSD, iPhone, Android, Cygwin, and Windows; servers for all of the OS platforms and the desktop platforms except for the mobile clients.  Doesn't really make sense for obviously a mobile client to have a server.  GnuPG support, HMAC support, client NAT penetration support, server-side NAT support.



Anyway, it is a beautiful complete win for anyone having a need that this particular approach solves.  And again, in this day and age, where we've got people brute-forcing servers that are sitting exposed to all IPs when they don't need to be, I kind of wanted to remind everybody, this thing's been around since the early 2000s.  I mean, the concept has.  It's just it's very clever in terms of allowing authenticated otherwise stealth access to servers operating at arbitrary ports.  And that's port knocking.



LEO:  Neat.  Very neat.  You can have a rolling port knock, I guess, like a TOTP, sort of.



STEVE:  I don't think you really need one.



LEO:  You don't need it.  I'm just thinking of the replay attack issue.



STEVE:  Yes, very good point.  And the way this guy solves the problem, he has solved it, is it records a log of all the previous packets that have authenticated, and it will never allow the same one to be used a second time.



LEO:  Perfect, perfect, yeah.



STEVE:  And it would have to be the same IP, after all.



LEO:  Right.



STEVE:  So but he actually did think about the replay problem, and he does it just by logging successes.  He logs a small hash of a success, so the log isn't big.  And if the packet, it first has to match all the other proper criteria, which means it could only be a replay.  And if it is, if it matches all the criteria, then it checks to make sure it's never seen that before, so you don't end up with a big log in any event.



LEO:  Steve, you've done it again.  Another great, thrilling, gripping edition of Security Now! for all of our listeners.  Steve lives at GRC.com, the Gibson Research Corporation.  That's where you'll find SpinRite, the world's finest mass storage maintenance and recovery utility.  Version 6 is current; 6.1 is imminent.  If you buy 6 now, you'll get 6.1 automatically for free.  You can also participate in the development, if you want.  GRC.com.



While you're there, check out all the freebies, the forums, ShieldsUP!, all the utilities Steve writes, like InControl.  It's all there, GRC.com, along with this show.  He hosts a couple of unique versions of the show on his website, a 16Kb audio version for the bandwidth-impaired, and he also has a transcript carefully crafted by humans, well, a human named Elaine.



STEVE:  A loving human.



LEO:  A lovely human.  And you can read along as you listen or search to find parts of the show and so forth.  That's all at  GRC.com.  He also has a 64Kb audio version, as we do.  We have video as well at TWiT.tv/sn.  There's a YouTube channel dedicated to Security Now!.  You can watch every show there, all 865 of them.  And, well, they're not all video.  So maybe all 810 of them or whatever it is.  I don't remember when we started video.  And then of course you can subscribe in your favorite podcast client.  You'll get it automatically that way.  And if your client allows reviews, please leave us a five-star review.  Share the good news about Security Now!.



If you want to watch us do the show live, we do it Tuesdays right after MacBreak Weekly.  That's usually sometime between 1:30 and 2:00 p.m. Pacific, 4:30 and 5:00 p.m. Eastern, 20:30 UTC at live.twit.tv.  You can chat with us live.  We still use IRC, yes, we do, at irc.twit.tv.  But if you're a more modern type, and you kind of prefer to do the Discord thing, Club TWiT members get access to a wonderful Discord, which is not just about the show, but about every other aspect of geek life, including coding and beer and wine and cocktails and ham radio.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#866

DATE:		April 12, 2022

TITLE:		Spring4Shell

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-866.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  We'll wrap up this week's podcast by revisiting Spring4Shell.  Last week, when we first mentioned it, it was just a questionable itch.  Now, a week later, it's a full-blown outbreak deserving of today's podcast title.  But before we roll up our sleeves for that, we're going to examine credible reports of a zero-day in the Internet's most popular web server platform.  We're going to take a look at Microsoft's newly announced "Autopatch" system, and the rapidly approaching end-of-security life of some Windows 10 editions.  We have another instance of an NPM protest-ware modification of a highly used library, and I want to share a bit of miscellany and listener feedback.  Then we'll finish by looking at what one week has done to Spring4Shell.



SHOW TEASE:  It's time for Security Now! Patch Tuesday edition.  Steve Gibson is here.  We're going to talk about Spring4Shell.  We talked about it as a concept last week.  Well, the concept one week later is now a reality.  We'll also take a look at a problem, a zero-day perhaps, in Nginx, the most popular web server on the web, on the Internet; and Microsoft's newly announced Autopatch system.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 866, recorded Tuesday, April 12th, 2022:  Spring4Shell.



It's time for Security Now!.  I know you've been waiting all week long.  He's finally here, Steve Gibson of GRC.com.  He actually is in this TV all the time.



STEVE GIBSON:  It's lonely in here.



LEO:  All by himself.



STEVE:  I've got some blinky lights to keep me kind of entertained.  I wait for something to happen.  Every week it turns on, and I go, oh, hello.



LEO:  Hey, I'm on.  No, not true.  Steve is a very busy guy.  In fact, we're very grateful that he takes the time to do this show every week.  I hope I say that enough.  Thank you.



STEVE:  It's been a very good thing for my life, Leo.  I do not regret it for a moment.



LEO:  Me, too.  Me, too.  I feel the same.



STEVE:  So we're at Episode 866.  I think I've got the number correct on the show notes this week.  I didn't advance it last week, and then I always am reediting the same doc file.  So after I stripped everything out of this single doc file last night, I thought, ooh, I could have changed the number and made another PDF, and the show notes would be correct forever.  Now they will be wrong forever.  But I guess I could get some Scotch tape or something.



Anyway, this is April 12th.  It is Patch Tuesday, which is apropos of the Picture of the Week, which we will show in a minute.  We're actually going to wrap up this week's podcast by revisiting the first topic of last week's podcast.  That was when we mentioned what was at the time a somewhat questionable itch.  Now, a week later, it's a full-blown outbreak, deserving of the podcast's title.  That is to say, Spring4Shell is no longer just theoretical.  



LEO:  Ugh.



STEVE:  But before - I know.  It's just - it's amazing.  Before we roll up our sleeves for that, we're going to examine credible reports of a new zero-day in the Internet's most popular web server platform.  And that's not where you want to have a zero-day.  We're going to take a look at Microsoft's newly announced Autopatch system and, like, what is that all about?  And the rapidly approaching end of security life for some Windows 10 editions.  We have another instance of an NPM protest-ware modification in a highly used library.  And I'm going to share a bit of miscellany and listener feedback before we plow into taking a look at what one week has wrought in this next Spring4Shell vulnerability.  So a lot of interesting stuff to share with our listeners.



LEO:  It's going to be another thrilling, gripping edition, as the announcer says, of Security Now!.  Very excited, as always.  Thank you, Steve.  On we go.  Your turn, Steve.



STEVE:  So our Picture of the Week, it looks like somebody actually made this shirt.  It looks like a photograph of an actual T-shirt.  It's a black T-shirt.  It's got the well-recognizable Windows logo, you know, the four different-colored squares.  And all it says on it, it's got the Windows logo, and it says "Exploit Wednesday."



LEO:  It's the day after Patch Tuesday.



STEVE:  It's the day that follows Patch Tuesday.  So it's the answer to the question, what follows Patch Tuesday?  Well, Exploit Wednesday.



LEO:  Nice.  Nice.



STEVE:  And, boy, you know, we just cover story after story, concrete example after example of how that's exactly true.  And we're going to see a couple of those today.



LEO:  This first one scares me because this is what I use.



STEVE:  It's what I use, yes.



LEO:  Yeah, everybody, practically.



STEVE:  So for those who don't know, Nginx is spelled N-G-I-N-X.  It's a web server and more that's been steadily growing in popularity.  When I installed GRC's GitLab instance on a FreeBSD Unix box, Nginx was, and is, providing that platform's web services.  Apache, which like forever, for like decades, was the leader, is no longer so.  Nginx is now the de facto web platform for new installations, and it's now the most commonly used web server on the Internet.  It's got a 33.2% share overall.  And of the top 1,000 sites, it's just shy of half.  It's 45.2% share of the top 1,000 sites.  And it can serve as a reverse proxy, a load balancer, a mail proxy, and an HTTP cache.  So lots of different things it can do.



And over time, large projects tend to get pushed to do things they were not originally designed to do.  New chunks get added onto them here and there.  And what might have once started out as an elegant and straightforward architecture ends up becoming awkward, riddled with special-case exceptions, and it becomes increasingly difficult to maintain.  And of course, as we know where security is important, simplicity is really key.  So in other words, these big projects almost inevitably begin to show their age.



And one such example is OpenSSL.  As we've covered here, it's become so old, cumbersome, and creaky that various lean and streamlined alternatives have been built.  It remains an amazing toolkit.  I use it at the command line from time to time to do stuff with certificates that you just - there's like no other way to do them conveniently.  But if all one wanted now was a fast, clean, and simple way of getting secure TLS connections, OpenSSL may no longer be the best choice.



So what of web servers?  I thought it was interesting that the official Apache.org site claims, they said:  "Co-founder Brian Behlendorf first came up with the name 'Apache' for the server.  The name Apache was chosen out of reverence and appreciation for the people and tribes who refer to themselves as Apache."  Okay, well, I think that indigenous Native Americans are a great and noble people.  But those of us who have been around for a while will recall that that's not the case.  The Apache web server got its name because, literally, it was "a patchy web server."



And the Internet archive doesn't lie.  I've tracked it down.  Question #4 on the Apache's FAQ from July 9th of 1997 asks:  "Why the name 'Apache'?"  And it provides the answer.  It said:  "A cute name which stuck.  Apache is 'A PAtCHy server.'  It was based on some existing code and a series of patch files."  Okay.  So my point is that it did get rewritten at one point when the world became aware of how important it was going to be to have a strong, robust server.  But it's getting a little bit long in the tooth, and we have a new kid in town.



F5 Networks, which focuses upon web application security, needed a platform.  So just over three years ago, in March of 2019, they purchased Nginx for $670 million.  Which, again, we've talked before about how interesting it is that you buy an open source thing for that much money.  It's like, uh, okay.  Anyway, as a result, today it is they who are investigating a credible-appearing report of a zero-day in Nginx.  And again, there are credible-appearing reports, which we'll talk about in a second, including reports of successful breaches, using a zero-day in what is now the Internet's most popular web server and in use by nearly half of the top 1,000 Internet sites.



A spokesperson asked by the press yesterday, on Monday, said:  "We are aware of reports of an issue with Nginx web server.  We have prioritized investigating the matter and will provide more information as quickly as we can."  Well, okay, that's good.



Now, the problem first surfaced on Saturday when a Twitter account connected to a U.S.-based group known as BlueHornet tweeted about an experimental exploit for Nginx v1.18, which is the current release.  The group tweeted, they said:  "As we've been testing it, a handful of companies and corporations have fallen under it."  They didn't respond to requests for further comment.  But a different researcher shared a conversation they had with the people behind BlueHornet about this issue.



The group explained that the exploit has two stages and starts with an LDAP injection.  LDAP stands for Lightweight Directory Access Protocol, and an LDAP injection is an attack used to exploit web-based applications that construct LDAP statements from whatever it is the user supplies.  And so if there's some tricky way of supplying some information that can in some way abuse what LDAP is doing with that user-supplied information, that's your way in.



BlueHornet said that they would share the issue with the Nginx security team through HackerOne, presumably for a bounty, which in this case seems fair if they've got a zero-day for Nginx, or through F5's internal platform.  And BlueHornet later created a GitHub page where they explained in detail how they discovered the issue and how it works.  For anyone who's interested I've got a...



LEO:  Sounds like it's an LDAP exploit, though; right?  I mean...



STEVE:  Yes. 



LEO:  Not necessarily a flaw in Nginx.  You have to exploit LDAP first.



STEVE:  I think that's the case.  



LEO:  Yeah.



STEVE:  So they wrote:  "We had been given this exploit by our sister group, BrazenEagle, who had been developing it..."



LEO:  Okay.



STEVE:  I know.



LEO:  That's not their real name, for sure.  Okay, go ahead.  That's as bad as Apache.  Okay, go ahead.



STEVE:  Yes.  Nor is it an Indian name.



LEO:  No.



STEVE:  Or, they said, at least since Spring4Shell came out.  Although this bears no relation to that.  Spring4Shell we'll get to later.  That's a Java problem.  They said:  "We are still in the early stages of usage and understanding it, as we are working on another vendor vulnerability."  So, you know, what are you going to do?  You've just got too many vulnerabilities to handle at once.



They said Gitworm - and that's that other entity that shared some details.  "Gitworm was allowed to share that information with permission from our DMs.  We were initially confused as LDAP doesn't [does not] interact much with Nginx.  However, there is an LDAP-auth daemon used alongside Nginx which allows for this to be used.  It primarily," they wrote, "is used to gain access to private GitHub, Bitbucket, Jenkins, and GitLab instances."  They said:  "Further testing is required in due time."  So that was how they opened their GitHub posting.



Then they posted Update #1:  "As some further analysis is ongoing, the module related to the LDAP-auth daemon with Nginx is affected greatly."  Then we have a smiley face.  They said:  "Anything that involves LDAP optional logins works, as well.  This includes Atlassian accounts.  Just working out if we can bypass some common" - and they said WAFs, so that's Web Application Firewalls.



And they said:  "Default Nginx configs seem to be the vulnerable type, or common configs.  We highly recommend disabling the ldapDaemon.enabled property.  If you plan on setting it up, be sure to change the ldapDaemon.ldapConfig properties flag with the correct information, and don't leave it on default.  This can be changed until Nginx respond to their emails and DMs."  And which is a bit of a story.  They were having a bad problem getting a hold, getting anyone to respond from F5 which, you know, I don't understand that.



"Update 2:  Been talking to some infosec people about this, some mixed responses.  Some are saying it's a problem with LDAP itself and not Nginx, while ldapDaemon isn't always used.  The exact quote is 'CI/CD pipeline hardens the instance.  One of the steps is to completely strip out the LDAP module.'"  And they said:  "This is partially correct.  In fact, it's an option when compiling Nginx.  However, it could be a problem with LDAP itself.  The issue with this is that it only works with Nginx instance using LDAP, such as any login portal that supplies that authentication method.  Further analysis and testing is required."  They said, again:  "Looks to only be affecting this version.  If it affects updated versions of the LDAP protocol, then we'll see what comes of that.



"Update 3:  I (as I'm still in ATW, but I'm just the only one online) have forwarded our own questions, community concerns, further testing questions to BrazenEagle via email.  They have yet to respond, as they are U.S.-based.  Hopefully they can provide some further answers."  They said:  "While I'm still skeptical on the workings of this issue, it would explain the companies that were breached in under an hour during testing by BrazenEagle.  They stated that they had passed this exploit to us as they were 'working on more lucrative exploits.'  What that means, I'm not sure," this person posts.  "Some few individuals were clearly told about this being in the works some months ago via Telegram chats, which maybe perhaps some Twitter infosec people were talking about it and ATW."



Update 4 briefly says:  "Regarding what people have been suggesting on Twitter and on the issues page about this only being a LDAP issue, the problem with this is, during testing phases, it's only working on Nginx, not on Apache or other web servers.  Also, Nginx have still not" - oh, yeah, also Nginx have still not replied.  They really meant F5.  Anyway:  "DMs or email.  We've emailed some companies that are affected that we've not breached, since that's," they said, "heavily against our ideals for support on the matter regarding security around this exploit."



The fifth update:  "So we've been followed by an employee of Nginx on Twitter.  Threw them a DM asking about the situation.  No response yet.  We've been working on another exploit for MongoDB and another database management framework.  Looking to have the proof of concept out in a week's time.  Video as well.  Will be working on it with [this Gitworm guy] on Twitter."



And then finally:  "Update 6:  We got a DM from Nginx on Twitter regarding the issue."  And I grabbed a screenshot of this Twitter dialog.  They originally said:  "Hello.  Does Nginx have a vulnerability disclosure program, or a bug bounty program?"  The replay back was:  "Please report any security-related issues concerning Nginx to," and then they have "mailto:security-alert@nginx.org."  And then the hackers replied to that DM:  "Already have done.  Did you get the email?  Is there a template you wish us to follow?"



So, and then lastly Nginx tweeted:  "Addressing Security Weaknesses in the Nginx LDAP Reference Implementation."  And their tweet included a bit.ly link.  And so the end of this GitHub stream is the eighth update, reading:  "As Nginx have now released a blog post about the public releases of information, we've emailed them with a description, some familiarities of the issue that they highlighted over and assets affected.  However, people are quick to jump on the 'This is fake' or 'This isn't anything' bandwagon.  As we got no answer to if there is any bounty offered by Nginx for the findings, we've not shared any deeper information about this.  If there's no bounty or even reward, we've looked at other options that would be to sell the exploit on either breached.co, exploit.in, or other sites."



LEO:  See, that does make me suspicious.  That's kind of blackmail almost; right?  Now they're saying...



STEVE:  Yeah, well, and I notice that that apparently is not against their ethics, although they're saying that using it directly is.  And then they said:  "We've been offered about 200K in XMR" - which is Monero - "for the exploit."  And then they finish:  "If you're thinking that we're only interested in money," then they said, "yes, what do you expect?  We're a threat group, LOL."  So okay.  Take that for what it's worth.



As for their part, Nginx is playing this as though they don't think that this is much of a threat.  Their posting about this is titled, as I mentioned, "Addressing Security Weaknesses in the Nginx LDAP Reference Implementation," and it starts out saying:  "On 9 April 2022, security vulnerabilities in the Nginx LDAP reference implementation were publicly shared.  We have determined that only the reference implementation is affected.  Nginx Open Source and Nginx Plus are not themselves affected" - just as you suggested, Leo - "and no corrective action is necessary if you do not use the reference implementation."



LEO:  Do they mean the config file?  What do they mean by "reference"?



STEVE:  Well, I got a kick out of this because in other words, "You weren't dumb enough to actually use the sample code of the way this should be used, were you?"



LEO:  Yeah, that would be a bit - who would do that?  I mean, c'mon.



STEVE:  And, you know, we've seen this over and over; right?  The classic example from early in this podcast was when Intel published a reference implementation for UPnP which all router vendors naturally copied and pasted into their code.  Then, when it was later found to be horribly defective, Intel said:  "Well, we didn't mean for you to actually use it.  We just offered it as a reference."  Right.  Okay.



LEO:  Okay.



STEVE:  So Nginx said:  "The Nginx LDAP reference implementation uses LDAP to authenticate users of applications being proxied by Nginx."  Okay, right.  "It is published as a Python daemon and related Nginx configuration at" - and then they have a link - "and its purpose and configuration are described in detail on our blog," and another link.  And the blog is "nginx-plus-authenticate-users."



They said:  "Deployments of the LDAP reference implementation are affected by the vulnerabilities if any of the following three conditions apply.  Below, we further discuss the conditions and how to mitigate them."  So the three conditions are command-line parameters are used to configure the Python daemon.  Which is what they do in the reference implementation.  Or there are unused, optional configuration parameters, as there are in the reference implementation.  Or LDAP authentication depends on specific group membership.



So, and they finish:  "Note:  The LDAP reference implementation is published as a reference implementation and describes the mechanics of how the integration works and all of the components required to verify the integration.  It is not a production-grade LDAP solution.  For example, there is no encryption of the username and password used for the sample login page, and security notices call this out."



So, okay.  At this point we'll have to see how this all plays out.  Nginx has a corporate interest in, to some degree, I mean, they have to take responsibility, but they would like to downplay it.  And they appear to be sliding the responsibility for this mess onto the shoulders of those who actually implemented their reference implementation.



But for all of us here, there's a larger takeaway lesson, I think, to be learned.  It's sort of a variation on the tyranny of the default; right?  And that is, never, for the sake of simplicity or clarity, offer an insecure example or reference implementation that you're not prepared to have pretty much everyone use, for real, exactly as it is offered without modification in a real production environment.  Right?  Because that's exactly what's going to happen.



Everyone's in a hurry.  No one's as much in love with your stuff as you are.  No one else knows it as well as you do.  They don't want to make a career out of setting it up.  They just want to get it going, you know, install it.  Okay, fine, it works good.  And then move on to the next thing.  So it's necessary to assume that all of the default settings are going to be left as-is, including any code or examples that are provided as samples of this is how you set it up.  Because that's what's going to happen.



LEO:  Yeah, I mean, that's, when I set it up, that's exactly what I did.  I used their conf, their example conf, I'm sure.



STEVE:  Yes.  And I followed a recipe when I set up GitLab.  And Leo, this thing, I just shudder when I think of what's going on.  It's got so many moving pieces.  I'm typing, it's like, you know, you're the Sorcerer's Apprentice, and you're typing, you're casting spells.



LEO:  No idea what you're doing.



STEVE:  Into the console.



LEO:  Yeah, yeah.



STEVE:  You know?  And you don't even dare type them yourself, so you copy it out of the recipe and paste it over here, and you hit ENTER.  And then the screen scrolls.



LEO:  I'm glad to know that you feel that way, too, because I just assume that I'm an idiot and that I just don't know what I'm doing.



STEVE:  Oh, nobody knows.  No.



LEO:  Yeah.  No.



STEVE:  And like when I compile stuff, you just see like the compiler is just gone...



LEO:  Look at the makefiles.  Who even knows what all those settings, all those parameters mean?  I have no idea.



STEVE:  No, and this is why Windows doesn't really work anymore.  They press "Build," and they just stand back.  It's like, okay, you know.  And then, like, does Notepad run?  Okay, thank god.  So on Sunday the hacking group claimed that they had tested the zero-day on the Royal Bank of Canada...



LEO:  Ooh, oopsies.



STEVE:  ...but didn't explain whether the bank had actually been breached.  It later said it did breach the systems of the Chinese branch of UBS Securities.  Neither of those institutions responded to requests for comment from the press.  But none of us should be surprised if we learn in the coming weeks, or maybe it'll be the title for next week's podcast, right, of sites breached by leveraging this newly uncovered zero-day in Nginx's reference implementation of LDAP-based authentication.  Clearly, some people have just used it the way Nginx said, "Here it is."  You know?  Don't use it.  But, well, if it's here, why should we not use it?  So that's how the world got UPnP from the beginning.  Remember back then in the beginning of the podcast, Leo, when it turned out that Intel posted something that you should not use?



LEO:  Oh, yeah.  Don't use this.



STEVE:  It was in every router.



LEO:  This is how you don't want to do it, a reference implementation.  How you don't want to do it.



STEVE:  Wow.  Let's take a break.  I'm going to wet my throat, and then we're going to talk about Microsoft's new Autopatch system and have some more fun with that.



LEO:  You know, no one can understand everything that they're doing.  And, you know, there are some risky things that everybody does.  Whenever I, you know, what you don't want to probably do is copy and paste a cURL command from a website to install their software.  But, you know, a lot of software you feel like, well, I know this site.  I'm sure it's okay.  It's executing a shell script with often admin privileges on your machine.  We do that.  I look at makefiles, they're hundreds of lines long.  You're supposed to, when you're updating in Linux, when you're updating from the user repositories, you're supposed to read the scripts ahead of time to make sure they're not doing anything malicious.  Ain't nobody got time for that.  C'mon.



STEVE:  No, it's really true.  And when you think about it, I mean, this podcast is getting to me, I have to say, because I'm, like, I'll be looking for some utility somewhere, something that does something, and I find it.  And it looks good.  But, you know... 



LEO:  But do you dare use it?



STEVE:  Exactly.  You know?  I want it.  I don't have time to write it by myself.  So, well, you know.  And so what I do now...



LEO:  That's why I like open source, because then somebody else can be looking at it.



STEVE:  I find that I am dragging and dropping more things on VirusTotal these days.



LEO:  Yeah, yeah.



STEVE:  I'll get something, I'll just go, what do you guys think?



LEO:  That's probably a good idea.  At least do that, yeah, yeah.



STEVE:  And if it lights up like a Christmas tree, oops, maybe not.



LEO:  Yeah, yeah.  We do the best we can.  But honestly, it's such a complex world.  



STEVE:  It's really escaped everyone's control.



LEO:  Yeah, yeah, yeah.



STEVE:  Okay.



LEO:  Okay.  All right.



STEVE:  A bunch of the tech press covered the news of Microsoft's new Autopatch system.  And some noted that it was going to make Patch Tuesdays much less exciting.  Because, you know, it's excitement that you're hoping for whenever Microsoft updates your system.  Like, "Where did all my desktop icons go?"  Okay.  No one ever said that using Windows was boring.  That's not something you hear often.



Lior Bela, a Senior Product Marketing Manager at Microsoft, explained.  He said:  "This service will keep Windows and Office software on enrolled endpoints up to date automatically, at no additional cost."  This is still him.  "The second Tuesday of every month will be 'just another Tuesday.'" Right, like the Pandora's Box that it's been recently.  Or other than, unlike the Pandora's Box that it's been recently.



So, okay.  What is this?  It's going to be interesting to see how this goes.  Microsoft explained - because, you know, I thought we already had automatic updates; right?  Autopatch.  Okay.  Microsoft explained that:  "Windows Autopatch manages all aspects of deployment groups for Windows 10 and 11 quality and feature updates, drivers, firmware, and Microsoft 365 Apps for enterprise updates.  It moves the update orchestration from organizations to Microsoft, with the burden of planning the Update process, including rollout and sequencing, no longer on the organization's IT teams."



Okay.  But anyone who's been, like, around in the industry knows that the whole point of giving control to IT teams was to allow them to carefully roll out Windows updates to enterprise machines only after first vetting those changes to make sure they didn't break anything mission critical.  And yes, it's a big pain in the butt.  But it's proven to be necessary over time since Windows updates have established such a track record of breaking things.  Like you roll up an update, and now nobody in the organization can print anymore.  That might be a problem.  



So, okay.  What's Autopatch?  How does it work?  What does it do?  Microsoft plans to automate the process that IT teams have been performing for themselves in-house.  The service automatically divides an organization's entire population of Windows machines into four groups known as "testing rings."  Microsoft likes their rings, so we have more rings now.  We have the test ring, the first ring, the fast ring, and the broad ring.



LEO:  No.  Really?



STEVE:  I'm not kidding you.



LEO:  Oh, man.  I want to be in the broad ring, whatever that is.



STEVE:  I want the broad - yeah.  If it's last, that's where I want to be.



LEO:  Yeah.



STEVE:  So you got test, first, fast, and broad.  The "test ring" will contain a minimum number of devices.  The "first ring" will contain about 1% of all endpoints that need to be kept up to date.  The "fast ring" will have around 9%, and the "broad ring" will have the remaining 90% of all devices.  So a few in the test ring, 1%, 9%, and 90%.



Lior Bela said:  "The population of these rings is managed automatically.  So as devices come and go, the rings maintain their representative samples."  Samples.  "Since every organization is unique, though," he said, "the ability to move specific devices from one ring to another is retained by enterprise IT admins," even though he started off by saying the population of these rings is managed automatically.  So I guess that means automatically unless someone moves something somewhere because they don't want it in that ring.



Okay.  Once these testing rings are set up, updates will be deployed progressively, beginning with the test ring and moving - presumably, if it doesn't melt down - and moving to larger sets of devices following a validation period through which device performance is monitored and compared to pre-update metrics.  Which there's some corporate speak for you.  So it's like, huh.  All I'm getting now is a blue screen.  I don't think this compares favorably to my pre-update metrics.  What do you think?



So anyway, Microsoft announced that this new Windows Autopatch service will be released this summer in July.  Either way, the good news is it won't bother non-enterprise end-users, like hopefully most of us, since it will be a new managed service offered for free to all Microsoft customers who already have a Windows 10 and 11 Enterprise E3 or above license.  Whatever that is.  If you have one, you probably know.



Okay.  Now, the good news is, Autopatch includes "Halt" and - I wanted to say "Halt and Catch Fire," but no - "Halt" and "Rollback" features that will automatically block updates from being applied to higher test rings or rolled back automatically.  Okay, that's good.  But listen to this one.  The product manager said something that I had to decode.  He said:  "Whenever issues arise with any Autopatch update, the remediation gets incorporated and applied to future deployments, affording a level of proactive service that no IT admin team could easily replicate.  As Autopatch serves more updates, it only gets better."



Okay.  What I think he said was that, when Autopatch breaks something, it learns about that breakage and doesn't do it again.  What, on that one machine?  Or on any of that enterprise's other machines?  Or on similar machines globally? This is beginning to feel like more of that "We don't know for sure where Windows 11 will run" hocus pocus.  Like where did all of the actual computer science go?



It sounds like Microsoft is using their telemetry feedback, and the fact that updating their operating system has become so problematic that they're going to turn all of the machines owned by all of their Enterprise customers into a gigantic neural network of "let's try this and see what happens."  Anyway, I've never felt so happy to be a lowly end-user.  This is going to be interesting.  And I'll be listening to Windows Weekly to see what Paul and Mary Jo think about this because, wow.



You know, I guess if you had sort of a mid-size enterprise that didn't have the excess revenue to staff this kind of IT admin team that you now need, as evidenced by this, I mean, this is responding to need; right?  So where every second Tuesday of the month this team stops their regular business in order to figure out what this month's updates will do to their enterprise, and so they've got a set of representative machines, you know, endpoints, running their corporate stuff.



And so they first install the updates there, and then see if everything works.  It's like, did this break anything?  Can we still print?  Can we log in?  Does our app go?  And if so, then I'm sure they hold their breath, and they say, okay, let's roll this out to the fourth floor and see if it survives.  And if so, then they continue.  And so it's interesting to me that Microsoft has decided, okay, yeah, we're going to do that now.  Just Autopatch.  It ought to be Autoprayer.  Anyway, we'll see.  It'll be interesting to see what happens.



As I mentioned, we have another instance of Russian protest that has appeared in JavaScript's open source repository.  On March 17th, so today's the 12th, almost a month ago, the Russia-based developer Viktor Mukhachev, who's also known as "Yaffle," and since that's much easier we'll call him Yaffle, altered his popular NPM library known as "event-source-polyfill."  This change, which was introduced into v1.0.26 of "event-source-polyfill," will cause web applications built with this now-latest version of the library, and it's still current, by the way, nearly a month later this is still in place, it will cause web applications built with this update to display antiwar messages protesting the "unreasonable invasion" of Ukraine, to Russia-based users 15 seconds after a webpage which incorporates this code is displayed.



Okay, now, "polyfill packages," we might also call them "backfill packages," but polyfill's their official name, implement sets of newer JavaScript features on web browsers that do not yet support them.  In this case, the "event-source-polyfill" package that's been deliberately polluted by its developer implements the very useful JavaScript "EventSource" API.  This API allows a webpage to open a persistent connection back to an HTTP server, which then sends events to the browser.  And it's a one-way connection which remains open until it's explicitly closed by calling EventSource.close() function.



So what's interesting is why anyone would need to backfill this particular API, since it's been present in all major browsers for quite a while.  It was first adopted by Chrome and Firefox - get this - in their respective version sixes.



LEO:  Whoa.



STEVE:  I didn't even - yeah.



LEO:  We're up to 100 now.



STEVE:  Yes.  Firefox is at 98, and Chrome is at 100.  But what's interesting, Leo, is the chart showing the API's adoption profile had a single interesting and glaring exception:  Internet Explorer.  And it is probably the case that Russia remains the surviving bastion of Internet Explorer use.



LEO:  Oh, isn't that funny.  Oh, my god.



STEVE:  And by the way, Leo, if you didn't see Colbert last night, oh, goodness.  He had something - they resubtitled Putin talking to the camera.



LEO:  Oh, that'll be fun.  I will watch that.



STEVE:  It was quite good.  He was trying to raise money for  Russia and was, for example, offering a box puzzle of, and he said, "with only four pieces missing."  And oh.  And that's just a tip of the iceberg.  It's, you know, Colbert and his writers at their finest.  So anyway, in order for Russia's inventory of IE to be able to run web applications that rely upon the EventSource API, that support needs to be "polyfilled," provided by this library.



But given how pervasive the use of Yaffle's "event-source-polyfill" package is, and given that it's only needed by IE, since all other browsers have incorporated the API natively for years, I mean, I didn't go back to figure out when version 6 of Firefox and Chrome were.  But, I mean, it was a while ago.  It must mostly be due to other developers not having yet proactively removed it from their own package dependencies because, get this.  It is currently used by more than 135,000 GitHub repositories.  135,000.  



LEO:  Wow.  Wow.



STEVE:  Individual repositories.  And it's being downloaded more than 600,000 times every week on NPM for incorporation into those other packages whenever they're rebuilt.



Now, of course, the bigger concern here is the use of what should be a rigorously politically neutral software API being repurposed to inject its author's political sentiment, whether or not we agree with it - I happen to, but still, you know - into the use of their software package.  The users - and think about this.  The users who receive this sudden antiwar protest pop-up have no idea where it's coming from.  They don't know that it was buried in some inter-package API dependency, and that it wasn't put up and reflective of the website or web app they're using.  In fact, since everything else they see is coming from the website or web app they're using, that's exactly what they're going to think.



So it really seems wrong.  It's the abuse of the implicit trust by the developers who have chosen to use and depend upon this package that's the problem.  And over time, with repeated incidents like this  this is the third one recently, you know, that we know of, deliberate alteration of the package for this purpose - the abuse of this trust is going to weaken the entire ecosystem.  And maybe that's not, in a way, such a bad outcome. Perhaps it should be weakened.  That is, perhaps we need to revisit all of this.  The very fact that a package's author and maintainer was able to cause their package to behave in a way that its dependent users may well disapprove of should serve as further demonstration of just how rickety, from a security standpoint, this entire package repository, you know, dependency tree ecosystem has become.



In the case of NPM and the browsers that run this code, they're going to start needing to not trust the code that's being sourced by the same-origin server.  Not just sequester non-same-origin code, but the stuff coming from the origin server.  And if that has to happen, that's a game changer.  And of course NPM is only one instance from the world of open source public repository supply chains.  There are many more.  Maven, Java's similar supply chain, is equally prone.  So what we've built is not robust in the face of an active adversary.  And unfortunately our adversaries are becoming more active.



I did want to quickly note, for anybody who might be hanging back, that April 2022, next month - oh, no, we're in April now.  Sorry, we're in April now.  Next month, May, will be end of service life for Windows 10 20H2, and a different form of it for 1909.  For Win10 20H2, which was also known as the October 2020 update, it reaches end of service life for Home, Pro, Pro Education, and Pro for Workstations users.  The Enterprise, Education, and IoT Enterprise editions receive one additional year of support, so they will be reaching their end of life on May 9th, 2023.



And this also means that next month the already end of service for Win10 1909, which previously ended for Home, Pro, Pro Education, and Pro for Workstations users, will also finally be ending for their Enterprise, Education, and IoT Enterprise editions next month.  So next Patch Tuesday, May 10th, will be the last round of updates for anyone still on Windows 10 20H2 who's not using Enterprise, Education, or IoT.  And so, you know, that means you get two months, basically, 60 days from now until you would have received an update, which you won't, until you update.  So just a heads-up for anybody who may have been holding back and for whatever reason deciding that you wanted to stay where you are.  You will stop being able to get security updates.



And just a random little bit of miscellany.  We have a neighbor whose son uses Coinbase to manage and retain all of his cryptocurrency, and he's been urging them to buy and hold some bitcoin.  Okay.  Independent of the value of any cryptocurrency as a buy-and-hold asset - which, Leo, I'm as dubious about as you are.



LEO:  Yes.



STEVE:  I made a comment about the general inadvisability to them of leaving any sizable investment in crypto online, noting that many exchanges had been breached, and that Coinbase had not escaped from that.  They suffered a breach back in 2019.  And exactly a year ago, between March and May of 2021, they acknowledged that more than 6,000 of their customers were hacked in a large-scale email phishing campaign which tricked their customers into giving up the email addresses, passwords, and phone numbers associated with their accounts.  I explained to my neighbors that the only safe practice was to remove any especially large amount of crypto...



LEO:  Put it on a hard drive.



STEVE:  Well...



LEO:  And then stick it in the corner of your office.



STEVE:  Well, and as we know, the only thing you really need to hold onto is your address; right?



LEO:  Yeah, yeah.  The wallet itself, if you look at the wallet.dat file in many wallets, it's just a long digit, a long number.  And that's your account number.



STEVE:  Right.



LEO:  I'll hold his password for him, if he wants.



STEVE:  So, well, and so I thought it was also interesting that the 6,000 customers a year ago were phished; right?



LEO:  Yeah, isn't that interesting, yeah.



STEVE:  Yes.  This discussion made me a bit curious.  So I went over to Coinbase.com and attempted to sign in without an account.  And I discovered in two seconds...



LEO:  Here's the problem.



STEVE:  ...that they made one of the cardinal mistakes of online security.  And, you know, if it was a site where you log in to post about recipes or something, fine.  This is Coinbase.  They show an attacker when they have guessed wrong about an account's email.  So I went to Coinbase, and I put in "bingo-zonk-dingo."



LEO:  Ooh, I have to try that in my password for my wallet.  That's good.  I like it.



STEVE:  Bingo-zonk-dingo...



LEO:  Bingo-zonk-dingo.



STEVE:  ...@gmail.com.



LEO:  And what happened, Steve?



STEVE:  And I pressed the button, and it lit up in red, and it said:  "No Coinbase account exists for this email.  Please check your spelling or create an account."



LEO:  Oh, that's the worst possible thing they could do.



STEVE:  Yes, because now it means that any group of attackers or bots can now guess email addresses.  And they will be told, you know, it's fine to take the email or account name first and separately.  But never tell the user that their account is unknown.



LEO:  Because then I could just enter in emails until I get one that says, oh, that's not your password.



STEVE:  Yes.



LEO:  And I know it's an address of an actual user.



STEVE:  Yes.  And now you begin the phishing campaign.  So always ask for their password, regardless of whether or not the account is known.  Then tell the user that there's a problem logging in.  Please check their account name and password.  Now, I understand from a customer service standpoint it's much less confusing to a user to provide them with immediate feedback when they've mis-entered their email address or their username at the first stage.  But the reason it's much less confusing is exactly why it provides an advantage to any attacker, who is now able to probe that service for its database of existing accounts.  And in the case of Coinbase, an attacker might know someone's email address and wish to know whether they have an account on Coinbase.  Coinbase lets them know immediately.  I just couldn't believe it.  So hacked recently?  Yeah.  Guess why?  Wow.



Okay.  Three bits of closing the loop, and then we will do our third sponsor and talk about what's happened with Spring4Shell.  Mementh tweeted me, @Mementh, M-E-M-E-N-T-H.  He said:  "Time-based port knocking.  You have an authenticator app, and port knocker gets that and generates the ports to knock."  And I thought, that's kind of brilliant.  I asked Mementh whether he'd come up with this on his own or may have seen it somewhere since I wanted to give him credit for a brilliant and clever solution.



It solves and resolves the static-knock replay attack and the brute force knock guessing problems in the same way that a one-time password solves the same problems for passwords.  Assuming that the client and server are able to both obtain an awareness of the time of day, so they're synchronized, the port listening server could be continuously receiving incoming port knocks into a ring buffer.  And whenever it adds a knock to the buffer, it would scan the buffer for the current knock sequence all coming from the same IP address.  And if they're present, you're in.



And something else occurred to me as I was writing this up that I hadn't seen anywhere.  One nice bit of client-controllable data that's also logged in the typical firewall log, and that's for the implementations where you simply want to be watching the firewalls log and process the log on the fly, most firewalls log not only the source IP, but also the source port.  And although ICMP-based knocking doesn't have any port, both UDP and TCP do.  So manipulating the knocking packet's source port doubles the number of entropy bits per packet from 16 to 32 without any other additional complexity.  You don't have to worry about the content of the packet.  It might just be SYN packets; right?  And so you can control the source port of the SYN when you're generating it.



So anyway, we got a lot of interesting feedback from people.  It turns out that a bunch of our listeners had never heard of port knocking before, and they were grateful for the episode, and also a bunch have implemented it in different ways.



LEO:  I like the time-based port knocks.  That's a really clever idea.



STEVE:  Isn't that cool?



LEO:  Yeah.



STEVE:  Yeah.  Basically you take the one-time password concept and employ it.  I think it's very neat.



Vitrapemli said:  "Re port knocking, Episode 865," he said, "I do something a little bit different, but I think it's just as cool.  I have iptables log all the packets just above the drop.  This is a little messy in the logs, but I have Fail2ban watching the log.  If someone is port knocking or scanning my host, three  failed attempts on any closed port, I block that IP for a week.  The idea is, the people I want to talk to my services on random ports know what port they're on.  So they have no reason to try other closed ports."  So anyway, I thought that was an interesting approach, too.  And I did agree that a mature port knocking system ought to definitely have an IP-based lockout just as an additional layer of security.



And finally, someone who's using the moniker "Lay the proud usurper low."



LEO:  Wow.  That's Shakespearian.  Wow.



STEVE:  Yeah.  That's @ehastings.  He said:  "Dear Steve.  Regarding SpinRite 6.1 and successors, is there any spot on your timeline for a version that is Apple Silicon native?"  He said:  "Before the new systems I had hoped that there would be a Mac native release.  That would seem to be far off, at best.  What can you report?  Thanks.  Gene."  And the bad news is no.  I can pretty much assert absolutely that I will not be doing an ARM-based version for native Apple Silicon.  The next SpinRite will run on Intel Macs for sure.  That's definitely on the short-term timeline.  But I just, you know, I'm still writing it largely in assembler, and life is too short.  It's not fun to hand-code ARM in assembler.



LEO:  It would also be really tricky because people who are using Macs almost always are using the Mac Apple controller, which I'm sure is very closed and hidden.  I mean, there's USB drives, and there's Thunderbolt drives.  You could diagnose those.  But they can just put those in a PC.  But the drives internal to your Mac, good luck.  Good luck.



STEVE:  Yeah.



LEO:  That's not going to be easy.  And Steve, by the way, there is no truth to the rumor that he is going to rename his Twitter account "Tyrants fall in every foe."  But I think, @ehastings, it was a nice try anyway.  On we go with the show.



STEVE:  So as I noted at the top of the podcast, Spring4Shell is no longer theoretical.  Attacks have begun.  Last week we introduced the latest Java-based flaw that has been found in VMware's Spring.io web framework, which at the time was still only theoretical.  Recall that there had been some questioning even about just how bad this potential RCE (Remote Code Execution) exploit would turn out to be.  Flashpoint had said:  "Current information suggests in order to exploit the vulnerability, attackers will have to locate and identify web app instances that actually use the DeserializationUtils, something already known by developers to be dangerous."  And I was a little skeptical of that, whether developers even know that.  I doubt it.



And Rapid7 said that despite the public availability of proof-of-concept exploits:  "It's currently unclear which real-world applications use the vulnerable functionality."  It's less unclear now.  And they also said:  "Configuration and JRE version may also be significant factors in exploitability and the likelihood of widespread adoption."  I don't know why everybody was, you know, the security firms were downplaying this.



But CERT's Will Dormann tweeted that:  "The Spring4Shell exploit in the wild appears to work against the stock 'Handling Form Submission' sample code from Spring.io."  And gee, do you think anybody would have taken that sample code and just modified it a little bit for their own purposes?  Hmm.  I don't know.  But maybe it's a reference implementation.  "If the sample code is vulnerable," he said, "then I suspect," he tweeted, "that there are indeed real-world apps out there that are vulnerable to remote code execution."  And recall that this one got a 9.8 on the CVSS scale.



Now CISA is warning of active exploitation of the critical, it's now considered obviously at 9.8 it's critical, Spring4Shell vulnerability.  So it appears that 9.8 was prescient and is being earned.  CISA has added it, the Spring4Shell vulnerability, to its Known Exploited Vulnerabilities Catalog, that's with capital K, Known Exploited Vulnerabilities Catalog, based on "evidence of active exploitation."  



Praetorian researchers Anthony Weems and Dallas Kaman noted that:  "Exploitation requires an endpoint with DataBinder enabled, in other words an HTTP POST request that decodes data from the request body automatically and depends heavily on the servlet container for the application."  Okay, now, the automatic decoding of a POST fits in with Will Dormann's observation that Spring.io's sample code for handling form submission is itself vulnerable.  Although details of in-the-wild abuse are still a bit unclear, the information security company SecurityScorecard said:  "Active scanning for this vulnerability has been observed coming from the usual suspects like Russian and Chinese IP space."  And so I guess Russia's still connected to the Internet.



Anyway, Spring4Shell vulnerability scanning activities have also been spotted by Akamai and Palo Alto Networks' Unit 42, with the attempts leading to the deployment of a web shell for backdoor access and to execute arbitrary commands on the server with a goal of delivering other malware or spreading within the target network.  So no big surprise there.  Spring4Shell has created yet another new way of jimmying the front door lock in order to install a permanent backdoor.  Check Point Research said:  "During the first four days after the vulnerability outbreak" - again, first four days.  This is why that T-shirt at the top of this show notes is so relevant, Exploit Wednesday.  "First four days after the vulnerability outbreak, 16%" - 16% - "of organizations worldwide were impacted by exploitation attempts."



And they added that they had detected 37,000 Spring4Shell-related attacks over the weekend.  I have a graph of the explosion of the scanning for this vulnerability in the show notes, showing on March 31st the little tiny bar, maybe 5,000.  Then the next day, April 1st, it jumps to 10,000 in that day.  On the 2nd looks like it's around 13,000.  And on the 3rd it's a little more than 14,000 per day.



Microsoft 365 Defender Threat Intelligence Team chimed in, stating it has been "tracking a low volume of exploit attempts across our cloud services" - that is specifically Microsoft's - "for Spring Cloud and Spring Core vulnerabilities."  There are, by the way, a pair of Spring vulnerabilities, both 9.8.  And according to statistics released by Sonatype, potentially vulnerable versions of the Spring Framework account for 81% of the total downloads from the Maven Central repository since the issue came to light on March 31st.  Let me repeat that.  Sonatype-tracked vulnerable versions of the Spring Framework accounted for 81% of the total downloads from Maven Central repository since it came to light at the end of March.  So since that time, four out of five of all downloaded were potentially vulnerable.



Cisco, which quickly jumped to investigate its own lineup to determine which of its products might be impacted, confirmed that three of its products are affected:  the Cisco Crosswork Optimization Engine, Cisco Crosswork Zero Touch Provisioning, and Cisco Edge Intelligence.  All are vulnerable.  So if you know if you or your organization is using those, make sure that they're patched because this thing is exploding in terms of bad guys looking to exploit it.



VMware, Spring.io's parent company, has said that three of its products are vulnerable:  their Tanzu Application Service for VMs, the Tanzu Operations Manager, and Tanzu Kubernetes Grid Integrated Edition.  They've made patches and workarounds available as needed.  VMware said:  "A malicious actor with network access to an impacted VMware product may exploit this issue to gain full control of the target system."  Okay.  Saying that the way they would say it if it weren't VMware, anybody on the Internet who is able to access an unpatched VMware instance can gain full control of the target system.  So that's not good.  So again, look how quickly we moved from "There may be a problem here, but we're not sure," to "Oh, crap, cancel Christmas."  That's the reality of today's world.



To flesh this out a bit further, SecurityScorecard wrote that on Thursday, March 31st, a patch for a widely used Java framework called the Spring Framework was given the designation, and then they list the CVE, it's 22965, with a CVS Score of 9.8.  That's the bad news, they said, for a lot of companies that make use of this framework for delivery of their web applications, services, and APIs.  They said this is a remote code execution vulnerability, and the ease of exploitation is partly why it has earned a 9.8 out of 10 on the CVSS Score.



And they reminded us that way back in 2010 there was a remote code execution for the Spring Framework v2.5 which fixed the vulnerability discovered then about unsafe class.classLoader.URLs.  That was where the problem was.  This new remote code execution is related to that vulnerability.  The fix 12 years ago was to forbid jumping from a class to classLoader, and the fix this time is to forbid jumping from a class to a module.  So basically one step up in the hierarchy.  That was, you know we talked last week about the fact that there had been a problem 12 years ago, and that the new exploit was a workaround of that problem.



So the point is this has been present and vulnerable for 12 years.  It's just that no one stumbled on it.  They were blocked by the change that was made 12 years ago.  So let's go up a level in the hierarchy and go in there.  So the saving grace is that this only became present in JDK9 and hence.  I actually saw some suggestions that if for some reason it was not possible for an enterprise to update their instance of Java, for some reason, they suggested if you recompiled around JDK8, and your app was compatible with JDK8, that was another way of solving the problem since it did not have the bug.



So if anyone's interested in much more detail, the deepest level of nitty-gritty about this was in the SecurityScorecard site.  I've got a link in the show notes.  But they said in their conclusion, they said:  "If this feels all too familiar and is reminding you of the Equifax hack that was due to an exploitation of the Apache Struts 2 framework, then your instinct is spot-on.  This is the same kind of vulnerability."



And on top of everything else, the Spring4Shell vulnerability is also now, since the start of April, being actively exploited by threat actors to execute the Mirai botnet malware and for some reason focusing at the moment at least within the Singapore region.  In their posting titled "Analyzing the Exploitation of Spring4Shell Vulnerability in Weaponizing and Executing the Mirai Botnet Malware," Trend Micro researchers said that:  "The exploitation allows threat actors to download the Mirai sample to the /tmp folder and execute them after making a permission change using chmod."



They wrote that they began seeing malicious activities at the start of April, and they also found the malware file server with other variants of the sample for differing CPU architectures.  And of course that makes sense since Java is a multi-architecture language that's executed by its own JVM.



Trend Micro's write-up is by far the most in-depth, even more so than SecurityScorecard, and it's the most detailed analysis that I have encountered.  So I've got a link in the show notes for anyone who's interested.  And it makes sense that botnets would be quick to jump on this because it's going to be to some degree a time-limited vulnerability.  And it's not the first time we've seen this.  In December of last year, multiple botnets including Mirai and Kinsing were found to be leveraging the Log4Shell Java vulnerability to breach susceptible servers on the Internet.  And as we know, Mirai, which means "future" in Japanese, is the name given to the Linux-hosted malware which has continued to target networked smart home devices, you know, IP cameras, routers, and then link them into botnets primarily for DDoSing.



Intel 471 researchers said last month that:  "The Mirai code is so influential that even some of the malware offshoots are starting to have their own code versions released and co-opted by other cybercriminals."  Remember that Mirai source code escaped and was then found in the wild, and some other offshoots of Mirai were created.  In January, CrowdStrike noted that compared to 2020, malware targeting Linux-based systems had increased by 35% during 2021.  Intel said that:  "The primary purpose of these malware families is to compromise vulnerable Internet-connected devices, amass them into botnets, and use them to perform distributed denial-of-service attacks."  And of course we all know all too well just how powerful and prevalent DDoS attacks have become today.



So anyway, once again we see, you know, it was the first item that we talked about last week was that someone had discovered a new way around a problem that had been patched 12 years before.  The security community was like, well, we're not sure.  Maybe it depends upon the settings.  But Will Dormann said, you know, the default sample code is vulnerable.  What do you know?  Yeah.  The reference implementation, it's vulnerable.  You think that might be a problem?  Uh-huh.



LEO:  Wow.  Wow.



STEVE:  Wow.  So, yikes, 37,000 compromised attacks at this point.



LEO:  You kind of feel like you're hearing news happen right in front of your eyes.  You know?  It's kind of amazing.



STEVE:  Yeah.  Yeah.



LEO:  Yeah.  It's why it's worth listening to Security Now! every Tuesday.  We do it around 1:30 Pacific, 4:30 Eastern, 20:30 UTC, if you want to watch or listen live, at live.twit.tv.  If you're watching live, chat live at irc.twit.tv or join our Club TWiT.  You can chat in the Discord.  Actually that's just a small fraction of the things that happen in the Discord.  It's a very active place to go to talk about all kinds of geeky subjects.  And you get ad-free versions of all the shows, and you get the TWiT+ feed, which is full of stuff that didn't get on-air, or shows that we're preparing for a future in the public, like the untitled Linux show and Stacey's Book Club, and This Week in Space recently came out of the TWiT Club, is now public.  All of that for seven bucks a month at TWiT.tv/clubtwit.



You can also get copies of the show after the fact from Steve.  He's got two unique versions of the show, a 16Kb audio version for the bandwidth-impaired.  He also has beautifully crafted, human-crafted versions of the transcript at his site:  GRC.com.  While you're there pick up a copy of SpinRite.  That's his daily  bread, the world's finest mass storage maintenance and recovery utility.  6.0 is the current version, soon to be 6.1.  You'll get 6.1 for free if you buy today.  But you'll also get to participate in the development of it.  That's at GRC.com, along with ShieldsUP! and all of this free stuff and lots of good information:  GRC.com.  You can leave Steve feedback there at GRC.com/feedback.  But it's even easier to do it on his Twitter account.  He's @SGgrc, for Steve Gibson, GRC, @SGgrc.  Steve Gibson, Gibson Research Corporation, on the Twitter.  And his DMs are open.  So slide on in.  Leave him a message.



We have copies of the show, 64Kb audio and video, at our website, TWiT.tv/sn for Security Now!  There's a dedicated YouTube channel, of course, as there is for all of our shows.  Best way to do it, though, as with any podcast, is get a podcast client, there are very many, and just subscribe to Security Now!.  That way you get it automatically.  You don't have to think about it.  You just know it's there of a Tuesday, ready for your listening.



Steve, have a great week. I'm going to go back to v2 of the Bobiverse, or Volume 2.



STEVE:  Oh, good, yeah.



LEO:  Catching up with you.



STEVE:  I'm continuing to wade through #4.



LEO:  Four, yeah.



STEVE:  I'm at like 83%, and it's like, okay, well, I have to finish this, but...



LEO:  There's no #5; right?  Four is the last one?



STEVE:  Yeah, there's not.  He's actually talking about, threatening, I should say, a fifth one.



LEO:  Okay.



STEVE:  But he's busy doing some other stuff.



LEO:  Okay.  But the first, at least I can vouch for the first two.  They're great, yeah.



STEVE:  Oh, Leo, it is definitely fun.  The trilogy is worthwhile.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#867

DATE:		April 19, 2022

TITLE:		A Critical Windows RPC RCE

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-867.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine Chrome's third zero-day of the year, followed by Microsoft's massive 128-patch fest last week, and we note that we don't even bother counting Windows zero-days, though there were another two this month amid the 47 critical vulnerabilities that were patched, one of them being so worrisome that it captured this week's podcast title, which we'll cover at length before we conclude.  We also have more WordPress add-on trouble, the return of a longstanding problem in Apache Struts, and we have some interesting commentary about the current hackability status of the United States nuclear arsenal.  I want to share a bit of closing-the-loop feedback with our listeners and give everyone a snapshot into the recent work on SpinRite.  Then we're going to take a close look at the one flaw, out of 128 that Microsoft patched last week, that truly has the entire security industry on pins and needles because it enables a zero-click Internet worm.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There is another Chrome zero-day, its third of the year.  But that's nothing compared to Microsoft's 128-patch fest last Tuesday.  Steve takes a look at some of them, including 47 critical vulnerabilities, and one he'll take a look at in greater detail, the RPC RCE.  Plus, yes, another problem with WordPress plugins.  It's all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 867, recorded Tuesday, April 19th, 2022:  A Critical Windows RPC RCE.



It's time for Security Now!, the show where we protect you, your loved ones, your privacy online with this guy right here, Steve Gibson.  It's a Tuesday.  It must be Stevie G.  Hi, Steve.



STEVE GIBSON:  Yes, indeed.  In fact, this is the third Tuesday of the month, which makes it the Patch Tuesday retrospective edition.  Oh, and, boy, we have really - in fact, there was one that was so bad that it became the title of the podcast.  We're going to examine Chrome's third zero-day of the year, followed by, as I said, as you led me into, Microsoft's massive 128-patch fest last week.  And we note that we don't even bother counting the Windows zero-days.  Yes, it's a big deal that it's Chrome's third.  Windows, eh, why bother counting?  They had two this month alone.  And that was amid the 47 critical vulnerabilities that were patched, one of them being so worrisome that it captured this week's podcast title, which we'll cover at the end before we conclude.  But we also have more WordPress add-on troubles, the return of a longstanding problem in - and everybody knows this term - Apache Struts, which of course gave Equifax some serious heartburn back in 2017.



We got some interesting commentary about the current hackability status of the United States Nuclear Arsenal, which of course is sort of an issue now that Putin's been rattling his sabers.  I want to share a bit of closing-the-loop feedback with our listeners and give everyone a bit of a snapshot into the recent work on SpinRite, which I haven't mentioned for a few weeks.  Then we're going to wrap by taking a close look at the one flaw out of the 128 that Microsoft just patched last week that truly has the entire security industry now on pins and needles because it enables a zero-click Internet worm.



LEO:  Whoa, that's not good.



STEVE:  That'll ruin your day, or some people's days.



LEO:  Something, yeah.



STEVE:  And we do have a fun Picture of the Week, as always.



LEO:  Okay.  Picture of the Week time, Steve?



STEVE:  I wonder if .cloud is a top-level domain?  That would kind of make sense.



LEO:  I feel like there is.  That's a good question.



STEVE:  Yeah, then they could be cloud.jumpcloud.cloud.



LEO:  That's a few too many clouds.  All right.  I'm ready for the Picture of the Week.



STEVE:  So I was saying to you before, looking at this Picture of the Week, that this made me ask myself if I like physical humor.  Because, you know, one of the things I got the biggest kick out of, one of our like legendary Pictures of the Week, was that ground wire stuck into a pail of dirt.



LEO:  You love this stuff.  I know.



STEVE:  You know?



LEO:  You love this stuff.



STEVE:  It's like it's grounded.  And then the other favorite one of mine was when the hardware store had bolt cutters which it was securing against theft by looping a completely bolt cutter-able security cable through the bolt cutters.  It's like, uh, you know, the bolt cutters are designed to cut the cable that you're using to hold them.  So I don't think that works.



Anyway, this one is sort of another.  We have like the third one here.  This is like, it looks like an old-school industrial control panel with big indicator lights, and it's got those labels that were engraved where the top plastic layer of the label is white, but it's on a black plastic backing so when you engrave it, the black shows through so they can't fade.  They're like made forever.



LEO:  It looks like it's Soviet era technology, is what you're saying.



STEVE:  Yeah, okay, perfect, yes.  And the controls are these big clunk switches that you would turn left or right, rotate left or right in order to engage the dump of the bad fluids or whatever out of some, you know, turn the pumps on and then the lights dim a little bit.  Everyone's like, oh, the pump's on.



LEO:  Good, satisfying chunk when you turn it, yes.



STEVE:  Anyway, somebody wanted to make absolutely sure that one of these twisty knobs would never be twisted, so they took a big, is that a crescent wrench or a plumber's wrench?  I'm not sure exactly.



LEO:  Yeah, a pipe wrench, yeah.



STEVE:  A pipe wrench.  And it looks like it's been around for  while because it's kind of rusty.



LEO:  It's pretty rusty.  I know, I love that.



STEVE:  Yeah.  And so it kind of completes the picture.  And so they first screwed this wrench down onto the handle of this twisty knob so that it was not going to move anymore.



LEO:  It cracks me up.



STEVE:  Then they took a piece of metal, kind of sheet metal strapping tape, I guess, but metal strapping tape, and bent it over the handle of this wrench and then screwed it to the front of the panel.  And, you know, as I'm looking at this, I'm thinking, could you not have opened up the panel and jumped across that switch if you wanted it to always be on, or disconnected it if you wanted it to always be off.  But apparently that was beyond these people, who decided instead we're going to screw a permanent wrench onto the outside of this and then affix it to the panel with some sheet metal strapping tape.  It's like, okay, well, it did make for a great Picture of the Week, so...



LEO:  It's really bizarre.



STEVE:  And somebody just tweeted it to me in the last couple days.



LEO:  It's very funny. 



STEVE:  So thank you very much for that.



LEO:  Love it.  Love it.



STEVE:  To the rest of our listening audience, you now know where my funny bone is.  It is in a pail of dirt that has a ground wire stuck into it, which that's classic, and similar ilk.  I also love those water flow ones where like the pipe is broken, but somehow the water is going through the air into a hole that someone cut in the pipe in order to get the mission to be accomplished.  Anyway.



Okay.  So on our zero-day watch, last Thursday Chrome received an emergency update bringing desktop Chrome to v100.0.4896.127.  And that's for Windows, Mac, and Linux desktop Chromes.  The exploit is once again a type confusion flaw which we've talked about in detail before so I won't go into it again, also found in Chrome's V8 script processing engine.  And this one was found by Google's own Threat Analysis Group.  But it's a zero-day because they found it in use.  Somebody - and I love it when their description says this could do this or could do that.  Well, you think?  Because someone was actually using it to do exactly that.  So yes, it's not that, if exploited, it could do this.  It's like, well, we watched it doing that and were embarrassed.  We're sorry that that's the case, but update Chrome.



Anyway, this is the third this year.  It's the second type confusion flaw.  And the first one was a use-after-free in Chrome's animation support.  And as usual, users of Chromium-based browsers - the other ones, right, Edge, Brave, Opera, and Vivaldi - will be getting updates, probably have hopefully already, since this thing would have been able to be leveraged against any Chromium-based browser.  And Leo, I've heard you several times lament this monoculture, the browser monoculture that we're slipping into.  And here's another like perfect example is the bad guys see that everybody now except Safari and Firefox are going to be victims to a single flaw, if they can find it.  Which of course is the liability.  The good news is, when you fix one, you fix it for all the others.  But on the other hand, they were all vulnerable in the first place.  So still, a browser monoculture is a bad place to be.



Anyway, Google never tells us more about this because why would they?  They say that they're not going to talk about it until everybody's been well patched.  And then by then no one cares.  So they don't talk about it.  So basically never.  Okay.  This is the official name of the week after Patch Tuesday has become Follow-Up Tuesday because in this case we get to talk about what happened.  And this one won't disappoint.  Users of Windows received a total of 128 - this is like, I think it's since 2020 was the previous month that had this number of problems fixed all at once, 128 fixes for known security vulnerabilities pretty much everywhere - desktop and server, Defender, Office, Exchange Server, Visual Studio, print spooler.  Windows DNS Server had a bunch we'll talk about.  And, you know, "V" everything.



So yeah, pretty much if you're going to have 128 fixes, at least they're well distributed.  Of the 128 bugs which were addressed, 10 were rated critical, with a few even earning that rare 9.8 CVSS, which frankly is fortunately rare for Windows.  We don't actually see a lot of 9.8s in Windows.  This time we got a couple.  We'll get back to those in a moment.



A whopping 115 of the others - so we had 10 that were critical.  115 others were important.  And the final three were considered to be moderate.  As I mentioned, we got a pair of zero-days, one that's known to be actively exploited in the wild, so that we kind of call a real zero-day, and then what Microsoft calls a zero-day because even though it hasn't been seen in use, somebody talked about it, and that upset them.  It's been publicly disclosed.



So the spread of these problems break down as follows.  There are a total of 47 remote code execution vulnerabilities.  Okay.  Just take that in for a second.  47 remote code execution vulnerabilities that they have fixed, and the same number of elevation of privilege vulnerabilities, also 47.  And remember, lest anyone thought that troubles with Print Spooler were all behind us, 15 of those 47 privilege escalation or elevation vulnerabilities were found in Print Spooler.  There were also 13 information disclosure problems fixed, nine denial of service, and three spoofing vulnerabilities.  And aside from all that, Edge also separately had 26 flaws fixed.  You know, and Chrome hasn't had that many.  So as we know, Microsoft is kind of trying to Edge-ify the Chromium core.  Sounds like or looks like maybe they're having some problems doing that securely.



The flaw that's being actively exploited, that is, the true zero-day, is an elevation of privilege vulnerability occurring in the Windows Common Log File System.  And that one, even though it's the zero-day, is not one of the 9.8s.  It's a 7.8.  And it was, interestingly, found and reported both by the NSA, the U.S.'s National Security Agency, and some researchers at CrowdStrike.  So several people saw that in use and said, uh, Microsoft, there's a little problem over here in the Windows Common Log File System.



Now, the other zero-day which has been publicly disclosed, but is not known to be under active exploitation, most likely because it's difficult to leverage, is also a privilege escalation.  This one occurs in the Windows User Profile Service where actually we've been seeing some problems recently.  Since its successful exploitation requires an attacker, they said, to win a race condition where timing is critical, that probably explains its somewhat lower CVSS of 7.0, and probably also why, although it's been talked about publicly, it hasn't, as far as we know, been used, probably because it's tricky to somehow win that race condition.



However, there are two biggies among the crop, both coming in with, as I said, rare for Windows CVSSes of 9.8.  The least worrisome of the two is in Windows Network File System.  And the other 9.8er that has the security industry, as I said at the top of the show, a bit on edge is a remote code execution flaw in the often exposed Windows Remote Procedure Call Runtime Library.  That's the RPC of this podcast's title, Remote Procedure Call.  Other RCE flaws, remote code execution flaws, were fixed in Windows Server, Windows SMB, and Microsoft Dynamics 365.



DNS, Microsoft's DNS server.  We touched on this in a couple podcasts ago, so that suggests that people had been looking at it more recently.  And as we have seen, it's often the case that when attention gets turned to a specific item in Microsoft's repertoire, lots of problems get found there.  Well, that's the case here again.  The well-known security researcher Yuki Chen focused on DNS.  Microsoft credited Yuki with discovering and responsibly disclosing a total of 18 previously unknown problems in their DNS server, one being an information disclosure flaw, but the other 17 being remote code execution flaws.  And that's, again, 17 remote code execution flaws in their DNS server coming to light now.  We don't know how long they've been there.  It's just astonishing to me.



Anyway, I mentioned that Yuki was well known, and apparently especially so to Microsoft.  Last summer he tweeted:  "Made #1 this year."  He said:  "Thanks to @msftsecresponse and the bounty team.  Congrats to all researchers on the list."  He said:  "I personally known some researchers who reported nice bugs but are not on the list this year.  Hat tip to their great work, too."  So that was gracious of Yuki.  And we haven't shown this for a while.  We did several times before.  This is MSRC's, the Microsoft Security Response Center, 2021, basically their MVP, their Most Valuable Security Researchers list.  In this case it's the top 58.  Yuki is number one.  And then there's just 57 others.



So anyway, I think it's cool that Microsoft is giving them recognition.  There was an interesting story that didn't have enough meat in it to make the podcast, but it was that Microsoft had increased the bounty for some of their products, some of their 365 things.  So anyway, that's Patch Tuesday.  And we will be getting to this most worrisome of all RCE in the remote procedure call runtime library by the end of the show.



Okay.  So WordPress once again is in the crosshairs.  And it's not surprising.  There are just a constant barrage of problems with their plugins, and WordPress is so highly used on the Internet that the vulnerabilities matter there.  A site and a service named Plugin Vulnerabilities - I don't think that was wisely named.  If I were to tell my trademark guy that I want to call a site Plugin Vulnerabilities, he would say, no, that's a bad idea.  You probably can't actually trademark that because it's just not unique enough.



Anyway, they explain themselves as "a service to protect your site against vulnerabilities in WordPress plugins."  And as we'll see, not only will that keep them busy, but what's happened is we've seen sort of a sub-industry get created from third-party services that are all jumping in to perform that function.  This Plugin Vulnerabilities site and service most recent posting from last week is titled "5+ Million Installed WordPress Plugin Elementor Contains Authenticated Remote Code Execution (RCE) Vulnerability."



They wrote:  "Late last week, third-party data we monitor showed what was possibly a hacker probing for usage of a WordPress plugin named Elementor, which has more than five million active installs according to WordPress."  And this hacker was probing for the file /wp-content/plugins/elementor/readme.txt.  So that makes sense.  If that directory were world-readable, and I guess it sounds like it is, they thought that they were going to get it, that would allow somebody to pull the contents of readme.txt.  If they were able to do that, that would tell them that that site had the Elementor plugin installed, and then they would do whatever it was they were going to do.



The guys at Plugin Vulnerabilities said:  "We couldn't find any recent disclosed vulnerabilities that should explain that, so we started doing our standard checks we do in a situation where a hacker may be exploiting an unfixed vulnerability in a plugin.  What we immediately found was that the plugin isn't handling basic security correctly" - shock, I know - "as we found many functionalities where capabilities checks were missing where they shouldn't be.  While some of those were not accessible to users that shouldn't have access, we found at least one that is, and the functionality accessible leads to one of the most serious types of vulnerabilities, remote code execution.  That means that malicious code provided by the attacker can be run by the website."



They wrote:  "In this instance, it is possible that the vulnerability might be exploitable by someone not logged into WordPress, but it can easily be exploited by anyone logged into WordPress who has access to the WordPress admin dashboard.  Unless another plugin restricts access to the admin dashboard, that would mean anyone logged into WordPress would have access.  The vulnerability was introduced in the plugin in version 3.6.0, which was released on March 22nd."  So that's interesting.  That's a little over a month ago, or rather just about exactly a month ago.



And they said:  "According to WordPress's latest stats, 30.3% of users of the Elementor plugin are now on version 3.6.something."  So that's an interesting stat all by itself.  So that says that a month after the 3.6.0 was published, one month later only 30.3% of users of that plugin were using 3.6.something meaning that, first of all, 70% that had not upgraded had avoided vulnerability as a consequence of that.  But unfortunately in this case, because a vulnerability was introduced with 3.6.0, and we don't know when it was removed, then - actually we're going to find out in a minute.  But 30% of users were vulnerable.  And apparently some bad guy perhaps knew that and was poking around the Internet trying to find victims.



Anyway, they conclude:  "Based on what we saw in our very limited checking, we would recommend not using this plugin until it has had a thorough security review" - meaning that overall they were not impressed with the design of the plugin that they found - "and all issues are addressed.  That it has five-plus million installs and hasn't been properly secured should be very concerning," they wrote.  "It certainly isn't for a lack of money at the developer, as they raised $15 million in 2020.  It also isn't for a lack of reason to be concerned, as two years ago it was claimed a zero-day vulnerability in the paid version of the plugin was being exploited."



So their English is a little spotty, but they appear to be a legitimate security concern.  Although, having said that, as I dug more deeply into this, I became a little bit, I guess, their background became a little more questionable.  They posted snippets of code which demonstrated and detailed the vulnerability.  And apparently this disclosure, or their disclosure, was made deliberately and irresponsibly over a dispute with the WordPress plugin forum moderators.  It seems that these Plugin Vulnerabilities guys have long been unhappy with the way WordPress manages the reporting of security vulnerabilities.  And reading between the lines, it sounds as though WordPress downplays, in their opinion, WordPress downplays vulnerability reports, which annoys these guys.  And it sounds like there's been sort of a back-and-forth clash of egos.



A different firm known as Patchstack, in the same business as the Plugin Vulnerability guys, that is, another one of these companies in the add-on WordPress "we're going to try to keep you safe" business, wrote, they said:  "The widely popular WordPress website builder plugin Elementor, which has over five million active installations, has recently released version 3.6.3, which contains an important security fix."  So that tells us that .0, .1, and .2 were not fixed; .3 was.  They said:  "This vulnerability allows an authenticated user, regardless of their authorization, to upload arbitrary files to the site.  The arbitrary file upload vulnerability could allow someone to take over the entire site or perform remote code execution.  Please update immediately!"



Then they added that:  "Patchstack Pro and Business users" - and this is them promoting their service.  "Patchstack Pro and Business users have received a virtual patch to be protected from this vulnerability."  And the previous group, the Plugin Vulnerabilities guys, who have their bruised egos, also produce what they call a WordPress Firewall to autonomously protect their subscribers from this danger when it's been configured to do so, in this case to limit the types of files which can be uploaded because it's by allowing you to upload something that you're then able to execute that, the attacker is able to cause it to get executed, thus running their code on your site.



So WordPress's undeniable vulnerability, I mean undeniable popularity, excuse me, their undeniable popularity, coupled with the constant stream of problems mostly created by insecure and poorly written WordPress plugins, has, as I said, spawned an industry of add-on WordPress protectors.  In the past we've often referred to the firm Wordfence, where a lot of these vulnerabilities were found and reported, always responsibly.  Those are good guys, and their business is protecting the sites that are their subscribers until the add-in has been patched.  So they're as proactive as they can be.



And of course we just talked about Plugin Vulnerabilities and Patchstack.  There are many others.  I saw a list of about 30 recently.  And although it was obviously self-serving, recall that it was Patchstack who, and we cited them a month ago, released a research whitepaper observing that last year saw a 150% increase in reported WordPress vulnerabilities compared to the previous year, 2020, with the alarming news that 29% of the critical flaws in WordPress never received a security update.  You know, they're plugins that their developers have wandered away from, yet later a critical flaw is found, and it never gets fixed.



They also observed comfortingly that only 0.58%, so, what, about one in 200 problems, were found in the WordPress core, with the rest being in themes and plugins written by anyone else and typically offered without review.  They also noted that almost all of the problems, 91.38% of the flaws, were found in free plugins; whereas paid/premium WordPress add-ons only accounted for 8.62% of the total.  So the takeaway, walk away advice is stick with the most barebones WordPress installation possible because the WordPress core is solid.



If you want more, take your time, look around, find the most reputable source of WordPress add-ons that you can, which probably means you have to pay for it.  But you're paying for some security that's probably worthwhile.  And in making this tradeoff accept that, as always, there will be a tradeoff between security and ease of use.  You just need to decide where you want your site to sit along that tradeoff.



When we hear the phrase "Apache Struts," certainly long-time listeners to this podcast will immediately think back to the historic Equifax breach of 2017, which was the result of Equifax not having updated their Apache Struts Java web framework for several months after a critical "must patch" update had been published.  Wired Magazine's coverage of the event at the time was titled "Equifax Officially Has No Excuse," with the subtitle "A patch that would have prevented the devastating Equifax breach had been available for months."  So as our listeners know, that infamous breach compromised the data of 143 million users as hackers exfiltrated the names, Social Security numbers, dates of birth, addresses, and in some cases driver's license numbers.  In other words, everything you need to perpetrate identity theft, which of course was the big problem there.



What wasn't mentioned at the time was that the flaw was in an historically troubled Struts component, and actually not only Struts, but an historically troubled component known as OGNL.  That's the Object-Graph Navigation Language.  And guess what.  It's a powerful interpreter and then some.  OGNL is an open-source Expression Language for Java which simplifies the range of expressions used in the Java language.  Among other things, OGNL enables coders to more easily manipulate arrays.  But as it turns out, parsing OGNL expressions based on untrusted or raw user input has long been dangerous.



I got a kick out of what Wikipedia had to say.  Listen to this, and don't try to understand all of it.  Just sort of let it wash over you.  Wikipedia said:  "OGNL began as a way to map associations between front-end components and back-end objects using property names.  As these associations gathered more features, Drew Davidson created Key-Value Coding Language (KVCL).  Luke Blanshard then reimplemented KVCL using ANTLR" - which, you know, antler - "and started using the name OGNL.  The technology was again reimplemented using the Java Compiler Compiler (JavaCC).



"OGNL uses Java reflection and introspection to address the Object Graph of the runtime application.  This allows the program to change behavior based on the state of the object graph instead of relying on compile time settings.  It also allows changes to the object graph."  And finally:  "Due to its ability to create or change executable code, OGNL is capable of introducing critical security flaws into any framework that uses it."  Yeah.  We'd like two, please.



"Multiple Apache Struts 2 versions," Wikipedia writes, "have been vulnerable to OGNL flaws.  As of October 2017, the recommended version of Struts 2 is 2.5.13.  Users are urged to upgrade to the latest version, as older revisions have documented security vulnerabilities.  For example, Struts 2 versions 2.3.5 through 2.3.31, and 2.5 through 2.5.10, allow remote attackers to execute arbitrary code.  Atlassian Confluence has been affected by an OGNL security issue that allowed arbitrary code remote execution and required all users to update."



Okay.  In other words, as we've seen before, not all ideas are good, and OGNL appears to be way too powerful and also particularly gifted at being bad.  Wherever it's used it appears to wreak havoc.  Back in 2020, a different OGNL Injection Bug, which is what all of these OGNL problems are called, was assigned CVE-2020-17530 with an attention-grabbing severity rating of 9.8.  Back then, a pair of researchers responsibly reported to the Struts team their discovery of what they called a "double evaluation" flaw in Struts 2 versions 2.0.0 all the way through 2.5.25, so sweeping, basically from the beginning of Struts 2 to then, which could occur under certain circumstances.



The advisory for that CVE states that "Some of the tag's attributes could perform a double evaluation if a developer applied forced OGNL evaluation by using the %{...} syntax."  And "Using forced OGNL evaluation on untrusted user input can lead to a remote code execution and security degradation."  I guess you'd call it a security degradation if somebody's remote code was being executed on your system.



Okay.  So finally, although Apache had resolved that known 2020 flaw back then, in the succeeding release, which was Struts 2.5.26 since everything before that had this problem, researcher Chris McCown later discovered that the applied fix was incomplete.  And, you know, that's what you get when these things are just this complicated, like this is mind-bogglingly crazy.  So he responsibly reported to Apache that the so-called "double evaluation" problem could still be reproduced in Struts version, which they thought they fixed, 2.5.26 and above, which resulted in the assignment of a recent CVE.  Actually it was last year, but it's just been patched.  It was CVE-2021-31805.



So today, users are advised to immediately upgrade to what is now current, Struts 2.5.30 or greater, and to avoid using forced OGNL evaluation in the tag's attributes based on untrusted user input because that's never going to turn out well.  It's just not safe.  And given what happened after Equifax apparently ignored similar advice back in 2017, I would be inclined to do whatever was necessary to stay current with Struts 2 if you or your organization or anybody you know or care about is using it.  Be sure that you're running the latest, which is now 2.5.30, because my sense is this is exceedingly difficult to execute.  This is not the low-hanging fruit that the script kiddies are going to use.



But if it was found that a site is using a vulnerable version of Struts 2, and based on the statistics we keep seeing of upgrade inertia it's doubtless that some are, it's then going to take somebody who really understands this stuff to engineer an exploit.  But it's clear it's possible.  And it's obviously even hard to fix this thing.  So it's unfortunate that this OGNL is anywhere near Struts.  But it sounds like it's very, very much more power than probably is necessary or that should be in there.  But it is.



Okay.  Are America's nuclear systems so old that they're unhackable?  Which is an intriguing question.  And I was put in mind of my meeting back in 1984 a dear friend of mine who was incredibly non-computer savvy.  She was a realtor.  We met when I purchased my home from her in 1984.  She needed to access the realtors' MLS, the Multiple Listing Service, and she was using something at the time which she called her "modem."  She just called it "the modem."  It was actually, I was humored to see, a Texas Instruments Silent 700 thermal printing terminal, and remember with those original rubber cups on top that you would press the telephone's handset down into after first dialing into some computer.



Well, we became lifelong friends.  And when the Internet happened, this MLS service moved online.  So I set her up with a Windows 95 machine.  Windows 95 was current at the time.  Then many years went by with everything working fine until someone she had over to her home was shocked that she was still using Windows 95.  I don't recall where the rest of the world was by then.  Probably at least on XP.  So she immediately phoned to ask why her computer had been allowed to become obsolete.  And I said, "Judy, does it work?"  Which gave her pause.  She said, "Uh, yes."  And I said, "Is there anything you need to do with it that it doesn't?"  And she said, "No."



So I explained that there was a growing problem on the Internet with security.  People were getting their computers hacked by clicking on the wrong thing.  And she said, "Oh, yeah.  That's happened to several of my friends."  She said, "They got viruses in their computers that were sending out emails to all of their friends and infecting them, too."  So I said, "Judy, did you receive those emails?"  And she said, "Yes."  And I asked, "And did you get infected, too?"  And she said, "No."  And I said, "That's right.  And that's because your computer is too old to get infected by modern viruses.  It uses an older and original sort of code which is in many ways better, especially for the few things you need your computer to do."



You know, basically she just used what she called "the Google," which she did not understand.  She did not know that the Google was not the Internet because that was what she addressed.  She confronted the Google.  So she continued to use that machine happily for many more years until someone convinced her to buy a new one, after which she pretty quickly got herself infected.  And I sort of felt a little responsible for that because I had never needed to lecture her on safe computing.



LEO:  She had no antibodies.



STEVE:  Exactly.



LEO:  She'd never been exposed.



STEVE:  I had exactly that thought, Leo.  Exactly.  Okay.  So that brings us to a really interesting piece that ran in The Record last week that I wanted to share some parts from.  It was intriguing to me, and I think it will be intriguing to our listeners, because it was interesting to see how much of the general philosophy of complexity and security, and the "if it's not broke, don't fix it" philosophy which we've developed on this podcast as a matter of self-preservation through the years, and how much of that was in quotes from this story.



So to set up their piece, The Record first establishes the context of the moment.  They wrote:  "As the Cold War drew to a close, a surprising contender emerged as the third largest nuclear power on earth:  Ukraine.  The country was home to some 5,000 nuclear weapons, placed there by Moscow when Ukraine was still part of the Soviet Union.  Kyiv sent the weapons back to Russia in exchange for security guarantees from the U.S. and Britain and a promise from Moscow that it would respect Ukraine's sovereignty.



"Then President Vladimir Putin invaded in February.  The nuclear option, which many thought had been largely removed from the table, was one of the first sabers Putin chose to rattle when he announced that Russian troops were moving into Ukraine in February.  He reminded the world that not only did Russia possess nuclear weapons, but it was prepared to use them.  Anyone who 'tries to stand in our way,' he said, will face consequences 'such as you have never seen in your entire history.'



"The threat raised an uncomfortable question.  After decades of pursuing disarmament talks and assuming nuclear confrontation was a bridge too far, was the United States ready for the ultimate confrontation with Russia?"  And of course nobody wants that.  Okay.  But so that's interesting.  Amid all the talks of cyber offense and defense, I have found myself wondering, as I imagine our listeners may have, how strong our nuclear deterrent is in the face of reportedly quite capable foreign cyber attacking adversaries?



What I'm going to share from what The Record wrote, and which I've edited for the podcast, speaks to exactly that.  They said, and again I've edited this to make it more understandable verbally:  "Right up until three years ago, U.S. nuclear systems were using eight-inch floppy disks in a IBM System 1 computer first introduced in 1976.  It was not connected to the Internet and required spare parts often sourced from eBay.  Some analysts think America's slow walk toward modernization of its nuclear systems may turn out to have been a canny strategy.  Because the systems are so old" - much like Judy's old Windows 95 machine - "they are practically unhackable."



Herb Lin, a professor at Stanford University and author of a new book titled "Cyber Threats and Nuclear Weapons," said:  "There is a truism about computers, which is that when we have a computer, we always want it to do more."  His book looks at the risks of cyberattacks across the entire nuclear enterprise.  He says the "more" problem inevitably introduces vulnerability into systems, and defense officials have to think carefully about how to modernize.  Amen.  Please.



He said:  "If you'll grant the point that the more you want a computer system to do, the more complex the system is that you have to build, then you take the second step, and you realize that complexity is the enemy of security."  Yeah.  Where have we heard that before?



LEO:  I think it was Admiral Adama in "Battlestar Galactica," actually.



STEVE:  Yeah?



LEO:  Remember, that's why Galactica survived the attack of the Cylons.  He still had phones with wires on them coming out of the wall.  They hadn't adopted the new technology, and they were the only ship in the fleet that survived.



STEVE:  Nice.



LEO:  You see?



STEVE:  Couldn't be hacked.



LEO:  Yup.



STEVE:  There it is.



LEO:  Yup.



STEVE:  So Lin says that this is where things start to go wrong, much as happened with the Galactica fleet.  He said:  "Run the probabilities, and there's a chance that one of those many complex components could be vulnerable to a hack in a way no one had considered before.  The cautionary tale is Stuxnet, the virus and worm that found its way into the Natanz uranium enrichment plant in Iran in 2009 and 2010.  Stuxnet" - which appears in this writing to have been the brainchild of U.S. and Israeli intelligence services - "was able to take control of centrifuges used to enrich uranium gas inside the giant plant and, without anyone noticing, get them to spin so fast they broke.



"For a long time, the cause of the centrifuge failures was a complete mystery.  Scientists were fired.  Officials thought they were sleeping on the job or not maintaining the systems properly.  It never occurred to anyone until much later that a cyber weapon could possibly find its way into a system that was air-gapped from the Internet and so closely watched.  Stuxnet had probably been in their systems a year before they even discovered it."  And of course this is a cautionary tale because it suggests that maybe sticking with eight-inch floppies was a good idea.



"The thinking has been," they write, "that America's geriatric nuclear weapons systems may actually provide an inoculation from this kind of attack."  Lin wrote:  "Many of the systems right now are so old that there's nobody, or few, very few people who know how to get at them.  So right now the current assessment is that the nuclear command and control system anyway is mostly robust against a cyber threat."  And of course that bar is high.  When you're going to say we're robust against the nuclear command-and-control system being compromised, good.



Hayat Alvi, a professor at the U.S. Naval War College, studies these kinds of nuclear weapons issues.  She spoke to The Record in her personal capacity. She says she has a mantra when it comes to our nuclear weapons systems, and I love it:  "If it's not broken, it doesn't need to be fixed."  Alvi says the calculus is pretty simple.  "Why try to change something that has worked for decades?  Assuming you change them to upgrade them to modern technology, you are actually inviting more risks and potential threats and sabotage into the system."



While officials have tinkered at the edges of the nuclear weapons systems, John Lauder, who used to direct the CIA's Nonproliferation Center, says:  "Most of the systems we're using are from the '70s and '80s."  And I'll just remind everybody that we've been staring at the chips on motherboards which all come from China, wondering and worrying whether that EEPROM might be more than it looks like, or whether the Ethernet connector could have a chip hidden inside it.  So it's like just, please, leave everything alone. 



He said there has been a general sense from people who worked in arms control that:  "We had put together a set of agreements that would keep peace and stability."  As a result, modernizing nuclear weapons systems seemed less important since there was a general sense that the weapons would eventually be phased out.  "But Ukraine," he said, "was a wake-up call."



In 1979, about three years after the U.S. nuclear weapons program adopted that state-of-the-art at the time IBM Series/1 computer, William Perry, who was a top Pentagon official at the time, got a phone call.  The voice on the other end identified himself as the watch officer.  Perry, who would later go on to be Defense Secretary in the Clinton administration, recounted in his podcast At the Brink, he said:  "The first thing the watch officer said to me was that his computers were showing 200 nuclear missiles on the way inbound from the Soviet Union to the United States."



LEO:  Yikes.



STEVE:  "And for one horrifying moment," he said, "I believed we were about to witness the end of civilization."  I mean, this actually happened.



As Perry weighed the possibilities, he concluded this had to be some kind of mistake.  There was nothing going on in the world at the time that would have caused the Soviet Union to suddenly strike.  Perry asked the watch commander to find out what had gone wrong with the systems so he could explain what happened to the President in the morning.  It turns out someone had accidentally put a training tape into the computer instead of an operating one.  As a result, what the computer saw was a simulation of an actual attack.  It looked real because it was designed to look real.



Perry says that night fundamentally changed the way he thought about nuclear weapons.  He came to the conclusion that simple human error could indeed lead to nuclear war.  He said:  "It has changed forever my way of thinking about nuclear weapons.  Up until this, a false alarm, an attack by mistake, starting a nuclear war by mistake was a theoretical issue."  Until it wasn't.  So there have been many questions raised and interesting movies made surrounding the question of whether a human being would be able to follow an order, would choose to follow an order to turn the keys to launch a strike.



So understandably there's a huge urge on the part of those who are given the responsibility of protecting us to remove the human factor from the loop on the basis that it introduces a wildcard, an unknowable uncertainty that cannot be relied upon.  The good news is that there are now so very many lessons which have since been learned about the true fragility of our supposedly advanced technology that it's at least reasonable to hope that ultimate control will not be centralized.  And after all, all of the strength of the Internet arises from its inherently and brilliantly decentralized design.  The good news is William Perry was in the loop, got the call, did like a reality check and said, wait a minute, it makes no sense that there's actually 200 inbound nukes from the Soviet Union.  Let's double-check our systems.



So anyway, it's going to be interesting to see what happens.  A little bit of I thought interesting insight that because our weapons just, you know, no one has believed we were going to probably ever need these things again.  I'm sure they're being dusted.  Oil is, you know, they're kept in functioning condition.  But the technology has been pretty much left as it was in the '70s and '80s.  And I say good because the only way, the only way it would be safe would be if we ourselves designed the chips and used our own foundries to make them and built systems that did it like from the sand on the beach up to working silicon in order to function.  Everything else, everything in our systems these days we get offshore.  And the world has changed too much recently.



Okay.  A couple of bits of feedback from our listeners.  Max Feinleib said:  "Re the Coinbase segment."  That's where I was talking about the unsatisfying login experience or trial login I had with Coinbase.  I mentioned that last week.  He said:  "Gmail also has the same login process where you enter your email, and then your password on another screen."  And so I thought, well, okay, they don't have to be the same screen.  It's just that entering your username, your email, has to not provide any feedback.



Okay.  Many other listeners have written since then to note that this or that online service does the same thing as Coinbase.  I distinctly recall, Leo, that once upon a time we used Gmail as an example of this being done right.



LEO:  Mm-hmm.



STEVE:  But sure enough, I tested Gmail.  I went to Gmail using an incognito mode so it wouldn't have any knowledge of who I already was.  And I entered the email "bingo-zonk-dingo-lingo@gmail.com," and I never had the chance to enter a login password.  Gmail immediately told me that they had no record of that account.  So it appears that over the years ease-of-use has prevailed, even at Google, over stricter but less user-friendly and less forgiving login policies.  And I guess, you know, it certainly is the case that not all websites should or need to set the bar at the same height.  I guess I could see, although I don't like the idea of Gmail having broken this login process into something which can be decomposed into two separate things.  What was it, it was not WEP, it was W, I want to say WPAD.  It was that WiFi protocol where you had to enter in the eight-digit code.



LEO:  WPS?



STEVE:  WPS, yes, WPS.  An eight-digit code.  Well, first of all, the final digit was a check digit for the other seven.  So that didn't count.  You always knew what that was.  And then what we learned was that you didn't - the protocol was broken such that you could separately guess the first three digits, or maybe it was the first four, separately from the final three plus the check digit.  And of course that hugely reduced the security, the fact that it was no longer all seven, because you could compute the eighth, all seven had to be right.  Instead, you could decompose it into four and three, which reduced the strength by a factor of a thousand.  Anyway, it does look, I just wanted to acknowledge, Max and everybody else who tweeted me and wrote saying Steve, you know, your favorite password manager does the same thing.  And it's like, oh.  Really?  Okay.  Well, that's too bad.



LEO:  Why do they do that?  I don't understand, besides the security issue, why ask for the email first?  I've never - everybody seems to don't do that now.  Is there some thinking?



STEVE:  Well, it is how they find your account; right?  So that's how they identify you.  But I would like them to be mum about it until they then ask you for your password.



LEO:  Right.



STEVE:  And then say, we're sorry, we had a problem.  Now, from a customer service standpoint, right, it's much easier if you tell the user, oh...



LEO:  Oh, we don't know you, yeah, yeah.



STEVE:  Yeah.  You have a typo in your email.  Then the user goes, oh.  But the problem is that's exactly that same ease of use makes it ease of hack.



LEO:  Right.  Yeah, because you can try email addresses till you find one that they do know.



STEVE:  Right.



LEO:  And now you're halfway there, yeah.



STEVE:  And if it's, I mean, it's one thing if its Gmail.  That's not good.  But if it's Coinbase, or if it's your bank, or like something where security really matters, wow.



LEO:  Now we can phish you, yeah.



STEVE:  Yes, exactly.  Dana J. Dawson, he wrote, he said:  "@SGgrc, on SN-866" - and today's 867, so that was last week - "you seemed unimpressed" - yeah - "with the new MS Windows Auto Update service."  You think?  "But in other SN episodes you've implored home router makers to include an auto update feature.  Could you elaborate on this apparent contradiction on an upcoming SN episode?"



Yes.  The issue, Dana and anyone else, the issue of auto updates is not settled because it's not black and white.  There are pros and cons and tradeoffs, and there isn't a single right answer.  The biggest difference arises due to the environment being addressed.  For example, a complex enterprise network has many moving pieces, and it presumably has some dedicated IT staff who know those pieces intimately and are competent to test any changes and validate that updates are safe before they're applied throughout the entire network.



Contrast that to a cute little router box, sitting alone and forgotten in a closet, which was installed by an elderly couple's grandson after they asked him how they could get WiFi.  If, when it was installed, that little router offered the option for automatically keeping itself up to date, we can hope that the grandson had the foresight to enable it if it wasn't the default.



My feeling is that given the "set and forget" environment which most routers find themselves in, coupled with the fact that those routers are bristling with features and run complex Linux operating systems where new problems are being found, and that there's active pressure to find and compromise these autonomous little devices for incorporation into botnets, cryptomining, and perhaps someday something more, I believe that the default setting should be to allow the router to repair itself in the event that a critical remotely exploitable flaw is discovered and reported to its manufacturer.



Yes, there should be an option to disable it.  You should be able to say, and maybe even explicitly ask the user if you're nervous about having it on by default, this router will update itself only if it really needs to.  Is that okay?  Or maybe even have it be like three settings:  off, all updates, or super ultra mission-critical updates only.  I don't know.  But the point is it feels to me like that's a case of the forgotten router in the closet, and Grandma and Grandpa don't even know what it is.  It probably has socks covering it up now so it's overheating.  



Okay.  The enterprise and the router in the closet scenarios are obviously different.  But what they have in common is their use of complex software which we still haven't figured out how to get right.  I think that the use case of Autopatch, that is, Windows Autopatch, is the very large middle ground.  I'm unconvinced that it will make sense for any significant enterprise that employs their own in-house IT staff to turn over control of updating to this level to Microsoft.  But most small businesses do not have in-house IT staff.  They've got some guy who comes in when they call to fix something that broke.  If Microsoft has found that their monthly updating is breaking more things than it's fixing, then having a way to detect when something has gone wrong and revert, in other words Autopatch, makes sense.  And "Autopatch" sounds more appealing than "Windows Un-Update."



So the problem is, to me, this whole thing, this Autopatch thing, and we're going to, you know, we've got four rings of testing.  We're going to stick our toe in the water and see if something breaks, and if so we're going to back out, and we won't go any further.  This all feels eerily like capitulation.  As if somewhere inside Microsoft someone acknowledged that they cannot actually fix Windows, so they're going to automate it not being fixed.  But, you know, I still love Windows.  I think it's an amazing piece of work.  It's in front of me right now.  I think it's incredible that it works as well as it does, and I very rarely have any trouble with it.  Although I don't know if you noticed, Leo, in the middle of this first sponsor break Zoom just shut down.



LEO:  No, I didn't.  Oh, how funny.



STEVE:  The window disappeared.  And I just, like, stood there with my mouth open, what?  That had never happened before.



LEO:  Well, it's perfect timing.



STEVE:  It was.  So I just restarted it and reentered our connection and...



LEO:  There you are.



STEVE:  It was transparent.  So okay.  Every so often my cable modem does the same thing, and I just kind of shrug and restart it.



LEO:  Computers.



STEVE:  I think it only happened once so far during - I know, Leo.



LEO:  Don't understand how we survive, really.



STEVE:  I've been silent for the last couple weeks about SpinRite.  But I thought I would share, like, what has been going on because it's kind of interesting.  And I know our listeners who are waiting for 6.1 will find it interesting.  Work has continued without interruption.  We've just emerged from an interesting sideline where we learned some surprising things about many of our systems.  One of the big changes being made in SpinRite 6.1 is the use, I've mentioned this many times, of very large 16MB transfer buffers.  The theory was that this would allow maximum performance by reducing the overhead in between many little transfers, you know, intertransfer overhead.



And for SSDs this theory has panned out big-time.  SpinRite is like two or three times faster than any BIOSes that anyone has.  And often, as I did mention months ago, we're like right up at the ceiling of the maximum theoretical transfer rate on the wire.  But this also holds for some spinning drives, which we now in the newsgroup refer to as "spinners."  But it turns out that not all spinning drives perform best when they're handed huge transfers to perform.  I don't recall why I thought to look, but we were finding that some BIOSes were outperforming SpinRite's own native drivers, which I have just written, which should not have been possible.  So I added technology to benchmark drives both ways, using SpinRite's new native hardware-level drivers and using the BIOS, with the intention to use the BIOS when it was faster than SpinRite because speed matters.



The BIOS has never been able to transfer in its history, still can't, more than 127 sectors at a time.  That's just a fundamental limitation of the BIOS's API.  But SpinRite can ask drives to transfer 32,768 sectors at once into a waiting 16MB buffer.  You know, now we've got lots of RAM, so why not?  Through experimentation, we discovered that some spinners would perform better when asked for 258 individual transfers of 127 sectors each, rather than a single transfer of 32,768 sectors.  But my code was still not keeping up with the BIOS.



When I realized that the way I was breaking up the large transfer into smaller transfers was introducing 258 times the intertransfer overhead, I fixed that.  I redesigned my drivers to be maximally efficient when large transfers needed to be divided into smaller pieces.  And then, finally, SpinRite's own drivers were always at least as fast as the BIOS and often, especially for SSDs, up to several times faster.  So SpinRite now never needs to use the BIOS when it's able to communicate with a drive's hardware directly.  And SpinRite now measures the speed both ways of each drive using smaller 127-sector transfers and these mega 32,768-sector transfers, and remembers which one is best for that drive and adapter.  So that's part of the drive enumeration process.  It basically figures out how this drive wants to be talked to, and then that's what it does.



So as for SpinRite's development, I know all of this is taking time.  The only thing I can say is that what is emerging is going to be a truly high-functioning piece of software, something that everyone will be proud to own and use.  And also it's also creating a strong new foundation for SpinRite's future in v6.1.



LEO:  Yay.



STEVE:  Yeah.



LEO:  That's exciting.  Speaking of RPCs and RCEs, Steve Gibson.



STEVE:  Yes, and you've right, Leo, about not letting things be exposed.



LEO:  No.



STEVE:  This is going to be another big lesson along those lines.  So it's CVE-2022-26809 with a CVSS, and we'll see why, of 9.8.  Last Tuesday, when along with 127 other lesser problems Microsoft patched this flaw in Windows' remote procedure call runtime library DLL (rpcrt4.dll), it was unknown to anyone other than Microsoft and its independent discoverer, "BugHunter010," who is with the Chinese Kunlun Lab.  He had privately and responsibly disclosed it to Microsoft.  What we've learned is you can look at patches and reverse engineer them.



As the implications of this freshly patched flaw began to emerge, the entire security industry started sitting up straighter in their chairs because what we have here is something rare.  In any not-yet-patched Internet-exposed Windows machines we have the basis for another zero-click Internet worm, reminiscent of 2003's MSBlast worm and the 2017 WannaCry attack.  CERT/CC's Will Dormann tweeted that right now 1,329,075 Windows machines are publicly reachable and, until patched, are vulnerable.  And all of those machines which remain unpatched, once the discovery is made of how to exploit this vulnerability, will be susceptible to full unauthenticated remote takeover.



Microsoft's own page disclosing and summarizing this vulnerability categorizes it as attack vector, network; attack complexity, low; privileges required, none; user interaction, none; confidentiality impact, high; integrity impact, high; availability, high; exploitation, more likely.  In other words, Microsoft is quite aware that it doesn't get any worse than this.  And at this very moment everyone, and I mean everyone, is working right now around the clock to figure out how to turn this into an active exploit.  Security companies are trying to do it in order to build proactive defenses for their clients.  They need to know what patterns to look for on the wire in order to block this.  And those who know say this is going to be very difficult to block.  We don't know what that means.  We also know that all of the usual suspects in North Korea, in China, and Russia are burning the midnight oil to design packets that they can send to any listening server to take it over remotely.



We all know that when we start off with more than 1.3 million vulnerable Windows machines, one week ago, all of those machines were vulnerable.  A month from now, somewhere between, what, 10% and 25% will still be vulnerable, and it will be their misfortune.  A working exploit for this would be extremely valuable, and it would likely fetch a high price even though a patch already exists since everyone knows that merely reduces the size of, but does not eliminate, the target population.  So if an exploit were developed it would be kept quiet since anyone selling it would want it to be rare, and anyone paying a high price would want to keep it for themselves or for their own selective resale.  So I actually wouldn't expect to see a worm appear initially.  Deploying this bad boy in targeted attacks makes a lot more sense.



Last Wednesday, the day after last week's patches became available, Akamai's security people, seeing how Microsoft had characterized this and seeing its 9.8 CVSS, became curious themselves about this change that Microsoft had made, and they posted a blog about the vulnerability because it's very likely going to become famous, or infamous.  I've excerpted and edited from their blog to give everyone a sense for what Akamai did.



They wrote:  "The CVE stated that the vulnerability lies within the Windows RPC runtime, which is implemented in a library named rpcrt4.dll.  This runtime library is loaded into both client and server processes utilizing the RPC protocol for communication."  They said:  "We compared versions 10.0.22000.434 (unpatched from March 2022) to 10.0.22000.613 (just patched in April 2022) to produce a list of changes in various functions.  The functions ProcessResponse and ProcessReceivedPDU caught our eye.  The two functions are similar in nature.  Both process RPC packets, but one runs on the client side and the other on the server side.  We went on to 'diff'" - as in difference - "to 'diff' ProcessReceivedPDU, and noticed two code blocks that were added to the new version.



"In the patched code we saw that a new function was added after QUEUE::PutOnQueue.  Zooming in on the new function and inspecting its code, we saw that it checks for integer overflows.  Namely, the new function was added to verify that an integer variable remained within an expected value range after having another value added to it.



"Diving deeper into the vulnerable code in 'GetCoalescedBuffer'" - and, by the way, that is where the problem is - "we noticed that the integer overflow bug could lead to a heap buffer overflow, where data is copied onto a buffer that is too small to populate it.  This in turn allows data to be written out of the buffer's bounds onto the heap.  When exploited properly, this primitive could lead to remote code execution.  A similar new call to check for integer overflow was added to three other functions, as well.  The integer overflow vulnerability and the function that prevents it exists in both client-side and server-side execution flows."  So that's what Akamai did.  Basically they did what anyone would start off doing, what everybody who is trying to figure out how to exploit that is doing right now.  It's only been a week.



Marcus Hutchins, our friend, has posted a 7.25-minute video on YouTube titled "Exploiting Windows RPC - CVE-2022-26809 Explained | Patch Analysis," where he basically walks his viewers through a 7.25-minute tutorial on exactly what I just talked about Akamai doing.  And the SANS Institute has posted an hour-long YouTube video titled "CVE-2022-26809 MS-RPC Vulnerability Analysis."  In the video's description, SANS introduced - oh, and by the way, I have links to both of those in the show notes for anyone who's interested.



In the video's description, SANS introduced the topic by writing:  "On Tuesday, April 12th, Microsoft released patches for CVE-2022-26809, reportedly a zero-click exploit targeting Microsoft RPC services.  At the time of the publication of this abstract, there is no proof of concept available in the wild.  However, based on the rating that exploitation is 'more likely,' we expect this won't last long."  And of course I expect they're correct.



I've gotten this far into the subject without having made any mention of ports.  Part of the reason is that RPC is not constrained to any single port, though it is typically carried over several, and one in particular.  Believe it or not, the number one culprit is SMB port 445 which is used for Windows' infamous file and printer sharing.  Everyone who's been listening to this podcast for any length of time will have heard me lament, many times depending upon how long you've been listening, that Microsoft's security for their Internet services has never, unfortunately, been worthy of trust.  I mean, come on, their DNS server just now had 17 remote code execution flaws repaired.  Just last week.  The troubles with their remote desktop server have been numerous, and file and printer sharing has never been trustworthy.  You cannot expose it to the Internet.  Everybody's heard me saying that.  Okay.  Despite that, obviously many people have not heard me say that.



The research company Censys said 1,304,288 hosts are running the SMB protocol as of last Wednesday.  Of those, 824,011, which is 63% of the total, were positively identified by Censys as running a Windows-based operating system, meaning that every one of those 824,011 machines was vulnerable last Tuesday.  And Censys noted that they were unable to determine the OS behind approximately 28% hosts running SMB.  So maybe some others were.  They don't know.  But they know that at least that 63%, 824,011 machines, were definitely running Windows.  The majority of the systems running SMB are in the United States, with a count of 366,000 systems.  And we've got a chart, a nice glow map of the world showing where they all are.



LEO:  China is completely dark.  But I don't think that's because they don't run SMB.



STEVE:  No.  366,000 systems in the U.S.  Russia came in second place with a count of 144,622.  And you can see it glowing there.  Hong Kong had almost 73,000.  Germany had just shy of 71,000, and France had 56,659.  So this is a widespread problem.



Last Wednesday the 13th, CERT's Will Dormann tweeted:  "CVE-2022-26809."  He said:  "Yes, blocking 445 at your network perimeter is necessary, but not sufficient to help prevent exploitation.  If by April 2022 you STILL" - and he has it in all caps - "have SMB exposed to the broader Internet, you've got some soul searching to do.  Now, about those hosts already inside your network...."  And the day after that SANS tweeted:  "Please remember port 445 is just one of the ports that may reach RPC on Windows.  MSRPC does port 135, or in some cases HTTP as well."  And actually it's a weird port on HTTP.  It's 539 or something.  I don't recall.  And another researcher, Ned Pyle, tweeted: "CVE-2022-26809 has nothing to do with SMB.  It's an RPC vuln where a variety of transport can be used, like TCP/135 and SMB/445."  



Okay.  So no matter how much you want to, no matter how much you may need to, other than HTTP and HTTPS, and I suppose Exchange Server is necessary, too, Windows services and their ports must not be exposed to the public Internet.  This is why site-to-site packet filtering by IP and port is crucial, and it's a perfect solution when IP endpoints are fixed, or even relatively fixed, and some other solutions such as a VPN, SSH, or port knocking needs to be used when a roaming IP must be able to obtain access.



So I just wanted to say that my choice at this point would be to run a WireGuard VPN from a server.  We've talked about WireGuard before.  It's a lovely, recently written, state of the art, 100% free and open source VPN which exchanges certificates for authentication, not passwords.  So it is very strong.  And unlike OpenVPN, it deliberately leaves the kitchen sink where it belongs, in the kitchen.  It does just one thing and it does it well.  And there are clients for all platforms now so connecting to it is no problem.  Although it was originally built on Linux, Windows is now officially supported, and it's a single-click install.



So you could run a WireGuard VPN server on any Linux, Mac, or Windows machine on your network to which any number of roaming users could securely connect to obtain access to everything that you want them to on the inside.  It's www.wireguard.com, and there's an install page that asks you which platform you want to download the installer for.  There are now some "How to set up WireGuard VPN on Windows" pages.  I've got them in the show notes also.  But for those 1.3-plus million systems which CERT says were vulnerable last Tuesday, it's going to be interesting to see what we're talking about in the next few weeks to come.  This thing really looks like, once somebody figures out how to take advantage of it, it's going to get loose.



Before we wrap up this topic and podcast, I need to mention what others have also mentioned, which is that the threat from this unpatched RPC RCE exploit is not only external.  It's bad, yes, because it allows bad guys to get into your network.  But there is no chance that bad guys who gain entry into an enterprise's network, once this thing has been weaponized, will not be adding a test for this patch being present to their lateral movement toolkits.



Remember that many initial network penetrations are with low and limited privilege.  And the low and limited privilege is just the operating system trying to protect itself.  And this is why privilege escalation or elevation exploits are almost as highly prized as remote code execution exploits.  This RPC RCE will doubtless be added to all post-penetration kits because it would allow any unprivileged attacker who has obtained a minimal foothold inside a network to hugely expand their reach by moving laterally to other more valuable machines while simultaneously elevating their privileges on that destination machine probably to system level, which is where RPC typically runs.  In other words, as those tweets were reminding, this is not just about preventing external public exposure to these services.  The threat from internal abuse is very real, too.



And so we have last Tuesday an extremely rare, extremely worrisome problem that we just know there's no way that even in the U.S. that 866,000 machines are all going to be patched.  And the race is on now to find a way to exploit this.  Everybody is able to look at the code, reverse engineer the code, see what's gone on, and then work on figuring out how to send a packet in from the outside that will get some bad stuff done.  In fact, given that it's been a week, unless it's impossible to leverage this, and it seems unlikely to be impossible, unless it's impossible it's probably been done, and we just don't know it publicly yet.



LEO:  Well, there you have it.  The inside story of the RPC RCE.  M-O-U-S-E.  Thank you, Steve, as always.  This is why we look forward to Tuesdays.  Tuesday is Security Now! day.  We do the show at about 1:30, 2:00 p.m. Pacific, depending on when MacBreak Weekly is over.  That would be 4:30 to 5:00 p.m. Eastern time, roughly 20:30 UTC.  You can watch us do it live at live.twit.tv.  You can of course, if you're watching live, chat live, either at irc.twit.tv or, if you're in Club TWiT if you're a member, you can do it inside the Discord.  Always a lot of fun in there.  We've been having some good conversations.  In fact, we always do.  That's where Stacey's Book Club is.  Steve did an AMA.  I think at some point we're going to do, we've got to do a Vitamin D episode in the Club.  We have the Untitled Linux Show.  We have the Giz Fiz.  That's where we launched This Week in Space.  We'll be launching, we'll announce it soon, another show in the Club.  Because the Club members support it, we don't have to worry about when a show's starting out if we can get advertisers, that kind of thing.  It really is a very important part of our operation these days, and we thank everybody who's a member.



You can get a copy of this show directly from Steve.  He has actually two unique versions, the 16Kb audio, which sounds terrible, but it is small.  He also has 64Kb audio.  Sounds a little bit better, I must say.  He also has transcripts, which is really great, so you can read along as you listen or search those transcripts for a part of this show or any of the 867 other shows.  That's all at GRC.com.



While you're there, pick up SpinRite.  Yes, I know, it's only version 6 right now, the world's best mass storage maintenance and recovery utility.  But you will get a free copy of 6.1 when it comes out if you buy it now, and you'll get to participate in the development as we get closer and closer to the release of 6.1 and all its shiny new features.



Steve has lots of other free things there like ShieldsUP! and some great reading, all sorts of stuff:  GRC.com.  If you want to leave a message for Steve you could do it there, GRC.com/feedback.  But probably the best way to reach out to Steve is through his Twitter account.  He is @SGgrc on Twitter, and his DMs are open.  So if you've got a Picture of the Week or a question or a comment or a suggestion, just DM him there:  Twitter.com/SGgrc.



We have copies of the show at our website, of course, TWiT.tv/sn.  There's a YouTube channel with all the videos.  You can also subscribe; right?  Sometimes that's the easiest thing to do.  The RSS feed is available from that web page.  Put that into your favorite podcast client, and then you'll automatically get Security Now! the minute it's available, which is a nice thing.  You can, in some of those clients, review the show.  And if you would leave us a five-star review, I'm sure Steve would just be glowing with happiness.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#868

DATE:		April 26, 2022

TITLE:		The Zero-Day Explosion

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-868.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're going to take a close look at the U.S. Cybersecurity and Infrastructure Security Agency's mandated must update list, including some recent entries.  We're going to examine the somewhat breathtaking mistake that Lenovo made across more than 100 of their laptop models, and a cryptocurrency wallet implemented in a web browser.  What could possibly go wrong?  Then we're going to look at another startling vulnerability that was recently discovered in Java versions 15, 16, 17, and 18.  We have a bunch of interesting listener feedback, a brief sci-fi interlude, and the announcement of a major milestone reached for SpinRite.  Then we're going to wrap up by taking a look across the past 10 years of zero-day vulnerabilities thanks to some recent research performed by the security firm Mandiant.  The title of this week's podcast gives away what's been happening.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll take a look at the CISA-mandated must-update list.  Why are there some very old flaws on there?  Lenovo has a problem with more than 100 laptop models.  A vulnerability in Java 15, 16, 17, and 18 that has a score of as high as 10 critical.  Critical.  And then we'll take a look across the past 10 years of zero-days.  Why are there more now than ever before?  It's all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 868, recorded Tuesday, April 26th, 2022:  The Zero-Day Explosion.



It's time for Security Now!, the show where we're going to protect you, your loved ones, your privacy online with this guy right here, Steve Gibson of GRC.com.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  How are you?



STEVE:  Great to be with you again.  We're closing out April.



LEO:  Ah, the cruelest month of all.



STEVE:  With a bang.  So this is Episode 868.  It's funny, you were talking to the MacBreak guys and wondering how long they'd been doing their show.  And they were like, what was it, 815 or something that you just did with them?



LEO:  Yeah.  You're ahead of the game.



STEVE:  We're at 868, so not a whole year, but nearly that.



LEO:  Yeah.  You were the second podcast we did on TWiT was Security Now!, yeah.



STEVE:  Yes, yes.  Well, and it was in Toronto that, in between shooting the four episodes that we were doing of Call for Help, you were leaning on the stage prop, and you said, "What would you think about doing a..."



LEO:  Hey, Steve.



STEVE:  "...podcast on security?"  And I said, "A what cast?"



LEO:  And the rest is history.



STEVE:  And that was nearly 18 years ago, and both of us had darker hair.



LEO:  And we have no lack of material, either.



STEVE:  And more of it.  Apparently not.  So, and speaking of which, that's a perfect segue because today's episode is titled "The Zero-Day Explosion."



LEO:  Ugh.



STEVE:  Uh-huh.  We're going to take a close look at the U.S. Cybersecurity and Infrastructure Security Agency's - boy, that's a mouthful - mandated must-update list, including a couple of recent entries.  We're going to examine the somewhat breathtaking mistake that Lenovo made across more than 100 of their laptop models, as well as a - and this is separately - a cryptocurrency wallet implemented in a web browser, which is where we all say in unison, "What could possibly go wrong?"  Then we're going to look at another startling vulnerability that was recently discovered in Java versions 15, 16, 17, and 18, all the last four major versions.



We've got a bunch of interesting listener feedback, and one, I only have one representative sample of something that 200 of our listeners must have sent.  We really have amazing listeners who are paying attention.  I've got a brief sci-fi interlude, the announcement of a major milestone reached for SpinRite, and then we're going to wrap up by taking a look across the past 10 years of zero-day vulnerabilities thanks to some recent research performed by the security firm Mandiant.  And of course the title of this week's podcast gives away what's been happening, "The Zero-Day Explosion."



LEO:  Yeah.  We've been seeing it in action.  It's incredible.



STEVE:  Yeah, we've been talking about zero-days more in the last year or two than ever before.



LEO:  Yeah.  I'm curious, well, we'll talk about it.  I'm curious what your thoughts are on why it's happening, what's going on, yeah.



STEVE:  Yeah, we do talk about that.



LEO:  I figured you would.  And now, the Picture of the Week.



STEVE:  So this relates to our discussion a while ago about privacy enforcing or pro-privacy web search, of course.  DuckDuckGo is the service.  And I actually found myself thinking about it recently.  I clicked on some link from a Google search, and where I was going was blocked by uBlock Origin because I was like in some redirection chain.



LEO:  Yeah, I've had that happen, yup.



STEVE:  Bouncing, oh, it's just so wrong.  It's like, I'm looking at the link, right, and it's a fakeout link.  It's not the actual URL that you get when you click on it.  So I just thought, oh, and the sites that I was being bounced through that uBlock Origin was protecting me from were really sketchy-looking.  It's like, this is a link on Google Search.



LEO:  And you go through all those other things.  Yes.



STEVE:  Yes, yes.  So I thought, well, DuckDuckGo.  Anyway, the problem is, and we discussed this at the time, you can say to somebody with a straight face, you know, did you Google it?



LEO:  Or Bing it.



STEVE:  Yeah, Bing, yeah.



LEO:  Not quite so straight of a face, but...



STEVE:  But Duck?



LEO:  Did you Duck it?  Oh, don't say that.



STEVE:  Did you Duck it?  No.  You don't want to - that's too close to, you know.  And so then it was ducking or, you know, like what?



LEO:  That's bad, too.



STEVE:  I don't know.  Anyway, so somebody tweeted me a picture that is, because of what it is, it's got, as you said...



LEO:  It's a meme.



STEVE:  ...a common meme of someone like saying, no, no, no to DuckDuckGoing.  You don't want to say DuckDuckGoing.  That's clearly not helping.  Anyway, they're suggesting "quacking."



LEO:  Oh, I quacked it.  I quacked it, yeah.



STEVE:  So I quacked it.



LEO:  Yeah, quack me.



STEVE:  Do did you quack that?  Yeah, quack it.



LEO:  Yeah.  I like it.



STEVE:  Just go quack it.



LEO:  Yeah.



STEVE:  So thank you.  I think that probably solves our problem.  You don't want to duck it.



LEO:  Is that Drake?  I feel like that's Drake.  But I might be wrong.  I don't know.



STEVE:  I don't know who Drake is.



LEO:  The chatroom will tell me if I'm right, yeah.



STEVE:  There's something on his shirt that looks like a cheerleader or, man, I don't know what's going on.



LEO:  No, it's hoops, man, that's Nike Air or something, yeah, Air Jordans.  He's a hipster; right?  That's Drake.  



STEVE:  Well, you were certainly right about me not knowing about the meme.



LEO:  He's a well-known Canadian rap star, if that's not an oxymoron.



STEVE:  Yeah, well, that doesn't help either.  Okay.



LEO:  Oops.  I'm pressing all the buttons, but nothing's happening.



STEVE:  You've got all kinds of buttons over there.



LEO:  There we go.  There we go.



STEVE:  Okay.  So CISA's known exploited vulnerabilities catalog.  As we've noted from time to time, one of the services being provided by our awkwardly named U.S. Cybersecurity and Infrastructure Security Agency, CISA, has been the maintenance of a...



LEO:  You know, I never noticed the redundancy there.



STEVE:  Oh, it's so bad.  And I was thinking maybe, I was playing where if you said Instructure and Cybersecurity Agency...



LEO:  That would work.



STEVE:  ...then you get rid of one of the securities.  Because otherwise it's Cybersecurity and Infrastructure Security.



LEO:  You would write a letter to the Department of Redundancy Department because I think they need to fix this, for sure.



STEVE:  Actually I'll write another letter to the Department of Redundancy Department.



LEO:  Do at least two.



STEVE:  Yeah, because they wouldn't respond to only one.  So, okay.  So this is a growing list of actively, and this is the key, actively exploited vulnerabilities, not just all vulnerabilities, but the ones that are, like, being seen.  So we normally refer to this list in the context of CISA's what I like to call their "Christmas cancellation policy" from last year which is of issuing standing mandates to all federal agencies within its governance that they must patch this or that by some specific date.  Typically only a few weeks after the issuance of a mandate.



Now, at the end of today's podcast we're going to be talking a bit about patching philosophy and about the idea of, or the feasibility, I guess, of prioritizing patching to focus upon those issues that are being actively exploited.  The logic behind that's obvious.  In a bureaucratic environment it certainly makes the imposition of the inconvenience that's suffered by the need to take down and patch running systems and services justifiable.



But boy, CISA's list is growing now so large that it's being referred to as a "catalog," which is a better description than a list.  So at some point it loses some of its punch as it becomes easier just to patch everything, which, as we'll see, is the strategy that I think makes the most sense overall.



That said, there have been some notable new entries added to CISA's constantly growing catalog of "must patch immediately" mandates.  CISA informs us that the CVSS 7.8 vulnerability in Windows Print Spooler that was patched as part of February's Patch Tuesday  okay, so a little over 60 days ago.  We've had March and April patches since then.  But what they fixed in February is actively being exploited in the wild.  It's a privilege escalation vulnerability which a hacker will need to leverage if they're able to arrange to get into a system, but under limited protective privileges.  They need to escalate their privileges in order to perpetrate their nastiness.



So here's a perfect example of a more than 60-day-old patched problem, right, I mean, patches are available, and it's even on Windows, that's like aggressively broken down all of our resistance against applying patches.  So it's like, okay, fine.  Sixty days ago.  It was doubtless immediately reverse engineered and put to use.  The only place it's going to be effective is against machines that have not yet received their February updates.  Yet CISA says this thing is being exploited in the wild.  That's what's happening.  Back in February when Microsoft listed this defect as fixed, it was tagged as "exploitation more likely" and they were apparently right about that.



It's interesting, and I think somewhat sad, to look at the CVSS year dates for things the CISA adds to its "actively being exploited in the wild" catalog.  There were two others.  One was they just added a cross-site scripting vulnerability which was found in the Zimbra - I had to look it up - the Zimbra Collaboration Suite.  It's certainly not mainstream.  It's a Java-based suite which is hosted on Linux.  It's been around since 2005, and its ownership has changed hands many times.  I don't know what that says about it.  I have a relative who changes jobs often, and that kind of is a little sketchy.



Anyway, this thing was first written by a company originally named LiquidSys, who changed their name to Zimbra, which Yahoo! later purchased, before selling it to VMware, who then sold it to Telligent Systems, who then also changed their name to Zimbra before being sold to Synacor.  So I guess a lot of companies had great hopes for it, or maybe the price was right.  I don't know.  But in any event, this cross-site scripting vulnerability was identified and patched four years ago, in 2018, and CISA now tells us that it is currently being actively exploited in the wild.



And guess where?  Ukraine's Computer Emergency Response Team, somehow still in operation, CERT-UA, released an advisory last week cautioning about email phishing attacks targeting government entities with the goal of forwarding victims' emails to a third-party email address by leveraging exactly that Zimbra vulnerability.  So, okay.  It's not just theoretical.  As CISA said, it's actually happening right now.  Some Russian miscreant saw that someone was still using Zimbra.  So they checked their own catalog, which is titled "The Big Book of Every Possible Way to Hack Someone," and found a four-year-old cross-site scripting vulnerability that would come in handy if that Zimbra instance had not been updated in the past four years.  The most recent Zimbra update was just a little over a year ago, on April 7th of 2021.  So Zimbra could have been kept current, that instance that was apparently found being hacked.  But apparently it hadn't been.



So CISA's not wrong that this four-year-old obscure vulnerability is being exploited.  But are they right to add it to the U.S.'s emergency "must patch by May" mandate?  That's what I'm wondering about.  Are any U.S. federal agencies known to be using Zimbra?  I have no idea.  But if so, wouldn't it make much more sense, because it's got to be, like, what, one or two, if any?  Wouldn't it make more sense to be a little bit more proactive and give them a call?  I mean, like, target those agencies rather than everyone else with an entry that just adds unnecessary noise to the growing list?  And this thing's four years old.



Another just-added entry is a three-year-old stack buffer overflow - now, stack buffer overflows, they're not good.  This one carries a CVSS of 9.8, which as we know, those are the ones you pay attention to.  That's a "stand up and take notice" score.  And it occurs in WhatsApp's VoIP component.  How is it possible that anyone using WhatsApp will not have updated it since 2019?  I mean, this thing's a three-year-old problem that was patched.



So Leo, I guess that perhaps we're living in a bubble.  That's the only conclusion I can come to.  Perhaps we've been drinking our own upgrade Kool-Aid for so long that we're completely out of touch with the real world.  There must be a significant proportion of users who actively and proactively ignore, or perhaps mistrust, or just don't think they need offers and requests to update their software.  You know, it's working and not obviously broken.  So they think, I see no need to mess with it.



And as we know, it certainly is the case that having something work and having something that works also being actively impervious to abuse are two very different things.  But we know that.  That's one of the most important lessons that everyone listening to this podcast, myself included, has learned through example after example through the years.  But that's probably not at all obvious to the typical user who thinks, well, it works.  I can message and talk, and it seems fine here.  So whatever they're trying to sell me or to get me to do I probably don't need or want.  So I'm not going to change anything.



But again, even so, does a three-year-old vulnerability, maybe  if someone in Bangladesh had their Android phone compromised as a result of using an even older version of WhatsApp, does it need to be taking up cognitive space in CISA's catalog?  I think it's an open question.



Three or four years is a long time not to have updated software.  It's clear that there's a very long tail on many of these vulnerabilities.  We talked last year and the year before about critical flaws in a TCP/IP stack being widely used by $5 Internet of Things light switches and plugs.  None of those things are ever going to be updated.  So they will be latently vulnerable as long as they're in service.  But that isn't what CISA is targeting.



I don't have an answer.  Light switches and plugs cannot be updated.  But for federal agencies to never update in-use software for which updates are ready and waiting is unconscionable.  So I'm glad there's a mandate.  I hope it has some teeth behind it.  But as I was thinking through this, I thought, you know, maybe have these things expire after a year, but like get agencies, like force them somehow not to be more than a year out of date.  And they shouldn't be more than, you know, in some cases a few days out of date.  The rate at which we're now seeing patched vulnerabilities leveraged into exploits has accelerated dramatically in the last couple of years, and we're going to be touching on that shortly.



Lenovo.  Leo, I heard you refer to Lenovo's UEFI problems on some podcast recently.  So this has been in the news a lot.



LEO:  Oh, yeah.



STEVE:  It's not surprising.



LEO:  I'm aware of it because I buy a lot of Lenovo hardware.



STEVE:  Yes.  That's been the ThinkPad; right?



LEO:  Yeah, yeah, love the ThinkPad, yeah.



STEVE:  It's like it's been, yes, the premier laptop.  So as we know, when a PC is powered up, something needs to wake up and configure the various parts of the machine.  The video needs to be started.  The fans need to spin up.  All of the machine's various mass storage subsystems need to be initialized.  And then the firmware's configuration needs to be checked, the proper operating system needs to be located, and its OS boot code needs to be initially loaded into RAM so that control can be turned over to it to continue booting the machine.



The first PCs did that using their Basic Input Output System, B.I.O.S., or BIOS.  That was good for about five years.  It actually didn't last very long because the PC just exploded in terms of what everybody wanted to do with it.  So the limitations which had been built into the BIOS's assumptions began to cause more problems than they were worth almost automatically.  And various Mickey Mouse workarounds were created to overcome many of these problems while Intel worked on a wholesale replacement of the BIOS.  The initial attempt was the EFI, the so-called Extensible Firmware Interface, which quickly matured into the Unified Extensible Firmware Interface, UEFI.



And we find ourselves right back where we always do.  The original BIOS was so dumb that it could not be infected.  It was originally implemented in...



LEO:  Sometimes dumb is a good thing.



STEVE:  That's exactly.  It was originally implemented in  masked ROM, meaning that the firmware's bits were etched into a metal mask at the factory and could never be changed.  It did mean you had to get the code right the first time.



LEO:  No updates, yeah.



STEVE:  And that was something people used to be able to do.  But we don't do that anymore.  So that soon gave way to non-volatile FLASH ROM, which could be updated, but the code it implemented was still blessedly dumb.  Sometimes, for some things, the dumber the better.  Because if all you want is to boot an OS, you don't really need that much smarts.  The BIOS did it just fine.  And the lesson we keep falling into and we keep failing to learn is that the more complicated, fancy, capable, and "smart" we make things, the more leeway and latitude the system has to go very badly wrong.



So welcome to the Unified Extensible Firmware Interface where malware is also able to "extend" the firmware.  Lenovo has been most recently in the "We made a UEFI mistake" news recently.  Last week, the guys over at ESET, whose motto is "We Live Security," posted the results of their analysis of some widely used Lenovo UEFI firmware.  Their posting's title was "When 'secure' isn't secure at all:  High-impact UEFI vulnerabilities discovered in Lenovo consumer laptops."  And the story's tagline is "ESET researchers discover multiple vulnerabilities in various Lenovo laptop models that allow an attacker with admin privileges to expose the user to firmware-level malware."



Okay.  Firmware-level malware.  That's not what you want to hear.  That's even less what you want to have crawling around inside your machine.  Firmware-level malware enables the ultimate in rootkit techniques, in fact having its own worst name, "Bootkit."  The presence of firmware-level malware means, quite simply, that it's impossible to trust anything about what the machine might do.  Firmware-level malware is able to infect and compromise the operating system's own code during its boot process before it has had any opportunity to raise its own shields.  And reformatting the machine's mass storage and reinstalling an operating system or even removing and replacing a drive won't necessarily eliminate the problem because this malware has taken up residence in the machine's underlying firmware on the motherboard, on a non-volatile memory soldered to the main board.



Now, we know that anybody can make a mistake.  And as our listeners know, I am infinitely forgiving of mistakes.  But the most troubling aspect of what the ESET researchers found was that two of the three big mistakes Lenovo made were the oversight of leaving highly exploitable drivers in the UEFI firmware image which should have only been present during the firmware's development.  These drivers should have never left the factory.  So it's not like they got a loop condition wrong or something like a mistake.  They left stuff in there that should not be in there.  How do we know?  We know because the two drivers were actually named "SecureBackDoor."  In the UEFI firmware that's the driver's name.  Yeah, we're going to...



LEO:  Talk about an oxymoron.  SecureBackDoor.



STEVE:  Yeah, yeah.  Turns out it wasn't.



LEO:  Yeah.



STEVE:  And the other one was "SecureBackDoorPeim."  So here's what ESET said.  They said:  "ESET researchers have discovered and analyzed three vulnerabilities affecting various Lenovo consumer laptop models."  Various.  Yeah, we'll get to that in a minute.  "The first two of these vulnerabilities" - and we've got two CVEs from this year, 3971 and 72 - "affect UEFI firmware drivers originally meant to be used" - this is ESET - "only during the manufacturing process of Lenovo consumer notebooks.  Unfortunately," writes ESET, "they were mistakenly included also in the production firmware images without being properly deactivated [/or deleted].  These affected firmware drivers can be activated by an attacker to directly disable SPI flash protections" - that's using Control Register bits and Protected Range registers - "or the UEFI Secure Boot feature from a privileged user-mode process running OS runtime."



Okay.  So just to be clear about what ESET just said.  They said:  "From a privileged user-mode process in the OS."  In other words, a user, any user of these laptops mistakenly allowing some malware to run in their OS, which might innocently ask to be granted brief UAC privilege elevation to install something, that is, if it didn't bring along its own privilege escalation vulnerability exploit, as it might; or which might set itself up to run as a system service.  That code can disable all relevant UEFI write protections to then surreptitiously install semi-permanent hidden bootkit malware into the system's UEFI firmware.  And the user would be none the wiser.  And we don't know how to scan for that yet.  I mean, there's been some talk of scanning UEFI.  Nothing much has come of it.



ESET said:  "It means that exploitation of these vulnerabilities would allow attackers to deploy and successfully execute SPI flash or ESP implants, like LoJax.  To understand how we were able to find these vulnerabilities, consider the firmware drivers affected by" and then the CVE number 3971.  They wrote:  "These drivers" - imagine this, Leo - "immediately caught our attention by their very unfortunate, but surprisingly honest names:  SecureBackDoor and SecureBackDoorPeim.  After some initial analysis, we discovered other Lenovo drivers sharing a few common characteristics with the SecureBackDoor* drivers."  Those are Chg - I guess that's short for change - and then BootDxeHook and ChgBootSmm.  SMM is system management mode stuff, which is the OS under the OS.  "As it turned out," they write, "their functionality was even more interesting and could be abused to disable UEFI Secure Boot."  That's the CVE ending in 3972.  



"In addition," they said, "while investigating the vulnerable drivers, we discovered a third vulnerability:  SMM memory corruption inside the SW SMI handler function."  Thus we have CVE ending in 3970.  "This vulnerability," they said, "allows arbitrary read/write from/into SMRAM, which can lead to the execution of malicious code with full SMM privileges" - that's, again, that's like the chip-level privileges, nothing more privileged in the world than that - and, they said, "potentially lead to the deployment of an SPI flash implant.



"We reported all discovered vulnerabilities to Lenovo on October 11th, 2021."  And I didn't have it in the show notes, but Lenovo responded a month later.  "Altogether the list of affected devices contains" - and here it comes - "more than 100 different consumer laptop models, with millions [many] of users worldwide, from affordable models like IdeaPad 3 to more advanced ones like Legion 5 Pro or Yoga Slim 9.  The full list of affected models with active development support is published in the Lenovo Advisory.



"In addition to the models listed in the advisory, several other devices we reported to Lenovo are also affected, but won't be fixed due to them reaching End Of Development Support (EODS).  This includes devices where we spotted reported vulnerabilities for the first time:  IdeaPad 330 and IdeaPad 110.  The list of such EODS devices that we have been able to identify will be available in ESET's vulnerability disclosures repository."  And what this tells us, reading between the lines, is that these vulnerabilities have been there long enough for those machines which they started affecting to now have left, have gone out of their service life with Lenovo.  Thus they will never be fixed.



Oh, yeah, I do have in the notes Lenovo confirmed the vulnerabilities on November 17th, 2021, and assigned them the following CVEs.  And, I mean, they're coming right out with it.  CVE ending in 3970, LenovoVariableSmm - SMM arbitrary read/write.  The one ending in 3971, SecureBackDoor - disable SPI flash protections.  And 3972, ChgBootDxeHook - disable UEFI Secure Boot.



So given how incredibly active the cyber underworld is today, we keep encountering quite sobering evidence of it, you know, in every podcast now.  There's just no chance that these now fully disclosed and very well-documented vulnerabilities will not be used to compromise the interests of some of these millions of Lenovo laptop users worldwide.  And many of them are going to be serious users.  It will happen.  So here we are once more noting that there's something very wrong with our industry's current development model.  How can this be allowed to occur over and over and over?  ESET had to reverse engineer the proprietary code in this UEFI firmware in order to find these problems.  And it's affecting lord knows what multiple of millions of Lenovo laptop users.



Lenovo messed up big-time here, but for the record, they're not alone.  These newly disclosed vulnerabilities merely add to the recent disclosure of more than 50, five zero, UEFI firmware vulnerabilities which had been found in Insyde Software's - I-N-S-Y-D-E - Insyde Software's InsydeH2O, and HP and Dell laptops since the start of just this year.  Among those are six severe flaws in HP's firmware affecting both laptops and desktops which, when exploited, could allow attackers to locally escalate to SMM privileges, which as I said is as much as you can get on any hardware platform, and trigger at least denial of service and maybe more.  So Lenovo is in good company, or at least only the most recent member of this UEFI vulnerability dog house.  And as we know, it's not Lenovo's first instance of UEFI problems.  Years ago they've also had problems.



So we've managed to make our lovely little machines far more complex by designing in extremely powerful capabilities.  Yes, we get lots more flexibility.  We get remote management and remote maintenance.  And not surprisingly, it's also a mixed blessing.  So a heads-up to anyone using Lenovo laptops.  Regardless of the model you have, don't look at a list of affected models.  First of all, there's hundreds.  You should definitely check in to see whether your device has a firmware update outstanding.  And for that matter, HP and Dell users would be well advised to do the same.



LEO:  Do you think these changes are driven by the needs of enterprise?  In other words, are we personal and home users and geeks suffering because...



STEVE:  Yes, exactly that.



LEO:  ...[crosstalk] management capabilities built in for any of us.



STEVE:  Exactly that.  Exactly that, Leo.



LEO:  There should be, and there are a few, places where you can get simpler systems with simpler UEFI and core boot open source firmware, things like that.  And they are really not aimed at enterprise.  What was the other thing I wanted to mention?  Oh, yeah, firmware updates now, it's interesting, are increasingly part of the operating system update.  I don't know if you've noticed that.



STEVE:  Yeah.  Well, we know that Windows, for example, is patching the Intel chipset firmware.  Linux brings along the same thing.  And to their credit, although it is a little bit of a mixed blessing, Lenovo now has software that comes pre-installed on their machines which is taking responsibility for keeping your machine's firmware up to date.  So it makes it better than never ever having the opportunity to proactively inform Lenovo machine owners and having a problem like this out there that would make them persistently vulnerable.



LEO:  Yeah, yeah.



STEVE:  Okay.  So I read the title of this piece of news in The Record, and it just made me shake my head.  The item is titled "Everscale blockchain wallet shutters web version after vulnerability found."



LEO:  Okay.



STEVE:  I mean, really?



LEO:  I'm going to put my wallet on the web.  That's a good idea.



STEVE:  What moron could possibly think that offering a web  browser-based cryptocurrency wallet was sane?



LEO:  Well, it's easy.  It's convenient.



STEVE:  Anyone who was capable of beginning to create such a thing should know it's just a bad idea.  As we've often observed on this podcast, just because you can do something doesn't mean you should do that.  Here are the first two sentences of The Record's story.  They wrote:  "The company behind Ever Surf, a wallet for the Everscale blockchain ecosystem, is shuttering its web version after a vulnerability was found by Check Point researchers.  The Ever Surf team confirmed that the vulnerability allowed attackers to gain access to wallets."  Uh, yeah.



LEO:  Duh.



STEVE:  Because it's on a web browser.  Oh, my god.



LEO:  Uh, yeah.



STEVE:  Okay.  The Record is reporting on research which was performed by Check Point Research. The Check Point guys explained.  They said:  "Blockchain technology and decentralized applications provide" - and decentralized applications are, you know, web apps - "provide users with a number of advantages.  For example, users can utilize the service without creating an account, and it can be implemented as a single-page application written in JavaScript."  They're being very fair here.  "This type of application does not require communication with a centralized infrastructure, such as a web server, and it can interact with the blockchain directly or by using a browser extension like MetaMask.



"In this case, the user is identified using keys that are stored on a local machine inside a browser extension or a web wallet."  Okay, now, the phrase "web wallet" itself should be outlawed.  But okay.  "If a decentralized application or a wallet stores sensitive data locally, it must ensure this data is reliably protected.  In most cases, decentralized applications run inside the browser and therefore may be vulnerable to attacks such as cross-site scripting," just to name one of, like, countless.



"This research describes the vulnerability found in the web version of Ever Surf" - maybe we should call it Never Surf - "a wallet for the Everscale blockchain."  They finish:  "By exploiting this vulnerability, it's possible to decrypt the private keys and seed phrases that are stored in the browser's local storage."  Yeah.  "In other words, attackers could gain full control over victims' wallets."



Okay, now, Leo, you're going to love the details of this.  Okay.  It turns out that one of the code libraries the implementers used - you know how everybody now is just grab a library here, grab a library there and hope that it hasn't been compromised by some supply chain attack, which is another problem.  One of the code libraries the implementers used is not fully supported, or one of the functions in one of the code libraries is not fully supported in web browsers.  The code attempts to obtain a cryptographic nonce with a call to the function  "DeviceInfo.getUniqueId."  The problem is that this function requires access to its underlying device, so it's only defined when running natively in Java on Android, iOS, or Windows.  I have a snippet of the function, actually it's the entire function, it's a one-line function because JavaScript is crazy with the way it operates.



LEO:  I would never write a function like this.  This is ridiculous.



STEVE:  I know.  You're able to chain a bunch of ors, and the first or that succeeds is the one that gets taken as the value of the enclosing function.



LEO:  You can't really see it very well on the screen, not from the show notes.



STEVE:  No, you really can't see it.  Anyway, I've got a snippet of it.  What it shows is, for those who read JavaScript, that it is obtaining a value of the underlying platform's default dot unique ID, if there's an evaluation of the OS as Android, iOS, or Windows.  And otherwise it's also a conditional expression which is another creation of, well, it exists in several languages now.  It's a conditional expression.  If it doesn't exist in those languages, that is, the function is undefined, then it returns "unknown."  Literally the string "unknown."



Now, of course the string "unknown" never varies, right, from browser to browser or instance to instance or user to user.  So when the OS is not Android, iOS, or Windows natively, the function returns, as I said, "unknown" in quotes.  And thus that value is never unique, and that value is used to salt the hash.  As we learned years ago on this podcast, salting hashes is crucial to the security of hashed password storage because the salt effectively customizes the hash per use.  With the salt broken, Check Point was able to trivially brute force the user's six-digit PIN.  Yes, on top of everything else, even if the system was working correctly, its entire security was controlled by a six-digit PIN.



Check Point wrote:  "CPR (Check Point Research) roughly re-implemented the key derivation and keystore decryption in NodeJS and performed a brute force attack on the PIN code.  This resulted in a performance of 95 passwords per second on a four-core Intel Core i7 CPU.  Although this is not a very high speed, it is sufficient for the attack on a six-digit PIN code.  In the worst-case scenario, checking 10^6 possible variants means the entire attack takes approximately 175 minutes."  And that's worst case.



They said:  "For our experiment, we created a new key in Surf and dumped the keystore from the browser's unencrypted local storage.  In our case, the attack took 38 minutes.  At the end, we got the derived key and decrypted the seed phrase that can be used to restore the keys on another device."  In other words, this was never secure.  And in this case, I mean, first of all, as I said, the idea of doing a browser-based wallet is just nuts.  It'd be like, I don't know, putting a wallet on a lemonade stand in the front yard and trusting that no one is going to come along and take it.  I mean, it's just - it's insane.  Browsers struggle with security, and you do not want your cryptocurrency private keys anywhere near a browser.



And again, had this - it would have been a bad idea to implement it on a browser in any event.  But this is a classic instance of why it's a bad idea.  Libraries were used that were not fully understood.  They deployed this thing without ever verifying that the hash was never changing, and so the same hash was being used to always encrypt the user's data.  And it just meant that the whole thing could be brute forcible.  Check Point also noted that in the same way, back in the day, Leo, what were those tables called?  Rainbow tables.



LEO:  Rainbow tables, yeah.



STEVE:  Yes.  And so basically you could create a rainbow table using some GPUs in the cloud to come up with the hashes for all 10^6 possibilities.  That wouldn't take a lot of time.  Then you could simply decrypt everybody's wallet who has one of these things that you're able to get a hold of.  So just a bad idea.



Okay.  Java once again.  Java 15, 16, 17, and 18 received, the JDK, the developers kit, received "must updates" last week.  Neil Madden, the somewhat excitable guy at ForgeRock, who discovered a new and quite severe problem with Java, considers it to warrant a CVSS of 10.0.  I think we should reserve that for the software apocalypse, or perhaps when Skynet obtains self awareness.  The rest of the industry gave his discovery a still very healthy CVSS of 7.5, and in no event should this one be ignored.  Anybody doing Java development using security needs to update using from last week's critical update.



So here's what Neil wrote about his discovery.  He said:  "It turns out that recent releases of Java were vulnerable to a flaw in the implementation of their widely used ECDSA."  That's the Elliptic Curve Digital Signature Algorithm.  And by the way, that's now the default across, like, all state-of-the-art algorithms are using ECDSA.  And we'll see some examples in a second.  He said:  "If you are running one of the vulnerable versions" - that is, Java 15, 16, 17, or 18 - "then an attacker can easily forge some types of SSL certificates and handshakes, allowing for interception and modification of communications; signed JWTs, JSON Web Tokens; SAML assertions or OIDC ID tokens; and even WebAuthn authentication messages, all using the digital equivalent of a blank piece of paper."



He said:  "It's hard to overstate the severity of this bug.  If you are using ECDSA signatures for any of these security mechanisms" - and as I said, ECDSA is now the default standard - "then an attacker can trivially and completely bypass them if your server is running any Java 15, 16, 17, or 18 version before the April 2022" - that's last week's - "Critical Patch Update (CPU)."  He said:  "For context, almost all WebAuthn/FIDO devices in the real world, including YubiKeys, use ECDSA signatures; and many OIDC providers use ECDSA-signed JWTs," JSON Web Tokens.



LEO:  I use it for my SSH keys.  But that wouldn't be impacted by this.



STEVE:  No, because I would not imagine...



LEO:  I'm not using Java for my SSH.



STEVE:  Right, exactly.  You're probably not - you're not connecting to something that is Java based.



LEO:  No.  No.



STEVE:  So he says:  "If you've deployed 15, 16, 17 or 18 in production, then you should stop what you're doing and immediately update to install the fixes in the April 2022 Critical Patch Update."  And finally he says:  "Oracle has given this a CVSS score of 7.5, assigning no impact to Confidentiality or Availability."  Which I agree is questionable.  He said:  "Initially we at ForgeRock graded this a perfect 10.0 due to the wide range of impacts on different functionality in an access management context.  ForgeRock customers can read our advisory about this issue for further guidance."  So in any event, for any listeners, if anybody is a Java developer, if you don't already have it, if you haven't received a notice, definitely update your Java JDK to the latest.  And apparently 15 and 16 had gone out of support.  So they're never going to be fixed, as I understand it.  Hopefully everybody has by now moved to 17 and 18.



We have a bunch of interesting closing-the-loop feedback this week.  7337, that's of course LEET upside down, he said of SN-863 and use after free, he said:  "Why does the deallocated memory not get zeroed?  Why does malloc not also zero out the deallocated memory?  Would that not solve the use after free issue?"  It's a nice point, but here's the catch.  It turns out it's not the memory, it's not the contents of the memory that's been freed that matters.  It's that you have a pointer which used to point to some memory.



Modern automatic languages make it impossible or difficult or at least managed to get a pointer.  You have to get something that refers to something else, you get that from the underlying language.  Unlike C or obviously assembler, where a pointer could just be - you just make one.  The next-generation automatic languages are dealing with pointers on your behalf for you.  So it's the fact that you are able to retain something that you got from the language which was a pointer, and then what it points to got freed, means that you might get lucky, and that it will then in the future point to something that you shouldn't be able to point to.  Anyway, it's the pointer that is the key, rather than the contents of what was released.



Awk tweeted:  "Re:  Feedback on 867."  He said:  "On MS Windows Auto Update Service."  He says:  "My thoughts are on the routers.  I've heard Bruce Schneier articulate and give examples on how cheap IoT devices are designed and manufactured in publicly available talks and seminars."  He said:  "A team is assembled," as Bruce said, "a team is assembled and then immediately disassembled after the process, and there is no one left to actively package and push patches to these devices, unlike the teams at Apple, Microsoft, and Google."  Which, anyway, I thought that was really interesting.



He said:  "Coupled with this, I shudder at the recent incidence and demonstration from SolarWinds that your supply chain update servers and processes can be compromised and commandeered.  Schneier comes to the conclusion that market forces cannot help here, as consumers want their devices produced fast and cheap, and thus only regulation would be the cure."  Anyway, he asks, "I wonder what your thoughts are."  And of course we've talked around this certainly a lot.  My thoughts are that we're doing this all wrong, that it is wrong that it is necessary for security firms to reverse engineer proprietary software which tens of millions of users are actively using that nobody else had to sign off on, that nobody else had to look at, that the company said, "Trust us, it's secure."  You know, my favorite example is voting machines.  How is it that we're using proprietary voting machines that were never vetted in any way?  It's just, you know, it's broken.  It leaves me to conclude we must still be in the early days of all this because this can't continue.



Andy in the U.K. said:  "On Google Auth, email addresses can be checked just by sending an email to it."



LEO:  Oh, good point.



STEVE:  Yes.  Yes.



LEO:  Never thought of that.



STEVE:  Yeah.  He says:  "If it bounces, the account doesn't exist.  So it makes very little difference to Google if they reveal the address does exist during the Auth process.  However, others using email as ID shouldn't reveal if it's for a valid account."



And however, I just chose Austin out of a hat, and that hat was brimming with, I'm not kidding, like maybe 200 public and private tweets who all said, uh, Steve, here's what's going on.  Austin says:  "Listening to Episode 867."  That was last week.  "During your conversation about authentication processes and email addresses being revealed during the process, Leo asked why an application like Gmail would ask for an email address first before moving on to the next step of the authentication process."



LEO:  I see it more and more often, all the time now.



STEVE:  "This is because many apps use many identity providers and authentication workflows.  Your email address will determine which authentication workflow..."



LEO:  What to do next, yeah.



STEVE:  Yes, "...the application will walk down..."



LEO:  Do you want to go to Okta?  Do you want to do Duo?  Do you want to do a password?



STEVE:  Precisely.



LEO:  Yeah, that makes sense.



STEVE:  Are you using a YubiKey?  Are you going to go to a password?  Do you have a custom SAML identity provider and so forth.  So thank you, everybody.  I wanted to thank everybody as a group who said, uh, Steve?  Apparently you don't do this, but everybody else is having to do this more and more.



LEO:  Yeah.



STEVE:  And so makes absolute sense.  Thank you, everyone.



LEO:  Makes sense, yeah.



STEVE:  And then finally...



LEO:  Go ahead.  I was just going to offer a break, but I think you have another one.



STEVE:  We've got a couple more things.  CMe, he said:  "Hi, Mr. Gibson."  Hi, CMe.  He said:  "I just thought..."



LEO:  I CU.



STEVE:  I CU, "...you'd get a kick out of this."  He said:  "On your recommendation I put the McCollum Gibraltar trilogy in my wish list on Amazon.  It's only sold as individual books.  But wanting to save a few bucks, my son-in-law contacted the seller.  That turned out to be the author himself" - yeah, Michael McCollum - "who agreed to refund the difference in postage.  So I received a $7 check signed by McCollum, and three autographed books for my birthday."



And I just wanted to remind people he is one of my favorite authors.  His site is scifi-az.com.  He's a self-publisher.  He does offer all of his books in eBook format.  They are DRM-free using the "we trust you."  And he's got two trilogies.  There's the Gibraltar trilogy, and the Antares trilogy.  Both are on my "must read" reading list.  He is literally a rocket scientist.  He designed one of the pumps that's in use on the International Space Station.  He's an engineer.  But he is a beautiful storyteller.  And what I love about his stuff is they are really clever.  I mean, they've got lots of aha moments, and they're surprising and really, really gratifying.  So, and they're not expensive.  So scifi-az.com. 



Speaking of sci-fi, I wanted to note that I finished Book 4 of the Bobiverse, after doing the trilogy.  And I'm back to Ryk Brown's Frontiers Saga.



LEO:  You'll never run out of those.  Holy cow.



STEVE:  No.  Although I will run out of patience.  That's the problem.



LEO:  Yeah, yeah.



STEVE:  Because I should say - I should explain.  It never happens, at least not recently, that I stay up reading after Lorrie falls asleep on my lap.



LEO:  Do you put the book on her head?



STEVE:  No.



LEO:  That would be mean.



STEVE:  I hold it off to the side.



LEO:  You're a good partner, good.



STEVE:  I could not - this is last night.  I could not stop reading this first book of the third 15-book story arc.



LEO:  Doesn't your arm get tired?  We need to get an arm to hold your Kindle.



STEVE:  And I just did want to mention, too, that I have hopes for next week.  "Star Trek Discovery" was way too hyperactive for me.  It just seemed like a big, like, VR videogame.  I just didn't get into it.  But we've got "Strange New Worlds" starting, where it is a prequel to the original first Kirk and Spock and McCoy series.



LEO:  Yeah, the trailer looks good.



STEVE:  It really does.  I'm just - I would like a Star Trek for adults.  Maybe there's no market.  But I'm one.



LEO:  I would watch it, yes.



STEVE:  Oh, goodness, yes.  So it would be - if we could return to Star Trek's original roots, which the "Next Generation" was also faithful to, of actually telling stories rather than only having an excuse for special effects.  And I've not yet looked into the second season of "Picard" after the disappointing first season, although I know it's there.  And it's just so sad to see Patrick looking so old, frankly.  And I've not started the sixth season of "The Expanse," though it's also - I think it's probably done by now.



And finally I have a milestone.  The incremental development release of SpinRite which I posted at 1:47 p.m. last Friday afternoon really surprised me.  Tester after tester has been reporting that everything that's been completed so far is finally working perfectly, solidly, and better than ever for them.  The reason this surprises me is that the code SpinRite now has is not relying upon any BIOS to interface it to mass storage adapter and drive hardware, yet it is finally working on every piece of hardware that everyone has of any age and vintage.



As we know, operating systems are able to achieve this by bundling a raft of hardware-specific manufacturer-supplied drivers which the OS loads on demand based upon the hardware it detects in the system when it's booted.  But that's not a practical approach for SpinRite.  What I was hoping we would be able to achieve was the creation of universal native drivers, one for IDE parallel adapters with PATA drives, and another for AHCI adapters with SATA drives, where they would simply work everywhere on all hardware from the 1980s through the latest chipsets.  I'm not only surprised, but I'm also greatly relieved since after many months of work we finally have 100% success, and every indication is that this elusive goal has finally been achieved.



I've been stuck here for a while, perfecting this foundation, because everything that comes next  not just SpinRite 6.1, but 7.0 and all of SpinRite's future after that  will be built upon it.  And it's all based upon that new IO abstraction approach which means that new mass storage technologies, like native support for USB and NVMe and whatever comes after that can be easily added behind the abstraction.  So once I catch my breath, I can finish the work on the rest of SpinRite, building upon this new foundation.  And then we're going to have 6.1.



LEO:  That is, wow, that's great.



STEVE:  It was a good week.



LEO:  This is I think uniquely difficult because previous versions let BIOS do the matching, and now you've got to replace that BIOS call.



STEVE:  Yes.



LEO:  That's tough.



STEVE:  It was a mixed blessing.  On one hand we got ease of interfacing because the BIOS knew how to talk to its own hardware.



LEO:  Right.



STEVE:  The problem is we also got a growing thickness of insulation through which SpinRite couldn't really see what was going on.  And so for example, anyone who's ever seen Dynastat painfully slowly getting sector samples, it's like it just takes so long because you have to issue a reset through the BIOS any time you get an error.  And it can take like 20 seconds.  So that's all going to go away.  6.1 is just going to - it's going to do data recovery at a screaming rate that we've just never seen, and I've never seen before.  No one's ever seen it before.



LEO:  So you were using INT 13 pretty much for everything.



STEVE:  Yes.



LEO:  Which, I mean, in the first edition made it easier, a lot easier to write it because you didn't have to test a variety of hardware or anything.  Did the move away from BIOS make - or is INT 13 always supported regardless?  I guess it has to be; right?



STEVE:  It's still there.  



LEO:  In software.



STEVE:  Yeah, exactly.  It is what the boot sectors use in order to get themselves booted.



LEO:  Right, right.  It's got to be there, yeah.



STEVE:  But booting an OS these days is only a matter of reading a few K of code into RAM, and then it starts to bring the rest of itself into memory.



LEO:  Right, it's all hard drive.  So I guess, I would guess this would be a first step in making it Mac compatible, too, because of course Macs never had INT 13, never had BIOS.



STEVE:  This absolutely will run on a Mac.



LEO:  Nice.



STEVE:  Yup.



LEO:  Okay.  Wow.  You heard it here almost first.  I'm sure the forums know.  But that's the first I've heard.  So that's great.



STEVE:  Yeah, it was a good Friday.



LEO:  Get in there, get SpinRite 6 so you can get 6.1.  It's almost here, folks.  That's going to be a red-letter day.  Let me know ahead of time so I can get a cake and some balloons and confetti and stuff.  Got to plan it.  Now let's talk about The Zero-Day Explosion.



STEVE:  And you know, that's what you want computers to do; right?  That was always - we just don't, you know, computers have been a typewriter for so long.



LEO:  Right.



STEVE:  Basically that's all they did.  They weren't really extending our brain in a lot of ways.



LEO:  Well, and it's not just horsepower.  It's also memory, RAM.



STEVE:  Yeah.  True.



LEO:  And just software skill.  And Grammarly's been around to kind of...



STEVE:  Well, and clearly, if these guys are in the Ukraine, they've got some English speakers because this is not something that you can...



LEO:  Oh, yeah, yeah.  Well, they're a distributed, yeah, they're all over the world.  And in fact they come in many languages, too, by the way, I should mention.



STEVE:  Yeah, yeah.



LEO:  Really good stuff.



STEVE:  Wow, very cool.



LEO:  Yeah, it's impressive.  And LISP, don't you love that?  Well, maybe you don't...



STEVE:  Yeah.



LEO:  I have to say I'm a little bit of a LISP fan boy.



STEVE:  John McCarthy would be proud.



LEO:  Yeah, yeah, exactly.



STEVE:  Okay.  So the most interesting and important class of software and system vulnerabilities are those that are discovered when a security researcher watches something that's not supposed to be possible, happen anyway.  Like a specially formed packet hitting a firewall and being admitted through the firewall despite the clear firewall rule which blocks its entry.  Or the password challenge that is ignored by the attacker who's logged on with administrative privileges anyway.  Or the cryptocurrency mining malware which suddenly launches, springs to life, and begins operating on a system that was just reformatted and reinstalled from scratch.  When a researcher watches something that cannot happen, happen anyway, they may have just witnessed and discovered evidence of the exploitation of a previously unknown zero-day vulnerability.



Zero-day vulnerabilities are a constant topic of this podcast.  And just as this podcast appears to be in no danger of running out of security topics ever, recently posted sobering research from Mandiant and Google's Project Zero make it pretty clear that we also won't be running out of zero-days to discuss anytime soon.  I have a chart which is quite - it makes you gulp, in the show notes.  Ten years of zero-day tracking.  What is going on?  The show notes show this chart of Mandiant's sobering 10-year graph showing the number of zero-days discovered each year from 2012 through 2021 last year.  The counts for each successive year are two, three, eight, 15, 21, 17, 16, 32, 30, and 80.



So 10 years ago, the entire year of 2012, we encountered just two zero-days, and the next year only three.  Then the following three years rose to 8, 15 and 21.  And the next two years dropped back a bit to 17, and then 16 in 2018.  2019 doubled that to 32.  2020 dropped it a bit to 30.  But then last year exploded from 30 in 2020 to 80 zero-days.  So if you felt as though we have been talking a lot, and a lot more, about zero-day vulnerabilities recently, well, you would be correct.



Some interesting observations emerged from Mandiant's research.  They wrote:  "In 2021, Mandiant Threat Intelligence identified 80 zero-days exploited in the wild, which is more than double the previous record volume in 2019."  That was 32.  "State-sponsored groups continue to be the primary actors exploiting zero-day vulnerabilities, led by Chinese groups.  The proportion of financially motivated actors, particularly ransomware groups, deploying zero-day exploits also grew significantly, and nearly one in three identified actors exploiting zero-days in 2021 was financially motivated."  In other words, not espionage.



"Threat actors exploited zero-days in Microsoft, Apple, and Google products most frequently, likely reflecting the popularity of these three vendors.  The vast increase in zero-day exploitation in 2021, as well as the diversification of actors using them, expands the risk portfolio for organizations in nearly every industry sector and geography, particularly those that rely on these popular systems.



"Mandiant analyzed more than 200 zero-day vulnerabilities that we identified as exploited in the wild from 2012 through 2021.  Mandiant considers a zero-day to be a vulnerability that was exploited in the wild before a patch was made publicly available.  We examined zero-day exploitation identified in Mandiant original research, breach investigation findings, and open sources, focusing on zero-days exploited by named groups.  While we believe these sources are reliable as used in this analysis, we cannot confirm the findings of some sources.  Due to the ongoing discovery of past incidents through digital forensic investigations, we expect that this research will remain dynamic and may be supplemented in the future."  In other words, in the future they may learn more about what's happened in the past.



And they said:  "We suggest that a number of factors contribute to growth in the quantity of zero-days exploited.  For example, the continued move toward cloud hosting, mobile, and Internet of Things technologies increases the volume and complexity of systems and devices connected to the Internet.  Put simply, more software leads to more software flaws.  The expansion of the exploit broker marketplace also likely contributes to this growth, with more resources being shifted toward research and development of zero-days, both by private companies and researchers, as well as threat groups.  Finally, enhanced defenses also likely allow defenders to detect more zero-day exploitation now than in previous years, and more organizations have tightened security protocols to reduce compromises through other vectors."



So I thought those points were really interesting.  Of course we wonder whether our count of zero-days is recently higher because we're looking harder and more closely for them.  This makes sense given that we've established quite clearly that with all software in general, the closer we look, the more problems we find.  Some of those "more problems" will be exploitable zero-day vulnerabilities.



And the increasing level of specialization we've chronicled in recent years also leads to higher zero-day counts through the creation of this exploit broker marketplace.  Now, now that there's a marketplace, those who wish to deploy such exploits don't need to spend their time hunting them down.  And those who specialize in hunting for new ways in can spend all their time doing nothing else and then selling them into that marketplace.



And finally, so much of the lower hanging fruit has been found and pruned that zero-days are becoming the only remaining way to get in.  Not exclusively, but to an increasing degree.  This means that the pressure to discover new zero-days is greater than ever before.  And since, as we know, security is inherently porous, the harder you press on it, the more results will be obtained.



Mandiant said:  "State-sponsored espionage groups continue to be the primary actors exploiting zero-day vulnerabilities, although the proportion of financially motivated actors deploying zero-day exploits is growing.  From 2014 through 2018," they said, "we observed only a small proportion of financially motivated actors exploiting zero-day vulnerabilities.  But by 2021, roughly one third of all identified actors exploiting zero-days were financially motivated.  We also noted new threat clusters exploit zero-days, but we do not yet have sufficient information about some of these clusters to assess their motivation."



Okay.  So just to be clear about that, the primary motivation behind the use of zero-days has historically been state-sponsored espionage.  Things like breaking into military contractors' networks to steal plans for future weapons systems that are still on the drawing boards and things like that.  But while such espionage remains dominant by far even now, by two times more, last year saw the rise in zero-days being the enabling factors of so-called "financially motivated" extortion with ransomware and sensitive data exfiltration and threats of exposure being the post-zero day intrusion consequences.



And Chinese-based cyberespionage groups remain the number one exploiter of these vulnerabilities.  Mandiant said:  "Mandiant identified the highest volume of zero-days exploited by suspected Chinese cyberespionage groups in 2021, and espionage actors from at least Russia and North Korea actively exploited zero-days last year.  From 2012 through 2021, China exploited more zero-days than any other nation.  However, we observed an increase in the number of nations likely exploiting zero-days, particularly over the last several years, and at least 10 separate countries likely exploited zero-days since 2012.  From January to March of last year, 2021, Mandiant observed multiple Chinese espionage activity clusters exploiting four zero-day Exchange Server vulnerabilities collectively known as the ProxyLogon vulnerabilities.  Microsoft described activity linked to this campaign as 'Hafnium.'"



So I'll just note that we appear to be focused upon the right things on this podcast.  All of our listeners will recall the attention we gave to the constant stumbling Microsoft was making over their seemingly endless Exchange Server ProxyLogon vulnerabilities.  They just couldn't seem to get it right.  What we didn't and couldn't know at the time was that that string of Microsoft missteps was actually translating directly into a string of exploitation with Chinese espionage actors at the other end.  They noted that:  "While some of the threat clusters involved appeared to carefully select targets, other clusters compromised tens of thousands of servers" - that is, Exchange Servers - "in virtually every vertical and region."



And of course that makes sense; right?  No one entity owns these vulnerabilities, and we have a heterogeneous environment of uncoordinated groups in China, Russia, and North Korea.  Some are going to go for high-volume spray attacks, whereas others are going to go after specific targets.



And this little tidbit was somewhat worrisome.  Mandiant said:  "Chinese cyberespionage operations in 2020 and 2021" - so just the most recent two years - "suggest that Beijing is no longer deterred by formal government statements and indictments from victimized countries.  In addition to the resurgence of previously dormant cyberespionage groups indicted by the U.S. Department of Justice, Chinese espionage groups have become increasingly brash."



The problem is that the world, I think, is becoming inured to the whole concept of cyberespionage, cybercrime, and cyberattacks.  As the years go by and we keep talking about them, it's just human nature that they're going to become less and less frightening and exceptional.  They will simply be incorporated into our expectations.  Where previously they were a big deal, it'll be just like, uh, okay, fine.  You know, sort of like DDoS attacks are.  It's like, oh, yeah, that happens.



As for Russia, Mandiant noted that:  "In a sharp departure since 2016 and 2017," they wrote, "we did not identify any zero-days exploited by Russian GRU-sponsored APT28" - that's, you know, Fancy Bear - "until they likely exploited a zero-day in Microsoft Excel in late 2021.  However, open source reporting indicated that other Russian state-sponsored actors exploited several zero-days in 2020 and 2021, including possibly targeting critical infrastructure networks with a zero-day in a Sophos firewall product."  And as we know, through the past four years, Mandiant said that they had noted a significant increase in the number of zero-days leveraged by groups that are known or suspected to be customers of private companies that supply offensive cyber tools and services.  And we know who those guys are.



They said:  "We identified at least six zero-day vulnerabilities actively exploited in 2021, potentially by customers of malware vendors, including one reportedly exploited in tools developed by two separate vendors.  In 2021 at least five zero-day vulnerabilities were reportedly exploited by an Israeli commercial vendor."  Well, those two separate vendors they're referring to were the well-known Israeli NSO group and a second smaller and lesser well-known vendor of very similar exploit capabilities known as "QuaDream."  Like the NSO Group, QuaDream is also Israeli and competes in the same market as the NSO Group, primarily selling to government clients.



Mandiant also noted that, unlike in the past, zero-day exploits were no longer appearing in underground exploit kits.  Now, that's interesting.  They explained:  "Since 2015, we observed a sharp decline in zero-day vulnerabilities included in criminal exploit kits, likely due to several factors including the arrests of prominent exploit developers. However, as the criminal underground coalesced around ransomware operations, we observed an uptick in ransomware infections exploiting zero-day vulnerabilities since 2019.  This trend may indicate that these sophisticated ransomware groups are beginning to recruit or purchase the requisite skills to exploit zero-days that may have been formerly developed for exploit kits."  In other words, an exploit kit would have just been - it would have included zero-days because they exist.  That would have essentially been tantamount to giving them away.  Well, why give them away if there's now a powerful, active, profitable market for them?



They said:  "Mandiant has documented significant growth in ransomware in terms of both quantity and impact.  Substantial profits as well as the increasingly compartmentalized, outsourced, and professional ecosystem that supports ransomware may have provided operators with two viable pathways to zero-day exploit development and/or acquisition:  financial resources and actor sophistication."  In other words, ransomware operations increasingly have the money to purchase high-value, but also high-cost, zero-day exploits from underworld sources.  And those underworld sources increasingly have zero-day exploits to offer for sale.



So where are all these zero-days being found?  We have a chart in the show notes.  Mandiant says:  "We analyzed zero-days from 12 separate vendors in 2021, with vulnerabilities in Microsoft, Apple, and Google products comprising 75% of total zero-day vulnerabilities, likely as a result of the popularity of these products among enterprises and users across the globe."  So Microsoft with all of their many products, Apple with their family of iOS devices, and Google with Chrome and the Android platform.  Together, as this chart shows, those top three account for just a tad more than 75% of those 80 zero-days which occurred during 2021.  Microsoft has the most, though they also have the most hardware sprawl, so I guess it's not surprising.  Apple has the next most, with Google the fewest of the three.  And given the nature of Chrome and Android, that's pretty impressive, really.



There are nine other major and notable sources of zero-days finishing out that Top 12.  In order of decreasing zero-day counts, the remaining nine are:  Accellion, SonicWall, Apache, Qualcomm, Trend Micro, Adobe, the Linux Kernel, Pulse Secure, and SolarWinds.



Mandiant noted that:  "The threat from exploitation of these major providers remains significant" - meaning Microsoft/Apple/Google.  That's where the zero-days are.  That's the platforms that everyone's using, and that's where to really watch.  They said:  "In addition, we noted a growing variety in vendors being targeted, which can complicate patch prioritization and make it more difficult for organizations who can no longer focus on just one or two vendors as priorities."  Which is really interesting, meaning there's just more to patch now than there was before.



They said:  "From 2012 to 2017, Adobe was the second most exploited vendor, with nearly 20% of all zero-days exploiting" - wait for it - "Adobe Flash alone."



LEO:  Of course.



STEVE:  Yeah.  2012.  Remember those days?  "We observed," they said, "a significant drop in Adobe exploitation since then, almost certainly fueled by Flash's end of life."  Yeah, no kidding.  How many times did we lament the continued existence of Flash when it was so obviously obsolete while also being such a global menace?



So what's the future outlook for the world of zero-days? Mandiant says:  "We suggest that significant campaigns based on zero-day exploitation are increasingly accessible to a wider variety of state-sponsored and financially motivated actors, including as a result of the proliferation of vendors selling exploits and sophisticated ransomware operations potentially developing custom exploits."  In other words, zero-days are big business, and that business is currently seeing what can only be described as explosive growth.



As for what enterprises can do about this, Mandiant says:  "The marked increase in exploitation of zero-day vulnerabilities, particularly in 2021, expands the risk portfolio for organizations in nearly every industry sector and geography.  While exploitation peaked in 2021, there are indications that the pace of exploitation of new zero-days slowed in the latter half of the year.  However, zero-day exploitation is still occurring at an elevated rate compared to all previous years."



They said:  "Many organizations continue to struggle to effectively prioritize patching to minimize exploitation risks."  Again:  "Many organizations continue to struggle to effectively prioritize patching to minimize exploitation risks."  And we'll come back to that.  And remember that survey we talked about recently where CIOs and IT professionals confessed to just how bad their organizations truly were about applying patches in a timely fashion.



To this, Mandiant added:  "We believe it is important for organizations to build a defensive strategy that prioritizes the types of threats that are most likely to impact their environment and the threats that could cause the most damage, starting with the relatively few number of actively exploited vulnerabilities.  When organizations have a clear picture of the spectrum of threat actors, malware families, campaigns, and tactics that are most relevant to their organization, they can make more nuanced prioritization decisions when those threats are linked to active exploitation of vulnerabilities."



And, you know, okay.  That just seems so unrealistic to me.  I mean, in a perfect world, sure.  But we're talking about an organization dedicating someone to the full job of essentially continuously surveying the dynamic and constantly changing threat landscape and cross-checking it with all of the organization's potential vulnerabilities.  What organization is really going to do that?  The truth is that everyone in IT is overworked, and there's an awful lot of hoping for the best going on.  You know, hoping for the best was what that survey revealed; right?  It was like, well, nothing happened today, and it's quitting time.



So there's no argument that, all other things being equal, focus less upon theoretical problems and more on vulnerabilities that are actively being exploited makes sense.  Mandiant wrote:  "A lower risk vulnerability that is actively being exploited in the wild against your organization or similar organizations likely has a greater potential impact to you" - okay, yeah, no kidding - "than a vulnerability with a higher rating that is not actively being exploited."  Okay.  So thus CISA.



They said:  "A new CISA directive places a significant focus on those vulnerabilities that are reportedly actively exploited.  We believe this will help increase the security posture and strengthen patch management procedures."  Except as I said toward the beginning of the podcast, yeah, these things that are four years old, I don't know.  Cross-site scripting in a package that no one's ever heard of and is taking up space and sort of diluting the other more important things?



Anyway, they finish:  "While zero-day exploitation is expanding, malicious actors also continue to leverage known vulnerabilities, often soon after they've been disclosed.  Therefore, security may be improved by continuing to incorporate lessons from past targeting and an understanding of the standard window between disclosure and exploitation."  And of course we spend a lot of time here talking about that.  They said:  "Furthermore, even if an organization is unable to apply the mitigations before targeting occurs, it can still provide further insight into the urgency with which these systems need to be patched.  Delays in patching only compound the risk that an organization supporting unpatched or unmitigated software will be affected."  And again, yeah, obviously.



Having read all that and shared all that, and considering the impracticality, I think, of expending any great deal of time on prioritization, and also given that low-priority exploits are still exploitable, my own advice to any organization, especially in light of that survey we covered which confessed that patching was clearly not a priority, would be to first and foremost simply fix that.  Period.  Figure out how to arrange to keep the enterprise's software up to date.  Yes, systems need to be taken offline, updated, and rebooted.  Yes, it's inconvenient.  And yes, customers and employees and even upper management in the C-suites will complain.



But today's and tomorrow's reality is that last year the number of zero-day vulnerabilities which were being used in the wild exploded from 30 the year before to 80.  And those were only the worst of the crop.  There were a great many more than just those 80 zero-days; right?  I mean, non-zero-day vulnerabilities, many more vulnerabilities.  Microsoft themselves patched 128 vulnerabilities just two weeks ago.  It's only going to get worse.  So to me, I mean, I get Mandiant's position.  Yes, wouldn't it be nice if we, like, certainly you want to look at all the notices.  And if you see something which is a glaring collision between something that's just been patched and software that you know your organization relies on, and it's something that's exposed to the Internet, yeah, you know, shut it down.  Turn it off.  Patch it.



But in general, I think all the evidence we see says that shortly after patches are released, they are reverse engineered, and attacks begin.  So it is just a reality moving forward that there isn't an alternative to taking systems down routinely and updating them.  It has to happen.  And the survey that we talked about demonstrated that it hasn't been happening.  And, you know, I don't talk about the endless stream of ransomware attacks.  They're happening constantly, everywhere.  I mean, there's just no point in filling the podcast with them.  But they have not let up.  And it doesn't look like they're going to.



LEO:  Yeah.  So, I mean, some of this is just reporting bias; right?  Like we know more about it.  We're more aware of it.  Or do you really think there is an increase in zero-days?



STEVE:  I really do think things are getting more complicated.  Look at - the perfect example is UEFI.  Before we had it, you couldn't infect it.  You know?  Now we have it, and you can infect it.



LEO:  Yeah.



STEVE:  So we are seeing increased complexity.  We're also seeing an increasing use of this toolkit approach, right, where the so-called supply chain attacks.  Well, if you don't have a supply chain, you can't attack it.  If you're writing stuff in-house, you know how it works.  If you're grabbing modules off the NPM repository and dropping them in, then you're making mistakes.



LEO:  Yeah, yeah.



STEVE:  Because you didn't write it, and you don't know how it works.



LEO:  That's true.  I mean, our software is more complex than ever before, yeah.



STEVE:  Yes.  It is doing more than ever before.



LEO:  Right, right, right.



STEVE:  But that means it's going to be more complicated.



LEO:  Yeah, yeah, yeah, that makes sense.



STEVE:  And there is more pressure.  You know, remember how many years, Leo, did you and I sit here thinking, well, isn't that an interesting virus.  It's just moving around from place to place, and it doesn't do anything.



LEO:  Right.



STEVE:  Well, weren't those days quaint.



LEO:  There's also nation-state actors, which is a whole new player in this field; you know?



STEVE:  Yes.  That's right.  Yes.  The fact that Chinese espionage is like where more than two thirds of these things are being leveraged, that should be sobering.  And also the idea that there's sort of a laissez-faire attitude.  I mean, again, that's human nature, too.  It's like, oh, well, my computer works.  Good luck.



LEO:  Yeah, mm-hmm.  Well, you don't need luck if you listen to this show, and that's the truth.  Steve Gibson is the man of the hour.  Every Tuesday, 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC, we gather together to talk about the state of security in this world of ours.  And no surprise, it's getting more tenuous.  It's really a show you have to listen to every single Tuesday.  You can watch us do it live, if you want, at live.twit.tv.  If you're watching live, chat live at irc.twit.tv; or in our Club TWiT Discord.



After the fact, of course, we've got other ways you can chat with us, including our TWiT forums at TWiT.community.  Our Mastodon instance now proudly not owned by Elon Musk at TWiT.social.  And of course Steve's got his own forums at GRC.com, including very active forums discussing SpinRite, the world's best mass storage maintenance and recovery utility.  6.1 is coming.  So go over to GRC.com and get a copy of 6.0.  You'll get a free copy of 6.1.  And you can participate in these final stages of development.



While you're there, do get a copy of this show.  Steve has two unique formats, 16Kb audio for the bandwidth-impaired.  He's also got really useful transcripts written by a real human, Elaine Farris.  So she writes everything down.  And that means you can read along as you listen.  You can search the transcripts to find a part of the show.  I remember he said something about this.  You can go right there.  That is a very, very handy feature.  And that's only at GRC.com.



We have copies of the show at our website, TWiT.tv/sn.  There's a YouTube channel devoted to Security Now! with all the videos of all the shows that have video, anyway.  And of course you can subscribe.  If you want to get all the new shows the minute they're available, just subscribe in your favorite podcast player, and you'll get them pretty darn quick, audio or video, your choice.  That's just Security Now!, search for it, or search for TWiT on your podcast player.  You should find that easily since we have been around for, what is it, 17 years?



STEVE:  Coming up on 18.



LEO:  Yeah, wow.



STEVE:  Yup.



LEO:  Let's see.  What else?  Oh, if you would, leave us a five-star rating.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#869

DATE:		May 3, 2022

TITLE:		Global Privacy Control

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-869.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're going to examine the success of the abbreviation overloaded DoD's DIB-VDP pilot program.  We're going to introduce the relatively new OpenSSF - Open Source Security Foundation - and its Package Analysis Project.  We're going to look at some hopeful new privacy legislation recently passed in Connecticut's house which if signed into law would cause it to join four other privacy-progressive states, and we're going to look at Moxie Marlinspike's irreverent rationale for the need for port knocking.  Then, after sharing some interesting listener feedback, we're going to look at the background, implementation and future of a very encouraging development in user web browser and Internet privacy.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with an acronym soup, including DoD's new DIB-VDP program.  We're going to talk about OpenSSF, and then a simple way to tell the world you don't want to be tracked.  And this time it really works.  You can stay tuned, I hope, for this one.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 869, recorded Tuesday, May 3rd, 2022:  Global Privacy Control.



It's time for Security Now!, the show every Tuesday that answers the vital question, what the heck?  And here's the man with the answers to all your questions, Steve Gibson.  What could possibly go wrong is actually the...



STEVE GIBSON:  What could possibly, yeah, that really is our tag line.



LEO:  That's the question you really, yeah, you really do answer.  Steve from GRC.com.  Hello, Steve.



STEVE:  So I have sort of a blast from the past, some good news.  This is Security Now! Episode 869 for this is our first episode of May 2022 titled "Global Privacy Control."  And the subtitle is "Hope Springs Eternal."  I'm hopeful.  That's my nature.  I've always been hopeful.  I refuse not to be hopeful.  Why not have hope?  Anyway, rather than a ton of little topics, the way it turned out, we have a number of bigger topics.  We're going to examine the success of the abbreviation-overloaded DoD's DIB-VDP pilot program.  Boy, the government loves their initials.



We're going to introduce the relatively new OpenSSF, the Open Source Security Foundation, with the introduction of their first project, which they call the Package Analysis Project.  We're going to look at some hopeful new privacy legislation recently passed in Connecticut's house which, if signed into law, would allow it to join four other existing privacy-progressive states, among which California and Colorado number.  And now they're being called - they're being considered the Three C's:  Connecticut, California, and Colorado.  And we're going to talk about that.



We're also going to look - I got a kick out of this.  Someone must have tweeted this to me.  Moxie Marlinspike, who, of course, famous for being the lead designer of the Signal messaging platform, he developed his own port knocking solution a decade ago.  And I just, I loved - he asked himself at the end of his page on GitHub about this, like why do you even need this?  And I know that everyone will get a kick out of his answer.  We've also got a bunch of listener feedback.  And then we're going to talk about what Global Privacy Control is and why this time might be different.  So I think another great podcast for our listeners.



LEO:  Excellent.  Very excited.  I didn't even ask, Steve.  Do we have a Picture of the Week?



STEVE:  We do, as a matter of fact.



LEO:  Oh, good.



STEVE:  It'll be a bit of a surprise.



LEO:  All right.



STEVE:  One of our listeners posted this in the SpinRite development group, but I just got a kick out of it and thought...



LEO:  Oh, it's really good news.



STEVE:  It's SpinRite running on a MacBook.



LEO:  Wow.  Wow.  Look at this.  Holy cow.



STEVE:  Yeah.  And the other cool thing is I remember a long time ago guessing that I thought that the new SpinRite would be so fast that it could do - I think I had said half a terabyte per hour was my estimate of its speed.  As we can see from the screen here, and he's just run the benchmark where we're benchmarking the front, the middle, and the end of the drive, and we can see that this 250GB drive in the MacBook would take 7.9 minutes for SpinRite to scan.  So four times that is about 30.  So that would mean 30 minutes to do a terabyte.  In other words, we're doing two terabytes per hour rather than half a terabyte per hour.  So it really becomes far more feasible to run SpinRite on today's really massive drives.



LEO:  How are you running on a MacBook?  Do you have to boot into FreeDOS or...



STEVE:  You do need to have Boot Camp.



LEO:  Ah.



STEVE:  So it uses Boot Camp.



LEO:  Ah, okay.  So not the modern MacBooks, unfortunately.



STEVE:  Exactly.  Not the most recent MacBooks.  It did remove Boot Camp.  And you can see in the picture he's got a thumb drive plugged into the USB there in the back right of the keyboard.  And so he intercepted the boot and booted FreeDOS.  So Intel only, and you do need Boot Camp.  So certainly on all of the older MacBooks it'll run perfectly.  So anyway, just a very cool little picture.  Caught me by surprise.  I said, hey, cool, because I've actually run it on my MacBook years ago when I diddled the keyboard.  The reason it wouldn't ever run before was that once SpinRite starts going, it directly polls the keyboard hardware.



Well, PCs have keyboard hardware.  Macs don't.  They have a USB keyboard which interfaces and sort of emulates the BIOS.  But SpinRite wasn't - it was like pushing past the emulation so you couldn't run it, even though everything else worked about it, it just wouldn't, you know, the keyboard died after you started up SpinRite.  So I've made sure that won't happen in the future.  And it's, wow, it's going to be so much faster.



Okay.  The DoD's DIB-VDP Pilot...



LEO:  M-O-U-S-E, yes.



STEVE:  Yeah.  So the program of the DoD - that's our U.S. Department of Defense - was named the "DIB-VDP Pilot Program."  And despite suffering from program name abbreviation overload, it just successfully finished its year-long pilot test.  The DIB stands for Defense Industrial Base.  VDP is Vulnerability Disclosure Program, thus DIB-VDP.  It was a 12-month voluntary event established collaboratively by DC3's - oh, that's the - no.  Oh, DC3's, no, that's DCSA.  Anyway, it's here somewhere.  Oh, Defense Cyber Crime Center is the DC3.  Oh, my god.



So established collaboratively by DC3's, that's the Defense Cyber Crime Center, DoD Defense Industrial Base Collaborative Information Sharing Environment, and I kid you not, that's the DCISE.  So, oh, and the DoD Vulnerability Disclosure Program, thus the DoD VDP, and the Defense Counterintelligence and Security Agency, the DCSA.  Right.



That year-long bug bounty program scrutinized a fraction, and this is key because we're going to see what a fraction it was, a fraction of the U.S.'s massive defense industrial base.  Yesterday, on Monday, the effort's organizers announced that more than 400 valid vulnerabilities had been uncovered.



So during the 12-month pilot program, nearly 300 vulnerability security researchers from HackerOne - that's where they were sourced because you had to be vetted.  And we talked about this a year ago, that this was going to start, and you had to be vetted, you just couldn't come wandering in and attack the U.S. government and hope to get away with it.  So HackerOne was the coordinator. Those nearly 300 vulnerability researchers filed 1,015 reports during their examination of the networks of individual participating defense contractors to the DoD.  As a result of their scrutiny, those 401 vulnerabilities were deemed actionable and required remediation.



The pilot effort was run in coordination with the, oh, my god, the Defense Cyber Crime Center, that's the DC3; and the Defense Counterintelligence and Security Agency, the DCSA.  The program began with 14 companies and 141 public-facing assets.  That is to say, those were the targets.  Fourteen companies with 141 public-facing assets were initially offered to these HackerOne hackers.  And over the course of the year that effort was expanded to include 41 entities and 348 systems.



Now, the point of this is, this is minuscule.  What should give the program managers pause is that 401 actionable vulnerabilities were found in those 348 systems belonging to 41 entities, yet somewhere between 100,000 and 300,000 companies contract directly with the Pentagon and its components.  So like I said, you know, literally, so more than actionable vulnerability found per system on average, and about 10 per entity that contributed and participated voluntarily in the program.  And we have a minimum, and I'm not sure why the number that was quoted was so large, I mean, to say between 100,000 and 300,000 is like, okay, well, how can it be a three-to-one, give or take?  But anyway, that's the U.S. government for you.  So this legitimizes the long-running worry - which, yeah - about the latent digital vulnerabilities of the firms that make up the Department of Defense's supply chain, which has, as we know, been rocked by a number of major breaches over the years.



For example, one of the most notorious incidents, which I referred to sort of in passing last week, occurred back in 2009 when suspected Chinese hackers broke into one of the contractors working on the F-35 Joint Strike Fighter, which as we know is the most expensive weapons platform ever in U.S. history, and made off with its design data.  Whoops.  So Melissa Vice, the interim director of the DoD Vulnerability Disclosure Program, so she was directing this effort, said:  "The pilot bug bounty, which concluded at the end of April, intended to identify whether similar critical and high-severity vulnerabilities existed on small to medium cleared and non-cleared DIB - that's Defense Industrial Base, so that's collectively how these companies, these contractors are known - company assets with potential risks for critical infrastructure and the U.S. supply chain."  That was Melissa saying that.



So armed with this data, an analysis of the DIB vulnerability report management network, which is yet another abbreviation, will now take place in order to document the pilot program's lessons learned and inform the way forward for a fully funded program.  Alex Rice, the co-founder and CTO of HackerOne, who as I said was the organizing, you know, HackerOne provided the hackers, he said:  "With CISA now mandating vulnerability disclosure for government agencies and federal contractors, the DIB-VDP takes the practice a leap forward by demonstrating the efficacy of Vulnerability Disclosure Programs in the real world."



So the fiscal 2022 defense policy bill required the DoD to assess the feasibility of allowing a threat-hunting program on the defense industrial base to weed out foreign hackers and study the possibility of establishing a threat information sharing program for what has become a sprawling DoD enterprise.  Yeah, if you've got 300,000, well, 100,000 or 300,000, you're not really sure, you know, subcontractors, that's sprawling.  Earlier this year the DoD's CIO, John Sherman, said the Pentagon was "getting rolling on both of those" examinations, which, as I said, the fiscal 2022 defense policy bill required that they do this, so they did.  Now we've seen, and the bureaucracy has witnessed, the clear feasibility of these programs.



And when you stop to think about it, this approach makes so much sense.  No sane and talented white hat hacker is going to poke at government networks and systems at the risk of triggering a law enforcement action against them.  Right?  We've seen exactly that happen in the past.  So that must be done within the bounds of a sanctioned bug hunting program.  And similarly, no sane and talented white hat hacker, no matter how patriotic, is going to invest their valuable time doing such poking for free, when they could be potentially earning a bounty from poking at some private enterprise's networks and systems that pays a bounty.



So the government needs to pay.  And no private enterprise has as much money lying around as the government, or a government, so paying is not a problem.  So it really ought to be that fully sanctioned government bug bounty programs which allow hackers to earn bounties which are competitive with those in the private sector become a permanent reality in the future.  Successes such as we're seeing here, and what we saw during this DoD DIB-VDP  though they really do need a sexier name for it  move us another step in that direction.



So this was all good.  And a lot of people said, wait a minute, 10 vulnerabilities which needed action per contractor on average?  And we have how many hundreds of thousands of contractors?  So, yeah.



Okay.  We've never talked about the relatively recently formed OpenSSF.  Hard to pronounce, or say.  OpenSSF stands for Open Source Security Foundation.  It's a Linux Foundation project created in 2020 by a very large group of significant tech and financial firms with the charter to help steer, guide, and share open source security tools.  Which unfortunately are lacking.



Its membership is divided into two classes, Premier and General members, with the premier members in alphabetical order being 1Password, AWS, Atlassian, Cisco, Citi, Coinbase, Dell, Ericsson, Fidelity, GitHub, Google, Huawei, Intel, IBM, JFrog, JP Morgan, Meta, Microsoft, Morgan Stanley, Oracle, Red Hat, Snyk, Sonatype, VMware, and Wipro. And there are about three times as many general members, which I won't enumerate.  But many are equally notable, like there's Bloomberg, Comcast, Goldman Sachs, MongoDB, Spotify, SUSE, Tenable, Tencent, Carnegie Mellon, and Mitre.  So again, this is a heavyweight group.  I also saw in some reporting that Samsung was a member, but they weren't listed on the group's official page at openssf.org, by the way.



So that page, openssf.org, says:  "Open source software is pervasive in data centers, consumer devices, and applications.  Securing open source supply chains requires a combination of automated tooling, best practices, education, and collaboration."  And so they divide the effort that they're organizing into three components.  There's Working Groups, where they say:  "Collaborate on the planning, design, and delivery of security tooling and best practices that secure critical open source projects."  Then they have Town Halls:  "Stay informed about the latest happenings in open source security and engage with experts in our community."  And Training:  "Take free courses on secure coding practices as part of our Software Development Fundamentals Professional Certificate."



So all of that is nothing but good.  Again, it feels like it's coming along a little late in the game, but wow.  Better than never.  Okay.  So that's the OpenSSF.  We're talking about this previously undiscussed group today because last Thursday they announced the first results, which were a success, with some caveats that I'll discuss, of their project known as the Package Analysis Project.  I've got a link in the show notes for anyone who's interested.



To introduce it, they explain package analysis:  Scanning open source packages for malicious behavior.  And so they said:  "Today we're pleased to announce the initial prototype version of the Package Analysis project, an OpenSSF project addressing the challenge of identifying malicious packages in popular open source repositories."  And talk about something we've been talking about a lot recently, open source repositories.  They said:  "In just one month of analysis, the project identified more than 200 malicious packages uploaded to PyPI and npm."



Okay, so think about that for a minute.  Thanks to this project, 200 previously present but unknown malicious Python and JavaScript packages were found.  In a minute I'll share some of the details about what they found.  But their announcement continues, saying:  "The Package Analysis project seeks to understand the behavior and capabilities of packages available on open source repositories:  what files do they access, what addresses do they connect to, and what commands do they run?  The project also tracks changes in how packages behave over time, to identify when previously safe software begins acting suspiciously.  This effort is meant to improve the security of open source software by detecting malicious behavior, informing consumers selecting packages, and providing researchers with data about the ecosystem.  Though the project has been in development for a while, it has only recently become useful following extensive modifications based on initial experiences."



Okay.  So what they've built is a big heuristic engine which is capable of performing static code analysis, currently of packages residing in Python and JavaScript repositories. And, as is the case with heuristics, which we can think of as complex rules of thumb, tweaking and adjusting of weighting factors is typically necessary.  But if the signals they're able to obtain from their static code analysis are strong enough, robust determinations should be possible.



So they continue:  "The vast majority of the malicious packages we detected are dependency confusion and typosquatting attacks.  The packages we found," they said, "usually contain a simple script that runs during an install and calls home with a few details about the host.  These packages," they conjecture, "are most likely the work of security researchers looking for bug bounties, since most are not exfiltrating meaningful data except the name of the machine or a username, and they make no attempt to disguise their behavior.  Still, any one of these packages could have done far more to hurt the unfortunate victims who installed them, so Package Analysis provides a countermeasure to these kinds of attacks."



They said:  "There are lots of opportunities for involvement with this project."  So they're also soliciting from the community, and our listeners are part of that community.  They said:  "And we welcome anyone interested in contributing to the future goals of" - there are four of them - "detecting differences in package behavior over time, automating the processing of the Package Analysis results, storing the packages themselves as they're processed for long-term analysis, and improving the reliability of the pipeline."



They then finished:  "Check out our GitHub Project and Milestones for more opportunities, and feel free to get involved on the OpenSSF Slack.  This project is one of the efforts of the OpenSSF Securing Critical Projects Working Group.  You can also explore other OpenSSF projects like SLSA" - I didn't dig into what that was - "and Sigstore, which expand beyond the security of packages themselves to address package integrity across the supply chain."



So this is very welcome news.  As we know, for the past several weeks especially I've been lamenting the inherent lack of security of the systems that have been built to make sharing code libraries virtually effortless.  The trouble has been that polluting these code repositories is also effortless.  And recently we've seen this process even being automated with bots to further thwart code identification and its takedown.



I mentioned some specific examples from among the more than 200 malicious packages discovered during the first month of the system's use.  The first two attacks listed in their case study published on GitHub were against Discord clients and their users.  They explained that Discord attacks are focused on attacking Discord accounts or clients.  They said stolen accounts can be resold, used for fraud like cryptocurrency scams, or used to gain insider knowledge or secrets.



The first one that was over on PyPI is discordcmd.  And in their analysis results they said:  "This Python package will attack the desktop client for Discord on Windows.  It was found by spotting the unusual requests to raw.githubusercontent.com, Discord API, and ipinfo.io."  They said:  "First, it downloaded a backdoor from GitHub and installed it into the Discord electron client.  Next, it looked through various local databases for the user's Discord token.  It grabbed the data associated with the token from the Discord API and exfiltrated it back to a Discord server controlled by the attacker."  And what's so cool, of course, is that they found this with automated scanning heuristics.



The second Discord malware was over on npm, and it was titled "colorsss."  In their analysis result they said:  "Similar to discordcmd above, this npm package attempts to steal a Windows user's Discord account token and was discovered by identifying calls to the Discord API.  This package first searches through local browser databases for a token, queries the Discord server to discover details about the token, and exfiltrates the details to a Discord server controlled by the attacker."



They also during this first month of scanning discovered a couple of remote shells.  They explain, for those who don't know:  "A remote shell is used by an attacker to provide access to a command shell running on a target machine over the network.  They're usually 'reverse shells' that connect back to an attacker-controlled machine."



So we have over on npm, this one was roku-web-core/ajax.  And they explained:  "During install, this npm package exfiltrates details of the machine it's running on and then opens a reverse shell, allowing the remote execution of commands.  This package was discovered from its requests to an attacker-controlled address."



Another one in Python, over on PyPI, is secrevthree.  And the analysis says:  "This package opens a simple reverse shell when a specific module is imported from the library, allowing remote command execution.  It uses a simple obfuscation technique to make it harder for a reader to notice.  This package was discovered from the request to the attacker-controlled address."



And lastly, over on npm, this one was random-voucher-code-generator.  And they said:  "When the library is imported, it queries a server running on their Heroku cloud platform.  The server response includes a command and a flag indicating whether or not the command should be run.  In the response we observed, the command stopped a process managed by 'PM2.'  However," they said, "this same response can be changed to run any command the attacker wished to execute, including opening a reverse shell.  The library then uses voucher-code-generator to provide the advertised functionality."  So, you know, you get your voucher-code-generator.  But you also get a reverse shell that you probably didn't count on.  They said:  "This package was discovered from the unusual request to the Heroku server."



Their case studies page finishes by talking about dependency confusion and typosquatting, you know, where they just, you know, we've been talking about that recently where remember the dependency confusion.  They did note that packages typically used a high version number, so the package managers that don't know better see that a high version number is available, higher than the one they have, and say, oh, I'd better go get that.  And of course when they do they get themselves pwned.  So, not good.



As we've covered previously - oh, and I was just going to talk about exactly that, the use of high version numbers in order to trick package managers into obtaining it.



Google, who is an active participant in this effort, added a bit of additional background in their own blog posting.  They explained, they said:  "Despite open source software's essential role in all software built today, it's far too easy for bad actors to circulate malicious packages that attack the systems and users running that software.  Unlike mobile app stores that can scan for and reject malicious contributions" - and this Google speaking, we know how tough even that is for them - "package repositories," they write, "have limited resources to review the thousands of daily updates and must maintain an open model where anyone can freely contribute."  Thus one of the big problems.  "As a result, malicious packages like ua-parser-js" - which we actually had spoken of previously - "and node-ipc are regularly uploaded to popular repositories despite their best efforts, with sometimes devastating consequences for users.



"Google," they write, "a member of the Open Source Security Foundation, is proud to support the OpenSSF's Package Analysis project, which is a welcome step toward helping secure the open source packages we all depend upon.  The Package Analysis program performs dynamic analysis of all packages uploaded to popular open source repositories and catalogs the results in a BigQuery table.  By detecting malicious activities and alerting consumers to suspicious behavior before they select packages, this program contributes to a more secure software supply chain and greater trust in open source software.  The program also gives insight into the types of malicious packages that are most common at any given time, which can guide decisions about how to better protect the ecosystem."



Okay.  So this is all for the good.  The bad news is that this again puts us back into, I think, a pretty clear cat-and-mouse game.  Because the project is inherently open, the bad guys can learn what the system is keying on and simply arrange to avoid it.  How many times in those descriptions I just shared did we hear:  "This package was discovered from the request to the attacker controlled address."  Right?  Or another of those, several of those descriptions said:  "These dependency confusion attacks were discovered through the domains they used, such as burpcollaborator.net, pipedream.com, interact.sh, which are commonly used for reporting such attacks."



So the bad news is, once the bad guys learn that these simple factors, which did not previously interfere with their nefarious aims, have become problematic for them to use, they'll simply stop using such obvious targets by arranging to use non-obvious addresses, such as by proxying their connections through any of the gazillion compromised consumer routers on the Internet.  We've already discussed such readily available proxying being used to hide connections back to command-and-control servers.  In this case, it wasn't necessary to do that before.  Now it is, so it'll happen.



We already saw something similar with the adoption of package submission bots, which we were just talking about the other day, being used to create unique per-package-submission accounts.  This prevented casual linkage to other malware which had also been submitted by the same malefactor.  The underlying problem is that, at the moment, and as Google referred to, saying it was crucial and necessary, anyone is able to offer packages for submission.  There's no reputation system in place.  And the problem with reputation-based filtering is that, yes, it can be discriminatory, preventing valid unknown publishers who've not yet established a reputation from contributing their efforts.



So we still have some problems to solve.  Nevertheless, it is wonderful that the inherent value and power of open source software and its need for some form of security oversight is being formally recognized; and that we now have an OpenSSF, an  Open Source Security Foundation which will focus and work towards introducing improvements in this valuable ecosystem.  And again, if anybody is looking for something to contribute to, some way to apply their skills, I think this is great because the gotcha here in the whole OSS movement is that security is as big a problem there as it is for the closed source world.



LEO:  Hydration completion?



STEVE:  Yes.



LEO:  Yes.



STEVE:  Thank you.  So Connecticut, a state in the U.S. for those who are...



LEO:  I've heard of them, yeah.



STEVE:  ...not in the U.S.  It's difficult to spell, but once you get the trick it's okay.  The U.S. federal government is not moving forward on consumer privacy protections, so several states have taken matters into their own hands.



The state of Connecticut's General Assembly advanced a bill last Thursday that would offer residents a useful set of baseline privacy rights.  If signed into law by Connecticut's Governor Ned Lamont, Connecticut would join four other states which have already enacted similar legislation.  Those four are California, Colorado, Virginia, and Utah.



Connecticut's bill, which is their SB 6, which is named 



"Act Concerning Personal Data Privacy and Online Monitoring." If signed, and the governor's spokespeople are not committing but are not saying no, and it's expected to go, it's expected to pass, it would take effect July 1st of next year, 2023.  So not immediately even when signed.  And it closely resembles the privacy laws which have already been passed in Colorado, Virginia, and Utah in that it allows residents of those states to opt out of sales, targeted advertising, and profiling.  And after two years, by 2025, the law requires companies to acknowledge opt-out preference signals for targeted advertising and sales.



Now, websites and companies would be required to proactively obtain consent to process sensitive data, and need to offer Connecticut residents ways to revoke that consent.  And the law provides that organizations will have no more than 15 days to stop processing data as soon as consent is revoked.  Parental consent is needed for any website to collect personal data from children under the age of 13, but businesses are banned from collecting personal data and using targeted advertising on children between the ages of 13 and 16.



The bill forces companies to honor browser privacy signals like the Global Privacy Control, thus the title of this podcast, and we'll be talking about that here in a minute, so that consumers can opt out of data sales at all companies in a single step.  And as we know, Global Privacy Control - oh, yeah, I have in my notes Global Privacy Control is today's main topic.  So we'll be discussing it in detail shortly.



So Keir Lamont, the senior counsel at the Future of Privacy Forum, added that Connecticut's privacy bill goes beyond existing state privacy laws by "directly limiting the use of facial recognition technology, establishing default protections for adolescent data, and strengthening consumer choice, including through requiring recognition of many global opt-out signals.  Nevertheless," he says, "a federal privacy law remains necessary to ensure that all Americans are guaranteed strong baseline protections for the processing of their personal information."



Now, the law also joins California and Colorado in adding sunset clauses to the so-called "right to cure."  In other words, placing an end to that right which has been regarded and treated as a "get out of jail free" card.  This "right to cure" describes a process where companies are given a set amount of time to fix violations before enforcement action can be taken or lawsuits can be filed.  In other words, it basically allows them to bypass the law.  Its presence has been a hotly debated topic in many state assemblies across the country.  It's been the reason why several previous privacy bills have failed to pass.  Consumer Reports, which worked with Connecticut lawmakers on their bill, called the "right to cure" provisions in most privacy laws, they're the ones who coined the term, a "get out of jail free" card for companies violating consumer privacy.



However, Connecticut's right to cure provision sunsets on December 31st, 2024.  Colorado's provision sunsets the next day on January 1st, 2025.  But our, and I say "our," Leo, because you and I are both in California, California's sunsets the first of next year, January 1st, 2023, which puts teeth in this for California.  Once these sunset, the states will be able to take enforcement action against organizations that violate the law.



California, Colorado, and Connecticut are therefore being referred to as the "3Cs" because they all have useful sunset provisions.  A lawyer, David Stauss, who serves as chair for the Husch Blackwell law firm's Privacy and Cybersecurity Practice Group, said:  "Through joint enforcement, the 3Cs of state privacy law will be in a unique position to dictate the future of U.S. privacy."  Because as we know, for state laws, any business doing business in that state, which is to say all businesses, need to abide by the laws within that state.  So he said:  "Through joint enforcement, the 3Cs of state privacy law will be in a unique position to dictate the future of U.S. privacy, assuming the continuing absence of any overriding federal law.  But this will not be the case with Virginia and Utah, where 'right to cure' will endure."



So Connecticut's SB 6 also establishes a privacy working group to analyze a number of issues and provide a report by September 1st of this year.  And without federal support, it's all a bit of a sticky wicket.  Several states have spent years attempting to pass their own privacy laws due to the lack of any movement on privacy legislation at the federal level.  Calls for a federal privacy law have ramped up since the European Union's infamous GDPR, their General Data Protection Regulation, became enforceable in 2018, which has served as a model for similar laws in Japan, Brazil, South Korea, and elsewhere.



But New York, Texas, Washington, and dozens of other states have faced so far insurmountable pushback when attempting to move their own privacy legislation forward due to backlash from businesses that complain the bills will create a significant amount of extra work for effectively any business with a website.  Oh, boo hoo.  Right.  You have to pay attention to a signal, an opt-out signal from the browser, and then behave accordingly.  Gee. 



Anyway, the 3Cs - California, Colorado, Connecticut - won't care and won't need to.  Any web-based business with a customer in California, Colorado, and presumably Connecticut once this legislation passes, will need to get their digital act together soon  in the case of California, as I said, by the end of this year  or face legal challenges.  As we'll see, the best news is that the adoption of the new Global Privacy Control browser signal means that we're not all going to be needing to keep clicking something like "I want my privacy!" at every website we visit, unlike that horrible cookie mess that we're facing.



Okay.  Now, I ran across this, and I loved it.  This is Moxie.  We all know Moxie Marlinspike.  We've spoken of him often.  He's most famous for his hand in the development of Signal, which provides a secure technology for true end-to-end encryption of instant messages.  We did a podcast on it years ago.  If anyone is interested in how Signal works, of course WhatsApp is based on Signal.  Unlike Telegram, Signal employs standard, proven, and well-understood cryptographic primitives.  You can look at it and go, yeah, this is secure.  There's no hocus pocus, mumbo jumbo there.  It's just very clever.  And that's where that ratchet concept was introduced in order to allow secure messaging even in an asynchronous message-passing environment.



Well, it turns out that 10 years ago Moxie took his own stab at his own design and implementation of port knocking.  For anyone who's interested, it's on GitHub, and I've got the link in the show notes.  And I won't go into it in depth.  It's just another way to solve the same problem.  But what I loved, and the reason I'm mentioning it today, is that Moxie ended his page with a section titled "Why Is This Even Necessary?" which I wanted to share because it'll sound quite familiar.  In answer to that question, Moxie wrote:  "You are running network services with security vulnerabilities in them."  Period.  And then he said:  "Again, you are running network services with security vulnerabilities in them."



He said:  "If you're running a server, this is almost universally true.  Most software is complex.  It changes rapidly, and innovation tends to make it more complex.  It is going to be forever hopelessly insecure.  Even projects like OpenSSH that were designed from the ground up with security in mind, where every single line of code is written with security as a top priority, where countless people audit the changes that are made every day, even projects like this have suffered from remotely exploitable vulnerabilities.  If this is true, what hope do the network services that are written with different priorities have?"



He said:  "The name of the game is to isolate, compartmentalize, and expose running services as little as possible."  He says:  "That's where knockknock comes in."  That's the name of his solution.  "Given that your network services are insecure, you want to expose as few of them to the world as possible."  He finished, saying:  "I offer knockknock as a possible solution to minimize exposure."



So from the mouth of Moxie, there it is.  As I said, it should sound familiar to anybody who's tired of listening to me say the same thing on this podcast.  All of the evidence that we have, that we cover every week, proves this point over and over and over, exactly as Moxie stated.  So it's a reason why I'm a big fan, when you have fixed IPs on your endpoints, absolutely use IP-based firewall filters so that nobody anywhere else has any access to the ports which you have opened between two fixed locations.  And even if they're mostly fixed, you know, as I've said, if you've got a cable modem, that IP is not changing often, like nearly never.  So even there it's feasible to do IP-based point-to-point filtering.  And if you need to connect to clients that are going to have a floating IP, some form of port knocking, some form of out-of-band or kind of quasi in-band authentication in order to then give that IP and port temporary access, just it makes all the sense in the world.  And there are widely available tools for making it happen.  You just have to decide to.



Okay.  Some feedback from our listeners.  Dstacer wrote, from @dstacer, he said:  "How can a cryptocurrency wallet in a browser be a very bad idea, while LastPass and other browser-based password managers are a very good idea?"  You know, that's a really good question I had to think about a bit, about how to explain why I still feel that both of those apparently opposing evaluations can both be true at the same time.  I think it boils down to necessity and practicality.  It is not necessary to use a browser as a cryptocurrency wallet.  It would be far safer to perform a wallet's operations in a more secure container, in any other more secure container.  And it would be, and is, practical to do so.  Most people do.



I also agree that a browser is not as secure as I would wish it were for the storage of all of my  most precious static passwords in any password manager.  But it's pretty much necessary to integrate a password manager into our browser, and it would be far less practical not to have that integration.  If a password manager were not integrated, we would need to manually look up every site we were visiting, then display, copy, and paste our username and password for that site into the browser.  And it passes through the clipboard, which has been a problem in malware that monitors the contents of our clipboards.  We would also be at an increased risk from lookalike phishing domains, since we might be fooled by "paypol.com," whereas an integrated password manager would not be.



This podcast has well established that browsers are today's number one attack targets.  They are the attack surface which we all present to the Internet as we move about.  They're not perfectly secure.  They're not as secure as we could wish they were.  Yet we are not seeing reports of browser native password databases, nor third-party password manager extensions, being hacked. So it must be that, unlike the mistake we discussed last week that was made in the browser-based cryptocurrency wallet add-on, those who are authoring and maintaining both browser-native and third-party password managers are really taking their jobs and responsibilities seriously.  They may not be sleeping well at night, and I know if I was responsible for this I wouldn't be, but that's so that we can.



Alan Nevill tweeted:  "Hi, Steve.  Regarding SpinRite 6.1, I use a Raspberry Pi 4 as a NAS.  Will 6.1 be able to run on the Pi?"  No.  I replied to Alan, SpinRite is lovingly written in native x86 assembly language.  So it is, and has always been, intimately tied to the Intel x86 processors.  So there's no way that version 6.1, at least, of SpinRite will run on anything other than a system with an Intel processor.  The only way to run SpinRite on a mass storage device from a non-Intel platform will be to temporarily move the device to any Intel PC.  And of course the good news is that SpinRite 6.1 is so fast now that a great deal of testing and maintenance can be accomplished in very little time.  But, you know, never say never.  I don't know perfectly what the future of SpinRite is.  But for the foreseeable future at least, even 7 and 7.x and everything that I know of in the future, it's still going to be Intel-based.



Someone whose handle is @fizch, he tweeted:  "A few weeks ago, you mentioned a piece of networking equipment that you use for segmenting your network.  For the life of me, I cannot find that episode of Security Now!, nor do I remember what the device was.  Can you shed some light on this for me?"



And yes.  I recall pointing up over my shoulder at the little SG-1100 firewall router by Netgate which runs pfSense and can do everything you might ever want a little router to do.  Since it has separate physical LAN, WAN, and Auxiliary interfaces, it's perfect for creating an isolated IoT network whose devices can only see each other on the same sub-LAN, and they can see the Internet, but not across each other's network.  And if you just google "netgate sg-1100," it'll take you right there.  And literally it's what I'm using on my network right now.  Yup, there it is.  Neat little gizmo.  And it's a turnkey pfSense firewall, and pfSense will do anything.



Ben Hutton tweeted:  "@SGgrc," he says, "I'm a little behind, but in SN-856 you mention that you wouldn't want to be able to translate two-factor authentication generation data back into the original QR codes."  He said:  "But some apps allow for export and import.  Could you explain how this is different, please?"



Okay.  So a two-factor authenticator receives a QR code from a website to establish a two-factor identity for a user.  The site will generate and store a random secret key which it associates with a user's account. It will then display that random key in the form of a QR code.  The use of a QR code allows that key to be easily transmitted optically to an authenticator app on the user's smartphone using its camera.  If we didn't have QR codes or something similar, the site would need to have the user - it would have to display like the private key in hex or something, and have the user manually enter that key on the smartphone's touchscreen, which would be a nightmare.  So using a QR code for this is a huge convenience.  And at that moment, by the way, when the QR code is being displayed by the website, is when I press CTRL+P to capture and print that page for my own offline QR code authentication archive.



The reason I would rather use an authentication app that does not allow for private key export is that anyone who might be able to arrange to obtain brief unlocked access to my phone might export my collection of authenticator private keys, thus defeating all of the multifactor security I've come to rely upon.  The only valid reason to ever need to export an authenticator's private key repository would be to move them to another authenticator app, for instance.  But since I keep all of those keys printed on paper, I never need to export anything.  I only need to re-import my private key archive, which is in the form of a sheaf of printed papers, each containing one QR code.



Bruce Schneier's famous advice regarding passwords applies here.  Many years ago Bruce said:  "Do not memorize your passwords.  Write them down.  They should be so complex that you cannot possibly remember them all, and we're very good at managing bits of paper," Bruce said.  And note that printing these QR codes has the inherent advantage of moving the archive offline.  It's true that someone with physical access who knew where this little sheaf of pages was stored could obtain all my authenticator QR codes.  But that's not the threat model we're attempting to protect against.  And if it were, safety deposit boxes or a safe or any sort of physical lock box is a long-established means for protecting that kind of real-world asset.  The threat model we're attempting to thwart is online.  So moving their backup offline provides for added protection.



And one last point is that I understand that it's not as high-tech to print QR codes on paper.  But it's our technologies that often bite us in the butt.  Sometimes, especially where security matters, a bit of judiciously placed low tech is the more secure solution, even if it's less glamorous.



LEO:  And now, speaking of the possible or maybe the impossible, Global Privacy Control.  Hit it, Steve.



STEVE:  Okay.  So what is Global Privacy Control?  Believe it or not, it is what was clearly the right solution and what I was vocally arguing for as being obviously the right solution when its predecessor, known as Do Not Track, was first proposed 13 years ago, back in 2009.  Yes, during this podcast.  Leo, we've been around for a while.  We are finally going to get it.  It will have legally enforceable teeth, and I could not be more pleased.  I got a kick out of this.  This is what Wikipedia has to say about Do Not Track, and it's useful because it also reminds us of a few things that happened.



Wikipedia says:  "Do Not Track (DNT) is a no longer official HTTP header field, designed to allow Internet users to opt out of tracking by websites, which includes the collection of data regarding a user's activity across multiple distinct contexts and the retention, use, or sharing of data derived from that activity outside the context in which it occurred.



"The Do Not Track header was originally proposed in 2009 by researchers Christopher Soghoian, Sid Stamm, and Dan Kaminsky.  Mozilla Firefox became the first browser to implement the feature, while Internet Explorer, Apple's Safari, Opera, and Google Chrome all later added support.  Efforts to standardize Do Not Track by the W3C in the Tracking Preference Expression Working Group reached only the Candidate Recommendation stage and ended in September 2018 due to insufficient deployment and support.



"DNT," they write, "is not widely adopted by the industry, with companies citing the lack of legal mandates for its use, as well as unclear standards and guidelines for how websites are to interpret the header.  Thus, critics purport that it is not guaranteed that enabling DNT will actually have any effect at all.  The W3C disbanded its DNT working group in January 2019, citing insufficient support and adoption.  Apple discontinued support for DNT the following month, citing browser fingerprinting concerns."  Uh-huh.  "As of November 2021, Mozilla Firefox continues to support DNT.  In Firefox, the feature is turned on by default in private browsing mode and optional in regular mode.



"In 2020, a coalition of U.S.-based Internet companies announced the Global Privacy Control header that spiritually succeeds the Do Not Track header.  The creators hope that this new header will meet the definition of 'user-enabled global privacy controls' defined by the California Consumer Privacy Act (CCPA) and European General Data Protection Regulation (GDPR).  In this case, the new header would be automatically strengthened by existing laws, and companies would be required to honor it."



Okay.  So that was DNT.  What we will be getting is GPC.  If I go to the globalprivacycontrol.org website, so it's globalprivacycontrol.org, website with Chrome, the top of the page looks like, and I have a picture of it in the show notes, there's a red dot, and it says "GPC signal not detected."  And then, since this is the globalprivacycontrol.org site, it says "Please download a browser or extension that supports it."  But if I go to the same site with Firefox, after enabling Firefox's already present and built-in global privacy control settings, I'm greeted with a different banner at the top.  I get a green dot and the words "GPC signal detected."  So from a technology standpoint GPC has 100% of the elegant simplicity that made DNT such an obviously correct solution.



The specification's abstract reads - this is the abstract for GPC.  It reads:  "This document defines a signal, transmitted over HTTP and through the DOM" - that's one additional feature that we'll talk about in a second - "that conveys a person's request to websites and services to not sell or share their personal information with third parties.  This standard is intended to work with existing and upcoming legal frameworks that render such requests enforceable."



The specification's introduction explains.  They say:  "Building websites today often requires relying on services provided by businesses other than the one which a person chooses to interact with.  This result is a natural consequence of the increasing complexity of web technology and of the division of labor between different services.  While this architecture can be used in the service of better web experiences, it can also be abused to violate privacy."  Which perfectly contextualizes this issue.



"Several legal frameworks exist, and more are on the way, within which people have the right to request that their privacy be protected, including requests that their data not be sold or shared beyond the business with which they intend to interact.  Requiring that people manually express their rights for each and every site they visit is, however, impractical."



Okay.  Then they cite a chunk of the California Consumer Protection Act, which reads - this is from the CCPA.  "Given the ease and frequency by which personal information is collected and sold when a consumer visits a website, consumers should have a similarly easy ability to request to opt-out globally.  This regulation" - that is, the California Consumer Protection Act now in force and in law in California, and the get out of free jail card is sunsetting and ending at the end of this year - says:  "This regulation offers consumers a global choice to opt-out of the sale of personal information, as opposed to going website by website to make individual requests with each business each time they use a new browser or a new device."  So in other words, thank goodness we're not going to need to be doing something like clicking on "Yes, I'll accept your cookies," the equivalent of, wherever we go.



And then back to the specification for GPC, they said:  "This specification addresses the issue by providing a way to signal, through an HTTP header or the DOM, a person's assertion of their applicable rights to prevent selling data about them to third parties or sharing data with them.  This signal is equivalent, for example, to the 'global privacy control' in the CCPA regulations."  And I'll get to that in a second.



So as I said, the specification could not be more clear or easy to implement.  I have a sample header in the show notes.  Whereas Do Not Track was DNT: 1 to enable it, this one is Sec, probably short for security, Sec- then GPC: 1.  That's it.  Your browser, when it makes a query to any asset, it's going to say GET as the verb, or maybe POST, and then the URL.  Then there'll be a Host: header that tells the site which host it wants, so like example.com, and then Sec-GPC: 1 to say I am requesting of you the site associated with this query from my browser that you abide by these regulations.  So it defines and standardizes this new HTTP header, Sec-GPC, which when affirmatively set has the value of "1," meaning "true."



Okay, now, for a minute, recall the mess that ensued when Microsoft decided to have Internet Explorer 10 default to transmitting that DNT: 1 header.  Wikipedia reminds us of this. They wrote:  "With IE10, when using the Express settings upon installation, a Do Not Track option was enabled by default for IE10 and Win8.  Microsoft faced criticism for its decision to enable Do Not Track by default from advertising companies, who say that use of the Do Not Track header should be a choice made by the user and must not be automatically enabled.  The companies also said that this decision would violate the Digital Advertising Alliance's agreement with the U.S. government to honor a Do Not Track system because the coalition said it would only honor such a system if it were not enabled by default by web browsers.  A Microsoft spokesperson defended its decision, however, stating that users would prefer a web browser that automatically respected their privacy.



"On September 7, 2012, Roy Fielding, an author of the Do Not Track standard, committed a patch to the source code of the Apache HTTP Server, which would make the server explicitly ignore any use of the Do Not Track header by users of Internet Explorer 10.  Fielding wrote that Microsoft's decision 'deliberately violates' the Do Not Track specification because it 'does not protect anyone's privacy unless the recipients believe it was set by a real human being, with a real preference for privacy over personalization.'



"The Do Not Track specification did not explicitly mandate that the use of Do Not Track actually be a choice until after the feature was implemented in Internet Explorer 10.  According to Fielding, Microsoft knew its Do Not Track signals would be ignored, and that its goal was to effectively give the illusion of privacy while still catering to their own interests.  On October 9th of 2012" - so he patched it on September 11th.  On October 9th of 2012, just a little over one month later, Fielding's patch was commented out, restoring the previous correct behavior for Apache because nobody agreed with him.



"On April 3rd, 2015, Microsoft announced that starting with Windows 10, it would comply with the specification and no longer automatically enable Do Not Track as part of the operating system's Express default settings, but the company will 'provide customers with clear information on how to turn on this feature in the browser settings should they wish to."



Okay.  So to prevent any of this, the formal specification for the new Sec-GPC header clearly states, they said:  "A user agent MUST NOT generate a Sec-GPC header field if the person's Global Privacy Control preference is not enabled or defaulted to."  But they allow it to be defaulted to.  "A user agent MUST generate a Sec-GPC header field with a field value that is exactly the numeric character '1' if the user's Global Privacy Control preference is set.  A user agent MUST NOT generate more than one Sec-GPC in a given request and MUST NOT use a Sec-GPC field in an HTTP trailer."  They're rare.  Hardly anyone uses them.  We talked about them once before.



"A server processing an HTTP request that contains a Sec-GPC header MUST ignore it and process the request as if the header had not been specified unless the field value is exactly the character '1.'  If there are multiple Sec-GPC headers and at least one has a field value of '1,' then the server MUST treat the request as if there were only one Sec-GPC header with a field value of '1,' and as if there were none otherwise.  HTTP intermediaries MUST NOT remove a Sec-GPC header set to '1,' but they MAY remove Sec-GPC headers that contain other values.  Additionally, an HTTP intermediary that has reasons to believe that the person originating a given HTTP request has a do-not-sell-or-share preference MAY insert a Sec-GPC header set to '1.'"



So the specification also defines, as I noted, a new top-level JavaScript global value "globalPrivacyControl" which allows any script running in the browser to determine whether the query which loaded that script sent the Sec-GPC: 1 header to the server.  And the sample script that they had, they have some JavaScript.  They have an if statement with parens, and then they start with an exclamation point, bang, for negating.  So if (!navigator.globalPrivacyControl), and then in curly braces they have the comment "wonderful, we can sell this person's data!"  Meaning Global Privacy Control was not set in this browser or for this query.



Since the only thing that differentiates this from the earlier DNT effort is the promise of legal enforcement under the European Union's GDPR, and at least in California and soon Colorado and Connecticut, I was curious to look at that a bit more closely.  Wikipedia closes out their discussion by noting:  "GPC is a valid Do Not Sell My Personal Information signal according to the California Consumer Privacy Act, which stipulates that websites are legally required to respect a signal sent by users who want to opt-out of having their personal data sold.  In July of 2021, the California Attorney General clarified through an FAQ that under law, the Global Privacy Control signal must be honored."



Now, as a California resident, this made me curious to read the actual California legislation.  So I found it.  But don't worry, I'm not going to drag everyone through it.  I'll just note that I am completely satisfied with what the legalese says.  I'm not an attorney, but it was written in very clear and concise English which, to my untrained eye, leaves zero wiggle room this time.  They're not screwing around.  For anyone who's curious to see for themselves, I have a link to the 28-page PDF where, on page 17 - and this is just an excerpt from the entire legislation, so this is a small chunk of it.  That's why it's only 28 pages.  But on its page 17, paragraph 999.315 is titled:  "Requests to Opt-Out."  And anyone who's interested can follow that.



So it appears that we're about to get what we wanted and hoped for more than a decade ago.  If you're a Firefox user, bring up your browser's detailed configuration settings by entering "about:config" in the URL field, as we've talked about many times.  Then enter "privacy.global" as the search term to reduce those gazillion items that will come up to just two.  Then double-click on each to flip them from false to true, which will also make them bold.  Then head over to https://globalprivacycontrol.org, and at the top of that page you'll see that your Firefox browser is now, and hopefully forever, transmitting your legally enforceable assertion that you do not give your permission...



LEO:  It worked!



STEVE:  Yup, that you are affirmatively revoking permission and choosing to opt-out of any site that you visit's legal right to share your information with any other entity for any purpose whatsoever.



LEO:  And they will honor it.



STEVE:  And, well, as individuals we don't need to enforce that right since once California's "right to cure" provision sunsets at the end of this year, it seems clear that the biggest violators of consumers' asserted legal rights to privacy will be taken to task by California's Attorney General, who has clearly stated their intention to do exactly that.



Now, although Chrome is becoming increasingly conspicuous for not directly supporting the transmission of this GPC signal, plenty of Chrome-compatible add-ons and other browsers already do:  Abine, Brave, Disconnect, DuckDuckGo, OptMeowt and Privacy Badger.



Now, being a perennial minimalist, it would be nice if uBlock Origin were to add the GPC header for us since I already have uBlock Origin installed everywhere, including in Chrome.  But Gorhill was asked about six months ago whether or not he would consider doing so, and he declined on the basis that it was just another DNT fiasco.  That doesn't appear to be true, so there's some hope that in the future he might reconsider.  Although he is extremely curmudgeonly, so perhaps not.



LEO:  Highly unlikely.



STEVE:  Yeah.  For me, I'm liking the new add-on "OptMeowt" where the "u" of "out" is instead a "w," making it "meow," as in cat's meow.  So it's spelled O-P-T-M-E-O-W-T, OptMeowt.



LEO:  OptMeowt.



STEVE:  Yeah.  The reason is I like the way it looks, and it was developed by some guys who have been deep into the Global Privacy Control effort.  Their names are listed as contributors to the spec.  They're at Wesleyan University's privacy tech lab.  Wesleyan's a private college in Middletown, Connecticut, probably not coincidentally where this legislation is on the verge of making them the third C, with California and Colorado preceding.  OptMeowt is free and open source and hosted on GitHub.  It's about 2.5MB, so it's a relatively small download.  And their page at GitHub has a bunch of other privacy-related projects, for anyone who's interested.



It's available from the Chrome Web Store and supports all of the many Chromium-based browsers, so Chrome, Brave, Edge, Opera, and Vivaldi.  The Brave browser began testing default inclusion of the GPC header last October, so it's probably well in place by now.  And if you're a Brave user, just go to Global Privacy Control.  I expect you're already going to see a green light shining back at you without needing to add anything else.  So by all means, check to see whether your browser may already support it, like Firefox did, and I didn't know it and didn't have it turned on.  Now I do, and it is.



So if your browser doesn't have it, this little OptMeowt looks like a slick way to add it.  I've got a picture of its settings page here at the bottom of the show notes.  It's just got three buttons:  Enable, which sends Do Not Sell signals to every visited domain.  Or you can be selective if you like.  You can give it a Domain List which sends Do Not Sell signals according to the custom Domain List which you can provide.  Or you can Disable, does not send any Do Not Sell signals.  So it took us a long time to get here, Leo, but this is certainly encouraging.



LEO:  Yeah, yeah.



STEVE:  And, boy, this would just be great.



LEO:  Yeah, because what we've had to do is develop tools that block tracking fingerprinting and things like that because companies just, you know, they're not going to...



STEVE:  Right.  They want it.



LEO:  They want it.



STEVE:  Yeah.  If we don't tell them no, and if they don't suffer legal recourse in the event of ignoring what we're telling them, they're going to do it.



LEO:  Yeah.  That's awesome.  That's awesome.  So you think that thanks to Europe, Connecticut, and California, they're going to honor this because sites will have to.



STEVE:  Yes. 



LEO:  Great.  Good.



STEVE:  Yes.  Well, look what GDPR did to our cookies.  And it's a pain in the butt.



LEO:  I know.  Everybody does it.  Everybody does it.



STEVE:  Yes, exactly.



LEO:  We're all scared to death of them.



STEVE:  Yup.



LEO:  Even though it's a completely meaningless regulation.



STEVE:  And they will sue.  I mean, and it looks like California is saying, look, you're going to honor this law, or you're not going to do business in California.  Well, you can't not do business in California. 



LEO:  Right, right.  Wow, that's great.  Well, thank you, Steve.  As always, some great information.  And I am now going to go around to all my Firefoxes and do about:config and turn on those Global Privacy Controls.  There's two settings.  You just flip them to true.  And you're right, it says I'm green light now, which is amazing.  So another reason to like Firefox, although you said there are other browsers that do this.  Chrome is the only one that won't, probably.



STEVE:  Yeah, I think Brave is - well, now apparently they did.  They did support DNT at some point.  So I think they'll end up saying, okay, fine, you know, we're not happy, but...



LEO:  I hope so.  I hope so, yeah.



STEVE:  Yeah.



LEO:  Steve Gibson.



STEVE:  But Leo, that's big.  I mean, that shuts everything down.



LEO:  Yeah. 



STEVE:  I mean, it's really big.



LEO:  Yeah.  It's kind of stunning, yeah.  Wow.  Globalprivacycontrol.org is the place to go to learn more about it.  



STEVE:  Yup, yup.



LEO:  Steve Gibson, see, now you know why you have to be here Tuesdays.  You really do.  Listen to the show.  You don't have to listen to it live because you can get it of course downloaded from Steve's site.  He's got a couple of unique versions.  He has a 64Kb audio, that's the mainstream version.  But he also has a 16Kb audio file for the bandwidth-impaired, and transcripts, which make it very easy to search for something you're interested in.  You can even, correct me if I'm wrong, Steve, but I think you can search all the transcripts all at once with Google; right?  So you can find...



STEVE:  You can use Google.  And I also have a search box on GRC.



LEO:  Perfect.



STEVE:  And it knows, in order to constrain it just to Security Now!.



LEO:  So it'll get all of them, and you can find them.  "Did Steve ever talk about" - yes, he did, and it's right here.  That's great.  Great stuff.  GRC.com.  While you're there, pick up SpinRite.  We are inching closer, maybe even striding closer to version 6.1.  6.0 is current, though.  And if you buy it at GRC.com you will be freely upgraded to 6.1 as long as you buy it now.  You'll also get to participate in the upgrade to 6.1, so that's kind of fun.  There's a forum and all sorts of stuff.  GRC.com.  Steve's also on Twitter, if you want to leave him a comment, @SGgrc, and his DMs are open.  So slide on in, as the kids say. 



We have on-demand versions of the 64Kb audio and video, strangely enough, on our website, which is TWiT.tv/sn.  There's a YouTube channel full of videos.  And you could subscribe in your favorite podcast client.  That's another way to do it.  Steve has a link to the RSS, as do we.  Just paste that in, and then you'll get it the minute it's available.



If you do, if you are in a hurry, and you do want to watch it live, the freshest version of Security Now!, we do it on Tuesdays around 1:30 Pacific, 4:30 Eastern, 20:30 UTC at live.twit.tv.  There's live audio and video.  So you can listen at dinner or lunch, you know, and learn.  Listen and learn, we call it.  And if you are listening or watching live, please chat with us live at irc.twit.tv.  I think I've now plugged everything.



Well, one more thing.  There's I am told a few cabins left if you want to go with me and Paul Thurrott to Alaska in July on the Holland America Eurodam.  It's going to be a great cruise.  Lots of fun.  We're going to see, I'm told we're going to see bears, otters, bald eagles, as well as glaciers, what's left of them:  cruise.twit.tv.  Still some cabins left.  And I would like anybody who wants to go, to go.  They have inexpensive cabins, if you want.



STEVE:  How long are you away from TWiT?



LEO:  I'll just miss a week, so I'll miss one show, July 16th to the 23rd.  We're going to go early, I think Thursday we're going to go up to Seattle, spend some time there.  Mary Jo Foley tells about a good dive bar we can get wasted in, so that'll be fun.  Then we'll get on the boat all hung over.  I can't wait.  It's going to be a lot of fun:  cruise.twit.tv.  You do not have to be a club member to go.  By all means, everybody should join us.  I invited you.  But Steve said, "No, Lorrie won't let me go."



STEVE:  No.



LEO:  So you're stuck on shore.  We'll have to do a land tour for you, you and Lorrie.  We'll figure something out.  Thanks, Steve.  Have a great week, and we'll see you next time.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#870

DATE:		MAY 10, 2022	

TITLE:		THAT "PASSKEYS" THING

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-870.MP3

LENGTH:	108 MINUTES



DESCRIPTION:  This week we look at a patch to Android to thwart an actively exploited vulnerability.  We briefly revisit Connecticut's new privacy law, and we take a quick look at the raft of recent ransomware victims.  The U.S. State Department has added another ransomware group to its big bounty list, and we look at what's being called the biggest cybersecurity threat facing the U.S.  Meanwhile, the White House issues a memorandum about the threat from quantum computing, and we have the discovery of a new and pernicious DNS vulnerability that's unlikely to be fixed in our IoT devices.  And after looking at F5 Networks' new and quite serious troubles, we close the loop with some listener feedback, briefly discuss the past week of sci-fi news, then finish by looking at the past week's most tweeted-to-me question:  "What's that passkeys thing that Apple, Google, and Microsoft are adopting?"



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a patch for another Android zero-day actively exploited vulnerability.  We'll find out why President Biden says, "Yikes, quantum computing."  And we'll find out what this new FIDO initiative with Apple, Google, and Microsoft means.  Is it more secure?  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 870, recorded Tuesday, May 10th, 2022:  That Passkeys Thing.



It's time for Security Now!, the show where we cover your security, your privacy, your safety, how computers work, anything else Steve wants to talk about including science fiction.  Here he is, ladies and gentlemen, Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And we will have a little sci-fi segue since, if you did see it, as you said you were going to last night...



LEO:  I did.  We can talk about it, yes.



STEVE:  We will talk about that.  And today's main topic is a topic close to my heart.  I titled it "That Passkeys Thing" after the big announcement of what was World Password Day, it turns out, was not only that but Cinco de Mayo.



LEO:  Really.



STEVE:  Yes.  And this is sort of a non-announcement announcement.  I mean, it was okay.  And in fact, because as you and I were talking before we began recording, I sort of have a dog in this hunt because I spent seven years of my life designing a solution which FIDO is still working to solve.  I thought, well, I didn't want to get myself involved.  But it was the most tweeted thing of the past week.



LEO:  Well, yeah.



STEVE:  So I thought, well, okay.  And if I hadn't invested seven years in designing SQRL...



LEO:  You're an expert.



STEVE:  ...obviously I'd be talking about this.



LEO:  Right.



STEVE:  So I shouldn't allow that to dissuade me from doing so.



LEO:  Well, and you're an informed source.  I was saying on MacBreak Weekly earlier I really hope Steve fills us in on this.



STEVE:  Yeah.  So we're going to look at a patch to Android to thwart an actively exploited vulnerability.  We briefly revisit Connecticut's new privacy law.  And we take a quick look at the raft of recent ransomware victims - not in depth, I just kind of want to just say look at what's happening.  The U.S. State Department has added another ransomware group to its big bounty list.  And we look at what's being called the biggest cybersecurity threat facing the United States.  Meanwhile, the White House issues a memorandum about the threat from quantum computing, and we had the discovery of a new and pernicious DNS vulnerability that's unlikely to be fixed in our IoT devices.



And after looking at F5 Networks' new and quite serious troubles, we'll close the loop with some listener feedback, briefly discuss, or maybe not so briefly because I think you and I will have some things to talk about, the past week of sci-fi news.  Then we're going to finish by looking at the past week's most tweeted-to-me question, what's that passkeys thing that Apple, Google, and Microsoft are adopting?



LEO:  The new FIDO Alliance thing, yeah.



STEVE:  The new FIDO Alliance thing.



LEO:  I can't wait.



STEVE:  We do have a wonderful Picture of the Week thanks to somebody who knows that I appreciate what we would call physical humor.



LEO:  It's quite the head scratcher. 



STEVE:  It is.



LEO:  And now, the Picture of the Week, Steve.



STEVE:  So this is a puzzler.  For those who are not "seeing" the podcast or haven't seen the show notes, what we have is a roadway and a large wide sidewalk stretching across sort of a gorge.  And on the sidewalk, like to the side, is a bridge which is sort of the bridge to nowhere.  It goes up, and then it goes down.  But you don't have to take the bridge because it just parallels the walking path.  And Leo, while you were telling us about Zentry, I was thinking maybe this is of historical significance.  This roadway and pedestrian way, it feels like from what we can see on the left like it's bridging some gorge or a river or something.  Maybe once upon a time that little walkway was like down below, going across the river.  And they thought, well...



LEO:  Let's just save it, yeah.



STEVE:  For historical reasons, yeah.  



LEO:  Or maybe for the view.  It gives you a better view of the hole.  That may be it.  I don't know.  If that were the case you'd put it closer to the edge, though, wouldn't you.



STEVE:  It's not clear from any of the context of the photo where this is.  I can't tell, like, what country it's in.  But our listeners have quite a reach.  So I'm calling to our listeners.  You may know.  You may have walked past this bridge and thought to yourself, what?



LEO:  What the heck?  It looks like, on these road signs, looks like there might be Chinese.



STEVE:  Yeah, there are, like, distant road signs in the background.  So if anyone knows what this is, where it is, why it is, it would be interesting to find out.  And I will certainly share that with our - oh, you can almost read that, yeah.



LEO:  Almost see it.  It's either blurry or Chinese.  I can't tell.



STEVE:  Anyway, so I love the caption that came with this.  I tweaked it a little bit.  So here we have this bridge, right, that just, it doesn't really do anything.  You don't have to go up it.  You could if you want.  But you're just going to get back to where you were.  Anyway, so the caption reads, for our audience, I mean, for this topic:  "When you don't know what that code does, but you assume it must be important, so you just leave it alone."



LEO:  Yeah.  Don't take it out, yeah.



STEVE:  You know, like you might be afraid to remove this bridge, Leo, because it serves a mysterious purpose like, as you said, gazing into the hole from a better altitude.



LEO:  You'll find out, for sure.



STEVE:  Yeah.  So better just maybe give it a little bit of leeway and just say, well, okay, fine, we're not going to - you could change your mind.  Today I'll take the bridge; tomorrow I won't.  Either way, you end up at the same place.  Anyway, thank you to our listener who said, "Okay, Gibson, you're going to like this one."



Google updated Android to patch an actively exploited vulnerability, not for the first time.  This was just their monthly security patch release.  It fixed 37 flaws across various components.  One of them is a fix for an actively exploited Linux kernel vulnerability that came to light earlier this year.  It was a little odd that it took them as long as it did.  That vulnerability is CVE-2021, that's last year's CVE number, 22600.  It has a CVSS of 7.8, ranked as "high severity" because it could be exploited by a local user to escalate privileges or deny service.



And as we know, especially in the Android ecosystem, where it's not that difficult for malware to be running on your phone, right, I mean, like people download this crap from the Google Play Store all the time, saying, oh, look, it's going to improve your cell phone service and squeeze your memory down to give you more memory.  And when you're not looking, it's going to polish your shoes.  And someone says, hey, sounds like a good idea.  I want some of that.  And so they download it.  Well, the app is constrained by Linux's security rings; right?  Unless the app knows about this CVSS 7.8 high-severity exploit, which allows it to escalate its privileges to root and root around in your phone.



So the flaw was a double free vulnerability residing in the packet network protocol implementation in a Linux kernel.  And, you know, it's kernel-wide, so not really just Android.  It could cause memory corruption, potentially leading to at least denial of service or, if you're a clever hacker, execution of arbitrary code.  And as I said, it wasn't just Android that was vulnerable.  Patches were released by various Linux distros including Debian, Red Hat, SUSE, and Ubuntu, back in late 2021, in December, and also early 2022 in January.  And it's unclear why Google didn't patch this one sooner.  Maybe they just figured, well, as far as we know it's not being used.  So anyway, now it is, and so now they did.  They said:  "There are indications that CVE-2021-22600 may be under limited targeted exploitation."



So last month the vulnerability was added to CISA's Known Exploited Vulnerabilities Catalog due to evidence of its active exploitation in the wild.  Google also patched three other bugs in the kernel, as well as 18 other high-severity and also one critical-severity flaw which was in the MediaTek and Qualcomm components.  So, you know, update.  Actually a couple stories today we're going to have some fun with this issue of updating because, come on, folks.



Okay.  Connecticut's recently passed data privacy bill became law last Wednesday.  I talked about it last Tuesday.  I was incorrect in stating last week that Connecticut's Governor Ned Lamont would need to sign the recently passed legislation for it to become law.  That's normally the way it works.  It turns out that the state, Connecticut, has a rule that bills which have passed in the state assembly become law automatically five days after they're passed when during a legislative session.  So that seems like an expeditious thing to have.  We ought to all have one of those.



So consequently, Connecticut now joins California, Virginia, Colorado, and Utah to become the fifth state to create its own privacy law in lieu, and because the federal government isn't doing anything about this.  And there has been a specific reaffirmation that once that law has ramped up to full strength, that Global Privacy Control signal that we talked about last week, which will now, or soon, be sent by browsers, must be honored, and specifically without exception and without any further "are you sure" style prompting by anyone with whom Connecticut residents interact online.  So the point being that no one gets hassled with this like you are now with cookies.  If you're sending that GPC signal, the browser cannot by law challenge you about that.  



LEO:  Yay.



STEVE:  It just has to say, okay, darn, and go with it.



LEO:  By the way, web4849 in our chat room has discovered the location of this bridge to nowhere.  It is in Korea, the Dongan Bridge.  And in Seongnam City, and apparently is intended to not go anywhere.  When walking along the 72-meter-long bridge, passersby can choose to walk straight on or, if they "had a little more strength in their legs or a little more time," they could opt to cross over an overpass to get from one end to the other.  You could also observe from it the Pangyo Techno Park park.



STEVE:  Leo, you got it.  It's an observation point.  And when they call it an "overpass," we should note that it's not passing over anything.



LEO:  Nothing.  It's the sidewalk.  Yeah, apparently well known in Korea.  And thank you to one of our chatters, web4849.



STEVE:  Fantastic.



LEO:  Who took seconds, mere seconds.



STEVE:  I love the reach of our listeners.  Let me think of other things I don't know, and we could...



LEO:  Yes, exactly.



STEVE:  We could ask some other questions.  Okay.  I promise not to dwell at length, ad infinitum, on ransomware attacks.  But my concern is that in honoring that promise to the letter, we're downplaying it.  So I decided, okay, from time to time I'm just going to kind of give people a sense for it.  So, for example, Trinidad's largest supermarket chain was crippled by an attack.  The German library service is struggling to recover from a ransomware attack.  A major German wind farm operator confirms a cybersecurity incident, which was ransomware.  Austin Peay State University in the U.S. was hit with ransomware.  The ADA, that is our American Dental Association, confirmed a cyberattack after a ransomware group claimed credit.  Coca-Cola is investigating claims of a hack after a ransomware group was offering their stolen data for sale.  Whoops.



Conti ransomware has deeply crippled the systems of the electricity manager in a Costa Rican town, and the newly elected President of Costa Rica has since declared a state of emergency as a consequence.  The agricultural equipment maker AGCO has reported a ransomware attack.  A cyberattack has taken down the network at the State Bar of Georgia.  And classes have resumed at Michigan Community College after a ransomware attack, and classes at Kellogg Community College will be resuming tomorrow after two days of outages caused by a ransomware attack.  In Battle Creek, Michigan nearly 7,000 students were told last Monday, May 2nd, that ransomware had crippled its systems the previous Friday, April 29th.  The school was forced to shut down its main campus in Battle Creek as well as branches in Coldwater, Albion, and Hastings.  So there's your...



LEO:  Good lord.



STEVE:  ...snippet.



LEO:  What it tells you is it's so commonplace now that it's not worthy of note; right?



STEVE:  Yes, it's just like, it's bad and doesn't seem to be getting any better.  But so I just wanted to sort of like put a little punctuation on this, that because I'm not talking about it, it doesn't mean it isn't happening.



LEO:  Oh, no.



STEVE:  And that this isn't like a really big problem.  And which leads us to our next story, the U.S. State Department offering a $10 million reward for information about Conti members.  The  U.S. State Department has begun offering $10 million rewards - plural, so it's not just a first-come, first-serve, there'll be a line - for any information leading to the identification or location of people connected to the Conti ransomware gang.  And in addition, you can get an additional $5 million reward if any information you provide does lead to the arrest or conviction of a Conti member.  So a total of $15 million to anyone who can turn in a member of the Conti gang.



And, you know, this is U.S. dollars, 15 million of them.  You've got to think that this would make anybody associated with Conti quite uncomfortable.  There must be others outside of the immediate gang who are not themselves criminals so they don't face prosecution, but to whom members of Conti have bragged about, like, just over drinks or - I keep trying to say "pillow talk," but no.



So in a statement on Friday, State Department spokesman Ned Price told us something we already know, that Conti has been behind hundreds of ransomware attacks over the last several years.  He said:  "The FBI estimates that as of January 2022, there had been over 1,000 victims of attacks associated with Conti ransomware with victim payouts exceeding $150,000,000, making the Conti ransomware variant the costliest strain of ransomware ever documented."



The memo also notes that the group has recently claimed credit for that wide-ranging ransomware attack that targeted the government of Costa Rica as it was transitioning to a new president.  The attack crippled the country's customs and taxes platforms alongside several other government agencies.  And as I noted before, the attack also brought down one Costa Rican town's energy supplier.



Conti also attacked, as we documented at the time, Ireland's Health Service Executive a year ago, back in May of 2021, which resulted in weeks of disruption at the country's hospitals.  Ireland refused to pay the $20 million ransom and now estimates it may end up spending $100 million recovering from the attack.  Although as I recall, Leo, I think that was the one where they were like going to get all new computers as a result.  So it was like...



LEO:  Yeah, nice.



STEVE:  Maybe they're milking their insurance company a little harder than they...



LEO:  Who gets the old computers?  That's what I want to know.



STEVE:  The group similarly crippled dozens of hospitals in New Zealand, and the group has made a point of targeting U.S. healthcare and first responder networks - they're not nice, oh, and they're Russian, by the way - including law enforcement agencies, emergency medical services, 9-1-1 dispatch centers, and municipalities within the last year, so says the FBI.



The group has suffered a number of internal breaches over the years, the most notable of which occurred a few months ago, in February, after it expressed public support for Russia's, no surprise, invasion of Ukraine.  Within a few days of the message, the gang's internal Jabber, you know, XMPP server, which carried their private messaging channel, was hacked, and two years of the group's chat logs appeared on a new Twitter handle called @ContiLeaks.  The leaks revealed the group's inner workings and illustrated the way they chose their targets.  However, those leaks did nothing to slow the group down.  Last Wednesday they added New York-based architecture firm EYP to their list of victims.



So Conti now joins the ranks of those carrying a serious bounty on their heads.  Last November, the U.S. State Department offered a $10 million reward for any information that would lead to the identification and/or arrest of members of the DarkSide ransomware group with a similar bounty on the operators behind REvil, also the Sodinokibi malware.  So this is an interesting tactic that I have to imagine it would be effective.  U.S. dollars are valued globally still, fortunately.  And again, if nothing else, you have to imagine that this would weigh on the minds of anyone choosing to participate.  If they get in, they've got to be very circumspect within their own social sphere because here's the U.S. dangling $10 million just for turning you in.  And that's real money.



LEO:  So frustrating.  One of our chatters just sent me a link to a Bleeping Computer article about a college in Illinois, Lincoln College, one of the historically black colleges in rural Illinois, closing after 157 years.  It survived a major fire in 1912, the Spanish flu, the Great Depression, two World Wars, the 2008 global financial crisis.  But after two years of pandemic and finally getting hit by ransomware they decided to shut down after 157 years.



STEVE:  Wow.



LEO:  That is horrific and tragic.  Just tragic.  A cyberattack in December that thwarted admission activities, hindered access to all institutional data, creating an unclear picture of fall enrollment, this fall enrollment.  And so they're shutting down. 



STEVE:  Wow.  Wow.  Well, and we've covered when the ransomware was attacking the healthcare industry, it was clearly damaging the lives of people.  People were being hurt by the healthcare providers being offline.



LEO:  So, so sad.  Now they're going after schools.



STEVE:  Yup.  Yup.  Okay.  So what's the worst threat the U.S. faces?  It's from the Winnti Group, W-I-N-N-T-I, the Winnti Group, also known as APT 41.  APT, of course, is Advanced Persistent Threat.



Just how advanced and persistent are these threat actors?  Researchers with Cybereason recently briefed the FBI and the DoJ about Operation "CuckooBees"  funny name, not a funny operation.  This is an ongoing espionage effort by Chinese state-sponsored hackers with the charter to steal proprietary information from dozens of global defense, energy, biotech, aerospace, and pharmaceutical companies. The specific individual organizations affected were not named in Cybereason's report, but they allegedly include some of the largest companies in North America, Europe, and Asia. And the threat actor behind it all is the prolific Winnti Group, also known as APT 41.



Cybereason's CEO Lior Div said that the most alarming aspect of the investigation into Operation CuckooBees was the evasive and sophisticated measures used to hide inside the networks of dozens of the largest global manufacturing companies in North America, Europe, and Asia, dating as far back as 2019.  Lior said:  "The group operates like a guided missile; and once it locks onto its target, it attacks and doesn't stop until it steals a company's crown jewels.  Winnti pilfered thousands of gigabytes of data and, to add insult to injury, also made off with proprietary information on business units, customer and partner data, employee emails, and other personal information for use in blackmail or extortion schemes at a time of their choosing."



Cybereason said that throughout its 12-month investigation, it found the intruders took troves of intellectual property and sensitive proprietary data including formulas, source code, R&D documents, and blueprints, as well as diagrams of fighter jets, helicopters, missiles and more.  And remember, China.  The attackers also gained information that could be leveraged for use in future related cyberattacks, like details about a company's business units, network architecture, user accounts and credentials, employee emails, and customer data.  This group gets in your network, you're hosed, basically. 



And of greatest concern, according to Cybereason's CEO, was that the companies had no clue they had been breached.  In a pair of detailed reports, Cybereason attributes the attacks to Winnti based on an analysis of the digital artifacts the group left behind after its intrusions.  Several other cybersecurity companies have also been tracking Winnti since it first emerged 12 years ago in 2010, and researchers have observed that the hackers are clearly operating on behalf of Chinese state interests while specializing in cyberespionage and intellectual property theft.



The group used a previously unknown and undocumented malware strain called DEPLOYLOG, as well as new versions of malware like Spyder Loader, PRIVATELOG, and WINNKIT.  The malware included digitally signed, kernel-level rootkits, as well as an elaborate multistage infection chain that enabled the operation to remain undetected.  The group also managed to abuse the Windows Common Log File System (CLFS), which allowed the intruders to conceal their payloads and evade detection by traditional security products.  CLFS is a logging framework that was first introduced by Microsoft in Windows Server 2003 R2 and has been included in all subsequent Windows OSes.



Cybereason explained that:  "The attackers implemented a delicate 'house of cards' approach" - that was their term - "meaning that each component depends on the others to execute properly, making it very difficult to analyze each component separately."  And unsurprisingly, "Operation CuckooBees" generally took advantage of existing weaknesses including unpatched systems.



LEO:  [Grunting]



STEVE:  I know.



LEO:  [Grunting]



STEVE:  Yes, thank you.  Insufficient network segmentation, unmanaged assets, forgotten accounts, and lack of multifactor authentications.  In other words, stupid oversights.



LEO:  Yeah.  Easily remedied.  Easily.



STEVE:  Yes, exactly.  Cybereason said that the attackers generally obtained their initial foothold in the organizations through vulnerabilities in Enterprise Resource Planning (ERP) platforms.  Last month, FBI's director Christopher Wray told "60 Minutes" that the "biggest" threat American law enforcement officials face is from Chinese hackers stealing proprietary information.  He said that the FBI opens a new China counterintelligence investigation about every 12 hours.



LEO:  Oh, my god.



STEVE:  Think about that.  Every 12 hours another new Chinese counterintelligence investigation is opened by the FBI.  Wray said that:  "They are targeting our innovation, our trade secrets, our intellectual property on a scale that's unprecedented in history.  They have a bigger hacking program than that of every other major nation combined."



LEO:  Wow.



STEVE:  "They have stolen more of Americans' personal and corporate data than every other nation combined.  It affects everything from agriculture to aviation to high tech to healthcare, pretty much every sector of our economy.  Anything that makes an industry tick, they target."



LEO:  There is a difference, though, between this and the ransomware gangs we were talking about earlier, which are actually trying to steal from companies and bring them to their knees.  This sounds like it's intellectual property theft.



STEVE:  Yes.



LEO:  That the Chinese hackers aren't trying to destroy us or destroy our industry.  They're just trying to find out how we make things so they can make them cheaper, things like that.  And admittedly there's an economic consequence to that.  But that's not nearly as offensive as the Conti Group; right?  Or am I wrong?



STEVE:  Well, when you're digging into our military industrial complex.



LEO:  Well, if they go after the military, you're right, that's different because then that has...



STEVE:  And they're in there.



LEO:  Yeah.



STEVE:  That's where they're going.



LEO:  But if the military's getting hacked because they haven't patched and they're not using two-factor, we have other reasons to be upset.



STEVE:  And Leo, we have other reasons to be upset.



LEO:  Yes.  And we're going to get upset about something else in a minute, I'm sure.



STEVE:  We are.



LEO:  All right.  I'm ready to talk about quantum computing.  And in fact for a long time I've been wanting to ask you how - see, I look at this, and to me it looks like Google and everybody, it's just a way to get money from the feds.  Like they aren't really - are we close?



STEVE:  No, no.  Okay.  So this one made me shake my head.



LEO:  Okay.



STEVE:  The headline was "White House wants nation to prepare for cryptography-breaking quantum computers."  Okay.  To give everyone a sense for this, the reporting on this which appeared in The Record started out saying:  "A memorandum issued Wednesday by President Joe Biden orders federal agencies to ramp up preparations for the day when quantum computers are capable of breaking the public key cryptography currently used to secure digital systems around the world.  The document, National Security Memorandum 10" - so of course we have to have initials, right, NSM-10 - "calls for 'a whole-of-government and whole-of-society strategy'"...



LEO:  Oh, let me get right on that.  Whoa.



STEVE:  Yeah, huh?  "...for Quantum Information Science (QIS), including 'the security enhancements provided by quantum-resistant cryptography.'"  Uh-huh.



LEO:  Well, that's one reason I switched from RSA to, what is it, ESCD 50,000, whatever it is?  The elliptic curve?  I don't know what I'm doing.  But that's why I changed my SSH keys; right?



STEVE:  Right.  Well, so...



LEO:  Is that more quantum resistant?



STEVE:  I was looking at this, and I was thinking, okay, why don't we just cure cancer?  And then I thought, oh, wait.  That was what Biden was going to do while he was...



LEO:  Oh, we already did it.  Oh, good.



STEVE:  ...Barack's VP.  We going to take that, do that.  How'd that work out?  But seriously, okay.



LEO:  I did Ed25519.  Right?  That's the keys I should use; right?



STEVE:  Yes.



LEO:  Okay.



STEVE:  So he's ordering the federal government to ramp up preparations for a day when quantum computers are capable of breaking public key cryptography which, by the way, doesn't yet exist.  The federal government...



LEO:  Right.  And we don't know when it will exist; right?



STEVE:  No, no.



LEO:  It's like fusion energy.  Someday.



STEVE:  Well, actually that's in the notes, Leo, yes.  The federal government is apparently unable to update its own software when being handed patches to do so.



LEO:  That might be more important.



STEVE:  Someone somewhere says, eh, not today.  We haven't yet secured our computers for technology we already have against attackers we already have.  So I don't know.  How about having the White House use a memorandum ordering the various agencies in the federal government to please just reboot their computers.



LEO:  Even that would be a step forward.



STEVE:  How would that be?  You know?  We would actually get more security right now today if we did that.  And to your point, yes, sure.  Quantum computing technology shows promise.  But let's remember that it's been showing promise for quite some time.  We've had nascent quantum computing technology since around the late 1970s, so for more than four decades.  It's intriguing and interesting.  And it's been moving forward gradually.  You know, like most really big problems do, and like fusion power, exactly like that.  And the federal government should absolutely be funding ongoing research in universities to allow our nation's brightest young minds to continue pushing this frontier forward.  There's clearly something tantalizingly possible there.  And I agree that we should not forget that we have adversaries.  As we know, China is also working hard on this problem.  So Leo, if we patch and reboot our computers, we might be able to keep them from stealing it once we figure it out.



LEO:  Wow.  Wow.



STEVE:  But yes.  It absolutely is the case that at some point in the future - I think right now the most I saw was four cubits we were able to deal with.  I kind of have a hazy sense that I saw something about more.  But, I mean, it doesn't scale linearly.  Like if you get four, you can't simply say, oh, let's use 128 of those to get 512.  No.  They can't do that.  So we are so far away from, like, this actually being a threat.  So I just scratch my head.  It's like Joe Biden is issuing a memorandum telling us to scale up our preparedness, when we cannot reboot our servers?



LEO:  It's not a bad idea by itself, but it's not maybe the first thing we should do.  IBM says they've got a 127-cubit device.



STEVE:  Ah, that may be what it was that I remember. 



LEO:  Yeah.  IBM's selling these now.  $1.60 per runtime second on their 27-cubit Falcon r5 processor.  I mean, so maybe, maybe.  Maybe it's happening.  I don't know.  I don't know what you could do with 27 cubits.



STEVE:  No.  Basically you can absolutely simultaneously solve a symmetric crypto that uses an 8-bit key, I think, is what it comes down to.



LEO:  Okay.  So we've got to get to more than a thousand cubits before we're in trouble.



STEVE:  Yes.  We've got to get way...



LEO:  Am I right saying that if I did Ed25519 that...



STEVE:  Yes.  The technologies which do - so the concern is that the one thing which has been protecting us, the RSA crypto, is that we don't know how to factor big numbers.



LEO:  Right.



STEVE:  So the concern is a quantum computer could theoretically kill the factorization barrier.



LEO:  Right.



STEVE:  I mean, that was the trapdoor that you could only go through one way.  And the worry is that a quantum computer could just go, oh, you want to factor that?  Here.  I mean, like it doesn't even take any time.  Just say here.



LEO:  Here.



STEVE:  You know?  It's like...



LEO:  Here's the factors.



STEVE:  So we're wanting to get away from a crypto that is based on the multiplication of two primes that you're then unable to factor.  Well, elliptic curve crypto is that.  



LEO:  Oh, good.  Okay.



STEVE:  So you've got enough bits of elliptic curve crypto, and we're not worrying about factorization any longer.



LEO:  I think I'm doing 512.  No, 500 - it's weird, it's 521 bits for some reason.  I don't know what that means.  But like I said, I just put in the formula, that's all.



STEVE:  That's good.  Okay.  And that's what's so easy about this.



LEO:  Yeah. 



STEVE:  And this has been the point I'm making.  We have all the tools.  All the toolkits are there.  All the work has been done.  The academic guys have pounded on it, and they've said, "Here you go."  And so all it takes now is just plugging these in and using them correctly.  And unfortunately, well, and not making any bad mistakes, which continues to dog us.



Okay.  As I've often said, I am stunned by the elegance and fundamental simplicity of the Internet's design.  It was so beautifully conceived in the beginning.  But as we know, it's not without a few blemishes.  One of the original sins of the Internet's early design was, and still is, a lack of entropy in some fields which are critically important.  This entropy is crucially necessary for robust attack resistance.



And in defense of the Internet's early designers, the last thing they were thinking about while they were trying to get this whole thing to go was about active and aggressive adversaries.  They were trying to keep coincidental things from causing a problem, which is very different from preventing an active aggressor from leveraging their design to create mischief.  That wasn't on their map at all.  So they designed and built beautiful technology which has, against all odds, withstood decades of explosive growth being put to use in applications they could never have and didn't ever imagine.



But we've got a few problems.  For example, the endpoints of a TCP connection are identified only by an IP address and a port number.  And the progress of the connection's data flow is tracked by a 32-bit sequence number.  In the early days of this podcast we examined how the predictability of the sequence numbers being issued by TCP/IP software stacks could be weaponized and used by attackers to splice into existing TCP connections.  Since nothing identified the other endpoint other than its source IP address and source port, TCP packets carrying spoofed source IP and source port, and guessing a sequence number that would be expected by the receiving endpoint could  and did back then  succeed in injecting malicious traffic into established TCP connections.



Another quite famous lack of entropy may, and often does, exist in DNS queries.  Being UDP, the spoofing task is much easier.  If a DNS client emits a query to a DNS server having a knowable IP address, and if the 16-bit source port of its query and the 16-bit transaction ID are predictable, it's not difficult for an adversary to jam a bogus DNS reply back into that client which looks identical to the reply it's expecting to receive from the authentic DNS server.  And as we know, this form of DNS cache poisoning spoofing attack can have devastating consequences.  The IP being looked up will be altered, and traffic will be silently redirected.



It was the realization of that which hit Dan Kaminsky back in 2008, when nearly all DNS servers in the world were vulnerable to exactly that attack because their queries had very low effective entropy.  That caused the world to secretly prepare and then synchronize a simultaneous global update of all affected DNS servers.  It was a great example of global coordination.



But we missed something, something that today afflicts many IoT devices, such as routers by Linksys, Netgear, and those loaded with OpenWRT firmware, as well as Linux distributions like Embedded Gentoo.  This exposes many millions of IoT devices, right now today, once again, to this once-solved security threat.  What we missed, or at least weren't worrying about 14 years ago, was that it's not only DNS servers and our desktop operating systems which emit DNS queries.  They were all fixed.  But many other low-end IoT-ish devices also emit DNS queries, and we forgot about them.  Maybe it wasn't a big problem or concern back then.  But the crucial fact is, the lesson of the need to deeply randomize source port and transaction query IDs for DNS was not learned well enough.



Nozomi Networks Labs discovered a vulnerability, now being tracked as CVE-2022-30295, which affects the DNS implementation of all versions of uClibc and uClibc-ng which are very popular C standard library replacements used in many IoT products.  The C standard library is this big library which is very capable, highly cross-platform.  You can compile it just for about anything.  But it's huge.  And it does way more than a little IoT product with tight memory constraints needs.  And for example it supports memory mapping, which embedded IoT, there's a uC Linux that specifically doesn't have memory management  because a little embedded product isn't swapping stuff in and out of RAM.  So this uClibc and uClibc-ng are what embedded Linuxes use.  The flaw, which was found and fixed in all major DNS servers back in 2008, is the predictability of transaction IDs in the DNS requests generated by the library.



In the show notes I have a picture of the code from uClibc where we can see a variable local_id++;.  Well, that's a post-increment operation.  And since you're doing nothing but that on that particular instruction, it simply increments the value of local_id.  The next line reads local_id and then &=, with 0xffff.  Okay, so that masks and retains the lower 16 bits of the value local_id.



So essentially what it does is local_id is incremented, and that AND operation, then the logical AND of the lower 16 bits deals with the overflow when it tries to go to a 17th bit.  It discards that.  So we wrap from 65535 back around to zero because those transaction IDs are 16-bits long.  In other words, what this does of course is produce absolutely sequential DNS query transaction IDs.  The problem that took the wind and the breath out of the entire network world 14 years ago is present in this library, and apparently has been since its beginning.



While I was doing a bit of background research into this uClibc, I found that the original pre-forked uClibc's last update was May 15th of 2012, so 10 years ago exactly this coming Sunday.  Because this library had become unsupported, it was forked to create uClibc-ng.  Presumably "ng" stands for next generation.  The good news is that one's being actively maintained.  But under its home page's History section, talking about the history of uClibc-ng, it explains:  "uClibc-ng is a spin-off of uClibc from Erik Andersen," and then from http://www.uclibc.org.  "Our main goal is to provide regularly" - which in their write-up was misspelled - "a stable and tested release to make embedded system developers happy.



"The first release 1.0.0, with the code name Leffe Blonde, was made while visiting FOSDEM 2015.  It was prepared in a hotel room in Brussels on the first of February, 2015.  All releases are prepared while drinking a pair of Belgian beer since then."  Because you know, Leo, that's what you want in the replacement for the Standard C library that everyone's embedded IoT devices are using is for it to be maintained by a drunken Belgian.  "The idea to fork uClibc started in July 2014," they wrote, "and was discussed on the Buildroot and OpenWRT mailing lists."



Okay.  So we've identified a well-understood flaw that has been present in embedded Linux-based IoT devices which use the uClibc library or the uClibc-ng library for the past decade or so.  Apparently no one thought to look before now.  Now the world knows.  I'm pretty sure that the OpenWRT folks will get on this and fix it because that's who they are.  The fix, after all, is trivial.  The transaction ID sequence simply needs to be unpredictable. And it needs to be unpredictable per instance of that device booting; right?  You can't just like make a pseudo, like a fixed pseudorandom sequence because anyone can reverse engineer the firmware, see what the sequence is, and then go back to predictability.



An embedded device without a good source of local entropy, which a really low-end IoT electric plug, for example, might have, or might lack a good local source, could, as it's starting up, use something like high-resolution packet timings to obtain an unpredictable seed.  Send out some pings to some known static IPs and time their return at the device's full-resolution clock speed.  That will generate a value that's unknowable by any external attackers.  I'd then use that value to key a simple symmetric cipher which encrypts a sequential counter.  That will produce a per power-up, per boot unpredictable sequence that no one on the outside can know.



What we don't know is everywhere this embedded library has been used in embedded Linux systems.  We don't know whether Netgear and Linux, which both use it, will care to update.  And most importantly, where and how this flaw will surface in the future.  But the bad guys will make it their business to know because that knowledge is valuable to them.  And this is the legacy we're building which most worries me, the growing number of well-known problems that are accruing, mostly under the radar, and which are not being diligently fixed.  These things don't go away on their own.  They accumulate.



What's happening over time  and mark my words  is that one of the favorite vehicles of fiction writers, which is that anything can be hacked, offensive as I have always found that idea, is gradually becoming true.  Anything can be hacked.  And this is another perfect example of the way it's going to happen, little by little and bit by bit.  A problem like this that just doesn't really rise to the level of an oh, my god, run around, house on fire like we saw with the DNS servers.



But the same problem is in all the other little things that are making DNS queries and that could easily have their query poisoned by somebody who wanted to do that.  That would relocate whatever traffic they were looking at to an attacker-controlled IP rather than where it should be going.  And lord knows, then, what mischief they can get up to.  And you can't even count the hundreds of millions of devices running an embedded Linux that probably use this now known to be defective uClibc library.  Well, we're not going to be running out of things to talk about, Leo.  That's clear.



F5 Networks Remote RCE.  So here's another example of a serious vulnerability that's far more high profile and should get the attention of anyone using F5 Networks' so-called BIG-IP equipment.  They've had problems before.  We've talked about them before.  But both we and the bad guys already know that patching is badly broken, and that there will be F5 BIG-IP equipment online which remains unpatched.  Remember that list of ransomware victims from earlier?  This is exactly where attacks such as those begin.



Last Wednesday on May 4th, F5, a major cloud security and application delivery network provider, released patches to repair 43 bugs spanning its products.  Of the 43 issues they addressed, one is rated critical, this one; 17 are rated high; 24 medium; and one is rated low in severity.  But that critical one, oh, baby.  It carries a CVSS of 9.8, which arises from a lack of an authentication check which will allow attackers to take control of an affected system.  As we're seeing more often now, the flaw took only a few days to reverse engineer, and a working proof of concept has been made public.



So the use of terms such as "might," "may," or "could" in F5's bureaucratically worded disclosure should be replaced with "will," "did," and "have."  They wrote:  "This vulnerability may allow an unauthenticated attacker with network access to the BIG-IP system through the management port and/or self IP addresses to execute arbitrary system commands, create or delete files, or disable services."



Uh-huh.  And this security vulnerability appears to be longstanding since it affects all six most recent major version release chains, v11 through v16.  It doesn't appear that they'll be patching the oldest two major versions 11 and 12 since patches for the iControl REST authentication bypass flaw have been released for versions 17.0.0, 16.1.2.2, 15.1.5.1, 14.1.4.6, and 13.1.5, leaving the 12 whatever and 11 whatever vulnerable but unpatched.



So we can expect CISA to soon add an alert and a mandatory update commandment for this to their growing catalog of Known Exploited Vulnerabilities.  And in fact they just added five more to that catalog, three for which patches were made available in 2014, one in 2019, and another last year.  Yet all five are now under active exploitation even eight years, in the case of those three that were patched in 2014, after having been patched.  So, wow.  We need to patch and reboot.



Okay.  A couple pieces of Closing the Loop feedback from our listeners.  Someone whose name on Twitter is Lets Burninate, he said - yeah.  Let's Burninate.  "Hi, Steve.  Long-time listener."  He said:  "I just wanted to share this image when I tried to change my PayPal password and was shocked by this error message for obvious reasons."  And it is kind of entertaining.  So this is, you know, I'm an avid PayPal user.  They're a great solution for the problem that they're there to solve.  So this is the Change Your Password dialog.  And first they want you to confirm your current password.  That's good.  And he's done that.  Then it says "Enter your new password.  Keep your account more secure.  Don't use your name." 



LEO:  Good thinking.



STEVE:  Yeah.  And then he put in a password which it didn't like.  And then it explained why.  It turned it red with a red emergency symbol:  "Your password can only contain letters, numbers, and these characters."  And there's 10 of them.  And I thought, what?  They looked familiar to me.  Sure enough.  They are the shifted numeric characters on a U.S. Western whatever you call it, you know...



LEO:  Yeah.



STEVE:  ...arranged keyboard.



LEO:  Exclamation mark till left and right parentheses.  They leave out tilde for some reason.  Interesting.



STEVE:  I don't have tilde in my...



LEO:  Oh, it's to the left of one.  You're right.  It's just one through zero.  You're right.



STEVE:  Yeah, yeah, yeah.  So it's exactly the shifted 1 through 9 and 0 keys on our keyboard.



LEO:  Oh, that's bad.  I see this a lot, I think.



STEVE:  So you can't do colon or semicolon, apostrophe or double quote.  You can't use the curly braces or the squares.  Apparently you can't use dash, plus, undersign, or equals.



LEO:  That's weird.



STEVE:  No tilde.  No back apostrophe.  It's like, no vertical bar or backslash.  Why?



LEO:  Why, why.



STEVE:  No forward slash or question mark.  You can't use greater than or less, I mean, they're, like, they've discarded a bunch of really good ones.



LEO:  Why?  Yeah.  That's the question, why? 



STEVE:  Anyway, Lets Burninate, I agree with you.  Lets Burninate.



LEO:  Let's burn it.



STEVE:  I don't get it.



LEO:  It's a witch.  Burn it.



STEVE:  Weird.  Now, and I should note, if we didn't have browser-based hashing, then you'd have to think they were sending this back to central headquarters.



LEO:  [Crosstalk]



STEVE:  Yeah, who says, no, no, no, I don't know how to pronounce that squiggly one so we're not going to have a tilde.  You know, nobody knows what a tilde is, so it's like, okay.  Wow.  Again, there is no logic whatsoever.



LEO:  There's no reason, yeah.



STEVE:  To them excluding those.  Awk, whose handle is @adrianteri, he said:  "Re feedback on 869's Moxie's Knockknock."  He said:  "One can minimize their exposure of things on the Internet without punching holes on their NAT routers.  On 833 you mentioned a couple of more options to 'overlay networks' other than tailscale that might enable one even on a home/residential IP to be able to interconnect their devices across the Internet even with their IPs changing."  And Awk, you are absolutely right.  I wanted to make sure I mentioned that.  I thought that was, you know, thank you for the tweet.



He's of course right.  This new class of so-called "overlay networks," you know, Hamachi was the first one that appeared, and now there's a whole bunch that are free and public domain, and great-looking, by all appearances.  That is another way to allow a roaming device to participate in an overlay network which is being maintained by a machine on your network or a router on the border, and not have to do any sorts of hole punching.  So again, thank you.



And then Bob Grant asked:  "Hi Steve, thank you for your recommendation on McCollum," meaning Michael McCollum.  He said:  "Can you suggest a good starting point for reading him?"  And I think I'd start with his Gibraltar trilogy.  I think that was the first one that we all read here on the podcast.  You know, Michael McCollum writes old-school hard sci-fi where, for me, the joy...



LEO:  No otters drifting down rivers, is what you're saying.



STEVE:  We'll talk about that in a minute, yes.



LEO:  Okay.



STEVE:  The joy is in his plot devices.  I love being surprised, and I have always found Michael's work to be full of delightful moments.  And to your point, Leo, speaking of sci-fi, I told our listeners that there would be the first episode of a new Star Trek series that I had great hopes for, "Strange New Worlds," premiering last Thursday.  It did.  I watched it.  And I loved it.



LEO:  It's very much in the old school.  Every episode has a beginning, middle, and end.  There's a moral at the end.  You know, very much like the original series, I thought.  There's even apparently a fistfight with aliens.



STEVE:  You've got to have that.



LEO:  Got to have that.  No red shirt guys, but other than that.



STEVE:  Spock gets to do his neck tweak.



LEO:  Vulcan, yeah, and gets a good line off on it, too.



STEVE:  Yes.  So we do have a young Spock and a young Uhura.



LEO:  Spock is perfect, I have to say.



STEVE:  Yes.



LEO:  And we have yet to see, but they mention, I don't know if you noticed this, a Lieutenant Kirk onboard.



STEVE:  Oh, no.  He did come onboard toward the end.



LEO:  Oh, did we see him?



STEVE:  I believe that our Kirk, James T. Kirk, had a brother, as I recall.



LEO:  Oh.



STEVE:  And I think that when they talked about a Lieutenant Kirk, I thought, what? 



LEO:  Yeah.



STEVE:  But it wasn't James.



LEO:  And a young Nurse Chapel.



STEVE:  Yes, a young Nurse Chapel.  Anyway, Lorrie was a bit confused about like the timeline on this.  And so I had explained to her that this was a prequel to the original Kirk series.



LEO:  But only like 15 years before, right, because...



STEVE:  Well, and that Pike was the previous Captain of the Enterprise, to whom Spock had loyalty which superseded his to his own Captain Kirk at the time, and the Federation.  Spock stole the Enterprise.



LEO:  Right.



STEVE:  In that episode.



LEO:  Right.



STEVE:  And, boy, was Kirk pissed off.



LEO:  It was the pilot; wasn't it?



STEVE:  It was a pilot that was never aired.



LEO:  The pilot was never aired.  Okay.



STEVE:  Yes.  Anyway, I loved it.



LEO:  Somebody's asking is it the alternate timeline.  It is not the new reboot timeline.  It is the original series timeline; right?



STEVE:  It is a prequel.  



LEO:  It's a prequel.



STEVE:  Yes.



LEO:  It is not this new rebooted movie timeline.



STEVE:  Right.



LEO:  That, no, this is very traditionalist, except for the graphics are updated.



STEVE:  Oh, and Leo...



LEO:  The bridge looks nice.



STEVE:  Something about me, I don't know, I was getting choked up at various points.  It just is so exactly right so far.



LEO:  Spock is perfect, I think, in that.  He's just very...



STEVE:  Yes, and I like Pike.  I think he's got like the right mixture.



LEO:  Good hair.  He's got good hair.



STEVE:  And he's like, humility and okay, fine.  Anyway, I'm very, very, very, very, very hopeful.  We're only going to get 10 episodes.  So if you wanted to, first of all, you could sign up, if you wanted to pay Paramount, what is it, $5 a month or something for the three months that it'll be airing.



LEO:  It's pretty cheap, I think, yeah.



STEVE:  Or if you really wanted to be tricky, they offer you five free days.  So you could wait for three months, get your five free days, binge it during that time...



LEO:  Oh, I don't know.  That's a lot of Star Trek.



STEVE:  It is.



LEO:  It's a little, I'm going to say, a little cornball.  Because I think really it is so true to the original series that it has that - it's not modern in the sense that it's kind of corny like the original series was.  



STEVE:  And like me.  Maybe that's why, yeah.



LEO:  Yeah.  I think people who liked the original series will love it because it's much more in that spirit than even TNG or Discovery or any of the other more modern stuff.



STEVE:  So in discussing some of the newer shows, some writer said, "This is not your father's Star Trek."  And my point is, this is...



LEO:  It is.



STEVE:  ...your father's Star Trek.



LEO:  Absolutely is your father's Star Trek.  Or your grandfather's.



STEVE:  And apparently that's what I want because Discovery was like a sped-up videogame.  It was just - it was not - well, and I do like that episodic format where you don't have to like, last time...



LEO:  Each one's standalone, yeah, yeah.



STEVE:  Yes.



LEO:  No previously, yeah.



STEVE:  Now, it also is the case that the phrase "Picard Season 2" rhymes nicely with "And I hate Q."  God, do I hate Q.



LEO:  I never liked him, either.



STEVE:  I always have, and it turns out I still do.



LEO:  Yeah.



STEVE:  He is an annoying fly in the ointment.  And I suppose that I'm not a huge fan of John de Lancie, the actor.  But we're watching the second season.  We'll finish it tonight.  We have two episodes left.  It's better than the first one only because the first one was so horrible.  And I heard that the second one was better.  It's like, okay.  Yeah.  But it's got Q.  It's like, you know, you just can't have a really annoying omnipotent alien who wants to get in Picard's face all the time.  It's just annoying.



So anyway, there were also too many dumb scenes which it very much had the sense that they were trying to draw the episode out to fill the hour.  So I don't know.  For what it's worth, I'm very capable of disliking something that has Star Trek in its name, even something that has Jean-Luc Picard.  "Strange New Worlds" I thought was great.



LEO:  And Lisa said, "I don't want any more of the ugly old bald Captain Kirk captain.  I want the young hot captain."  And I think Pike fits this, although she really - I think she really wants Shatner back.  Ignore the bald old guy, that's it.



STEVE:  Well, I've got to give Bill Shatner credit for, like, hanging around.



LEO:  Legend.  Legendary, yeah.



STEVE:  He is wonderful.  When I was over at IMDB because I wanted to update for the show notes the current ranking of "Strange New Worlds," it is at 8.3.



LEO:  Oh, that's good.



STEVE:  Out of 10.  That's a very good rating.  And it's going up.  That's what you want to see.  You want to see them going up after the actual show has been out for a while.  What I stumbled on was the official trailer for the new Avatar movie called "Avatar:  The Way of Water."  And apparently this is number two.  It sure took Cameron a long time to get number two out.  But number three is already in post-production, four is filming, and five is in the pipe.  So we're going to get a bunch of Avatars.



And, I mean, it looked astonishing.  I remember when I was watching the first Avatar I just sat there thinking, how do you make this movie?  How do you make this?  I mean, it was just an astonishing piece of visualization.  But this sort of looks like the first one.  And, you know, okay, I guess maybe it's a new story for the kiddies, I'm afraid.  I'm not sure that it's going to, you know.  But Cameron has never disappointed me.  I mean, he gave us "The Terminator," "Aliens" the second one, "The Abyss," "True Lies," "Titanic," "Dark Angel" - that's where we saw Jessica Alba, ooh, baby - and "Avatar."  So, yeah.



LEO:  Mixed bag, I think.  Okay, Steve.  By the way, I did want to mention, you didn't like Book 4 of the Bobiverse.  That's where I was talking about otters floating down the river.  I liked it.  I liked it quite a bit.  So but now I'm waiting for Book 5.  Love the Bobiverse.



STEVE:  Good.  Good.



LEO:  Yeah.  Now let's talk about this FIDO thing because I'm very - I really want to get your take on it.



STEVE:  So Ars Technica's headline was "Apple, Google, and Microsoft want to kill the password with 'Passkey' standard.  Instead of a password, devices would look for your phone over Bluetooth."  Bleeping Computer said "Microsoft, Apple, and Google to support FIDO passwordless logins."  The Record said "Google, Apple, and Microsoft to expand support for passwordless sign-in standard."  And it made the headlines in all of the tech press.  And all of these headlines popped up last Thursday, May 5th, which as I said at the top of the show was not only Cinco de Mayo, but also World Password Day.  And the news of and questions about this new "Passkeys" was the most tweeted-to-me  item of the past week, with many of our listeners wanting to know what it was and what I thought.



Having spent seven years of my life designing, implementing, demonstrating, and proving a complete working solution to this need, I have a good grasp of the problem domain.  So I dug into this "Passkeys" news by going to the source, as I always endeavor to.  I first read the FIDO Alliance's May 5th press release which was titled:  "Apple, Google, and Microsoft Commit to Expanded Support for FIDO Standard to Accelerate Availability of Passwordless Sign-Ins."  This was the press release that everyone else was quoting in the news.  It appeared that whoever wrote it was being paid by the word, since it went on and on to make sure that its reader would come away knowing that all pre-FIDO systems were bad, and FIDO was the cure.



At this point it appears that regardless of whether or not it turns out to be the cure, it will at least be the next thing we try.  And I'm in the same boat as all of our listeners.  We're all avid users and consumers of the Internet.  So we're all hoping that the industry knows what it's doing.  But that press release wasn't going to get the job done.  Fortunately, it linked to the description of the FIDO Alliance white paper titled "Multi-Device FIDO Credentials."



The description of the paper that links to it said:  "The FIDO standards, together with their companion WebAuthn specification, are on the cusp of an important new development.  Evolutionary changes to the standards proposed by the FIDO Alliance and the W3C WebAuthn community aim to markedly improve the usability and deployability of FIDO-based authentication mechanisms.  As a result, FIDO-based secure authentication technology will, for the first time, be able to replace passwords as the dominant form of authentication on the Internet."  What a concept.



"In this paper," they say, "we explain how FIDO and WebAuthn standards previously enabled low-cost deployments of authentication mechanisms with very high assurance levels.  While this has proved an attractive alternative to traditional smart card authentication, and even opened the door to high-assurance authentication in the consumer space, we have not attained large-scale adoption of FIDO-based authentication in the consumer space.  We explain how the introduction of multi-device FIDO credentials will enable FIDO technology to supplant passwords for many consumer use cases as they make FIDO credentials available to users wherever they need them, even if they replace their device."



Okay.  So I have a link in the show notes to the PDF for anyone who wants the raw material.  Obviously this descriptive overview still doesn't tell us what we want to know.  So I dug into the whitepaper.  We get the Executive Summary, followed by "A Brief History of Online Authentication."  Then a section titled "FIDO: Starting from the Top," followed by "WebAuthn Level 3:  Bringing Up the Bottom."  So this brings us to the bottom of page four of the PDF, and we begin to frame the problem as follows.  The explanation explains:  "FIDO-based solutions can also increase the security of consumer two-factor authentication by providing phishing resistance, regardless of whether those use cases care about hardware-based sign-in credentials or not."



Now, I should mention that FIDO was always hardware based, which has been the problem that they've been struggling with is that the FIDO authentication standard was you will have a hardware dongle, a token, a something which, because it's hardware, because it's physical, it cannot be spoofed.  It cannot be, you know, no one in Russia can get the contents of what you have in your thing you're holding in your hand because you're holding it in...



LEO:  The YubiKey said there are some that are FIDO2 YubiKeys.  That's what you mean.



STEVE:  Yes, yes, yes.  And so...



LEO:  Which is that's good.  That's good security.  No one would deny that; right?



STEVE:  You could argue it's the best.  The gold security.



LEO:  Yeah.



STEVE:  Yes.  The problem is it's physical.



LEO:  And it makes people buy keys, $50 keys.



STEVE:  Yes.  Exactly.  The benefit is it's physical.  The problem is it's physical.  And so if you absolutely - so they said:  "FIDO-based solutions can also increase the security of consumer two-factor authentication by providing phishing resistance regardless of whether those use cases care about hardware-based sign-in credentials or not."  In other words, they're saying we're giving up.  We're going to back down from the position we had taken, I mean, you could still use hardware-based sign-in credentials, but now you're not going to have to.  We're not going to make you have to have a hardware dongle.  And this has been sort of in the air for a couple of years; right?  There's been talk about being able to use your phone as your FIDO authenticator.  So this notion isn't completely new.  It's been happening.



They said:  "However, we have observed limited adoption in this latter category, especially in the consumer space, because of the perceived inconvenience of physical security keys - buying, registering, carrying, recovering - and the challenges consumers face with platform authenticators as a second factor, for example, having to re-enroll each new device; no easy ways to recover from lost or stolen devices."  They said:  "While these drawbacks can make FIDO-based solutions, whether based on physical security keys or platform authenticators" - and I should explain this phrase "platform authenticators."  That just means your smartphone or your laptop.  They're calling that a "platform authenticator" as opposed to a physical security key.  So "...drawbacks can make FIDO-based solutions, whether based on physical security keys or platform authenticators, a tricky proposition for users already accustomed to two-factor authentication, they present an even higher barrier to adoption for users who don't or don't want to use two-factor authentication at all, and are stuck with passwords."



And so finally we get down to it.  The white paper explains:  "The FIDO Alliance and the W3C WebAuthn working group are proposing to address these gaps in a new version" - which they call "Level 3" - "of the WebAuthn specification."  They said:  "Two proposed advances in particular bear mentioning."  And so here they are, one and two.  Number one:  "Using your phone as a roaming authenticator."  That's the first of these proposed advances.



They said:  "A smartphone is something that end-users typically already have.  Virtually all consumer-space two-factor authentication mechanisms today already make use of the user's smartphone.  The problem is that they do this in a phishable manner.  You may inadvertently enter a one-time password on a phisher's site, or you may approve a login prompt on your smartphone not realizing that your browser is pointed at the phishing site and not the intended destination.  The proposed additions to the FIDO/WebAuthn specs define a protocol that uses Bluetooth to communicate between the user's phone, which becomes the FIDO authenticator, and the device from which the user is trying to authenticate."  You know, your laptop, for example.  "Bluetooth," they say, "requires physical proximity, which means that we now have a phishing-resistant way to leverage the user's phone during authentication."



LEO:  Yeah, the hacker has to be in physical proximity.  Which is good.  Right?  Because Bluetooth is not the most secure - well, go ahead, go ahead.



STEVE:  No.  Of course SQRL solved this with a QR code that you let your phone see, as we know.



LEO:  Right, right.



STEVE:  They said:  "With this addition to the FIDO/WebAuthn standards, two-factor deployments that currently use the user's phone as a second factor will be able to upgrade to a higher security level, phishing resistance, without the need for the user to carry a specialized piece of authentication hardware (security keys)."  Oh, thank god.  So yes, we'll be able to use our phones.  Wonderful.  That was point one.



Here's point two:  "Multi-device FIDO credentials."  Okay?  They say:  "We expect that FIDO authenticator vendors, in particular those of authenticators built into OS platforms" - this is, we've heard the names, right, Apple, Google, Microsoft - "will adapt their authenticator implementations such that a FIDO credential can survive device loss.  In other words" - and again, hasn't been done yet, but this is what they expect.  "We expect that FIDO authenticator vendors," blah blah blah.  "In other words, if the user had set up a number of FIDO credentials for different relying parties" - and "relying parties" is a term of art in this whole identity space - "on their phone."  If the user had set up a number of FIDO credentials for different relying parties on their phone.



And notice that in FIDO you need a credential per relying party.  That is, a FIDO credential for Amazon, a FIDO credential for PayPal, a FIDO credential for Facebook, a FIDO credential for Google, blah blah blah.  One each.  So it's a one-for-one mapping in FIDO.  "And then," they say, "got a new phone, that user should be able to expect that their FIDO credentials will be available on their new phone.  This means that users don't need passwords anymore."



LEO:  [Gasping]



STEVE:  "As they move from device to device, their FIDO credentials are already there, ready to be used for phishing-resistant authentication."  Okay.  Now, I'll just pause to note that I solved this problem with one-time password authenticators with my sheaf of printed QR codes; right?  We were talking about that last week.



When I'm enrolling on a site that offers me second-factor authentication with a one-time password, and it shows me the QR code which I can then capture with my authenticator on my phone, I also print the page.  I print the paper out, and it's securely stored.  I have a sheaf of them for all the places I use two-factor authentication.  So that, yeah, if I need to set up a new device that doesn't sync in some fashion with the authenticator in my phone, I can do that.  It's offline.  No one in Russia can get to it.  It's very secure.  But yeah, it's a little burdensome.  I had to do that.  Lots of people don't, and then they get stuck if their authenticator won't export or transport and sync.



So they say:  "For these multi-device FIDO credentials" - so this is their term.  Multi-device FIDO credentials just means cloud sync.  That's all that is, multi-device FIDO credentials.  "It is the OS platform's responsibility to ensure that the credentials are available where the user needs them."  And they said:  "Note that some companies are calling FIDO credentials 'Passkeys' in their product implementations, in particular when those FIDO credentials may be multi-device credentials."  So in other words, just for the record, passkeys is not a term of art in FIDO.  And I imagine that the company that has a trademark on "Passkey" is not very happy.  A lot of people noted that the government started to use the term "Shields Up" for one of their things.  And it's like, yeah.



LEO:  What are you going to do.



STEVE:  Okay, fine, I don't care.  Exactly.  So they say:  "Just like password managers do with passwords, the underlying OS platform will sync the cryptographic keys that belong to a FIDO credential from device to device.  This means that the security and availability of a user's synced credential depends on the security of the underlying OS platform's (Google's, Apple's, Microsoft's, et cetera) authentication mechanism for their online accounts, and on the security method for reinstating access when all old devices are lost.  While this may not always meet the bar for use cases that require physical key level security," they write, "it is a huge improvement in security compared to passwords."



They say:  "Each of the referenced platforms apply sophisticated risk analysis and employ implicit or explicit second factors in authentication, thus giving two-factor-like protections to many of their users."  So this is FIDO saying, well, it's not as good as physical keys.  We're kind of annoyed.  But look, it's going to work.  Like maybe someone will actually use FIDO because we're going to allow cloud syncing in this Level 3 mode.  And the people who are doing the syncing are being responsible enough.



So they said:  "The shift from letting every service fend for themselves with their own password-based authentication system to relying on the higher security of the platforms' authentication mechanisms is how we can meaningfully reduce the Internet's over-reliance on passwords at a massive scale."  In other words, they're saying that we will rely upon the user authenticating to their own device  smartphone or desktop  with biometrics or whatever, rather than authenticating to each remote site individually.  And yes, that sounds familiar.



Finally, they say:  "Syncing FIDO credentials' cryptographic keys between devices may not always be possible, for example, if the user is using a new device from a different vendor which doesn't sync with the user's other existing devices.  In such cases, the existence of the above-mentioned standardized Bluetooth protocol enables a convenient and secure alternative:  If the FIDO credential isn't readily available on the device from which the user is trying to authenticate, the user will likely have a device, for example a phone, nearby that does have the credential."  So in other words, if you're using Windows, and iOS won't sync to Windows, then you can use Bluetooth on your iOS device to get the credential over into Windows.  They said:  "The user will then be able to use their existing device to facilitate authentication from their new device."



Okay.  So it appears that what this press release and these so-called "Passkeys" - which again as the white paper explains don't actually have anything to do with FIDO, that is, the term doesn't - it's just the introduction of cloud syncing among devices to facilitate the transport of one's collection of FIDO credentials from one device to the next.  The other piece, well, and in the case of device loss, when you get a new one, you resync with the cloud and you get all of your FIDO credentials back.



The other piece is that the FIDO Alliance appears to have formally given up on the idea that we're all going to go out and purchase a hardware FIDO token when we all already own a smartphone that can serve the same purpose.  The use of a possibly available Bluetooth link allows one's smartphone to be used to authenticate to a website on a desktop that does not contain a FIDO authenticator with one's credentials.  And as we said, for clarity, that's what SQRL provides for with a QR code and the smartphone's camera.



And yes, speaking of SQRL, I know that the head of everyone out there who understands SQRL is exploding right now because FIDO still falls very far short of providing the complete solution that SQRL offers.  But having moved from simple usernames and passwords to password managers and multifactor authentication and then to OAuth third-party authentication, we're now going to get FIDO, though it will apparently be popularly called "Passkeys."  From the samples I've seen online, it appears that it will still be necessary to first identify oneself to the web site being authenticated to.  So FIDO with "Passkeys" replaces the password, but unfortunately not the username. So it will continue to be somewhat more cumbersome in that way.



The way FIDO's crypto works is that it randomly synthesizes a public and private key pair for each and every website the user wishes to authenticate with, and it gives that site the public key to retain while the FIDO authenticator stores the matching private key for each subsequent use for reauthenticating.  So it's this collection of individual private authentication keys  which are now being called "Passkeys"  that Apple, Google, and Microsoft will be obtaining and synchronizing in the cloud for their users.  This provides for same-platform cross-device FIDO credential synchronization which is crucial for FIDO since each new website authentication creates another public/private key pair. And it provides for credential recovery in the event of device's loss, and that's certainly needed to create a practical system.



As we know, I went a different way with SQRL.  SQRL uses a single master key which can be printed and stored safely.  Or it could be loaded in the cloud if you wanted, whatever.  From that one key, it deterministically synthesizes unique per-site public and private key pairs based upon the website's domain name; and, like FIDO, it gives each website the public key to use for future authentication.  But unlike FIDO, there is no growing collection of randomly synthesized per-site private keys that need to be retained and cloud-synced among devices.  So there's no need to back up a large collection of private keys to the cloud or anywhere.



The only thing a SQRL user ever needs for their identity to be secure and fully recoverable for all websites is one piece of paper.  And if you have multiple identities on multiple devices, you can log in for the first time on some other device that has your same SQRL identity.  And when you log on a different device, the identity works because multiple devices all synthesize the same private key. 



So backing off from that, overall, this whole big announcement of Passkeys appears to have mostly been a World Password Day-timed press event without much technology to back it up.  We're not getting SQRL.  We, all of us, we're getting FIDO.  And that means we need cloud-synchronized "Passkeys" to make FIDO's use practical.  The good news is we're going to get it.  I'll be interested to see how the login flow functions.  The other big thing FIDO is missing is it doesn't identify you to the site.  You still have to first identify yourself, then FIDO replaces your password.  SQRL did both, which was way more convenient.  But anyway, we're not getting SQRL.  We're getting FIDO.  And Passkeys basically makes FIDO feasible because you have to be able, since you are synthesizing completely random keys for every site you visit, you've got to collect them.  You've somehow got to cross-device sync them.  And Apple, Google, and Microsoft will be taking care of that for us.



LEO:  So it sounds like it's kind of less secure than if you used a YubiKey, I guess.



STEVE:  Yes.  This is absolutely FIDO group, the FIDO Alliance compromising themselves down from their ivory tower because...



LEO:  Which they needed to do because nobody would use it.



STEVE:  Because nobody wanted FIDO, yes.



LEO:  Right.



STEVE:  Nobody was going to do it.  I mean, yes, high-level, I know that there are Google employees who use their Titan keys to do things.  But I don't have one.



LEO:  It's not going to succeed if everybody - but, see, that's my other issue is not everybody has a smart device.



STEVE:  Correct.



LEO:  I guess would this work if you didn't have...



STEVE:  It's always possible to still use a username and password.



LEO:  Oh, okay.



STEVE:  That will never go away.



LEO:  Okay.



STEVE:  Never, never, never go away.



LEO:  Which means that's what people are going to do.



STEVE:  Yes.  Yes.



LEO:  So...



STEVE:  You know, my favorite example, Leo, is the person who said, "Oh, I don't need a password manager."  And I said, "You can't be using the same password everywhere."  And she said, "Oh, no, I don't."  And I said...



LEO:  How do you do that?



STEVE:  And she said, "Well, when I'm creating an account, I just bang on the keyboard a lot."  And I said, "Okay."  And I said, "So how do you log in again?"



LEO:  I forgot.



STEVE:  She said, "It always - there's a little line there that says 'I forgot my password.'"  She said, "And I never knew it, so I did forget it."



LEO:  Yeah.



STEVE:  And she said, "Then they send me a link, and I log in with that."



LEO:  That's actually - that's fairly secure; right?  I mean, honestly, yeah?



STEVE:  Well, you know, it uses an email confirmation in order to reassert that you have...



LEO:  As long as you don't lose control of your email, you're okay.



STEVE:  Correct.  And that is the segue to next week's Picture of the Week, which is already in the document and waiting to be displayed.



LEO:  You don't have anything else, but that's there.



STEVE:  That's right.



LEO:  I love it.  Obviously SQRL would be much more secure.  But SQRL has a similar problem which is it is not trivially easy to use.  And for that reason I think people are going to fall back to a password for almost anything.  Single sign-on's good.  You know, I use Microsoft now for login to Windows.  As you know,  sends to your phone an authenticator, sends it a digit, a two-digit number, and you say, yeah, I know that number, and you're in.  That seems like - is that the same thing as this FIDO thing?  It's similar.



STEVE:  Well, so it's specific to Microsoft.



LEO:  That's right, that's right, yeah.



STEVE:  Yeah.  And so we're looking for a broad-based solution which solves the phishing and the "I forgot my password" problem.



LEO:  Right.



STEVE:  Which is easy to use.  The fact is we'll have to see what the flow looks like.  It is certainly easy to do login with Facebook, login with Google.  We know that that's horrific from a tracking and privacy standpoint, right, because you're bouncing through them. 



LEO:  Yeah.  Oh, I don't do that, yeah.  I've stopped doing that entirely, yes.



STEVE:  Oh, my god.  And in fact I did hear you on TWiT last Sunday talking about how you were finally thinking maybe you should be taking privacy a little more seriously. 



LEO:  Yes, yes.  I admitted I was wrong.  And that because these data brokers selling information about who visited Planned Parenthood over the past week for 160 bucks.  And what that does is it puts you - if you live in Texas, and there are now other states, and soon it might even be...



STEVE:  Criminalizing.



LEO:  ...23 other states...



STEVE:  Criminalizing interstate travel for the purpose of terminating a pregnancy.



LEO:  So for 160 bucks anybody, the way this Texas law works, anybody can go after you.  So there's now probably a brisk business, people buying that information and then suing you, or law enforcement in Tennessee, for instance, going after you.  Or I guess it's Louisiana.  In any event, it suddenly became obvious that the government is now starting to go after people for things that they shouldn't be, and it is now dangerous to leave this stuff on, unfortunately.  



STEVE:  And that's really, I think that is, you're right, that's the takeaway is that, given a certain set of existing laws, you could argue that with those laws there's a reduced risk from lack of privacy.



LEO:  Yeah.



STEVE:  But if the laws change...



LEO:  Well, that's the problem.  Exactly.



STEVE:  And suddenly the previous assumptions no longer hold under the new regime.



LEO:  Exactly.



STEVE:  And that's the danger.



LEO:  Yeah.  If you trust the government, no problem.  I no longer trust the government, so problem.



STEVE:  Yeah.



LEO:  And that's too bad.



STEVE:  Yeah. 



LEO:  But now we have to pay more attention.  So you've been right all along.  I was a wide-eyed optimist.  I am no longer.  Steve, thank you, as always.  It's always eye-opening and always fascinating.



If you do not tune in every Tuesday to Security Now!, you really ought to.  If you're listening today, I think you know now.  Several ways you can do this.  You can watch live.  If you happen to be around 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC on a Tuesday, you could just go to live.twit.tv.  There's a live audio and video stream there.  You could watch that.  If you're watching live, chat live at irc.twit.tv.  After the fact, you can online go to TWiT.tv/sn.  That's our website.



Steve also has the show at his website, GRC.com.  In fact, he has two unique formats.  He has good hand-contrived transcripts by Elaine Farris, who listens to this show, she's listening even now, and writes it down and then puts it in a transcript.  That's really handy for searching.  You can also get a 16Kb version for the bandwidth-impaired.  Then of course the full 64Kb audio, also available at GRC.com.



If you're over there, you know, it might be a good idea to pick up a copy of SpinRite, current version 6.0.  If you buy today, you'll get a copy of 6.1 when it comes out, but you'll also participate in the final stages of that creation.  And everybody who has mass storage really needs SpinRite, the world's best mass storage maintenance and recovery utility.  There's lots of other great stuff:  GRC.com.  You can also leave him feedback there:  GRC.com/feedback.  Maybe the better way to do it is through Twitter.  He has his DMs open, as the kids say.  You can slide into them, @SGgrc on the Twitter:  @SGgrc.



We have copies of the show at the website, as I mentioned.  You know, the easiest thing might be just to subscribe.  Both Steve and I have links to the RSS feed.  You put that into your podcast client, and you'll get the show the minute it's available, each and every Tuesday so you can listen at your own  leisure, at your own pace.  I know some people who like to listen at 1.5.  In which case when you watch live, I've just got to warn you, we will sound drunk talking at normal speed.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#871

DATE:		May 17, 2022

TITLE:		The New EU Surveillance State

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-871.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look back at what no one wanted, an eventful Patch Tuesday.  Apple has pushed a set of updates to close an actively exploited zero-day.  Google announced the creation of their Open Source Maintenance Crew.  A ransomware gang wants to overthrow a government.  Google's Play Store faces an endlessly daunting task.  The predicted disaster for F5's BIG-IP systems arrived.  A piece of errata and some closing-the-loop feedback from our terrific listeners.  Then we're going to look at just how far afield the European Union has wandered with their forthcoming breathtaking surveillance legislation.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Big show planned for you.  Why the government is saying don't install the Microsoft patches from last Tuesday, a Patch Tuesday that didn't go as well as planned.	 We'll talk about the ransomware gang that wants the citizens of Costa Rica to rebel.  And a new proposed regulation in the EU that could mean disaster for privacy.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 871, recorded May 17th, 2022:  The New EU Surveillance State.



It's time for Security Now!, the show where we cover your security, privacy, safety online with this guy right here, Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.



LEO:  Careful.  The drone is right in front of you.  It could wake up at any time.



STEVE:  Well, it's tethered right now, so that's good.  It can't escape because it's got a tail on it.  For those who don't know, Leo's been playing with a little inexpensive sort of a selfie cam drone that doesn't really cut the mustard.



LEO:  But as you point out, I mean, given this thing is about four square inches, it's pretty impressive what it can do.



STEVE:  Yeah.  It's basically flying propellers that suspend a camera.  



LEO:  Yeah, and it has sensors, not only a camera but has sensors and things.  It doesn't run into things.



STEVE:  It's got to have position sensors, inertial sensors in order to be stabilized and all that.  Yeah.



LEO:  Pretty impressive.



STEVE:  But anyway, so we're at Episode 871 for mid-May.  And I titled this one "The New EU Surveillance State."  That was about the fourth title the podcast got.  And I kept changing the title as I read more deeply into the details of some proposed legislation in the EU which first leaked last Tuesday and then was - it was funny, too, because the leaked copy actually had the word "sensitive" on the front page.  Yes.



LEO:  Yeah.  Yes, it is.



STEVE:  And that was removed from the actual formal official legislation that came out the next day on the 11th.  Anyway, we've got to talk about that because it's been compared to the CSAM Apple stuff.  No.  This is way beyond what Apple was proposing.  It's, well, it's breathtaking.  Anyway, we'll get there.  First we're going to take a look back at what no one wanted, which was an eventful Patch Tuesday.  You don't want your Patch Tuesdays to be eventful.



LEO:  No.



STEVE:  You want them to be uneventful.



LEO:  Quiet, yes.



STEVE:  And we didn't get that.  Apple has pushed a set of updates to close an actively exploited zero-day across a bunch of their products.  We'll touch on that.  Google has announced the creation of their Open Source Maintenance Crew.  That's the formal name of it, the OSMC, the Open Source Maintenance Crew.  A ransomware gang has the temerity to call for the overthrow of a government.  Google's Play Store is facing an endlessly daunting task which we'll mention and talk about and look into.  The predicted disaster for F5's BIG-IP systems, which we expected last week, that arrived, right on schedule.



We've got a piece of errata.  I've got a bunch of closing-the-loop feedback from our terrific listeners.  Then we're going to look at just how far afield the European Union has now wandered with their forthcoming breathtaking surveillance legislation.  And last week I mentioned, because it was in the context that we were talking about, that I had already cued up this week's Picture of the Week, which talks about supply chain security, which actually we'll be talking about also in the middle of the podcast.  So I think another great podcast for our listeners.



LEO:  Yeah, boy, this Picture of the Week is not funny at all.  Not a comic this time.  Picture of the Week time.



STEVE:  So, yeah, this is a perfect snapshot to characterize the current state of the security of the open source software supply chain.  This was a legitimate - I actually dug into it and went to the NPM listing for this.  Someone named Lance R. Vick, he put up a note on mastodon.social.  He wrote:  "I just noticed 'foreach' on NPM is controlled by a single maintainer.  I also noticed they let their personal email domain expire, so I bought it before someone else did."



LEO:  Oh, my god.



STEVE:  He said:  "I now control 'foreach' on NPM."



LEO:  Oh, my god.



STEVE:  "And the 36,826 projects that depend on it."  And when I went over there, sure enough, more than six million downloads of this little "foreach" module per week.  So this thing is like deeply dependent.  All it does is iterate across...



LEO:  It's dopey, yeah.



STEVE:  ...an array like, I mean, it's like the dumbest thing.  It's like, why would you just not - why would you go get that?  But lots of people do.



LEO:  Well, Python has it.  A lot of languages have it.  I guess for some reason JavaScript doesn't.  So now you can.



STEVE:  Yeah, so it adds that to it because, you know, you wouldn't have to want to write an iterator on your own.  Oh, goodness, no.



LEO:  Heaven forfend.



STEVE:  Let's instead depend on somebody else's who apparently has wandered off and allowed his personal domain to expire.  But here's the point.  So this is the system that has evolved.  And we're going to be talking about this today, talking about the Open Source Software Security Foundation because there's news there.  And I'm really glad there's news there because open source software just sort of started off as being kind of a curio, and everybody would agree now it's become a real force.  The problem is it's much less fun to maintain and secure things than it is to just write them.  And so a lot of stuff is written and then, like, here you go, I'm going to go back to my regularly scheduled job.  And things kind of just get left.  So we have to fix this over time.  We'll get there in a minute.



First of all, I've observed in the past that what one looks for in a Patch Tuesday is a seamless and uneventful experience.  Either you check for updates and then decide to install them, if they're ready for you; or you receive a notice that updates have arrived and have already been installed and are just waiting for you to step away from your computer.  Again, uneventful.  What you don't want is to see a headline such as BleepingComputer ran yesterday, which reads:  "CISA warns not to install May Windows updates on domain controllers."



LEO:  Oh, boy.



STEVE:  In fact, CISA so much doesn't want May's updates installed that they went so far as to temporarily remove the listing of one of their "must patch" mandate's security flaws from their own catalog of known exploited vulnerabilities because they really can't have it listed there while they're also warning all users of Active Directory not to install those updates to fix the problem which is in their catalog because it's known to be exploited.  It's a mess.



The headline on CISA's published notice reads:  "CISA Temporarily Removes CVE-2022-26925 from Known Exploited Vulnerability Catalog."  And they wrote:  "CISA is temporarily removing CVE-2022-26925 from its Known Exploited Vulnerability Catalog due to a risk of authentication failures when the May 10th, 2022 Microsoft rollup update is applied to domain controllers.  After installing May 10, 2022 rollup update on domain controllers, organizations might experience authentication failures on the server or client for services, such as Network Policy Server, Routing and Remote Access Service, Radius, Extensible Authentication Protocol, and Protected Extensible Authentication Protocol.  Microsoft notified CISA of this issue, which is related to how the mapping of credentials to machine accounts is being handled by the domain controller."



So as we know, once upon a time it was possible to individually install security patches so that a troublesome patch could be manually avoided.  No longer.  Now everything is rolled up into a take-them-all-or-none solution.  And I don't blame Microsoft for that, frankly.  My mind was always boggled that Microsoft was even able to somehow consider offering a la carte patching of such an incredibly complicated codebase as Windows has become.  The complexity of offering that option to me was just astonishing.  And I don't know how they even did it.  And, as we know, it often didn't quite work as hoped.  So now it's all or nothing.  And in this case of Windows domain controllers, for the time being, "nothing" is what you want.



As enterprise admins last week began installing the May updates, problems quickly started surfacing, with admins sharing reports online of some Active Directory policies failing with the error message:  "Authentication failed due to a user credentials mismatch."  And this continues:  "Either the user name provided does not map to an existing account, or the password was incorrect."



So Microsoft explained that the issue is only triggered after installing the updates on servers used as domain controllers.  Okay, well, that doesn't help those domain controllers.  But the updates, they made a point of saying, will not negatively impact when deployed on client Windows devices and non-domain controller Windows Servers.  And this is an example of an instance where it's going to be really interesting to eventually watch and learn whether Microsoft's announced and forthcoming Autopatch system turns out to be a good thing or more trouble than it's worth.  Presumably, Autopatch is somehow going to handle unforeseen problems like this.  Right?  I mean, that's the whole point.  And at the moment it's unclear where this omniscience is going to come from, since it's apparently not coming from Microsoft.  So we're going to be writing that eventually.



The problem all of this is trying to fix, and in this case of this domain controller patch, it's a flaw that's being actively exploited, as I mentioned before, thus the reason it was on CISA's thou-shalt-patch-now list, actively exploited in the wild.  It's a Windows LSA, that's the Local Security Authority, spoofing zero-day which has been confirmed as a new PetitPotam Windows NT LAN Manager Relay attack vector.  The PetitPotam problem was discovered and named last July by the French security researcher Gilles Lionel.  We talked about it at the time, last July, which is probably why PetitPotam sounds familiar.  It's just fun to say.



And an NTLM relay attack, which is what this specific PetitPotam exploit allows, it allows bad guys to force devices, in this case domain controllers, to authenticate against malicious servers under their control, essentially joining the malicious server to the domain.  Once a device authenticates, the malicious server is able to impersonate the device and gain all of its privileges.  This in turn gives attackers complete control over the domain.  In other words, not good.  Not what you want.



Okay.  Now, as for the actual patch itself, we're back to that disheartening story where Microsoft patches to stop the proof of concept from functioning, while leaving the underlying problem unresolved.  Gilles has confirmed to the tech press that May's security update, the one that you're not supposed to install even, leaves the underlying problem unresolved.  It did finally fix that particular specific problem, and even that was Microsoft's second attempt.  They first tried to fix it last August.  He worked around that because they didn't really fix it.  And now he's saying it still isn't fixed.  They closed one specific attack vector in the encrypted file system, which is where this whole thing surrounds this.  But he said:  "Attack vectors still exist which will allow a slightly modified attack to continue to work."  He said:  "All functions of PetitPotam, as other vectors, still work except the EfsOpenFileRaw," which was the one thing they fixed.



So to me this feels like there's a bigger problem, that is, they posted workarounds for this and ways of locking things down so that this isn't a problem.  I mean, so it feels like it's kind of big, the way all of these printer server problems were that we faced at the beginning of last year, where it just took them basically all year to fix this problem because it was fundamental to some early assumptions that Windows had made back when security wasn't such a focus, and they're kind of stuck with the protocol.  Feels to me like they're kind of stuck with not breaking things, which is what would happen if they really locked this down as it needs to be, so they're just kind of trying not to.



So all of this is current as of yesterday.  Assuming that Microsoft is eventually able to fix and reissue May's security bundle in a way that doesn't break Active Directory servers, and hopefully they'll test it first this time to be sure, then they will either announce an out-of-band update, or maybe they'll just wait till next month, till June.



Okay, but this all happened last Tuesday, right, on Patch Tuesday.  What else happened?  We received fixes for three new zero-day vulnerabilities, one of which was that one, and patches for a total of 75 flaws across Microsoft's entire software suite.  Eight of those 75 flaws were rated "critical."  Twenty-six, which would have been more than one third of the 75, were remote code execution vulnerabilities, now closed.  Twenty-one were elevation of privilege vulnerabilities.  Seventeen were information disclosure.  Six were denial of service.  Four were security feature bypass, and I still love that, it's so generic.  We have one spoofing vulnerability.  Nothing was fixed in Chrome this month by Microsoft.



And remember that Microsoft classifies a flaw as a zero-day different than the rest of the world, if it's been disclosed publicly, even if it's not known to be actively exploited.  So somebody talked about two other problems, aside from this PetitPotam problem.  But as far as they know, it hasn't been exploited at the time that they closed it.  So, okay, fine.



So as we know, given that Exploit Wednesday now follows Patch Tuesday, the urgency to install updates in a timely manner has increased because we just see one example after the other, month after month, that the bad guys are jumping on patches, reverse engineering them, and working to exploit them as quickly as they can.



And speaking of zero-days, yesterday Apple updated watchOS to v8.6, tvOS to 15.5, and macOS Big Sur to 11.6.  So that would cover the Apple Watch Series 3 or later, Apple TV 4K 1st and 2nd gen, Apple TV HD, and Macs, which are running Big Sur.  In each of those cases the updates fixed an out-of-bands write which could be made to occur in the AppleAVD module.  That's a kernel extension for handling, as AVD sounds, audio and video decoding.



It will come as no surprise to our longtime listeners who have all learned to expect trouble to arise in complex media decoders which are inherently complex interpreters of encoded bitstreams.  In this case, remote attackers could have, and were known to be,  executing their arbitrary code with kernel privileges, which is never good.  Apple was as closed-mouthed as usual about this, only saying that they had added improved bounds checking.  Which, yes, you would want to add to an unbounded write in one of these modules.  So they never share very much.  They just update and say this is important, everybody please fix this.



Okay.  Google's Open Source Maintenance Crew.  Recall that two weeks ago we first talked about the OpenSSF, the tongue-twister, Open Source Security Foundation.  At that time I enumerated the gratifyingly large number of participating and supporting companies, pretty much a who's who, and even some you wouldn't expect.  And the occasion two weeks ago was their announcement of that Package Analysis Project which, in just one month, they said, had identified more than 200 malicious packages which were present in the Python and JavaScript repositories.



And recall that I was a bit wary at this point of getting too excited about this particular effort, although I applaud the whole concept of an Open Source Security Foundation, as we'll see.  But in this case it appeared that they were mostly just scanning, doing static code scanning for references to previously known malicious domains and IPs.  Okay.  Which would all be trivial to change once it became clear to the bad guys that this was the way to avoid being picked up by this particular detector.  So that's not going to take long to fix.  That malware will be back under a different name, using an unknown domain, and apparently not go detected.  Anyway, we'll see.



But as for the OpenSSF effort overall, I'm very bullish about the prospect of this.  It's what has been needed for now quite some time.  Those of us who are old enough to have our hair thinning will remember once upon a time when open source software was sort of a counterculture phenomenon, back in the days when source code was not commonly shared for any purpose, and the idea of doing that was kind of bizarre.  I mean, even shareware was still closed source.  It was just please pay me if you find this useful, but it's still mine.  And back then the idea of software being free represented a clear threat to the interests of commercial proprietary software vendors.  In fact, in February of 2001, Microsoft's Jim Allchin publicly stated that "Open source is an intellectual property destroyer."  He said:  "I can't imagine something that could be worse than this for the software business and the intellectual property business."  Well, yeah, I guess that's sort of obviously true.



And then early the following year, in January of 2002, one of Microsoft's chief strategists, Craig Mundie, addressing New York University's School of Business, said that releasing source code into the public domain is "unhealthy," causes security risks - yeah, you wouldn't want anybody else to look at your code and find all those bugs - and, he said, "as history has shown, while this type of model may have a place, it isn't successful in building a mass market and making powerful, easy-to-use software broadly accessible to consumers."



Okay.  Well, now that was then.  And no one is holding Microsoft responsible for anything that was said 20-plus years ago.  The world today is an entirely different place.  But it does remind us just how much things have changed in 20 years. And we know that change is slow.  We also know that the Open Source model has produced tremendous wealth, both intellectual and economic.  I saw somewhere, and it was an old stat, so I didn't add it to the show notes.  But years ago it was estimated that $60 billion of wealth had been created and just sort of dumped into the public community by open source software.  And as we know, it's become a crucial component of today's software technology landscape, which even Microsoft has now begun to embrace.  Today it is entirely possible to operate a major enterprise using nothing but open source software.  Which says a lot.



But its problems are also many.  The trouble is, as I mentioned at the top of the show, that volunteer effort is much more interested in creating new stuff than in maintaining and securing it.  It's not that maintenance and security focuses are absent; but as we've seen, so much maintenance and security focus is needed beyond just getting something to work that it's a big ask.  And truly securing software, understanding the many ways in which code which works can still be made not to work, requires an entirely different mindset and a very different type of specific education and training.



So many major organizations are now benefiting from the work that has been done for them that having them join a Foundation so that they have an organized platform for giving something back, especially when it's about improving the crucial security of the software they are now all using within their own enterprises and on their network borders, it's the right thing to do.  And the OpenSSF looks like it's the foundation that's going to succeed.



We're talking about this today because last Thursday Google made a major announcement about specific new support for this effort.  Google wrote:  "Today we joined the Open Source Security Foundation (OpenSSF), Linux Foundation, and industry leaders for a meeting to continue progressing the open source security initiatives discussed during January's White House Summit on Open Source Security.  During this meeting, Google announced the creation of its new Open Source Maintenance Crew," that is, the meeting on Thursday of last week.  Google announced the Open Source Maintenance Crew, they wrote, "a dedicated staff of Google engineers who will work closely with upstream maintainers on improving the security of critical open source projects.  In addition to this initiative, we contributed ideas and participated in discussions on improving the security and trustworthiness of open source software."  Which is why our Picture of the Week was so apropos.



"Amid all this momentum and progress," they wrote, "it is important to take stock on how far we've come as a community over the past year and a half.  In this post we will provide an update on some major milestones and projects that have launched, and look towards the future and the work that still needs to be done."  And I'm not going to share all of it, but just a little, it's sort of the preamble of that.



They wrote:  "A little over a year ago we published Know, Prevent, Fix, which laid out a framework for how the software industry could address vulnerabilities in open source software.  At the time, there was a growing interest in the topic, and the hope was to generate momentum in the cause of advancing and improving software supply-chain security."  And amen to that.  So they said:  "The landscape has changed greatly since then,"  a year and a half.  They highlighted three points.  They said:  "Prominent attacks and vulnerabilities in critical open source libraries such as Log4j and Codecov made headline news, bringing a new level of awareness to the issue and unifying the industry to address the problem."



Second:  "The U.S. government formalized the push for higher security standards in May of last year, 2021, with the Executive Order on Cybersecurity.  The release of the Secure Software Development Framework, a set of guidelines for national security standards on software development, sparked an industry-wide discussion about how to implement them."



And finally:  "Last August, technology leaders including Google, Apple, IBM, Microsoft, and Amazon invested in improving cybersecurity; and Google alone pledged $10 billion over the next five years to strengthen cybersecurity, including $100 million to support third-party foundations like OpenSSF that manage open source security priorities and help fix vulnerabilities."



So I'm finishing their quote, saying:  "In light of these changes, the Know, Prevent, Fix framework proved prescient.  Beyond just the increased discussion about open source security, we're witnessing real progress in the industry to act on those discussions.  In particular, the OpenSSF has become a community town hall for driving security engineering efforts, discussions, and industry-wide collaboration."  And again, I will say what I said two weeks ago.  I will encourage any of our listeners who are so inclined to go over to OpenSSF.org and poke around.  Consider perhaps getting yourself involved.  



Google's post goes into greater details about their plans for participation.  But I wanted to just follow-up on our introduction of the OpenSSF two weeks ago to note that this is looking like the organization that's going to succeed.  Previous efforts were well-meaning but premature; and as history shows, visionaries are often too far ahead of the pack.  As the saying goes, they're the ones who get the arrows in their backs just because they're out in front, and they get the problem, but there's just not enough yet mass behind them.



To me it feels like the open source movement is finally being recognized and is earning the respect it deserves.  And it may have taken something like the scare of the Log4j vulnerability at the beginning of this year to give major organizations a bit of a wakeup call, to realize just how dependent they had slowly grown on open source solutions through the years.  But either way, it appears that it's finally happening now.  And, you know, bravo.  We really need someone to take a look at the things that have sort of just been created with no oversight and no real focus on security in mind and get them strengthened.



As I have promised, I won't spend lots of our listeners' valuable time discussing boring details of endless ransomware attacks.  But when a ransomware gang gets so big for their britches that they suggest that perhaps a government which is refusing to pay their ransom should be overthrown by its citizenry...



LEO:  This was so ridiculous.



STEVE:  I know.  I think that rises to a new level of interest.



LEO:  Yes.



STEVE:  I referred to this drama for the first time, mostly in passing, last week.  Russia's Conti ransomware gang is behind the attacks on several Costa Rican government ministries.  Over the weekend they doubled their ransom demand from $10 million to $20 million.  The Costa Rican operations which have been affected are the Finance Ministry; the Ministry of Science, Innovation, Technology, and Telecommunications; the Labor and Social Security Ministry; the Social Development and Family Allowances Fund; the National Meteorological Institute; the Costa Rican Social Security Fund; and the Interuniversity Headquarters.



In two messages posted to Conti's leak site on Saturday, the gang, which has already leaked 97% of the 670GB stolen during their attacks, claimed the U.S. government was "sacrificing" Costa Rica, and that the country's government should pay for the decryption keys to unlock their systems.  As I mentioned last week, Costa Rica's new government had just taken office and immediately declared a state of emergency after refusing to pay the initial $10 million ransom demand issued by Conti.  Costa Rica has received assistance from officials in the U.S., Israel, and other countries.  And the context for me mentioning all this last week was the U.S. State Department's announcement of a $10 million bounty for information about anyone connected to Conti, with an additional $5 million payable for information leading to an arrest and conviction.  So Conti posted:  "Why not just buy a key?"



LEO:  Yeah, why not?  Yeah.



STEVE:  Yeah.  It's only $10 million.  Yeah, come on.



LEO:  Only $10 million, yeah, no big.  Come on.



STEVE:  And the person wrote in the first person, saying:  "I do not know if there have been cases of entering an emergency situation in the country due to a cyberattack."  I think they're like, again, getting a little ahead of themselves.



LEO:  Is this a ransomware for hire kind of ransomware, Conti?  Like it's a service?  So this could be just some guy who's being a jerk?



STEVE:  Good question.  I don't think...



LEO:  Or is this the Conti group themselves?



STEVE:  I think this is the Conti group themselves.  And actually they sort of mention...



LEO:  Probably Putin's henchmen, honestly.



STEVE:  They mention this.  So they said:  "In a week we will delete the decryption keys for Costa Rica."  And then followed up with a posting:  "I appeal to every resident of Costa Rica."



LEO:  So evil.



STEVE:  I know.  "Go to your government" - wherever that is - "and organize rallies so that they would pay us as soon as possible."  But apparently you only have a week because the decryption keys are going to get deleted.  Then it's too late.  They said:  "If your current government cannot stabilize the situation, maybe it's worth changing it."



LEO:  What jerky jerks.



STEVE:  Like I said, too big for their britches.



LEO:  Oh, my god.



STEVE:  In another message, the group called President Joe Biden a "terrorist," probably as a result of the State Department's new bounty declaration, and said it was raising the ransom to $20 million.  The group also implied that it would begin calling government officials to demand the ransom.  Yeah, like they've got a spare $20 million in their pockets.



And then, finally, the last message I'm quoting, they said:  "Just pay before it's too late.  Your country was destroyed by two people.  We are determined to overthrow the government by means of a cyberattack.  We have already shown you all the strength and power.  You have introduced an emergency."  Wow.



LEO:  So depressing.  They're such jerks.



STEVE:  Yeah.  It's true that Costa Rica is limping along at the moment.  The attack crippled the country's customs and taxes platforms alongside several other government agencies, even bringing down one Costa Rican town's energy supplier.  The country's treasury department has been unable to operate any of its digital services since the attack, making it nearly impossible for paperwork, signatures, and stamps, all required by law, to be processed.



More than three weeks after the attack began, the country is still facing significant struggles, particularly because of the damage done to the Finance Ministry.  Last week the country told its residents that taxes need to be calculated by hand and paid in person at local banks, as opposed to the digital system the country had previously used.  So back to the Stone Age.  Or pre-Internet, at least.  Wow.  And yes, Leo,  as you said, they're not going to get any money from this.  Costa Rica will get outside assistance and limp themselves back into existence.  And nothing will come of it.



Okay.  Policing the Google Play Store.  There's a probably intractable problem with the model we currently have - and I can't think of a better one, so I'm not criticizing the model, I'm just elucidating - for freely downloadable mobile device apps created by individuals lacking a reputation.  After all, everyone starts off with no reputation.  Android handsets are available for a fraction of the price of Apple's devices, and Android users typically cite the expansive freedom provided by the Android platform as their primary reason for preferring that much more open mobile environment.  But those listening to this podcast realize that with that freedom comes significantly increased danger.



I think it's clear that Google is doing the best they can to minimize this danger.  But a continuous daily incoming torrential flood of apps is arriving at the Google Play Store, and there is just no way for Google to deeply research the behavior in each and every one of these apps.  And to provide the useful and powerful freedom that Android users demand, apps must be given powerful enough access to the underlying hosting platform that a malicious app could be quite abusive.  So Google is always stuck playing catch-up.  And in addition to their efforts, thank goodness, they're able to rely upon the motivations and scrutiny that is also being offered by third-party security companies.



Yesterday, Trend Micro posted their piece titled "Fake Mobile Apps Steal Facebook Credentials & Cryptocurrency-Related Keys."  In their article, Trend Micro explained that malware that's expressly designed and intended to steal the Facebook logon credentials of Android phone users continues to pop up on the Play Store.  Such malware has become so commonplace, in fact, that it's now being called "Facestealer" malware.  But it doesn't say that on the cover, of course.  It's hidden in apps that otherwise look harmless, compelling, and are of course completely free.



Trend Micro recently identified more than 200 Facestealer variants in the Google Play Store, notified Google, and Google took them down.  But how long will it be before they're replaced by another 200?  Some of the apps that were just taken down had been installed more than 100,000 times.  The apps take the form of tools for editing, manipulating, or sharing photos, but they can take many other forms.  An example was "Daily Fitness OL" which appears to be a fitness app complete with exercises and video demonstrations.  I looked at the screenshots of this thing.  It's gone now.  But it looked, like, completely convincing.  But it was entirely designed to steal the Facebook credentials of anyone who used it.



The so-called Facestealer apps were first identified in July of last year and have been linked to Russian servers by researchers with the mobile security company Pradeo.  Attackers typically use the compromised Facebook accounts they acquire for various malicious purposes such as phishing scams, fake posts, and ad bots.  In the case of Daily Fitness OL, users are prompted to log into Facebook through an embedded browser.  Okay, what could possibly go wrong with that; right?  Then, not surprisingly, a piece of JavaScript is injected into the loaded webpage which, of course, steals the logon credentials entered by the user.  Easy peasy.



Trend Micro identified many other Facestealer apps with names like Enjoy Photo Editor, Panorama Camera, Photo Gaming Puzzle, Swarm Photo, and Business Meta Manager.  And in addition to these 200-plus Facestealer apps, Trend Micro noted they had also found 40 fake cryptocurrency mining apps that are designed to steal their users' cryptocurrency, not only what it mines, but what was there before.  Last month Google reported that last year they had removed more than one million malicious apps from the Play Store.  Think about that.  One million malicious apps in 2021 alone.  An intractable problem indeed.



And the trouble is, there is next to zero general awareness of this problem among the Android-using population. There are presently more than three billion - with a B - active Android devices being used worldwide.  There's no question that the majority of Google Play Store apps are legitimate and well meaning.  But when a malicious app is only removed after having been downloaded and installed more than 100,000 times, it's also clear that downloading Android apps carries a non-zero risk, and it's not clear that there's anything that can be done.  Again, it's a nice open model, but it is under constant relentless attack.



Speaking of relentless attack, the situation has indeed grown more dire for F5 Systems' BIG-IP boxes.  The day after we talked about this last week, CISA added that recently disclosed F5 BIG-IP flaw to its Known Exploited Vulnerabilities Catalog following reports of its active abuse in the wild.



The problem is CVE-2022-1388, bearing a well-deserved CVSS of 9.8, due to a critical bug in BIG-IP's iControl REST endpoint, which provides an unauthenticated attacker with a method to execute arbitrary system commands.  And I actually saw one that was posted in some of the write-ups.  They posted that one of the commands that was being issued was "rm -rf" - that "-r" as in recurse - \*, which of course will remove all the files on the system, in the file system, starting from the device's root.  Wow.  Anyway, the firm Horizon3.ai wrote:  "An attacker can use this vulnerability to do just about anything they want to the vulnerable server.  This includes making configuration changes, stealing sensitive information, and moving laterally within the target network."



Although patches and mitigations for the flaw were introduced by F5 on May 4th, the Wednesday before last, we know how that tends to go.  And in fact the F5 boxes have been subjected to in-the-wild exploitation ever since F5's announcement was followed by reverse engineering the fix and then going to town.  Some attackers attempted to install web shells that would grant backdoor access to the targeted systems, and others simply destroyed the device's usability by executing that recursive "rm," the remove files command, across the entire file system from the root outward.



Rapid7 wrote:  "Due to the ease of exploiting this vulnerability, the public exploit code, and the fact that it provides root access, exploitation attempts are likely to increase."  But their security researcher Ron Bowes added that:  "Widespread exploitation is somewhat mitigated by the relatively small number of Internet-facing F5 BIG-IP devices."  And, yeah, those that are still surviving on the Internet because, once wiped, they're no longer a BIG-IP device.  They're just a machine with no mission.  The SANS Internet Storm Center (ISC) wrote on Twitter that:  "Given that the web server runs as root, this should take care of any vulnerable server out there and destroy any vulnerable BIG-IP appliance."  And indeed that's what's happening.



Pursuant to CISA's addition of this vulnerability to their catalog, all Federal Civilian Executive Branch agencies have been mandated to patch all systems against this issue by May 31st.  But that's two weeks from today.  And of course by that time there will be nothing left standing to patch.  I did see GossiTheDog also tweet that he had used - I'm blanking on this service, the scanning service.  Oh, Shodan.  He had used Shodan to find the machines.  They were discoverable using Shodan.



LEO:  Oh, boy.



STEVE:  So, you know, it's just a matter of...



LEO:  They're out there.



STEVE:  Yeah, no time before they'll all be gone.  Okay.  I have a piece of errata.  Not often, but important we do this.  Since it's an interesting and important topic that's perfect for this podcast, I want to take a moment to talk about classical computing, quantum computing, and symmetric versus asymmetric cryptography.



Back in 1994, an American mathematician by the name of Peter Shor conceived of an algorithm for quantum computers which would be able to determine the prime factors of integers.  That algorithm worked, and it bears the name "Shor's Algorithm."  Wikipedia explains that:  "The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform and modular exponentiation by repeated squarings."  They write:  "If a quantum computer with a sufficient number of qubits [quantum bits] could operate without succumbing to quantum noise and other quantum-decoherence phenomena, then Shor's algorithm could be used to break public key cryptography schemes, such as The RSA scheme, the Finite Field Diffie-Hellman key exchange, and the Elliptic Curve Diffie-Hellman key exchange."



In other words, I was incorrect to state last week that the use of elliptic curve crypto was "post-quantum" safe.  It's generically any asymmetric public-key crypto that we're currently using that is not safe.  And I know better, so I wanted to correct the record.  It's symmetric crypto, interestingly enough, that remains safe in a post-quantum crypto world.



LEO:  What uses symmetric?



STEVE:  Well, everything does.  We start with asymmetric in order to share the key.  Then that key, because asymmetric encryption is so slow, we don't actually do the bulk encryption and decryption asymmetrically.  We only use the asymmetric encryption for the key.  So that decrypts the key.  Then we use symmetric encryption, like Rijndael AES, to perform the actual bulk decryption or encryption.



LEO:  Because with symmetric the real issue is the transfer of that key.



STEVE:  Correct.  Correct.



LEO:  So you use public key to kind of get the symmetric key across.



STEVE:  Precisely. 



LEO:  And then you can continue with the symmetric key.



STEVE:  Precisely.  So Wikipedia explains:  "RSA is based on the assumption that factoring large integers is computationally intractable.  As far as is known, this assumption is valid for classical non-quantum computers.  No classical algorithm is known that can factor integers in polynomial time.  However, Shor's algorithm shows that factoring integers is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer.  It was also a powerful motivator," writes Wikipedia, "for the design and construction of quantum computers" - yeah, let's crack crypto, that would be good - "and for the study of new quantum computer algorithms.  It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called 'post-quantum cryptography.'"



Okay, now, Leo.  The good news is - you're going to get a kick out of this - from a practical standpoint it still looks like we're well away from the quantum crypto apocalypse, since Wikipedia also reports on the recent progress being made in quantum prime factorization.  They write:  "In 2001, Shor's algorithm was demonstrated by a group at IBM."



LEO:  Wait'll you hear this.  Go ahead.  Go ahead.  What did they factor, Steve?



STEVE:  "Who factored 15..."



LEO:  Fifteen, the number 15.



STEVE:  "...into 3 times 5."



LEO:  Wow.



STEVE:  "That used nuclear magnetic resonance implementation of a quantum computer with 7 qubits."  Now, progress; right?  "After IBM's implementation, two independent groups implemented Shor's algorithm using photonic qubits..."



LEO:  Ooh.



STEVE:  Yeah, that's some fancy photonics out there.



LEO:  They could get a big number there, huh?



STEVE:  Well, "...emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits."



LEO:  Is that good or bad?



STEVE:  Who knows?



LEO:  Seems like entanglement would be bad, but okay.



STEVE:  I mean, this is all deep voodoo.



LEO:  I know.



STEVE:  Okay.  Now, so 11 years later, in 2012, the factorization of 15 was performed with solid-state qubits.  So, okay.



LEO:  Oh ho.



STEVE:  Yeah.  Also in 2012 - wait for it - the factorization of 21 was achieved.



LEO:  Oh, my god.



STEVE:  Setting the record, Leo.



LEO:  Wait a minute, was it 7 and 3?



STEVE:  Uh, that's good.  Those are both prime.



LEO:  I did that in my head.  I'm faster.



STEVE:  That's, you know, you are faster than a quantum qubit.  That set the record for the largest integer factored with Shor's algorithm.



LEO:  What is the largest integer ever factored?



STEVE:  Well, now, here's the problem.



LEO:  Yeah.



STEVE:  Three years ago - now we're up to three years ago, just in 2019 - an attempt was made to factor the number 35.



LEO:  Thirty-five, wow.



STEVE:  Yeah.



LEO:  "Attempt" sounds like they couldn't do it.



STEVE:  Yeah, unfortunately it was using Shor's algorithm on an IBM Q System One.  I hated Q, I told you.



LEO:  Yeah, yeah.



STEVE:  But the algorithm failed.  We were unable to factor, could not factor 35.



LEO:  Wait a minute.  Wait a minute.  Shor's algorithm doesn't work?



STEVE:  And Leo, it's not fair if you give IBM Q System One a hint on the prime factorization of 35.



LEO:  They're two prime numbers.



STEVE:  That would be cheating.  Don't give it a hint.  The algorithm failed because of accumulating errors before we got to the answer.



LEO:  So it's not the algorithm's fault.  It's those damn quantum bits.



STEVE:  You know, they're a little fuzzy on the edges.  And so they're not, you know, you want a zero, or you want a one.  You don't want, like, something in between there.



LEO:  No, yeah.



STEVE:  So these algorithms are similar to classical brute force checking for factors.  So unlike Shor's algorithm, they are not expected to ever perform better than classical factoring algorithms.  Okay.  So.



LEO:  The whole point of Shor's is down the road, using many, many qubits - we're going to find out how many in a moment, and you will love the number - this is going to be better than just brute force.



STEVE:  Well, better than prime factorization the old-school way, right.



LEO:  Right.



STEVE:  So quantum computers...



LEO:  Using Newton's method or whatever it is.



STEVE:  ...have successfully factored the four-bit value of 15.  Now they've done it several times.  And Leo, you did it right here in front of us, which was astonishing.



LEO:  Whew.  I thought that was impressive, wasn't it, yeah.



STEVE:  Wow.  And broke the record by factoring the five-bit value of 21 using Shor's algorithm.



LEO:  Oh, nice.



STEVE:  However, they thought, hey, we're on a roll here.  We did four bits.  Then we got five bits.  In an effort three years ago in 2019, couldn't quite make it to six bits to factor 35.  



LEO:  Darn it.



STEVE:  Which is binary 100011.



LEO:  Is that supposed to help me?  Because I think it's 7 and 5.  But I might - I could be wrong.



STEVE:  Oh, Leo, you spoiled it.  We had to wait.  We had to wait for the answer.



LEO:  Okay.



STEVE:  To come, you know.



LEO:  Okay, fine.



STEVE:  Now, one of our listeners is a crypto-aware physicist who wrote after last week's podcast.



LEO:  Oh, good, good.



STEVE:  And he raised a couple of very good points which I want to share.  We'll get to him in a minute.  But I want to finish up on the asymmetric versus symmetric crypto question.  Elsewhere, Wikipedia notes that:  "In contrast to the threat quantum computing poses to current public key algorithms" - and again, we should not be worried, we don't need to change our passwords, everybody's okay - "most current symmetric cryptographic algorithms and hash functions are considered to be relatively secure against attacks by quantum computers.  While Grover's algorithm" - I think he was that green creature, wasn't he, on 'Sesame Street'?  "While Grover's algorithm does speed up attacks against symmetric ciphers, doubling the key size can effectively block these attacks.  Thus," Wikipedia concludes, "post-quantum symmetric cryptography does not need to differ significantly from current symmetric cryptography."



And as we know, we already periodically double the lengths of our symmetric crypto keys and hashes as the speed of traditional computation begins to narrow their practical security margins.  So anyway, we can all collectively, Leo, breathe a sigh of relief.  I absolutely want to correct the record.  It's not that  ECC is better.



LEO:  It's just as vulnerable.



STEVE:  Yes.



LEO:  Basically any asymmetric scheme is just as vulnerable.  Is that what you're saying?



STEVE:  Well, there is, for example, lattice-based crypto.  There are asymmetric post-crypto.  So it's not...



LEO:  Yeah, because we kind of need asymmetric.



STEVE:  Yes.  Yes.  We need it, and we're going to have it.  And already the mathematicians are loving the fact that we're now at, well, not quite to six bits.  But that lets them dangle the threat in front of the budget committee in order to get more funding because maybe we'll get to six, and maybe to seven bits.



LEO:  The problem right now is just expanding the number of bits in the key, right, to 512, 1024, 2048.  



STEVE:  And Leo, at some point, we will get to a bit number that you will not be able to factor in your head.  And that will be a day to celebrate.



LEO:  I think we've gotten past that point, but okay.  What did your physicist friend say?  Because I'm very curious.  He's a crypto expert as well as a physicist, so he would have a good handle on all of this.



STEVE:  His name is Alim, and he's @dutchphysicist on Twitter.



LEO:  Oh, okay.



STEVE:  He said:  "Hi, Steve.  Hope that all is well with you.  As my weekly routine, I listened to your last podcast episode where you discussed Biden's memorandum on Quantum Computer threats on classical cryptography.  Being a physicist by education (Ph.D.) and having practiced PKI-related IT work" - public key infrastructure - "for the past five to six years, I wanted to make a few remarks on your comments."  And I'll just preface this by saying they're good ones.



"It is not only," he wrote, "RSA (based on difficulty of factorization problem), but also ECC (based on the discrete logarithm problem) which is vulnerable to Shor's algorithm, but a powerful enough quantum computer."  He said:  "Recently, Bruce Schneier also referred to an academic article where the authors discussed how much qubit capacity is required to achieve a reasonable attack on the Bitcoin blockchain."



LEO:  Oh.  There you go.  That's an interesting problem.



STEVE:  Ah, Leo.  You know, is that seven bits?  Is that eight?  You need 10?  How many you need?



LEO:  Is that also - is blockchain based on prime factorization?  It's the same idea?  I don't know.



STEVE:  That's a good point.  He says:  "Indeed, the required number of physical qubits is tremendous, on the order of 10^6."  So I think the blockchain also is safe because we don't yet have six, let alone 10^6.



LEO:  Because that's a lot.  Okay, yeah.



STEVE:  Yeah.  That would a million qubits.  And they can't be fuzzy little bits.  They've got to be, you know, they have to be sure of themselves.



LEO:  Well, stability is a big problem with these.



STEVE:  You've got to be sure of your bitness, yeah.



LEO:  Okay, all right.



STEVE:  "On the other hand, we know how fast it went," he says, "with the traditional silicon technology.  Remember Moore's law."  I'm not sure that applies here.



LEO:  I don't know if this applies, yeah.



STEVE:  It feels too - I don't think it does.  But the point here is now he really raises some good ones.  He says:  "I find Biden's statement correct from some aspects.  First, there is an attack called 'store now, decrypt later.'"  Of course we talked about that years ago.



LEO:  And that's why you want perfect forward secrecy; right?  Because yes.



STEVE:  No, actually.



LEO:  No, that doesn't solve it?



STEVE:  That doesn't work for that.



LEO:  Oh, okay.



STEVE:  Yeah.  He says:  "Any stored information today can be broken by a powerful quantum computer in the future.  Therefore, any confidential information that should stay confidential for a long period of time should today be protected against quantum computers of tomorrow."  And again, I mean, that's a good point.  Also, and this is really good, everyone will get this, he wrote:  "Achieving crypto agility is very difficult.  Most cryptographic algorithms are embedded deep in the protocols and products.  There are even cases that DES algorithm is still used in SIM cards and payment cards today."



LEO:  Right, right.  And that's been broken for years, yeah.



STEVE:  Yeah.  "This makes the lifetime of such algorithms very long," he says, "i.e., 40 years or more.  Therefore," he writes, "Biden's warning on the federal agencies, and thus the industry, is an early call for a very hectic and difficult transition."  He said:  "Though I also see some issues with Biden's statement."  He wrote:  "Two, NIST's competition on post-quantum algorithms has not announced the winners yet, and standard finalization is still a few years down the road.  From this perspective," he said, "any organization to attempt to implement a post-quantum solution is a premature action.  Things may still change."  Meaning let's not be too quick to force NIST to choose a winner here.



LEO:  Right.



STEVE:  And he finally said:  "I sincerely hope that Biden's administration will not implicitly try to pressure NIST to finalize the competition by this statement.  Just a few moments ago a weakness was found and reported in one of the post-quantum digital signature algorithms," he said, "Rainbow.  There should be no political pressure on such standardization activities, and researchers should be left free in making their decision and given enough/proper time."



LEO:  So we don't really have a quantum-safe algorithm yet.



STEVE:  No.  We haven't, like, NIST has not standardized on the way we have, like, Rijndael, which is now our chosen AES.



LEO:  And his point is you don't want pressure on NIST to propose something prematurely.  You really, I mean, you want to get this right.



STEVE:  Yes.  And get people using it.  The concern is that something will be standardized, and there will be a political mandate to start using it because, as he noted, there was just a problem found in one of the post-quantum digital signature algorithms.



LEO:  Yeah, so much for that.  Okay.



STEVE:  So, like, yeah, we don't want that to be in use when the problem is found.  And again, it's like, you know, 35 we still can't factor.  So just, you know, but he's also right about it taking - it being so difficult to change this stuff.



LEO:  Yeah.  Yeah.



STEVE:  So he finished, saying:  "Needless to say, I definitely share your opinion that there are way more fundamental issues to be addressed urgently," which was the point I was making when I was kind of poking fun at this last week.  He said:  "However, I believe that quantum computer threats on cryptography is very serious and should be given enough attention due to slow adaptation or adoption of new cryptographic algorithms in billions of computers, protocols, et cetera."



LEO:  But you can't rush it, either.



STEVE:  Exactly.



LEO:  So we're just going to keep looking for a post-quantum technology, and then I'm not too worried about my SSH keys being cracked by a quantum computer anytime in the next - in my lifetime, probably.



STEVE:  Apparently it's got a ways to go.



LEO:  Yeah.  But important point, the fact that I switched to elliptical keys isn't really germane.  



STEVE:  Yeah.  Actually, the elliptical keys are nice, mostly because they're so much shorter.



LEO:  They're small, yeah.



STEVE:  They're much shorter.  It's much...



LEO:  Yeah, I used to have to paste a paragraph in.



STEVE:  Right.



LEO:  Now it's one line.  So that's, I mean, that's a silly reason why.  And that 512 bits is better than the 2048 bits I had in RSA, I presume.  Or as good as.  Okay.  All right.



STEVE:  Yeah.  Also I got, just for the record, I got a bunch of tweets from people saying, "Steve, you'd better look at ECC again and see if you still think it's quantum-safe."



LEO:  But the thing I was worried about is somebody saying, oh, you're so wrong, quantum computing is just around the corner.  I don't see that yet.  But, you know.



STEVE:  And again, I think we framed it exactly right.  Yes, we need to be thinking about it.  Yes, it is advancing.  Yes, it is advancing slowly.  But yes, it does take a long time to take existing crypto out of circulation.



LEO:  Yeah.



STEVE:  And, you know.  So I would say as soon as we absolutely know that we have a bulletproof post-quantum crypto, where problems are not going to be found in it - and, see, and that's the other problem is we were, when we went through the Rijndael competition, or the AES competition which ended up choosing Rijndael, it was a whole bunch of ciphers.  And they were able to say, well, we used a Feistel network here, and we ran that through a triple scrambler, and blah blah blah.  I mean, like our understanding of how to do symmetric crypto, which is what AES and Rijndael are, it's so mature that we were like taking building blocks and mixing and matching them and understanding in detail how they worked.  We're in a whole new environment now with post-quantum crypto.  And we're at risk of making a mistake.  And that's almost worse than prematurely obsoleting something that isn't broken today.



LEO:  Right.



STEVE:  So, yeah.



LEO:  Yeah.



STEVE:  I got a nice note from someone whose name I thought I ought to redact, even though he didn't say I should.  He said:  "In Security Now! you've reported on a number of cybersecurity initiatives that the federal government has introduced this past year, including CISA's 'Known Exploited Vulnerabilities Catalog,' Congress's 'Strengthening American Cybersecurity Act,' and the White House's 'Executive Order on Cybersecurity.'  What I haven't heard you mention are the TSA's two 'Security Directive Pipeline' memorandums.  These are two successive directives, issued in response to the Colonial Pipeline compromise, that impose explicit cybersecurity requirements upon the midstream oil and gas pipeline industry."



And he said:  "One of the lesser-known regulatory mandates of the TSA," he says, "yes, that TSA, is the safety of interstate pipelines."  He says:  "I work in the midstream pipeline industry, and these TSA directives have been the bane of my existence for the better part of a year.  I'll reserve specific criticism, but will offer a recent Politico article which summarizes the situation nicely.  Unfortunately, I'm not able to go into many particulars because the government, in its infinite wisdom, has marked the entire second directive (SD02) as Sensitive Security Information which prevents me from publicly divulging details.  Suffice it to say that, yes, the government has instituted a cybersecurity standard that a segment of critical infrastructure must adhere to, but that can't be discussed except behind closed doors."



LEO:  See, that seems like a bad idea.  



STEVE:  I could not agree more.  Yup.



LEO:  I understand the notion of security through obscurity.  But honestly, anything that you're doing for security should be tested.



STEVE:  Needs oversight, yes, yes. 



LEO:  Other experts have to look it over, I think.



STEVE:  He said:  "One tidbit that I'm compelled to share is the role that CISA's Known Exploited Vulnerabilities Catalog plays.  SD02 requires that pipeline operators patch vulnerabilities published in the Catalog within certain timeframes.  Since you've mentioned the Catalog in several Security Now! episodes, I wanted to call out the fact that this applies not just to government entities, but also to private pipeline companies.  And yes, we are forced to review the list daily for new additions."



And he finished:  "Thank you for all you do, and especially for a wonderful and informative weekly podcast."  And I will say back to him, thank you.  You've just contributed to making it more informative.



LEO:  Nice.



STEVE:  So Liam Lynch tweeted from @L2actual, which I thought was a cool handle.  He said:  "Hi, Steve.  I only listened to Episode 869 the other day and heard you and Leo again refer to the GDPR as being the cause of cookie notices.  I kept meaning to contact you about this, as the only thing the GDPR did for cookie notices was to strengthen the consent requirement."  On the other hand, I would argue that is what created the notices.  He said:  "Cookie notices have been around for a lot longer than the GDPR has been in force, as they came from the ePrivacy directive."



LEO:  Yeah.  But we ignored it until GDPR.



STEVE:  Exactly.  And then he cited a thread.  So I wanted to for the record put that out there.  But again, it was the GDPR that gave it teeth and then forced it to be such an annoyance for all of us.



LEO:  It is kind of the canonical example of overregulation or privacy regulation gone wrong, I think.



STEVE:  Boy.



LEO:  Yeah, yeah.



STEVE:  And I wanted to officially note that the work on SpinRite's backend has officially finished.



LEO:  What?



STEVE:  SpinRite's new hardware drivers are working without any exception that the group's extensive testing has revealed.  In the case of two very old systems, they were like back when you could set the DMA speed in the BIOS.  You remember, like you could set it to 0, 1, 2, or 3 or something.  They were also the first VIA implementation chipset.  In two very old systems it was necessary to turn off the Ultra DMA setting in the BIOS to obtain reliable transfers.  SpinRite detected that they weren't reliable and refused to operate otherwise.  But when that was done, everything worked perfectly.



Throughout this work, SpinRite's new and much-improved benchmarking was used to exercise those backend drivers through the IO abstraction that I've talked about before.  So what that does is effectively isolate any backend devices from the front-end code.  When SpinRite 7 adds native hardware USB drivers and then NVMe drivers, nothing else about SpinRite's front-end needs to change because the IO abstraction provides a uniform interface to the front-end code.  The benchmark was the first client of that IO abstraction.  The actual SpinRite machine, with its multiple switchable screens, the grid display, the DynaStat data recovery, the detailed technical log and all the rest will be the second and final client.  So that's where I now turn my focus.  What we've just slogged and fought through has been by far the longest and toughest part, since it was where all of the hardware and machine dependency was.  Now that's all behind us.



Since the BIOS was historically SpinRite's IO abstraction, which is now gone, I now have a lot of rewriting to do to support SpinRite's new abstraction.  But it's not the sort of thing that will need constant interaction and iteration and tireless testing, the way the previous work on the backend did.  I'm sure that the SpinRite testing gang will end up finding things I've missed and will have ideas for improvements when they begin to see something that is actually operating, which is what will happen next.  But at this stage they're mostly going to be waiting for me, rather than me waiting to learn from them how the latest test release turned out.  So we're getting close.



LEO:  Getting very close.  Very exciting.  Yay.  All right, Steve.  I'm very curious.



STEVE:  Okay.



LEO:  We've been of course reporting on this story for the last week.  We talked about it Sunday on TWiT, on TWiG on Wednesday.  But I'd like to get your take on the new EU rules here.



STEVE:  So the title of today's podcast, "The New EU Surveillance State," might seem hyperbolic.  But just wait till you hear what the EU is proposing.  The European Union's proposed new legislation will not only require scanning encrypted communications for child sexual abuse material content, but believe it or not, actually reading all text messages with the goal of detecting any textual content that might be regarded as "grooming" a minor.



LEO:  Ugh.  That's such a nebulous term.



STEVE:  Oh, Leo, it's impossibly nebulous.  Okay.  So those who haven't read far into the legislation quickly and correctly recognize that accomplishing any of this is inherently, necessarily, and unavoidably violating - it requires some agency or entity to scrutinize all communications capable of conveying any graphical or textual material.  In other words, all of the social messaging platform used by European Union citizens.  And such scrutiny necessarily contravenes the well-established goals, intents, and capabilities of end-to-end encryption.  So, yeah, this would be the end of true meaningful privacy enabled and facilitated by end-to-end encryption.



But reading some of the proposed legislation, as I did, one discovers that it also requires that this surveillance goes beyond the matching of previously known content hashes to also include content that has not been previously seen.  So this would require either humans to view everything that everyone sends to anyone, or to train up machine vision and learning models to automate the identification of previously unknown child sexual abuse material.



Okay.  Listen to what Johns Hopkins cryptographer Matthew Green tweeted upon hearing of this last week.  Mathew tweeted:  "This document is the most terrifying thing I've ever seen.  It is proposing a new mass" - I mean, Leo, as I'm saying this, it sounds like fiction.  It sounds like I'm making this up.  This is the actual legislation.  He says:  "It's proposing a new mass surveillance system that will read private text messages, not to detect CSAM, but to detect 'grooming.'"



LEO:  That's the ridiculous part.  CSAM you could pretty much know that's CSAM.  But if I ask you, Steve, what's your age, sex, and location, am I grooming you?



STEVE:  Right.  Or what are your pronouns?



LEO:  Yeah.  It's very contextually dependent.  And I think it's a judgment call.



STEVE:  So it actually says, it actually says in the legislation...



LEO:  And, by the way, that eliminates E2E encryption entirely  because they say encryption's no excuse.



STEVE:  No.  In fact, they say we still want the best encryption possible.  Okay.  But we also want this.  So here's what Matthew highlighted from the legislation.  This is the legislation:  "As mentioned, detecting 'grooming' would have a positive impact on the fundamental rights of potential victims especially by contributing to the prevention of abuse; if swift action is taken, it may even prevent a child from suffering harm.  At the same time, the detection process is generally speaking the most intrusive one for users (compared to the detection of the dissemination of known and new child sexual abuse material), since it requires automatically scanning through texts in interpersonal communications."



I mean, so the point is the legislators know this.  And they continue:  "It is important to bear in mind in this regard that such scanning is often the only possible way to detect it; and that the technology used does not understand the content of the communications, but rather looks for known pre-identified patterns that indicate potential grooming.  Detection technologies have also already acquired a high degree of accuracy" - and they cite something, it's footnote 32 - "although human oversight and review remain necessary, and indicators of grooming are becoming ever more reliable with time, as the algorithms learn."  Give me a frigging break.



LEO:  It's pretty horrible.



STEVE:  Oh, Leo.  Matthew then issued a series of tweets in a thread, which I'll read.  He said:  "Let me be clear what this means.  To detect grooming is not simply searching for known CSAM.  It isn't using AI to detect new CSAM, which is also on the table.  It's running algorithms reading your actual text messages to figure out what you're saying, at scale.  It is potentially going to do this on encrypted messages that should be private.  It won't be good, and it won't be smart, and it will make mistakes.  But what's terrifying is that once you open up 'machines reading your text messages' for any purpose, there are no limits.  Here is the document.  It is long but worth reading because it describes the most sophisticated mass surveillance machinery ever deployed outside of China and the USSR.  Not an exaggeration."



The link Matthew shared last week was from a leak of the official legislation, which then appeared the next day.  They are the same, I looked at them both, 135-page document.  The title is "Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL laying down rules to prevent and combat child sexual abuse."



As I said, the legislation is 135 pages.  But its first two paragraphs set the stage, and they're worth sharing.  So here's the first two paragraphs:  "The United Nations Convention on the Rights of the Child (UNCRC) and Article 24(2) of the Charter of Fundamental Rights of the European Union enshrine as rights the protection and care of children's best interests and well-being.  In 2021, the United Nations Committee on the Rights of the Child underlined that these rights must be equally protected in the digital environment.  The protection of children, both offline and online, is a Union priority.



"At least one in five children falls victim to sexual violence during childhood.  A 2021 global study found that more than one in three respondents had been asked to do something sexually explicit online during their childhood, and over half had experienced a form of child sexual abuse online.  Children with disabilities face an even higher risk of experiencing sexual violence.  Up to 68% of girls and 30% of boys with intellectual or developmental disabilities will be sexually abused before their 18th birthday.



"Child sexual abuse material is a product of the physical sexual abuse of children.  Its detection and reporting is necessary to prevent its production and dissemination, and a vital means to identify and assist its victims.  The pandemic has exposed children to a significantly higher degree of unwanted approaches online, including solicitation into child sexual abuse.  Despite the fact that the sexual abuse and sexual exploitation of children and child sexual abuse materials are criminalized across the EU by the Child Sexual Abuse Directive 6, adopted in 2011, it is clear that the EU is currently still failing to protect children from falling victim to child sexual abuse, and that the online dimension represents a particular challenge."



Well, okay.  One of the most difficult lessons an ethical person learns and must come to terms with as they grow is that not all problems have workable solutions.  And it doesn't matter at all how big the problem is nor how much we want there to be a good solution.  The entire problem here can be broken down into two separate issues.  First, in order for some overviewing agency to obtain the raw data to be scrutinized, there can be no true and meaningful privacy between digitally communicating endpoints or individuals.  That must end.  Period.  Everything needs to be visible and visited.  And it's not sufficient to only surveil the devices used by minors because the original intent of detecting child sexual abuse material was to discover and apprehend those non-minors who were actively trading in such illegal content, thus curtailing the demand for the creation of more material.  This means that all of everyone's social media messaging content must pass through surveillance filters.  Which brings us to the second issue, what to do with that content once it's obtained.



Matthew put it bluntly in one of his tweets.  He said:  "It is going to do this on encrypted messages that should be private.  It won't be good, it won't be smart, and it will make mistakes."  Even if we agreed to voluntarily relinquish all of our privacy rights, it's not at all clear that the world has the technology to do what the EU's governing legislators want.  It's easy for them to write a law stating what they want.  But wanting it doesn't will it into existence, no matter how fervently and sincerely they want it.  So the technology is going to miss things that it should catch and flag things that it should not.  Humans will be required to examine the previously private photos that some image classifier believes to be salacious, and previously private text messages shared by consenting adults will be open to others' scrutiny.



And then there's the devil's advocate side which is also absolutely true and well-established.  Cryptography has already escaped.  The algorithms which are able to unbreakably encipher plaintext are already public.  So if the use of truly unbreakable end-to-end encryption is outlawed, then only the outlaws will be using it.  And, yes, that too could be prevented.  The next step in this escalation to doom would be for the communications carriers to refuse to transit any encrypted communications they cannot themselves decrypt.  That's possible.  In which case we might as well just turn back the clock to the 1970s and give up because the Internet would no longer be useful for commerce.



Matthew also noted the other elephant in the room by tweeting:  "But what's terrifying is that once you open up 'machines reading your text messages' for any purpose, there are no limits."  The distasteful issue of child sexual abuse is certainly real.  But it has also been observed that it serves as a convenient stalking horse for governments' much broader interests in monitoring and controlling speech of many other kinds.  Much of such speech could be criminal.  But much that might be of interest to censors would not be.



The EU's governors are wrong to want this legislation which can only be characterized as dangerous, wholly impractical, and impossible to implement as they hope.  So all we can do is pray that it dies.



LEO:  And that's a possibility.  It's not law now.



STEVE:  Correct.



LEO:  It's just a proposal.



STEVE:  Correct.  And, you know, the good news is it's now being aired.  Everybody is talking about it.  The EFF just, I mean, they just melted down, as you can imagine, over this.  And, boy, I mean, as we've said, when we were crossing into this next decade, encryption is this massive challenge.  I mean, it just is.  The governments do not like the idea that they have no means for seeing what their citizenry is saying.  And, I mean, and both sides have valid positions.  It's a tough one.



LEO:  Learn how to write your own encryption, kids.  You might want to invest in your own telecommunications network while you're at it.



STEVE:  Well, and would somebody who wanted to abide by the law feel okay about doing it?  I couldn't do that.  I mean, first of all, I'm just texting Lorrie when I'm going to be home for dinner.



LEO:  Right.



STEVE:  So it's not like I'm caring about that that much.  There's nothing I'm doing.  But we've been given this, and now it's looking like the EU wants to take it away.  I mean, Leo, literally, you have to, in order to do this "grooming detection," everything someone types on their keyboard of their  computer and their mobile device, it has to go through some censor's screening filter.



LEO:  Yeah.



STEVE:  Or the system isn't providing what the law requires.  And we know that users don't want them on their own phones.  Apple tried that, and everyone went, you know, pinched their nose and said, "Eww.  I don't want any form of kiddie porn on my phone in order to keep Apple from being involved."  So that says you have to have a third party somewhere in a cloud that all this runs through.



LEO:  Yeah.



STEVE:  So you have to have a man in the middle, a sanctioned man in the middle able to decrypt this in order for, you know...



LEO:  Unbelievable.  What a horrible idea.  All right, my friend.  We've wound people up enough.  I think it's time to stop.  That is Steve Gibson right there, man.  That's the guy.  If you like this show, go to his website, GRC.com.  Pick up a copy of SpinRite, the world's best mass storage, I almost said surveillance, mass storage maintenance...



STEVE:  It's not.



LEO:  No, no surveillance.  And recovery utility available at GRC.com.  Pick up 6.0.  You'll get 6.1 free if you buy it today.  And you get to participate in the development.  We're just, obviously, just around the corner.  While you're there you can also get this show.  He has two unique versions of this show at his website, a 16Kb audio file for the bandwidth-impaired and really nicely done transcripts which you can read along as you listen, or use to search for parts of any of the 871 shows.  That's all at GRC.com.  You can leave him feedback at GRC.com/feedback, or slide into his DMs on Twitter.  He's @SGgrc.



We also have copies of the show at our website, 64Kb audio, we have video, TWiT.tv/sn.  There's a YouTube channel also devoted to Security Now!.  If you want to share clips with friends, that's probably the easiest way to do it.  You can also subscribe with your favorite podcast client, get it automatically the minute it's available.  We do the show Tuesdays, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch us do it live.  We stream it live behind the scenes.



STEVE:  Right now.



LEO:  Right now.



STEVE:  It's happening right now.



LEO:  We're doing it as we speak.  Unless you're watching later, and then in this case it's not live.  You can watch that at live.twit.tv in those hours.  If you're watching live, chat live at irc.twit.tv.  Club TWiT members get their own special Discord.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#872

DATE:		May 24, 2022

TITLE:		Dis-CONTI-nued: The End of Conti?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-872.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we'll start by following-up on Microsoft's Patch Tuesday Active Directory domain controller mess.  We're going to look at several instances of the Clearview AI facial recognition system making news, and at the systems which fell during last week's Vancouver Pwn2Own competition.  We cover some welcome news from the U.S. Department of Justice and some disturbing news about a relatively simple and obvious hack against popular Bluetooth-link smart locks.  We have some closing-the-loop feedback from our listeners, including a look at what's going on with the Voyager 1 space probe, and another interesting look into the looming impact of quantum crypto.  Then we finish by sharing an in-depth examination of the surprisingly deliberately orchestrated shutdown of the Conti ransomware operation.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Well, well.  What do we have to talk about?  We're going to say hello to Clearview AI's facial recognition.  They're using it in Ukraine, but is it okay?  And then it's the results of Vancouver's Pwn2Own competition.  We'll talk about Steve's bizarre theory about what's going wrong with Voyager 1, and then a look at the Conti ransomware operation.  It looks like they've gone out of business.  But have they?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 872, recorded Tuesday, May 24th, 2022:  Dis-CONTI-nued:  The End of Conti?



It's time for Security Now!, the show where we cover your privacy, security, and safety online with the man, the myth, the legend, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Apparently we are a well-caffeinated team this morning.



LEO:  Ah, yeah, a little bouncy right now, yes, I am.	



STEVE:  Or this early afternoon, rather, whatever it is.  So you agreed with me that the title for the show is a bad pun.



LEO:  It's godawful, yes.



STEVE:  Dis-CONTI-nued.



LEO:  Dis-CONTI-nued, yes.



STEVE:  Yes.  The end of CONTI, question mark.



LEO:  This has become I think kind of a new MO for these bad guys.



STEVE:  Yes.  Thank you for the perfect foil, segue, Leo, because that's actually what hooked me on the story and what I thought would make it so interesting for our listeners is that there is evidence that this whole Costa Rican debacle was a setup designed to obscure the reformation of CONTI.  And what's really...



LEO:  Oh, interesting.



STEVE:  Yes.  And remember how it didn't, like something seemed off about it.



LEO:  Yeah.



STEVE:  It was like, what?  You know, it just...



LEO:  Yeah, exactly.



STEVE:  It was weird when we talked about it last week, actually for the last couple weeks in various aspects.  And it's looking like it was actually, well, okay.  I don't want to give it all away here at the beginning because we have lots to talk about.  We're going to follow up on Microsoft's Patch Tuesday Active Directory domain controller mess.  We're going to look at several instances of the Clearview AI facial recognition system making news, and at the systems which fell during last week's Vancouver Pwn2Own competition.



We cover some welcome news from the U.S. Department of Justice.  They're rethinking what it means to be an ethical hacker, which is really good news.  And some disturbing news about a relatively simple and obvious hack against popular Bluetooth-linked smart locks.  We've got some closing-the-loop feedback.  Actually only three pieces, but one of them ends up being, well, actually two of them.  Two of the three end up really big, expanding into something because we're going to take a look at what's going on with the Voyager 1 space probe which has just started to act a little wonky.



LEO:  You mean Veeger?



STEVE:  Oh, please don't say "Veeger."  That was, god, was that an awful movie.  Thank goodness somehow they were forgiven for producing that first atrocity.  And then we got Khan, so...



LEO:  Khan.



STEVE:  Yes, Veeger.  I have a theory about what might have happened which you're going to get a kick out of.  



LEO:  Oh, interesting.



STEVE:  Yeah, well, anyway, I'll save that, too.  And another interesting look into the looming impact of quantum crypto.  We then are going to finish by sharing an in-depth examination of the surprisingly deliberately orchestrated shutdown of the CONTI ransomware operation.  It turns out it was far more well-planned than was known.  And essentially the guys that have been watching this have spoiled what CONTI was trying to do.



LEO:  Oh, good.



STEVE:  And it's, yeah, good.  And we have a pretty funny Picture of the Week.  So I think another great podcast for our listeners.  What a shock.



LEO:  Spoil the spoilers.  All right.  All coming up.  Picture of the Week time, Mr. Gibson.



STEVE:  So, okay.  So everything about this picture to me looks authentic.  It's showing a standup card, sort of, not an ad, like a...



LEO:  It's a table card.  They put them on the table or a placard they put outside the event, the conference.



STEVE:  Yes, thank you, yes, for a Microsoft MVP Global Summit.  And again, so this was in a conference in a hotel.  You can see this godawful hotel carpet underneath it.



LEO:  Isn't it always ugly?



STEVE:  Oh.



LEO:  It's always the worst.



STEVE:  I think it's so that it hides barf and anything else that, you know...



LEO:  He's exactly right.  Chewing gum.



STEVE:  Spilled cocktails.



LEO:  Exactly.



STEVE:  Nothing is going to - no spots will show through this horrible print that this thing has.  Anyway, so we have this sign.  And it was originally boring signage.  It probably said, you know, Security Summit Conference 203B or something.  Anyway, some person with a sense of humor changed around the normal heading and everything, which looks 100% authentic.  They changed the sign of the Microsoft MVP Global Summit announcement to say, or to caution:  "Unattended laptops will be upgraded to Windows 11."



LEO:  Ooh.



STEVE:  Which, you know...



LEO:  Mean.



STEVE:  ...is not what anyone wants.



LEO:  No.



STEVE:  So do not leave your laptop unattended, children, or you may find out that your menu can no longer be docked to the left-hand side of the screen.  You must have it in the center because we're sure that's better for you.  Okay.  So speaking of Microsoft, last week we noted that the previous week's Patch Tuesday had been overly eventful, with admins reporting that the month's patch rollup had adversely affected the operation of some of their enterprise's critical authentication infrastructure.  The malfunctioning patch was intended to close an actively abused security vulnerability.  And while there were some pre-patch workarounds, most admins were choosing to simply roll back their systems by removing the entire May Day patch bundle.  So that's where we were last week.



Last week I wondered out loud whether Microsoft would attempt to fix the fix, and hopefully to test it more thoroughly this time, or whether they would prefer to wait until June's Patch Tuesday.  One problem with waiting is that June begins next Wednesday, which places June's Patch Tuesday as late in the month as possible, nearly in the middle, on the 14th.



And in the past, Microsoft's patching logic has been, well, calling it "inscrutable" is being kind.  Sometimes they wait half a year before patching something.  Other times they patch it in a hurry.  Fortunately for Active Directory users, the latter was the case here, with Microsoft fixing the trouble in about a week and releasing a mid-cycle emergency patch last Thursday.



However, presumably since it was only domain controller servers that were being adversely affected, that point that Microsoft took great pains to point out throughout all this, it's like, oh, no, it's all fine, don't worry about it, it's only these people, like the admins of the largest enterprises in the world, who have been brought down by this.  So as a consequence the emergency update is not being made widely available through the regular Windows Update channel.  They're not wanting to like update everybody again because most people aren't affected.



So enterprising users, if you'll pardon the pun, will need to get it from Microsoft's Update catalog by going there directly.  But you can also, those users in the enterprises who have Windows Server Update Services or Microsoft's Endpoint Configuration Manager can import the update from the catalog into those services, and then it'll fix their networks.  So since CISA was forced to pull the requirement of patching this flaw from their catalog of known exploited vulnerabilities due to May's debacle we can assume that it will be returning to the catalog now that it is possible to actually patch against this, remember, actively exploited flaw in Active Directory.



In any event, it should be obvious to everyone these days that, more than ever, patching has become everything.  Patch, patch, patch, patch, patch.  You just, I mean, it's better to patch and regret it than it is not to patch at all because you can unpatch.



LEO:  That should be on a pillow or something.  Good.  That's a good motto.



STEVE:  Well, it's the motto that evolved from this podcast because just look around.  Wow.  Okay.  We've talked about  Clearview AI on several occasions.  Recall that they're the company which provides what have turned out to be quite controversial facial recognition services to apparently anyone who wants them.  The controversy surrounds the perceived privacy invasion created when an army of both visible and invisible cameras might be scanning and cataloging the real-world identities of everyone who crosses through their fields of vision.



Thirty years ago we lacked the ability to automate anything like this at scale because it all comes down to economics.  We didn't have inexpensive cameras.  We didn't have inexpensive communications bandwidth, or inexpensive and apparently endless amounts of storage or computation.  Today we have all of that, essentially for next to nothing.  So what could not be done in the recent past is now feasible at a mass scale.  Global.  During a recent interview, Clearview AI's CEO, a guy by the name of Hoan Ton-That, explained that he intends to have 100 billion images in their database within a year.  Currently they have about 20 billion.  And 100 billion would represent 13 photos of each person on Earth.  Obviously not all equally represented.  But probably the more important you are, the more photos you've got in the database.



Okay.  So the potential for the abuse of this technology is breathtaking due to its sweeping scope.  One of the times we've talked about these guys in the past was when the ACLU, the American Civil Liberties Union, and others brought a lawsuit against Clearview AI in Illinois, thanks to Illinois's very restrictive Biometric Information Privacy Act (BIPA), which became law in 2008.  Thanks to BIPA, any entity wishing to collect biometric identifying information  and a photograph is that.  I knew it was you, Leo, when you sat down, just by looking through the camera.



LEO:  Very high intelligence.



STEVE:  Any entity wishing to collection biometric identifying information for any Illinois citizen must first obtain their explicit consent before doing so, which of course Clearview AI never does.  They're scanning online image sources, so consent is never requested nor received.  Thus they immediately fell afoul of Illinois's very restrictive BIPA.  Earlier this month Clearview AI agreed to a settlement in that lawsuit, which was brought by the ACLU, that would largely prohibit sale of services to private companies which previously weren't prohibited, and people, similarly.  And you were just talking recently about how it turned out that you could get location, like individuals could purchase location information on other individuals.



LEO:  Oh, yeah.  It's nothing.  Data brokers are amazing.



STEVE:  Yes.  And so Clearview AI is clearly a massive cloud-based broker that also provides image matching.  So the terms of the suit prohibit the sale of Clearview AI services to private companies and people in the U.S., as well as to law enforcement throughout Illinois, for the next five years.  However, Clearview AI's CEO said that the settlement would not change anything on a material level because they could continue to sell to any government clients elsewhere.  So they took what they consider to be a little hit, and they'll continue unabated.



At the same time, Clearview AI is increasingly facing pressure from various nations' privacy regulators who are pushing Clearview to remove their data on their citizens from its systems.  Both Australia and Canada last year ordered the company to delete information on their residents.  And now last week the U.K.'s government announced that they are levying a fine of more than 7.5 million pounds sterling, that's around $9.4 million USD, against Clearview AI, and have ordered it to stop collecting information about U.K. residents and to delete all of the data that it already has from its database.



The U.K.'s Information Commissioner John Edwards said in a press release:  "The company not only enables identification of those people, but effectively monitors their behavior and offers it as a commercial service.  That is unacceptable.  People expect that their personal information will be respected, regardless of where in the world their data is being used.  That's why global companies need international enforcement."  Edwards added that he would be meeting with European regulators in Brussels later this week to "collaborate to tackle global privacy harms."  And I didn't mention, these guys are the biggest now globally.  They are the number one facial recognition service provider in the world.



These recent moves by the U.K.'s Information Commissioner's Office follow a joint investigation by the agency and the Office of the Australian Information Commissioner which launched in July of 2020.  So that's been going for some time and was completed last November.  A provisional notice from the Information Commission's Office to Clearview that month warned the company to stop processing and to delete U.K. resident data, as well as suggesting a substantially larger - at the time they were saying they wanted 17 million pounds sterling.  It ended up being 7.5.  But still, you know, it'll get your attention.



In reply, again, Clearview's founder and their chief executive, that Hoan Ton-That, said he was "deeply disappointed" that the U.K. data authority "misinterpreted" his company's technology and their intentions.  Uh huh.  He said:  "I would welcome the opportunity to engage in conversation with leaders and lawmakers so that the true value of this technology which has proven so essential to law enforcement can continue to make communities safe," he said.



Okay.  So there's one side.  There's also been Clearview, although not by name until recently, in the news relative to Russia and Ukraine.  There's no question that facial recognition technology has the potential to be quite useful.  It is Clearview AI's facial recognition technology that has been allowing the Ukrainian government to identify both their own citizen casualties of war, obviously in their war with Russia, as well as the Russian soldier casualties who've been left behind by their comrades.



Thanks to Clearview AI's knowledge of Russian citizens, Ukraine has been able to mount a potentially powerful political counteroffensive by identifying these Russian war casualties, notifying their families of their demise, and offering to allow them to travel to Ukraine to reclaim their fallen family member.  Some 400 Ukrainian investigators are currently using Clearview AI's technology to build much more airtight war crimes cases as a consequence.  It really helps to know who you're talking about.  And certainly Russia's not providing that information.



The publication The Record recently interviewed Clearview AI's founder and CEO, this Hoan Ton-That, whom I referred to before.  I think that the details revealed by that interview are worth sharing.  So I've edited it a bit to bring it up to the level of our listeners.  So in the interview the question is posed:  How did Clearview come to play such a major role in Ukraine?



Hoan replies:  When the war started, it was really shocking to me and a lot of members of our team to see especially the video footage of women and children suffering.  And it made me think, how can we help?  Originally, I would see photos of captured Russian soldiers, and I realized that with that kind of photo quality, our facial recognition technology could be helpful.  So I reached out to a lot of people on our advisory board to ask them, do you know anyone in Ukrainian government?  And one person, Lee Wolosky, he's on the National Security Council under three presidents, he quickly said yes.  We thought that it could be helpful in identifying deceased soldiers and track misinformation.



So how could it be useful to track misinformation is asked.  Hoan says:  You'd see a lot of things on social media saying this is a captured Russian soldier.  But you might see people on the other side saying actually that's a paid actor, and here's the name of the paid actor.  So with this technology, the level of accuracy can be used to identify if someone is who they say they are.



So the question is asked:  Who in Ukraine has logins to Clearview AI?  Hoan:  It's in six different agencies, including the National Police of Ukraine.  Then the people on the ground would be using it.  So we gave a demo.  We'd give them training on how to use facial recognition technology responsibly.  So part of the training is that they would send photos of unidentified people and run it through the system, and we could show them, "Hey, this is how you verify who it is."  So for example, if they have a tattoo that matches online, there's a very high chance it's the same person.



So the questioner:  You inserted this technology into a war zone, which presents a lot more problems than having a police department in the United States using it.  How are you accounting for that?  Hoan responds:  You want to be careful to make sure that they really know how to use it properly, and so there are all these scenarios that we want to make sure don't happen.  For example, what if someone takes a photo of someone and says, "Hey, I think you're a Russian traitor," and then they detain them.  And it's all incorrect based on incorrect information.  So we'd never want something like that to happen.  As long as these investigations are done by trained investigators, this can be a very useful tool.  If it was used by everybody, I think that's when problems happen.



So the interviewer:  What's the most surprising use you've seen in Ukraine?  Hoan says:  War crimes investigations.  We were talking to people in the National Police of Ukraine and others where they have video footage from surveillance cameras.  Smartphones are more prevalent, so there's a higher chance of something being recorded.  Now, with this technology, if it's easy to identify someone, I think people are going to think twice about war crimes.  So that was a surprise to me, he said.



Okay, so the interviewer asks:  So this is a subscription service, and you say that gives you more control if someone is misusing it.  How does Clearview AI work?  He said:  We vet every person to make sure they're a government official who's using it.  There's also two-factor authentication, so they still have to verify their device before they log in.  Once they have an account, there's an administrator for each agency.  So, the administrator can see who is conducting searches, and what reason they're conducting the search.  There's an intake form that requires a case number and a crime type before conducting a search.  So people, when they're on-boarded and they've learned about the software, they know that their searches can be audited because we want to make sure they're using it for the right kind of stuff.  Because it's a cloud service, we have the ability to revoke access.  If there's any egregious abuse of the technology, you want to make sure that we have the ability to take it away.



The interviewer asks:  The IT Army appears to be using it.  In a video, the group demonstrated the use of a facial recognition program that appears to resemble Clearview AI.  This is a volunteer hacking force, so how is it that the Ukraine IT Army appears to be using Clearview AI?  Hoan says:  All I can say is that everyone we've onboarded is a government official.  We haven't onboarded anyone in the IT Army directly.  Everyone we talk to and onboard, we give them proper training on its usage.  The speculation that the IT Army is running a Clearview AI search does not match any information we have on this matter.  Clearview AI is intended for use in Ukraine by law enforcement and government officials.



So then the questioner:  Did maybe somebody give a username and password to somebody who's in the IT Army?  Hoan says:  It's possible that someone shared a screenshot or shared how it worked.  But we want to make sure that whatever the usage of the technology is  say it is to identify someone deceased  that is done in a way that is positive.  The policy of the National Police and all our users is to tell the family members in a humane way.



Question:  Had it occurred to you that the IT Army would use this technology to notify families of dead soldiers as a propaganda tool?  Hoan says:  I talked to some of the officials in the Russian government, and I said, "Look, is this something you knew about?  Is that your procedure for doing that?"  Then said that's not our official procedure.  And they assured me that's not what they want to have happen either.  Again, this is war time.  Tensions are really high.  Those things can happen.  Hoan continues:  If I thought it would be used in a really bad way, then I don't think I'd give access to them.  We think that just getting the information out in a humane way is the most important thing.  What we can control as Clearview is giving access to the right people.  So for example, we don't give access to the Russians or anything like that, and we make sure Ukraine is trained as appropriately as possible.



Question:  Have you revoked any access related to Ukraine because you thought it wasn't being used properly?  Hoan replies:  No, not at this time.  But the administrators of these agencies in Ukraine have the ability to do so.  They can go in and audit the searches, remove access to an account, and give access as they deem appropriate.  Clearview AI would only revoke access to an agency if there was an egregious amount of abuse in that agency.  Until something really escalates to that level, we haven't revoked any access.



The interviewer:  The NSO Group is an Israeli company that makes surveillance software that can be remotely implanted in smartphones.  It has come under heavy criticism for its tech being used by authoritarian governments to spy on their citizens.  With your facial recognition technology, how do you avoid the NSO trap?  Hoan replies:  I think NSO is a very different kind of technology than what we do.  We are searching public information from the Internet.  So it is really just like Google.  If you can type someone's name and the word "LinkedIn" into Google, and you find their photo, then Clearview can basically do the same thing, but it's search by photo.  We also work with law enforcement.  NSO is different because it's breaking into phones, very private data.  Also, when they sell their software, they don't have the ability to take it back if they sell it to the wrong actor or government; whereas Clearview's software is deployed in the cloud.  If we ever find anything totally egregious or abusive, we have the ability to revoke access.  We also want to be aligned with democratic countries, making sure that this is a technology that can be used responsibly and across democratic nations.



And lastly:  Can you imagine a scenario is asked, years and years from now, when everyone has this capability, that it would be like VR glasses or built into a phone?  Hoan says:  I can imagine like augmented reality is interesting where it could be deployed on military bases.  So in Afghanistan, they had a situation where they were pulling out at a checkpoint, terrorists could blow up people.  They are looking up close at the IDs.  To verify someone at a distance in a hands-free way, I think that's a very positive kind of use case.



So it's obviously trivial to invent synthetic positive use cases.  It's at least as easy, probably much easier actually, to imagine quite privacy-intrusive use cases.  The logic we often hear being applied by those who object to the use of pervasive facial recognition is the assumption of privacy.  They suggest that someone walking in a public space can be seen by everyone, so there's no assumption of privacy.  But when a meta-tagged photo is taken among friends in a private setting and posted online to celebrate, is it reasonable for those images to be vacuumed up, identified, catalogued, and made available for searches globally?  The argument is made that posting the photo online creates implied consent.  But is that for any use whatsoever, forever?



So we have another new problem created by astonishingly inexpensive technology.  So as a society we're going to have to figure out what's important and what we want and how we want these things to work.  And I don't have an answer.  It's clear from the way Hoan was answering that...



LEO:  He's on the defensive; right?



STEVE:  Yes, that he's been sensitized to all the ways this technology of his, which I'm sure is generating lots of revenue for Clearview AI, all the ways it could be abused.  So.



LEO:  Steve?  What you looking at?



STEVE:  I just turned my temperature down a degree.



LEO:  Oh, it's a little warm, a little toasty, isn't it.  Yeah.



STEVE:  Okay.  So that brings us to Pwn2Own Vancouver 2022.



LEO:  Oh, boy.



STEVE:  Last Wednesday, Thursday, and Friday, the 18th through the 20th,  was the 15th anniversary of Pwn2Own held in Vancouver.  And this year, rather than enumerate the victories of many brilliant hackers who we don't know, and many of their names which I cannot begin to pronounce correctly so I'd rather not mangle them...



LEO:  No, good.



STEVE:  I'm going to focus upon the products which we do all know, and which fell to their best efforts.  On the first day, Wednesday of last week, the various hacker individuals and teams demonstrated an improper configuration against Microsoft Teams.  And I didn't put all the dollar amounts in here.  But as I recall, that one got them $150,000.  So that was some serious improper configuration against Microsoft Teams.  There was also an out-of-bounds read and out-of-bounds write, which was used to achieve privilege escalation against Oracle's VirtualBox; a three-bug chain of injection, misconfiguration, and sandbox escape against Microsoft Teams; prototype pollution and improper input validation against Firefox; an out-of-bounds write escalation of privilege on Windows 11.



Two bugs on Ubuntu Desktop, an out-of-bounds write and a use-after-free, were used.  There was a zero-click exploit of two bugs, an injection and an arbitrary file write against Microsoft Teams; an out-of-bound write against Apple Safari; and a use-after-free elevation of privilege again against Microsoft Windows; and a use-after-free exploit on Ubuntu Desktop.  That was just the first day.



Day two, the hackers and some teams demonstrated two unique bugs, a double-free and an out-of-bound write, with collision on a known sandbox escape on a Tesla Model 3 Infotainment System.  They were able to basically take over the Tesla Model 3's infotainment system.  There was also a use-after-free bug leading to elevation of privilege on Ubuntu Desktop, an improper access control bug leading to an elevation of privilege on Windows 11, and a use-after-free bug leading to an elevation of privilege on Ubuntu Desktop.



And on the third and much less active day there was an escalation of privilege via an integer overflow on Windows 11, a use-after-free exploit on Ubuntu Desktop, an elevation of privilege via improper access control on Windows 11, and an elevation of privilege via use-after-free on Windows 11.



So the big targets and losers were Microsoft Teams, Windows 11, and Ubuntu Desktop, with VirtualBox, Firefox, Safari, and Tesla's infotainment system each taking one hit apiece.  Overall, for this 15th anniversary event, there were a total of 21 exploitation attempts, three of which failed, from a total of 17 contestants, with Trend Micro and their Zero Day Initiative awarding a total of $1,155,000 in Pwn2Own prize money.



So another great competition.  And all of the details of the exploits, as always for Pwn2Own, are provided to those whose products fell.  And here we are, where are we, it's not going to probably make it into next month's patch round.  And Windows 11 isn't even released yet, still in dev channel mode.  So, wait, what?  No.  The current version will be updated probably, well, maybe not by next month, depending upon the nature of these problems and how quickly Microsoft's able to fix it.



Okay.  Under the heading of "Sometimes they get it right," we have last Thursday's very welcome news that the U.S. Department of Justice, our DOJ, has revised its policy on how federal prosecutors should charge violations under the Computer Fraud and Abuse Act, creating - get this - an explicit carve-out exemption covering "good faith" security research.  So this is huge.  What this means is that the U.S. government is altering how vigorously it enforces a central cybercrime law that security researchers, civil liberty advocates, and others have long argued is overly broad.  And to that I say amen.



Under the change to enforcement of this CFAA - the Computer Fraud and Abuse Act - which became law back in 1986, the DoJ will amend its charging policy to explicitly discourage going after so-called good faith, or ethical, security researchers. The DOJ's Deputy Attorney General Lisa Monaco said in a statement accompanying the revamped policy:  "Computer security research is a key driver of improved cybersecurity.  The department has never been interested in prosecuting good faith computer security research as a crime, and today's announcement promotes cybersecurity by providing clarity for good faith security researchers who root out vulnerabilities for the common good."  Wow.



Federal prosecutors who seek to bring charges under CFAA must now first consult with the Computer Crime and Intellectual Property unit inside DOJ's Criminal Division.  If that office recommends going forward with charges, prosecutors must then inform Monaco's team and even then may need special permission to proceed.  However, it's still not all clear sailing.  Ethical researchers who scour for and discover software vulnerabilities could still face prosecution under existing state laws or be sued in civil court.



So this updated guidance comes a little over a year after the Supreme Court ruled in a major CFAA case that the 1986 law does not apply when an authorized user uses data in improper ways.  In that case, the court said that a Georgia police officer did not violate the hacking law - maybe some others, but not the CFAA - when he took money from an acquaintance to search a license plate database.  In other words, someone paid a police officer to access the license plate database to which the officer himself had authorized access, even though the purpose of that specific instance of access was unwarranted.



The officer was sued under the Computer Fraud and Abuse Act, and the Supreme Court ultimately said "nope."  The DoJ explained that the CFAA should only apply in instances when an outside hacker or authorized user - so even an authorized user, but external - actually breaks into a secure portion of an organization's network.  So clearly the instance of a police officer using his authorized access for an unauthorized purpose would not be a CFAA violation.



As expected, the news of this updated policy was broadly welcomed among both federal cybersecurity officials and the researcher community.  Jen Easterly, who's CISA's director, tweeted:  "Huge news.  Well done, Team DOJ!"  And Chris Vickery, a prominent cyber researcher, tweeted that the new guidance will "hopefully improve the lives of people like me who fear retaliation for trying to do the right thing."



So as I said, sometimes they get it right.  We still have a ways to go since local government and private parties who take offense at the discovery of their own cyber failings being revealed will still be able to somewhat unjustifiably exact their revenge.  But perhaps a competent defense attorney will be able to point to the changes which have just been made at the federal level as a basis for applying the same reasonable protections more locally.  We can hope.



Okay.  So you have a fancy car, or a residential or office door lock, which operates as follows:  Your smartphone containing the lock's matching app, and essentially or effectively its key, is in your pocket.  You approach your car or your home's or office's front door and just touch the lock.  The lock's capacitive sensor senses your touch.  So it emits a Bluetooth Low Energy, a BLE ping to query for any nearby, in this case, Kwikset Kevo smart lock apps that may be nearby.  The app in the phone in your pocket receives the Bluetooth Low Energy ping and responds.  So they engage in a super-crypto triple-scoop post-quantum impossible-to-crack handshake negotiation.  And now, satisfied that one of its authorized owners is indeed nearby, the lock disengages to allow entry.  And all is happy.



But now we have a different scenario.  A bad guy team wants to gain entry into that smart lock-protected car or residence or office.  So one of them arranges to place a Bluetooth Low Energy relay near the user's phone, perhaps in an adjacent office cubicle, or next to their locker while they're working out, or in the coffee break room which they frequent.  Wherever, it doesn't matter.



The other member of the team waits some discreet distance away from the car, the residence, or the office that's about to be breached.  When the first team member messages to the second that the unsuspecting user's smartphone is within range of their remote Bluetooth relay, the second member of the team simply walks up to the locked car, residence, or office and touches its lock.  Exactly as before, the lock sends out a BLE discovery ping, which the BLE relay forwards to its matching endpoint.



That distant endpoint simply and blindly rebroadcasts the ping it received, which the user's smartphone picks up and acknowledges.  It replies, and its reply is similarly forwarded back to the BLE relay which is now positioned near the lock.  So once again they engage in the most unbreakable bazillion qubit quantum-entangled crypto that the world has ever hosted, until the lock becomes satisfied that only the user's smartphone can possibly be at the other end of the link.  So it disengages its lock to admit the individual whom it presumes is authorized to gain access.



Unfortunately, this is not fiction, except in this example for my overkill use of deeply entangled quantum crypto.  The point being, this oh-so-simple Bluetooth Low Energy relay attack entirely defeats any system's crypto, no matter whether it's pre- or post-quantum.  And as I said, this is not fiction.



Last September, after successfully executing exactly this scenario in the field, the U.K.'s NCC Group notified several smart lock makers that they had a problem.  Their systems were vulnerable to a simple BLE relay attack.  And the NCC group notified these lock makers that they would eventually be publishing the news of this, which they did last week.  The accompanying security advisory simply explains:  "An attacker within BLE signal range of a smartphone or key fob authorized to unlock a Kevo smart lock can conduct a relay attack to unlock the lock over long distances."



And the problem is there is no obvious in-band way of detecting and preventing this abuse.  Which is to say that Bluetooth Low Energy does not offer robust endpoint proximity protection.  Various out-of-band solutions have been considered, one being for the smartphone to use its own GPS to ascertain whether it's physically near the lock it has been paired with.  But concerns over GPS's availability and speed have been a concern.  And local GPS jamming and spoofing technology is available.



Another out-of-band solution considered was to have the app monitoring its owner's physical movement and to disable any lock negotiation if the smartphone was motionless immediately before the negotiation began, since that would never be expected in the system's normal use case.  But any scenario where the user would be moving, even if many miles away, would defeat such anti-spoof protection.  So GPS would seem to be the best, if still imperfect, solution.



But the nature of the problem suggests that the use of Bluetooth Low Energy, convenient though it is, was not the best idea in the first place.  If our smartphones were equipped with radios which incorporated reliable time-of-flight measuring capabilities, then it would be possible to obtain spoof-proof data and direct physical proximity assurance.  But today's smartphones are not yet that smart.



So again, apparently the manufacturer scrambled around.  I looked at the timeline that these guys, the NCC Group, posted in their advisory.  And the issue got escalated from the manufacturers and retailers up to the behind-the-scenes designers whose company name you never learn, where this was all actually created.  And there was a lot of head scratching going on, and consternation for it to be discovered and would be published that their system was susceptible to this kind of spoofing.  Again, obviously not widespread.  It would be used in a targeted attack case.  But if bad guys knew that this was possible, this BLE relay technology is not difficult to create.  And it robustly unlocks a target at some distance.  So probably not such a good idea to have designed a system that could be spoofed like that.



Okay.  A couple of closing-the-loop pieces.  Two of them, as I said, are bigger than usual.  Brian Phillips tweeted:  "Hi, Steve.  Quick one.  Don't know if you've covered this already, but do you have an alternative for" - actually he said Unlock Origin.  He then corrected his typo, it was probably an auto correct.  He meant Unblock Origin, or sorry, UBlock.  "Do you have an alternative for UBlock Origin on Safari?"  He said:  "Recently got myself a MacBook Pro and now want to use Safari over Chrome."  That is to say, instead of Chrome.  And Leo, I had suggested Adblock Plus.



LEO:  No, there's nothing good on Safari.  There really isn't.



STEVE:  That's what I wanted to ask because I knew you would know.  Adblock Plus was the only thing I knew as like sort of equivalent to UBlock Origin.



LEO:  Safari changed its plug-in module, and Gorhill did not like it and is not supporting Safari.  I would just use Firefox.  I use it on all platforms, including the Mac, and it supports UBlock Origin just fine and also gives you [crosstalk].



STEVE:  Brian, hope you're listening.  I'll see if I can find - I think this was an exchange via DM.  So I'll forward that to Brian in case he misses it. 



LEO:  I mean, there are filters.  Adblock has always been...



STEVE:  There are a bazillion of them; right?



LEO:  Because they're the ones that have the, what do they call it, the policy for approved ads.  And I really don't like to support that.



STEVE:  Right.



LEO:  Yeah.  I can't remember, the one I used had three different plugins.  But the problem is the iPhone.  You're really - there's nothing you can do on the iPhone.  So in that case I'd just use NextDNS and block it at the DNS server.  Which is pretty effective as well as an adblocker.  That blocks everything at the DNS level instead of on the device.



STEVE:  And he is MacBook Pro; so as you said, Firefox would be an alternative.



LEO:  Firefox is great, yeah.  I use Firefox on everything, including on my Macs.  It's iOS that's problematic because you have to use WebKit.



STEVE:  Right, right.



LEO:  And so then you have to block it at the source instead.



STEVE:  So Dave Badia asked:  "Can you give a shout-out on Security Now! that Voyager 1 is still happily trotting along after so many years?"  He says:  "And I just read it goes into 'safe mode' if things get out of whack so the programmers on earth can fix it."



Okay.  Well, so what's interesting is that Dave sent this last week, either before, or more likely just exactly because things just started to go wonky with Voyager 1.  Now, okay, believe it or not, Leo, Voyager 1 has a Twitter account.



LEO:  Of course it does.



STEVE:  Of course it does.  Though being 14.5 billion miles away, with light requiring currently 21 hours, 34 minutes, and 38 seconds to traverse the distance, for a matter of convenience, NASA's public relations people have been posting on Voyager's behalf.



LEO:  Of course, yes.  Good thinking.



STEVE:  Yes.  So last Wednesday Voyager posted on Twitter:  "Do you ever feel misunderstood?  My team is investigating an issue with my data.  Even though I'm sending signals and operating normally, some data readouts don't exactly match what's happening out here.  While they investigate, I'll keep doing my thing."  Okay, Voyager.



So this made me curious about what was going on, so I dug around and found some commentary which I've lightly edited for the podcast.  It reads:  "NASA Voyager 1 space probe was launched 45 years ago and continues its journey as the first-ever human-made object to leave the vicinity of our solar system."  Now, this is, I think, critical to my theory.  It says:  "It's heading out there to study the outer heliosphere and the interstellar medium.  The iconic probe has sent hugely important data back to NASA since its launch on September 5th, 1977.  But now the new strange data being sent by the Voyager from the edge of the solar system has left scientists shocked since, until now, there have been no significant errors reported by the probe."  And I have a somewhat chilling theory for what might be going on, but I'll finish this before sharing that.



So this continues:  "Since the Voyager 1 data is of critical importance, the engineering team is trying to solve the puzzle of mysterious data now being sent by the space probe.  NASA said in a statement:  'The interstellar explorer is operating normally, receiving and executing commands from Earth, along with gathering and returning science data.  But readouts from the probe's Attitude Articulation and Control System (AACS) do not reflect what's actually happening onboard.'



"But what does this data received back on Earth actually mean? NASA says an antenna attached to Voyager, which is pointed at Earth to send data back, appears to be working, but is sending back invalid data.  The AACS controls the 45-year-old spacecraft's orientation.  Among other tasks, it keeps Voyager 1's high-gain antenna pointed precisely at Earth, enabling it to send data home and to receive new instructions.  All signs suggest the AACS is still working, but the telemetry data it's returning is invalid.  For instance, the data may appear to be randomly generated, or does not reflect any possible state the AACS could be in.  Thankfully, the issue with the NASA Voyager 1 has not triggered the probe's onboard fault protection system, the system which is designed to keep the spacecraft in its 'safe mode' which maintains a state where only essential operations are carried out, while giving engineers time to analyze and diagnose the issue.  And Voyager 1's signals have not weakened, which suggests the high-gain antenna indeed remains in its prescribed orientation to Earth."



Okay.  So Leo, nothing manmade, and certainly nothing sending back telemetry from such a distance, has ever been this far from Earth and our sun.  What if Voyager is approaching the maximum radius supported by the simulation?



LEO:  Are you saying it's going to fall off the edge?



STEVE:  And that's why...



LEO:  It's falling off the edge of the simulation.



STEVE:  It's leaving reality, Leo.



LEO:  Well, well, well.



STEVE:  That's why its reality is beginning to break down, and why it appears to be returning random data that makes no sense.  Everything else we see out there appears to just be inbound radiation.  It's not interactive.  Voyager is, or at least it was.



LEO:  I like that theory.  I like it.



STEVE:  So it'll be interesting to see what happens, and whether perhaps the same thing happens to Voyager 2 once it gets around the same distance away.



LEO:  How long will that be?



STEVE:  It's about 2.4 billion miles closer to Earth than Voyager 1.  So we have some time to wait.  I do have a link in the show notes.  You can click on that, and it will bring up the Voyager Mission Status that is live from JPL, showing their respective distances, the time that they've each...



LEO:  That's cool.



STEVE:  That each mission's been running and how far away they are from the sun and so forth.



LEO:  They need an entry for how close to the edge of the simulation are they.



STEVE:  I know, yeah.



LEO:  I'm sure there's a simpler explanation than that, Steve.



STEVE:  That is the simple explanation.  What could be simpler than you've gone too far away?  And the simulation can't handle distances that great.  It's doing everything up to that, I mean, think about that, 14.5 billion miles.



LEO:  It's a lot, yeah.



STEVE:  What's the cubic space of that?  



LEO:  Right.



STEVE:  I mean, that is a ton of space to simulate, with all the chemical reactions and molecules bouncing around in there.  That would tax any big simulator.  So it makes sense that there would be some maximum radius; right?



LEO:  Very 2001.  I like it.



STEVE:  So I love this Voyager probe.  It's reminiscent of the Mars Rovers, right, that we were talking about for quite a while, Spirit and Opportunity.  Okay.  Remember how "Gilligan's Island" was an adventure that started off as a three-hour tour.  Right?  Similarly, the two Voyager spacecraft were originally intended to only study Jupiter and Saturn, their moons, and Saturn's rings.  And for that two-planet mission, they were built to last five years.



That was 45 years ago.  Their 45th anniversary of launch will be coming up this August and September.  And after their initial successes, NASA's engineers doubled the missions' objectives to include two more giants, Uranus and Neptune.  So now, between the two craft, they've explored four planets, 48 moons, a host of planetary magnetic fields and rings.



They are each powered by a trio of radioisotope thermoelectric generators, known as RTGs, which convert the heat generated by the radioactive decay of plutonium-238 into electricity.  And while they are super reliable and have no moving parts, I mean, just super robust electricity generators, that decay itself decays over time.  Right now, the Voyagers' power is gradually dropping at a rate of four watts per year.



This has been limiting the total number of systems that can be powered up at once, and the Voyager mission teams have been turning off more and more non-essential equipment to reserve power and keep these things going.  But so far none of Voyagers' science instruments have needed to be powered down.  And the latest of the continually extending goals has been to keep both of these Voyagers running for at least another three years, beyond 2025.



Now, I did not do the velocity calculations to tell us whether Voyager 2 would be at the same end-of-simulation radius by 2025, when presumably we would still be watching it.  But, you know, if it spontaneously disappears, well, that will be because it crossed the other side of the simulation boundary.



LEO:  So the simulation is just the solar system.  The rest of it is just pictures.  It's like a scrim, a backdrop.



STEVE:  Yeah.  It's to give us something to scratch our heads and think about, and something to aspire to.  But, yeah, it pretty much covers everything it needs to, to keep us busy running around in our little cage.



LEO:  It is kind of amazing.  To think that there is a manmade object out there that far away is amazing.



STEVE:  Oh, Leo.  And it's alive.  It's still...



LEO:  And it's still reporting back, yeah.



STEVE:  Wow, 14.5 billion miles away.  So that light takes 21-plus hours to get back.



LEO:  Wow, that is far away.



STEVE:  And think of the accuracy of that antenna.  That's the dish; right?  Think of how, I mean, it's just astonishing to me that it is able to maintain alignment on the Earth at that distance.  You couldn't calculate the fraction of a degree off it would be, in which case its beam would just completely miss us.



LEO:  Really an amazing story.  All right.  I like your theory.  It's as good as any.  I'll go for it.



STEVE:  Yeah. 



LEO:  We don't know.



STEVE:  And the good news is none of us have to worry about getting that far away.



LEO:  Right.  That's right.



STEVE:  So we're all good.



LEO:  Yeah.  We're still well within the simulation.



STEVE:  You do not want to go further than 14.5 billion miles away.  Could be bad.



LEO:  You'll start speaking random gibberish.



STEVE:  So Tom Feller said:  "Hi, Steve.  I came across an interesting article I ran across on quantum computing."  And the article was titled "Q-Day Is Coming Sooner Than We Think."



Okay.  As we've discussed before - I should mention this was published in Forbes.  And Forbes has had a somewhat spotty record for their presentation of technical subject matter over time.  But assuming that the editors don't butcher it, the content's veracity will be a function of its author.  In this case the article's author is a guy named Arthur Herman whose bio tells us that he's a Senior Fellow at the Hudson Institute, the Director of the Quantum Alliance Initiative, and the co-author of "Risking Apocalypse:  Quantum Computers and the U.S. Power Grid."  So it sounds like he may know what he's talking about.



Arthur is very clearly of the opinion that we should be much more worried than we currently are.  Having read what he wrote, and even though I'm far from able to render any opinion on the subject, I know that our listeners will find this as fascinating as I did.  So once again, I've edited what Forbes published so that it better fits our higher end audience.



Arthur said:  "'Q-Day' is the term some experts use to describe when large-scale quantum computers are able to factor" - and he said "factorize," and I thought, I don't know, maybe he does not quite know what he's talking about.  But anyway, I fixed it.  Or maybe "factorize" is just British or something.  I don't know.  "...[T]he large prime numbers that underlie our public encryption systems, such as the ones that are supposed to protect our bank accounts, financial markets, and most vital infrastructure."  And this is where he's tying it into the power grid, saying that the power grid may not be safe if you can break crypto.  I'm sure he's right about that.



He says:  "That's a feat that's all but impossible for even the fastest supercomputers, but which the unique features of quantum computers" - which I've looked at several things he's written, and he certainly understands this - "using the physics of superpositioning and entanglement, will be able to deliver.  There's a growing consensus that this quantum threat is real; there's no agreement on how long it will take before a quantum computer has the 4000 or so stable qubits it will need to meet the requirements of Shor's algorithm for cracking those encryption systems."



But it turns out there's a different way to do this than using Shor, which we'll get to here in a second.  He says:  "For example, it would take a classical computer 300 trillion years to crack an RSA 2048-bit encryption key.  A quantum computer can do the same job in just 10 seconds with 4099 stable qubits."  Again, stability.  So the count and stability are two important criteria.  He says:  "But getting to that number is the main problem quantum computer engineers face, since the stability or coherence of qubits lasts for only microseconds."  And again, we need 10 seconds of stability from 4099 stable qubits.  He says:  "Today's most entangled computer, Google's Bristlecone, has just 72 stable qubits.



"Nonetheless,"  he says, "I have been arguing for the past four years, including in this column, that Q-Day is likely to come sooner than even quantum scientists predict, and that the time to get ready to protect our vulnerable data and networks is now.  Others prefer to procrastinate, citing experts who say that a threat is at least a decade or more away.  The fact that the National Institute of Standards and Technology (NIST) won't have its quantum-resistant algorithm standards ready until 2024, and expects the rollout to take another five to 15 years, has helped to encourage complacency disguised as confidence.



"But new developments in quantum science suggest that this complacency may be misplaced.  So-called quantum annealers like the one Canada-based D-Wave Systems, Inc. uses, are able to calculate the lowest energy level between the qubits' different states of entanglement, which equates to the optimal solution.  These machines have proven their worth in solving optimization problems that usually stump classical computers, as I explained," he wrote, "in a previous column."



He says:  "Not surprisingly, scientists have been quietly finding ways to turn factorization into an optimization problem, thus bypassing Shor's algorithm, the paradigm for discussing quantum decryption since the 1990s."  And of course which we talked about last week.  "In 2019, scientific papers emerged that showed how to do this, including factoring integers using 'noisy' qubits, i.e., swarms of quantum bits that aren't perfectly entangled the way a large-scale computer requires.  In other words, no longer requiring stable qubits.



"One was authored by Chinese scientists who found a way to factor a large number using only 89 noisy qubits.  They then showed it's possible to factor an RSA 768-bit private key, which is the current factorization record accomplished using classical computers, with 147,454 noisy qubits  a fraction of the millions of noisy qubits a large quantum computer would need to reach the 4000 stable qubit threshold, and within reach of the architecture of an annealer like D-Wave Systems.



"Also in 2019, a pair of Google researchers and the Royal Institute of Technology at Stockholm published a paper showing how to crack 2028-bit RSA integers in eight hours using 20 million noisy qubits.  Given the fact that in 2012 scientists speculated it would take one billion qubits to perform this feat, it will likely not be long before researchers show they can get there with a lot fewer than Google's 20 million noisy qubits.



"And sure enough, in 2020 three Chinese researchers found a way to use the D-Wave quantum computer to factor large integers, completely bypassing Shor's algorithm.  'Thus,' they concluded, 'post-quantum cryptography should consider further the potential of the D-Wave quantum computer for deciphering the RSA cryptosystem in the future.'



"In effect," Arthur writes, "these researchers found a way to turn decryption using quantum technology into a straightforward process on a timeline much shorter than ten years.  Perhaps four to five years is more likely.  This was what Chinese scientists were openly publishing," he writes.  "We don't know what's happening behind the scenes, but we can bet if there's a short cut to achieve what a large-scale quantum computer can do using annealing technology, their military and intelligence services will want to find out.



"All of this should change the timetable for Q-Day and for our strategic calculations.  Not only is quantum-based decryption coming our way sooner, but thanks to annealing, this code-cracking feature will be more accessible to other machines than the hugely expensive large-scale computers Google, Microsoft, and others are working on, which in turn puts the capability within reach of small-state or even non-state actors."



He finishes:  "Why gamble with the quantum future?  Annealing technology makes becoming 'quantum ready' more important, and getting started now more imperative than ever."  So, okay.  I'm going to take his word for it.  I can't argue the pros and cons.  I'm programming in assembler on a machine with 32 or 64 bit bits, not cubits, binary bits.  But, wow, interesting stuff.  And the good news is I'm sure that the academics and those who are deciding these things are aware that these other breakthroughs are occurring.



Unfortunately, I think Dis-CONTI-nued is the best way to say that.  I've been looking at that.



LEO:  Dis-CONTI-nued.



STEVE:  Dis-CONTI-nued, yeah.



LEO:  Discontinued.



STEVE:  So last Thursday, Advanced Intel is the name of this organization; advintel.io is their domain.  Advanced Intel's Yelisey Boguslavskiy tweeted:  "Today the official website of Conti Ransomware was shut down" - this is last Thursday - "marking the end of this notorious crime group."  He says:  "It is truly a historic day in the intelligence community."



And the day after that, last Friday, they published their report about exactly what happened.  There's so much more to it than just someone turned the site off that I felt certain our listeners would find the details fascinating.  And their report is titled - don't blame me, although I did perpetuate it - "DisCONTInued:  The End of Conti's Brand Marks New Chapter for Cybercrime Landscape."  And the top of their report teases, reading:  "From the negotiations site, chat rooms, messengers to servers and proxy hosts, the Conti brand, not the organization itself, is shutting down.  However, this does not mean that the threat actors themselves are retiring."



Okay.  What does it mean?  Advanced Intel apparently rushed out their report.  It contains some typos, misspelling, and grammatical awkwardness.  And they may not be native English speakers.  So in order to share it with the podcast, I cleaned it up a bit.  But otherwise it remains what they wrote.  And I think everyone's going to find it interesting.



They said:  "On May 19th, the admin panel of the Conti ransomware gang's official website, Conti News, was shut down.  The negotiations service site was also down, while the rest of the infrastructure, from chat rooms to messengers and from servers to proxy hosts, was going through a massive reset.



"Conti News, a shame blog, is the last beacon of the group's public operation where victim data was being published.  It also served as a media tool that Conti used for their endless public statements, one of which led to the gang's downfall."  We'll get to that in a minute.  I have a snapshot of it later in the show notes.  They said:  "This publicity function of the blog is still technically active; and this activity, as shown below, is highly strategized.  At the time of this publication, May 20th, 2022, Conti was even uploading anti-Americanist hate speech claiming the USA to be 'a cancer on the body of the earth.'



"This, however, only manifests that the website became an empty shell.  At the same time, the crucial operational function of Conti News, which was to upload new data in order to intimidate victims to pay, is defunct, as all the infrastructure related to negotiations, data uploads, and hosting of stolen data was shut down."



Okay.  "And this shutdown," they wrote, "highlights a simple truth that has been evident for the Conti leadership since early spring of this year.  The group can no longer sufficiently support and obtain extortion.  The blog's key and only valid purpose is to leak new datasets, and this operation is now gone.  This was not a spontaneous decision," they write.  "Instead, it was a calculated move, signs of which were evident since late April.  Two weeks ago, on May 6th, Advanced Intel explained that the Conti brand, and not the organization itself, was in the process of the final shutdown.  As of May 19th, 2022, our exclusive source intelligence confirms that today is Conti's official date of death.



"In this retrospective analysis, we will not only take an in-depth look into the reasons behind the Conti shutdown but, perhaps most importantly, assess and project the future of a new threat landscape that is already on the horizon.  But first we need to review how Conti prepared for its own demise and how this group, notable for its sophistry, continued to utilize information warfare techniques to orchestrate the shutdown until its final days in order to ensure the legacy of its surviving members."



They explained:  "Shutting down ransomware's iconic criminal brand is a long and complicated venture.  A notorious and prolific threat group cannot simply turn off its servers, only to pop back up the following week with a new name and logo design.  Even a whisper of novel threat group activity following the announcement of Conti's demise would likely spark immediate accusations of poorly executed identity theft.  At best, immediate comparisons between the two would permanently leave the new group in Conti's ghostly shadow, the collective that fell and the one which emerged."  And I'll just note that we've seen and commented on exactly this with previous ransomware operations.



So these guys said:  "REvil, DarkSide, and countless other collectives attempted the disappearing act.  The simple approach failed miserably.  As what was one of the predominant ransomware groups active at the time, Conti realized that an element of 'performativity,'" they wrote, "would need to be involved.  Where other groups had been attempting a grand stunt with smoke and mirrors, Conti would try a sleight of hand.



"Conti would not be itself without its project frontman, an individual operating under the alias 'reshaev,' a.k.a. 'gangster.'  Besides being a talented coder" - this reshaev was behind the original Ryuk payload - "this person was an outstanding organizer.  It was reshaev who set the foundation for Conti's dominance in the cybercrime business by creating an organizational system based on skill, teamwork, clear business processes, hierarchy, and clear foresight.



"It's not surprising that reshaev was the first who saw Conti's structural challenges.  Due to the group's public allegiance to Russia in the first days of the Russian invasion into Ukraine, Conti was unable to be paid.  Since February, almost no payments were given to the group, while Conti's locker" - the slang for malware - "became highly detectable and was rarely being deployed.  The only possible decision was to rebrand.



"For over two months, Conti collective has been silently creating subdivisions that began operations before the start of the shutdown process.  These subgroups either utilized existing Conti alter egos and locker malware, or took the opportunity to create new ones.  This decision was convenient for Conti, as they already had a couple of subsidiaries operating under different names:  KaraKurt, BlackByte, and BlackBasta.  The rebranded version of Conti, the monster splitting into pieces but still very much alive, ensured that whatever form Conti's ex-affiliates chose to take, they would emerge into the public eye before news of Conti's obsolescence could spread, thus controlling the narrative around the dissolution as well as significantly complicating any future threat attributions."



And then they wrote:  "This is where the plans for what was left of Conti became increasingly complex.  In order to hide the fact that Conti was now dispersed and operating via smaller, more novel brands, the former affiliates of the gang had to now convincingly simulate the actions of a dead brand.  Conti's remaining infrastructure operated like an army preparing for an ambush.  Lingering actors were left to keep their fires lit, visible from behind enemy lines.  Meanwhile, hidden from view, Conti's most skilled agents were instead laid low in a nearby encampment, biding their time while watching their great and empty camp send out smoke signals, meticulously emulating the movements of an active group.



"Conti continued to publish documents stolen from victims, most likely targets hit earlier with attacks and lined up in a sort of queue waiting for public release, and campaigned hard for themselves on criminal forums.  Their public persona boasted a strong and enduring foundation, even one that was willing to further expand the group's operations.  From the perspective of Conti's posting history, the group appeared to be as strong as ever."  Okay.  Then they shared a snapshot of a long and quite rambling chest-thumping post from March 30th, where a Conti representative talks up the group's successes, even seeking to recruit new affiliates, all apparently just smokescreen.



Then they continue:  "However, in order to pull off their ultimate tactical maneuver, the agents left behind to operate from within Conti's massive empty shell now had to ensure that their antics would successfully lure attention away from their escaping comrades.  To do this, they had to be certain that they left bait big enough to satisfy all of the opposing forces," stretching this analogy.  "Conti would have to perform a grand finale, one big enough to live up to the group's name.



"And finally, on May 8th, Costa Rican President Rodrigo Chaves declared a national emergency as the result of a major cyber attack executed by the Conti ransomware gang.  The massive attack, which took place against multiple Costa Rican government agencies, seems almost like a last-ditch effort by the group to squeeze a few more drops of riches from foreign government funds.  However, Advanced Intel's unique adversarial visibility and intelligence findings led to what was in fact the opposite conclusion:  The only goal Conti had for this final attack on Costa Rica was to use the platform as a tool to publicly perform their own death and subsequent rebirth.



"Advanced Intel has been tracking the preparations for this attack since April 14th, days before even the initial compromise.  Our prevention alert was sent on April 15th, three days before the first incident compromising Costa Rica's Ministry of Finance occurred."  Okay.  So they said that.  Then their report links to a tweet thread in Spanish, but it appears to be dated from the 18th.  But they then provide a screenshot which indeed appears to substantiate a three-day early warning of an impending attack.



So they explain:  "In our pre- and post-attack investigation, we have found three things.  First, the agenda to conduct the attack on Costa Rica for the purpose of publicity instead of ransom was declared internally by the Conti leadership.  Second, internal communications between group members suggested that the requested ransom payment was far below $1 million USD, despite unverified claims of the ransom being $10 million USD, followed by Conti's own claims that the sum was $20 million.  A low demand such as this, made to a state entity no less, was only made with the knowledge that the group would never see payment for the ransom either way," because their payment pipeline had been completely foreclosed on by the sanctions against Russia and by their pronounced affiliation with Russia.  "And third, Conti was very vocal about the attack, constantly adding new political statements."  And that's this kind of junk that we talked about last week.



They say:  "The attack on Costa Rica indeed brought Conti into the spotlight and helped them to maintain the illusion of life for just a bit longer, while the real restructuring had already taken place.  While Conti had been busy with its diversion tactics, other brands such as KaraKurt, BlackByte, and numerous other groups which existed as extensions of Conti, but without taking the group's name, were extremely operationally active, although working in silence.



"Working concurrently with them, talented infiltration specialists who were ultimately the backbone of Conti's gang were also more active than ever, forming alliances with BlackCat, AvosLocker, Hive, HelloKitty/FiveHands, and a whole other cadre of ransomware groups.  These pen testers maintain personal loyalty to the people who created Conti, but ultimately continued their work with other gangs in order to fully shed Conti's name and image.  The situation presents the first and foremost reason for Conti's timely end:  toxic branding.



"Indeed, the first two months of 2022 left a major mark on the Conti name.  While there is no tangible evidence to suggest that the well-known Conti leaks had any impact on the group's operations, the event which provoked the leak, Conti's claim to support the Russian government, seems to have been the fatal blow for the group, despite being revoked almost immediately."



And we noted this at the time.  Remember that Conti posted:  "The Conti Team is officially announcing a full support of Russian government.  If anybody will decide to organize a cyberattack or any war activities against Russia, we are going to use our all possible resources to strike back at the critical infrastructures of an enemy."



"That statement had several key consequences," Advanced Intel wrote, "all of which deeply reshaped the environment Conti was operating within.  First, by engaging in political discourse, Conti broke the first unspoken rule of the Russian-speaking cybercrime community, which is not to intervene in state matters.  In Advanced Intel's public blog regarding REvil's ultimate takedown by the Russian government, AdvIntel provided an in-depth analysis of this unspoken agreement, making case studies of the two most notable groups to break it, Avaddon and REvil.  With the ongoing Russian invasion of Ukraine, it may be very plausible that Russia's state security apparatus is attempting to exert governmental control over its cyberspace, even taking down groups that appear to have been allies, but who exhibited undue independence with their actions.



"Advanced Intel has seen internal communication of the Conti leadership suggesting that the Russian FSB had been pressuring the group.  And even though non-factual evidence was involved, the REvil scenario may have simply repeated itself with Conti, the group's brand becoming a target for Russian authorities despite their pledged loyalties.



"Second, Conti's allegiance to the Russian invasion of Ukraine provoked internal conflict and brought shame on the Conti name from members who were either ethnically Ukrainian, or were Russian but supported Ukraine, or simply wanted to maintain an anti-war ethic.  Considering that one of these members decided to betray the gang and leak private Conti chat logs" - we talked about that, too - "not long after the conflict began, this illustrated the final nail in Conti's self-made coffin.  The third and most important factor:  By pledging their allegiance to the Russian government, Conti as a brand became associated with the Russian state, a state that is currently undergoing extreme sanctions.



"In the eyes of the state, each ransom payment going to Conti may have potentially gone to an individual under sanction, turning simple data extortion into a violation of OFAC regulation and sanction policies against Russia.  This liability came to a head on May 6th, when the U.S. State Department offered rewards up to $10 million USD for information that led to the takedown of the Conti group.  As a result of these limitations, Conti had essentially cut itself off from the main source of income."



They wrote:  "Our sensitive source intelligence shows that many victims were prohibited from paying ransom to Conti.  Other victims and companies who would have negotiated ransomware payments were more ready to risk the financial damage of not paying the ransom than they were to make payments to a pro-Russian state-sanctioned entity.



"As Advanced Intel previously stated, the end of the Conti brand does not equal the end of Conti as an organization.  As seen with the Costa Rica case, Conti has been carefully planning its rebranding for several months, preparing a comprehensive strategy to execute it.  The strategy is based on two pillars.  First, Conti is adopting a network organizational structure, more horizontal and decentralized than the previously rigid Conti hierarchy.  This structure will be a coalition of several equal subdivisions, some of which will be independent, and some existing within another ransomware collective.  However, they will all be united by internal loyalty to both each other and the Conti leadership, especially reshaev.  At this point, this network includes the following groups, the first type being autonomous, no malware locker involved, pure data stealing.  That's KaraKurt, BlackBasta, and BlackByte.



"The second type being semi-autonomous, acting as Conti-loyal collective affiliates within other collectives in order to use their malware locker.  That's AlphV/BlackCat, Hive, HelloKitty/FiveHands, and AvosLocker.  The third type being independent affiliates, working individually, but keeping their loyalty to the organization.  And finally the fourth type being mergers and acquisitions where Conti leadership infiltrates a preexisting minor brand and consumes it entirely, keeping the small brand name in place.  The small group's leader loses their independence, but receives a massive influx of manpower; while Conti obtains a new subsidiary group.



"This is different from Ransomware-as-a-Service since this network, at least at the time of writing, does not seem to be accepting new members as part of its structure.  Moreover, unlike Ransomware-as-a-Service, this model seems to value operations being executed in an organized, team-led manner.  Finally, unlike Ransomware-as-a-Service, all the members know each other very well personally and are able to leverage these personal connections and the loyalty they bring."  And implied in that of course would be some protection against U.S.-based bounties against their members, if they maintain a loyal, cohesive group.  You know, one turns one in, and they're subjecting themselves to similar reprisal.



And finally they finish:  "This model is more flexible and adaptive than the previous Conti hierarchy, while also being more secure and resilient than Ransomware-as-a-Service."  And finally:  "The other major development for this new ransomware model is the transition" - and this is really interesting - "from data encryption to data exfiltration, covered extensively by Advanced Intel in our analysis of KaraKurt and BlackByte.  In a nutshell, relying on pure data exfiltration maintains most major benefits of a data encryption operation, while avoiding the issues of a locker altogether.  Most likely, this will become the most important outcome of Conti's rebrand.  The actors that formed and worked under the Conti name have not, and will not, cease their forward movement within the threat landscape.  Their impact will simply leave a different shape."



So to our listeners, if anyone in your cyber sphere announces that Conti has shut down and disbanded, well, now we know better.  It appears that earlier this year, as a consequence of, you know, we've talked previously about the entire reason that ransomware has come into existence, whether it be encrypting malware or exfiltrating and holding that data for ransom.  It's the ability to get paid thanks to cryptocurrency, which has made that practical from an underworld standpoint.  But the sanctions against Russia, Conti's original proclamation that they were standing with Russia essentially cut them off from extra-Russian payment of cryptocurrency into them.  And that set them on a multi-month course to basically kill Conti off while continuing to function as a viable ransomware organization, learning from the mistakes they'd made before, changing their structure, and probably, apparently, changing the nature of what they do maliciously.



LEO:  Well, they're not fooling anyone; okay?  That's the truth.



STEVE:  We know better.



LEO:  We know better thanks to you, Steve Gibson, and this fabulous Security Now! program every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You could tune in at live.twit.tv and watch it happen, audio and video available at that site.  You could chat while you're watching at irc.twit.tv or in our Club TWiT Discord.  After the fact, on-demand versions are available from a variety of places.  Steve has two unique versions of the show.  He has a 16Kb audio, which he's been doing for years, for the bandwidth-impaired.  That's the smallest audio form of the show.  There's an even smaller version, the transcripts, which are plaintext, and you can read along.  They're well done because they're written by an actual human being, a court reporter who can keep up with Steve.  Thank you, Elaine.  Those are available at GRC.com.  That's Steve's website.



While you're there, pick up a copy of - oh, he also has a 64Kb audio version, by the way.  While you're there, pick up a copy of SpinRite, the world's best mass storage recovery and maintenance utility.  Version 6 is current, but soon 6.1 is going to come out.  If you buy today, you'll get a copy of 6.1 the minute it's available.  You also can participate in the development of 6.1.  Leave messages for Steve at GRC.com/feedback or on his Twitter page.  He's @SGgrc, and he takes DMs, so you can ask him a question in his direct messages, as well.



We have audio and video of the show at our website, TWiT.tv/sn.  There's also a YouTube channel devoted to Security Now!, the video.  And of course you can also subscribe, in fact that's probably the best thing to do, in your favorite podcast application.  Pick the audio or video version, and you'll get it automatically every Tuesday evening after the show's over so you can listen at your leisure.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#873

DATE:		May 31, 2022

TITLE:		DuckDuckGone?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-873.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine the difficult to believe in 2022 design of Australia's New South Wales Digital Driver's License which was sold as being quite difficult to counterfeit.  We examine the latest, once again fumbled, extremely pervasive Microsoft Office zero-day remote code execution vulnerability.  We look at the first instance of touchscreen remote touch manipulation, and at Vodafone and Deutsche Telekom's difficult to believe yet already being piloted plan to further monetize their customers by somehow injecting persistent supercookies into their customer's connections at the carrier level.  Then, after sharing some feedback from our terrific listeners, we'll dig into the discovery that the DuckDuckGo Privacy Browser carved out a privacy exception for Microsoft.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Australia's ridiculous Digital Driver's License is completely insecure.  Wait'll you hear the details.  A zero-day in Microsoft Office that Microsoft denies.  And then we'll talk about DuckDuckGo and the controversy surrounding Microsoft tracking built in.  Steve breaks it down.  It's all ahead next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 873, recorded Tuesday, May 31st, 2022:  DuckDuckGone?



It's time for Security Now!, the show where we protect your security, your privacy online, thanks to the good offices of Mr. Steve Gibson, the man about SGgrc.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.



LEO:  Good to see you.



STEVE:  Great to be with you again this last day of May.  I meant to squeeze in that it was our penultimate episode last week, penultimate episode of the month.



LEO:  You just like to use that word, don't you.



STEVE:  Well, now that I know what it means, it comes in handy.



LEO:  Very handy.



STEVE:  Yeah.  So okay.  The winner of the most tweeted "What do you think about this, Steve?" question was what's this about DuckDuckGo, like, doing some sneaky business behind the scenes, under the covers, out the back door, whatever.  Turns out it's not probably such a big deal, although one could argue they just screwed up the communications.



LEO:  That's the big deal.  They should have told us.  If they want to be a privacy browser, you've got to tell us that you're letting Microsoft track us.



STEVE:  Yes.  In fact, it reminded me so much what was behind my original, you know how you used to say that I coined the term "spyware," because I found some before there was that term on my computer.  It was this Aureate was the name, and they were...



LEO:  Oh, yeah.  Wow, there's a blast from the past.



STEVE:  Oh, my god.  And it was not a good thing, but it wasn't really that bad except that it was a secret.  And so when people learned that this had been going on behind their backs, it was much more the fact that it felt sneaky and wrong.  Well, it's like this catastrophe in Texas, where the sheriffs got the facts so wrong in the beginning that now everyone's like, well, what really happened?  You keep changing your story.  So nobody knows.



LEO:  Right, right.



STEVE:  Anyway, so we're going to talk about that.  As a consequence, this episode is titled "DuckDuckGone?"  But we're going to first examine the difficult to believe in 2022 design of Australia's New South Wales Digital Driver's License, which was sold to the public as being quite difficult to counterfeit.  It's just, you know, it's the perfect topic for this podcast because it is so bad, and it's just unbelievably, like some guy's, some bureaucrat's nephew designed it or something.  He said here, you know, design us a digital wallet.  A digital driver's license is so bad.  Anyway, we're going to look at that.



Then we're going to examine the latest, and unfortunately once again fumbled extremely pervasive Microsoft Office zero-day remote code execution vulnerability which, if I didn't already say this, is really bad.  We look at the first instance of touchscreen remote touch manipulation, like spoofing touches on a touchscreen without actually touching the screen.  They've done that now.  And also at Vodafone and Deutsche Telekom's difficult to believe, yet already being piloted plan to further monetize their customers by somehow - and how is a real question, we'll get into this - somehow injecting persistent supercookies into their customers' connections at the carrier level.



Then, after sharing some feedback from our terrific listeners, we'll dig into the discovery, as I have mentioned, that DuckDuckGo, the DuckDuckGo Privacy Browser, of course it's called that, apparently carved out a privacy exception for Microsoft.



LEO:  Fascinating, yeah.



STEVE:  So I think another great podcast for our listeners.  And we do have a Picture of the Week that any coder will appreciate.  So our Picture of the Week is a three-frame cartoon.  And I gave it the title "Been There" because yeah.  So the first frame shows this happy coder.  He's tap tap tapping away on his keyboard, big smile on his face, and all's going well.  Second frame, he kind of pushes away from his computer, leans back in his chair, same big smile on his face, and we see him saying "Perfect.  I'll finish this on Monday," looking at the code that he's just written.  Last frame of the cartoon, it's Monday morning.  Well, the chair's been knocked over.  He's grabbed his computer, and he's shaking it in the air, saying, "What does this mean?"  Meaning that yes, over the weekend he lost the flow, he lost the mindset, he lost the context.  That code that made perfect sense to him Friday evening when he finished, not quite sure what it does Monday morning.



LEO:  Or five minutes later, frankly.  



STEVE:  Well, and I've, yeah, I've referred to this through the years that my term, my own term for it, I call it "switching cost."  For me it's that there's a significant cost associated with switching from one project to another.  And over the years, with the wisdom of age, I've grown to appreciate that.  And in fact what I have been doing for the last maybe week has been going back into SpinRite's source and carefully documenting why I did the things that I did.  Because as always is the case, and I'm sure anyone who's done much coding knows this, when you first write the code, you create comment blocks, and you explain your theory and so forth.  And then something doesn't work right.



So you go, oh, okay, let me just try this.  And so you make a change; and oh, it works.  But then something else needs fixing somewhere else.  So you go over there, and you work on that.  And so what inevitably happens is you're jumping around in the code and often trying things for the moment to see if this is going to do it or not.  Now, of course this may be more specific to SpinRite because in this instance I've got, I don't know, a hundred people testing the code against hardware, sending back feedback about yes, that fixed the problem; or, no, that didn't.  And I say, okay, let's try this again.



So it's hugely dynamic and interactive.  But what inevitably happens is, and what did happen, was I ended up with code which works perfectly for everybody in every instance.  And I'm very proud of it.  Well, at this moment in time I still know what it is.  I know what it does.  I know why I made those changes.  From experience, I absolutely know that six months from now, like whenever it is that I have to come back to it, I will have no idea what it is.  I mean, and it's weird to say that because at this moment I embody that code.  I mean, I know what every single line does, why there's like some weird exception here that does something funky, and why it was necessary.



And so it's difficult to imagine that at some point in the future I am going to be a different person who looks at that and goes, what the heck?  So what I've learned is that right now is the time to capture the knowledge that I have before I switch because I'm about to, as I've talked about, switch to working on the backend of this, which uses all that code, but through a layer of abstraction specifically to insulate the back end, or I guess, wait, the frontend rather, the frontend that uses the code on the backend to...



LEO:  You can't even remember which end the code comes out.



STEVE:  Anyway, I love that somebody created a cartoon for this because, yes, it does capture the essence of - I think the reason coders like us love to code is that it really can tax our brain more than anything else.  It's like as much as I can give it, it will take.  And I love that about it.  But it'll take it back if you're not careful to hold onto it.



LEO:  This is why as I get older I like functional code better and better because you write small routines that always produce the same result.  When the same stuff goes in, the same stuff comes out.  You can document in Lisp is nice because you can have a code string.  I guess you could do that in Python, too, where it's the second line of the function.  You say, this is what this function does.  And even if I don't understand why the function works, I've tested it.  I know it works.  I know it does what it says it'll do.  So I know that component.  And that's I guess the same idea behind encapsulation and object-oriented coding is that once you make an object, it's a black box, and you don't have to remember how it works as long as you know and can trust that it works; right?  But you're doing it in assembly.  I don't even know how you could - I guess you could write in that style in assembly.  People do.



STEVE:  And actually, as has often been observed, my assembly code looks much more like high-level language than most assembly code.



LEO:  You use macros; right? 



STEVE:  Well, yes.  I use macros, and also Microsoft's macro assembler has if-then-else loop while, I mean, it's got high-level language control flow constructs.  They have zero overhead.  That is, the if is just a branch instruction, but it will intelligently do, like build an expression for doing compound testing in a single line.  So I've seen other people's assembly is just like opcodes running down the left-hand margin of the page.



LEO:  That's a recipe for disaster, yeah.



STEVE:  And you just look at it, and it's like, what is this?  I mean, it's ugly looking.  Who could be proud of that?  But some jocks think that that's...



LEO:  Hey, it works, man.  It works.  Well, that's what, yeah, see, and that's exactly - I think sometimes older coders probably are better, not just from experience, but because they can't trust themselves to remember the complex spaghetti that they wrote.  So they have to rely on...



STEVE:  They've learned not to.



LEO:  They've learned not to, yeah, yeah.



STEVE:  So anyway, love that cartoon.  Okay.  So get a load of this.  This piece was stiff competition for this week's main story.  But as I said, it lost out to an explanation of what was discovered about the operation of DuckDuckGo's Privacy Browser.  So I decided to lead with this one as the runner-up because, wow, it's just perfect for this podcast.  A penetration testing and secure app development group known as Dvuln (D-V-U-L-N) recently took a close look at the somewhat new, it was released in 2019, so three years ago, New South Wales government's Digital Driver's License (DDL).  And putting it mildly, they found its security to be wanting.  They documented the system's various troubles in a blog posting which was titled "ServiceNSW."  ServiceNSW is NSW is New South Wales.  They said:  "ServiceNSW's Digital Driver's License Security Appears to Be Super Bad."



So, okay.  To set the stage, they explain in their posting:  "In November 2019, the New South Wales government (ServiceNSW) introduced the Digital Driver's License, or DDL for short, as a means to make it easy for people to access a digital version of their driver license.  Upon the launch of ServiceNSW's Digital Driver's License there were multiple security researchers who publicly reported a number of security issues including but not limited to the ability to manipulate digital license data and create fraudulent digital identities."  What?



They said:  "As far as we can see, there appears to be no formal public response from ServiceNSW regarding the acknowledgement or remediation of these issues.  As of February 2022, according to the Minister for Customer Service, there have been 3.9 million people who have opted-in for the Digital Driver's License.  To put this into perspective, we can assume around 70% of people in New South Wales use and trust the Digital Driver's License as a means of identification and verification in their day-to-day lives."  Meaning that 3.9 million people is 70% of the population of New South Wales covered by this DDL.



They said:  "During Dvuln's analysis of the ServiceNSW mobile application" - it's for iOS - "we discovered that due to the existence of several security design flaws, it is still possible" - meaning today - "for malicious users to generate fraudulent Digital Driver's Licenses with minimal effort on both jailbroken and non-jailbroken devices without the need to modify or repackage the mobile application itself."  So of course that would be one thing, right, to create a fake Digital Driver's License app which could show anything you want it to show.  But no.  Don't have to do that.  Use the real app and just change the data.  And it doesn't care.



Okay.  So back in 2019, not long after this DDL (Digital Driver's License) first appeared, during a security conference a security researcher, as part of his conference presentation sort of on a larger topic of digital identity security, he demonstrated to the audience, in public, his ability to modify this New South Wales Digital Driver's License details locally on his mobile device, causing it to display false information.  And displaying accurate information is the whole point because you, like, show your phone to somebody, and they go, okay, yeah, that's you.  And, oh, look, you're 21 years old.  Go ahead.  Buy alcohol.  Go into the club, whatever.



And although during his talk he mentioned that he had reported these troubles to ServiceNSW, the New South Wales government, there were no apparent public updates on the matter since then.  So it's unclear whether these bugs were considered an accepted risk - which okay, this is 2022, everybody, that's insane if that's the case - or if any sort of remediation was ever attempted by the presiding government.



So now we jump forward three years to 2022 where there are rumors circulating regarding underage people using false digital licenses.  No.  Underage kids could be spoofing their digital identities?  No.  The Dvuln posting contained an authentic-appearing Twitter posting made on November 25th, 2021 where the poster is annoyed that a bouncer at a club denied access to one 18 year old when others using fake digital licenses are apparently regularly admitted.



I have the tweet in the show notes for anyone who's interested.  This was posted by Sydney 2100 is the Twitter name.  And this was sent to @TheSteyneHotel.  And the tweet reads, "18 year old went there last night with three forms of ID, and you wouldn't let him in because you don't count a physical New South Wales driver's license as valid ID.  Really?"  He says:  "I know 10 kids that you let in regularly with fake digital licenses because they are easy to make.  No idea."  Meaning you have no idea what you're doing.  So apparently this 18 year old had a physical real-world, like old-school plastic New South Wales driver's license, and the guy said no.



And actually, Leo, completely off topic, I may have mentioned this before, it seems familiar, that Lorrie and I were renting a car, I don't know, a couple months ago, to move some stuff out of her parents' condo in L.A.  So we got the biggest, it was a Yukon or something, thing that we could rent.  And it turns out in order to do that I had to have a banking application on my phone which showed my name.  And, I mean, I had driver's license, I think I may have even had my passport with me.  But, I mean, I had a wallet full of credit cards.  I had, you know...



LEO:  You needed this Hawaiian Driver's License for McLovin.  If you'd had this, everything would have been simple.



STEVE:  Unbelievable.  I actually...



LEO:  That's bizarre.



STEVE:  And I said to this little young gal at the terminal, she said, oh, everything was fine.  She said we need one more thing.  You need to show me a banking app, like Chase or whatever, Visa or something, for your account with your name on it.



LEO:  They didn't tell you that ahead of time?  That's bizarre.



STEVE:  No.  Didn't tell me ahead of time.  It was apparently new.  And as it happened, I think I had one, or I have an account with Chase, but I didn't have their app.  So I had to...



LEO:  Download it.



STEVE:  ...install it, log in.  Then I had to go to LastPass and get LastPass to log me in to Chase in order - it was like, oh, my.  Anyway, point is, apparently, no.  Now we're believing digital over real-world old-school physical.



LEO:  That's bizarre.



STEVE:  Anyway.  Okay.  So...



LEO:  That's good to know.  I'll make sure to keep my banking app on my phone, yeah.



STEVE:  Yeah.  And it's funny, too, because - oh, and actually she finally just said, "Okay, never mind."  I was making such a valiant effort to do this and kept getting stuck for reasons like my own security was getting in the way.  She said, "Okay, never mind."



LEO:  Right.  Oh, for crying out loud.



STEVE:  So as we're walking to the car...



LEO:  You must be Steve Gibson.



STEVE:  As we're walking to the car, she says, you know, and I'm saying to her, I said, "Well, you know, I guess I'm just old, and that's why I don't have, like I don't do banking on my phone, like, ever.  I'm not going to do banking on my phone."  And she says, "Yeah, well, you are old.  Because," she said, "everybody I know has banking apps on their phones."



LEO:  Yeah, yeah.



STEVE:  And I said, well, okay.



LEO:  Yeah.



STEVE:  Okay.  So not having seen the IDs themselves, that is, this particular tweet and what the guys were referring to, the Dvuln guys wrote about this Twitter posting, they said:  "We cannot confirm whether or not they were exploiting the poor security design or similarly using a static photoshopped image."  Because again, if you're just showing like a screenshot, it could be faked; right?  Like, you know, just Photoshop what this digital ID is showing, and that's what you present.  So clearly there are problems with the whole concept of, like, so many ways you could do a digital ID incorrectly.  Anyway, he says:  "At the same time, although due to ease of exploitation, it is entirely possible that these kids were using the same method detailed in this blog."



Okay.  So the Dvuln guys quoted one of the security claims made about the new Digital Driver's Licenses.  They said:  "According to a press release from the New South Wales government, the Digital Driver's License implemented is 'hosted securely on the new ServiceNSW app, locks with a PIN, and can be accessed offline,' and 'will provide additional levels of security and protection against identity fraud, compared to the plastic driver's license.'"



And the Dvuln guys explained that, in fact, real world physical driver's license counterfeiting is actually far more difficult than spoofing the content of a physical driver's license.  They wrote:  "Given the Digital Driver's License's current state of security" - by which they mean, as we'll see in a minute, shocking lack of security - they said:  "We believe it would be far more difficult for an average fraudster to obtain the equipment necessary to produce high-quality plastic NSW driver's licenses.  A fraudster would need to source and obtain hardware such as but not limited to, a card printer, New South Wales holographic security foil, and other security features developed uniquely for the New South Wales identification cards such as the middle green layer, none of which are commercially or legally available outside of the printing hardware."



So, yeah.  Like, you know, if you've looked at your driver's license, it's got all kinds of wacky stuff going on now.  Weird reflected angles and things embedded in different layers and stuff that's shiny.  It's like, okay, you can imagine duplicating that physically is not going to be easy, compared to, what, taking a screenshot of the secure ID on the phone?



Okay.  So what are the specific problems with New South Wales Digital Driver's License?  Okay.  First off, the DDL-stored license data is encrypted, but not very well.  On iOS, the Digital Driver's License data is stored in a JSON-formatted file, which is encrypted using AES-256-CBC.  So 256-bit AES cipher, that's state of the art.  Cipher Block Chaining mode, CBC, that's fine.  And then that's combined with Base64 encoding.  Nothing wrong with any of that.  The encryption will turn anything, even if it was ASCII textual content to begin with, into binary data.  So the Base64 encoding converts that back into ASCII text so that it can be stored in a JSON text format file if desired.



But here's the problem.  The encryption key is the four-digit PIN that's initially set during onboarding when the user first installs and sets up the app.  That's it.  A four-digit PIN.  Believe it or not, that's the password.  A four-digit PIN.  Since a four-digit numeric PIN can be anything from 0000 to 9999 there's, gee, let's see, 10,000 possible PIN combinations.  One of those will correctly decrypt the JSON file.  And the use of the correct decryption will be readily apparent because what's not decrypted correctly, that is, with the wrong PIN guess, will be gibberish.  In other words, if an attacker is able to obtain the encrypted data, either through accessing an iPhone backup, direct access to the device, or a remote compromise, it will take only a few minutes to brute-force the correct four-digit key to that encryption.  There's not even any password-based key derivation function, you know, a PBKDF, to slow down the guessing.  Nope, just use the four-digit PIN, see if that decrypts the blob back into something that looks correct; and if not, try the next one.



During Dvuln's testing, their brute-force process took only a few minutes to decrypt the Digital License Data, which could then be edited, reencrypted and used to change the Digital Driver's License details on the mobile device.  In other words, no sign of any authentication, no digital signature or anything else to protect against user tampering and manipulation of the stored and then displayed data.  The only protection was this four-digit PIN.  And when you use it, it decrypts the data, bringing it back into plaintext, which can be modified.  Unbelievable.  Again, this is 2022.



Next problem is a lack of any client-side validation.  As they said, the Digital Driver's License data is never validated against the back-end authority which issued the license.  So not only is there no local authentication, it's not signed, nothing.  But there's no ongoing period or ever verification with the original issuer.  They wrote that this means that the application has no native method to validate the Digital Driver's License data that exists on the phone and thus cannot perform further actions such as warn users, or anyone relying on this data, anybody else, when this data has been modified.



Since the Digital Driver's License data is stored on the client's device, validation should take place to ensure the local copy of the data matches the Digital Driver's License data that was originally downloaded from the ServiceNSW API.  Or, I would add, locally verify a signature.  No such verification takes place,  An attacker's able to display the edited data on the DDL app without any prevention.



Okay.  Now presumably, whatever moron designed this system figured that since it was encrypted with military-grade 256-bit AES encryption, and thus could never be modified, there would be no need to verify it.  And one of the features they boasted was that it could run completely offline.  Right.  No verification needed.  And speaking of verification, one of the key "verification features" of the digital license is its so-called "pull-to-refresh" functionality, which is used to ensure that anyone relying on it is viewing the most current license information.



However, the Dvuln guys noticed that refreshing the application's driver's license data only updates the QR code which is displayed on the license, and that the QR code only contains the license holder's name and whether they are under age of 18 or not.  That's the only thing in the QR code.  So if a fraudster had modified their license details and photo by decrypting it, modifying, and then reencrypting the data, this fraudulent data would remain visible on the screen even after the QR code, date, and time had been refreshed and updated.



And not surprisingly, in still another example of incredible sloppiness, the license data is, indeed, exported in iPhone backups, making its modification outside of the phone trivial.  As we know, when a secured mobile device is jailbroken, it's reasonable to assume that any security features an application may have could be bypassed because an attacker has obtained root-level access to the device's storage and various services.  But conversely, as long as a secured mobile device is not jailbroken, apps should be able to be reasonably secure that their users are protected against misuse and various types of client-side vulnerabilities.  However, in the case of ServiceNSW's application, the Digital Driver's License data is included in device backups, which means that attackers or anyone wanting to commit fraud can obtain and modify their license details without ever needing to jailbreak their device.



There is no way that this system was ever reviewed by any competent security expert.  These days we've got competent security experts coming out of our ears.  Anybody who had any training in digital application security would immediately see what an embarrassment this thing is.  And the publicly known problems with it are now three years old, with rumors that these Digital Driver's Licenses are being readily spoofed.  Of course they are.  So we have to ask ourselves, how did this happen?  Was it created by some government bureaucrat's unemployed nephew?  And why doesn't anyone appear to care?  It seems quite likely that someone will be caring very soon, thanks to the bright light that the Dvuln guys have finally aimed at this mess because the tech press has picked this up and run with it.  So this is going to be an embarrassment to New South Wales and whatever clown wrote this thing.



Okay.  So because this is the Security Now! podcast, we can ask, so what's the answer?  How do we solve this problem?  Since I've been called a competent security expert, I'll take a stab at it.  How about this?  All that's needed is a certificate, a standard X.509 format certificate.  This has all been worked out already.  Certificate fields can contain binary data.  They already do, like they have a public key in them.  So the certificate owner's photo can easily be contained within the certificate, as can dates of starting and ending validity and anything else that might be needed, like a timestamp for when the certificate is signed, the owner's legal name, their physical address, their date of birth, and so on.



What makes the certificate special is that its entire contents is signed by a recognized and trusted authority.  As we know, the process of signing is that the certificate's contents are hashed to create a digest of that content.  Then that hash is encrypted with the signer's private key.  Anyone wishing to later verify the certificate's authenticity, meaning the contents of everything in the certificate, simply creates their own hash of the certificate's contents, then uses the signer's public key to decrypt the hash that came with the certificate. If the new hash and the decrypted hash match, then we know that not one single bit of the certificate's contents have been modified since it was signed.



So to build a simple and practical system, an existing trusted root authority, a certificate authority, any existing certificate authority that's already trusted by the mobile iOS and Android platforms, take my favorite one and chosen certificate authority DigiCert, issues an intermediate certificate to the New South Wales administration, which is itself permitted to sign these special purpose end DDL (Digital Driver's License) certificates.



Anytime someone out in the world, like a DDL license holder's DDL application, wishes to update and reverify their license, that app sends a request to the government's server.  The server pulls together all of the relevant data it has for the individual from its database, including their latest photo, their date of birth, the driver's license initial and expiration date, and builds a new certificate which also contains a current timestamp.  They use the private key that was obtained from DigiCert in this example, to sign the resulting certificate.  They then bundle that certificate with their intermediate certificate's public key and send the package back to the DDL application.



Since the bundle contains a short certificate chain whose intermediate certificate is already trusted by the root certificate store in the mobile device, and since that intermediate certificate can verify the DDL end certificate, any iOS or Android device has everything it needs to verify the authenticity of the DDL end certificate.  And there is no way for that DDL license certificate to be modified or tampered with in the field without breaking its signature.  In this system, there's no need for a PIN to decrypt the certificate's data because there's no need for any certificate encryption in the first place at all.



And as for that QR code that apparently only contains the user's name and age, that's also, frankly, ridiculous if its data can be spoofed.  If you want to have some sort of verification reader that reads the QR code, then the QR code can simply contain the individual's driver's license number.  And we know that that cannot be spoofed.  And if it were, it contains the driver's license number.  So the reader, the QR code reader obtains the license number that is being claimed on the screen, makes a query to the government's server to obtain their signed DDL certificate, and displays the user's name, age, photo, and everything else so you can make queries on the fly.



And if the QR code system needs to work offline, which was apparently a feature of the current system, then the QR code can simply contain a signed subset of the user's information, like only their name and date of birth, which is separately signed by the government's intermediate certificate.  The QR code reader then scans the QR code, which is itself a tiny certificate.  It uses the government's intermediate certificate to verify the QR code certificate and can then trust that the data contained in the QR code certificate subset has not been tampered with.



Again, none of this is rocket science.  As I've said many times, we now have an incredible toolkit of technology components that can be applied individually and collectively to solve any of these sorts of problems.  Nothing needs to be invented anymore.  And the beauty of this system is that all of the well-tested and bulletproof crypto libraries already exist.  In fact, the iOS and Android platforms already contain down in their kernels all of the required crypto machinery APIs.  None of that needs to be created.  So hopefully, when this ridiculous New South Wales Digital Driver's License disaster fiasco finally comes to light, the existing ridiculous system will not be salvaged.  It is unsalvageable.  It needs to be entirely scrapped and replaced with a simple bulletproof system like the one I just described.  There's nothing to it.



LEO:  Seems simple.



STEVE:  Yes, Leo.  Unbelievable that, like, this was designed three years ago.  This all existed three years ago.  This is not new.  This is just the obvious way to solve this problem.  



LEO:  Did I hear you correctly that the data is unencrypted with a four-digit PIN?



STEVE:  Yes.



LEO:  You enter your PIN, and now I can modify my driver's license.



STEVE:  Yes.



LEO:  Well, that's, I mean, anybody looking at that would see the problem.



STEVE:  I know.  It's insane.



LEO:  It just assumes that, well, why would anyone want to modify their driver's license?



STEVE:  Yeah, I can't imagine why.



LEO:  Why?



STEVE:  An 18-year-old would want to spoof their age.



LEO:  Why would they modify their - yeah.



STEVE:  No one's ever...



LEO:  No one's ever done that before.



STEVE:  No.



LEO:  That's just bizarre.  Oh, well.



STEVE:  So we have the latest Microsoft Office zero-day remote code execution vulnerability.



LEO:  Of the week.



STEVE:  Oh, my god.  This is a head-buried-in-the-sand quite pervasive problem in Microsoft Office.  By far the best researcher on this has been Kevin Beaumont.  We've talked about Kevin from time to time. He's a great security researcher.  He tweets using the handle GossiTheDog for some reason.  And he even, Kevin, tracked down a bachelor's thesis authored by a guy named Benjamin Altpeter on August 1st, 2020, so nearly two years ago.  On page 29 of his thesis, of his bachelor's thesis, Benjamin writes:  "Windows includes the ms-msdt://" - so it's a URL scheme - "protocol that opens the Microsoft Support Diagnostic Tool which provides the troubleshooting wizard to diagnose WiFi and audio problems and the like.  This protocol directly passes the string it is given to the msdt.exe program.  The attacker now needs to find an included wizard that allows the execution of arbitrary programs, preferably even remote ones.  The program compatibility wizard fits this description.  Luckily for the attacker, all user input can also be prefilled from the command line."



Okay, now, this MSDT thing was new to me.  At that point I opened my own command-line window on my Windows machine, entered "msdt" and hit ENTER.  And sure enough, up popped a "Microsoft Support Diagnostic Tool" dialog which I had never encountered before.  So this MSDT thing is alive and well and living in all of our Windows machines right now.  It turns out you can access this, which is to say bad guys can access this, through an Office document using the ms-msdt:// protocol and use it to remotely execute code.



Okay, so back to Kevin.  Kevin's name for this exploit has stuck.  It's now semi-officially known as the "Follina" (F-O-L-L-I-N-A), the Follina exploit, because Kevin spotted a reference to 0438 in the sample exploit file, and 0438 is the area code of the city of Follina, Italy.



LEO:  Okay.  Hey, as good a reason as any.



STEVE:  That's the way we name these things these days, folks.



LEO:  Yes, it is.  Hysterical.



STEVE:  So Benjamin's bachelor's thesis was nearly two years ago.  The reference in it was obscure, it was on page 29, and we'll never know whether someone saw it and recognized its significance, as he did, or may have independently invented an attack.  Given the nearly two-year interval, I'm inclined to think that this was an independent discovery because these sorts of things happen all the time.



But either way, last month on April 12th, the leader of Shadowchasing1, an advanced persistent threat (APT) hunting group, reported the active exploitation of this vulnerability in the wild to Microsoft's MSRC, their Microsoft Security Response Center.  That's April 12th.  The Shadowchasing1 report provided a copy of the in-the-wild, real-world Microsoft Office document exploit which was targeting Russia, themed as a Russian job interview.  Nine days later.  Nine days go by.  On April 21st, Microsoft's MSRC blew it off and closed the ticket, saying that it was not a security-related issue.



I have a picture in the show notes, a screenshot of this.  It reads - it's signed by MSRC, so it's written in the first person.  It reads:  "I finally had time" - after nine days.  "I finally had time to look at this critically and have decided it is not a security-related issue.  Msdt is indeed executed, but it requires a passcode when it starts, and the one provided in this sample does not work for me.  I will be closing this case, but appreciate you submitting it.  Regards, MSRC."



Okay.  Now, I'm not sure what the phrase "looking at this critically" means, exactly.  But apparently it doesn't mean "closely examined and worked to understand what the underlying problem might be."  And, you know, this might represent another sample of the pattern that seems to be emerging, where Microsoft increasingly seems to be needing the external security research community to solve all of its problems for it, and hand it everything on a silver platter.  Apparently saying "Hey, have you considered that allowing Word's remote template feature to retrieve an HTML file from a remote server, which in turn uses the ms-msdt:// URI scheme to load some code and execute some PowerShell, even when Office Macros have been disabled and even able to bypass Protected View by giving the document an RTF extension?" is no longer sufficient to get Microsoft's attention.



Now, in fairness, the provided exploit sample apparently did not work instantly for the overworked MSRC guy.  And we don't know how many false-negative reports the MSRC guys might be fielding every day.  Perhaps they face a constant deluge of bogus reports.  But we do know about economics, and we do know about incentives.  So we know that if this sort of behavior is never met with consequences, it will continue and even be de facto encouraged.  And Microsoft has arranged to completely insulate themselves from any consequences ever.  So all we can ever do is hope for the best.



Since we're apparently on our own, what do we do?  Kevin reports that the vulnerability has been proven to work against Office 2013, 2016, 2019, 2021, Office ProPlus, and Office 365.  In other words, all of them.  It also applies to Windows itself because this thing, this exploit can be invoked from, believe it or not, our old friend the .lnk link file.  Yup.  That hasn't gone away.  So there are two different issues:  Office itself using the ms-msdt: scheme protocol while allowing loading unfiltered HTML Word templates and Outlook links; and the fact that the MSDT executable allows code execution.  All flavors of Windows Defender also completely miss detecting this, except yesterday that changed.  Which is not surprising given that Microsoft decided last month that this was not a security issue.  So that was true until yesterday.  Now Defender's awareness has been updated, and I'm sure other AV companies are on this also.



And Microsoft released a workaround in the meantime in the form of a registry script, also yesterday.  That registry script simply deletes the ms-msdt protocol handler reference from the registry.  I have it.  You run the command prompt as admin, and then you do a "reg delete HKEY_CLASSES_ROOT\ms-msdt /f."  But get this from the show notes if you want to try it.  And as always, it's good to back up your registry beforehand.



So Kevin wonders out loud how this might evolve.  He says:  "We'll see.  Microsoft are going to need to patch it across all the different product offerings, and security vendors will need robust detection and blocking.  Microsoft will probably point towards Protected View; however, Protected View also applies by default to all macros, and Office macro malware is most definitely a major problem regardless."



Then in an update to Kevin's original posting, he added:  "Microsoft have indeed pointed to Protected View, saying it 'prevents' the attack."  Kevin writes:  "I think this is stretching the truth.  For example, if the document is a .RTF file and is opened by Preview in Explorer, Protected View does not apply, and it becomes a zero click exploit.  Microsoft knows this.  They just aren't mentioning it to customers."  And then Kevin provides a tweet from our well-known CERT/CC guy, Will Dormann, and Will agrees.  Will tweets:  "This language is a bit misleading in not really describing what 'calling application' means.  If you preview a file in Explorer, which uses Office to render the document, Protected View doesn't do a damn thing," says Will.



The very latest is that Microsoft, as I said, has finally awoken to this threat, a CVE has been assigned, and Windows Defender's detection signatures have been updated.  But the underlying trouble is that the use of MS Office protocol is extensive and pervasive, so it cannot be shutdown without breaking all sorts of other things that depend upon it.  It's a bit like last year's printer spooling Catch-22 fiasco.  It's another of those problems that isn't really a bug that can be patched because it's the abuse of a deliberately designed-in feature.  And this is what results from systems that become too complex.  It begins to be impossible to understand and anticipate every possible interaction among components when there are just so many different components.  So it's the world we are in today.



Anyway, Defender's updated.  Other AVs are being, if they're not already, updated.  We've got now the maximum time for Microsoft to create a patch because today is the last - today this Tuesday is the last day of the month, which puts the first on a Wednesday, meaning that Patch Tuesday will be the 14th, the latest date it could possibly be in the month.  So Microsoft has a full two weeks.  Maybe they'll be able to get a patch out for Office across the board by then.  We'll see.



Okay.  Ghost Touch.  Get a load of this one.  This is not yet a real-world threat due to the potential attack's very short range which is currently around 40mm, or just over an inch and a half.  But as we know, "impractical" is how many eventually practical attacks began.  Here's the abstract of the 17-page exploit research paper describing this new and very clever Ghost Touch attack which was invented by a team of Chinese and German security researchers.



They explain:  "Capacitive touchscreens have become the primary human-machine interface for personal devices such as smartphones and tablets.  In this paper we present Ghost Touch, the first active contactless attack against capacitive touchscreens.  Ghost Touch uses electromagnetic interference (EMI) to inject fake touch points into a touchscreen without the need to physically touch it.  By tuning the parameters of the electromagnetic signal and adjusting the antenna, we can inject two types of basic touch events, taps and swipes, into targeted locations of the touchscreen and control them to manipulate the underlying device.



"We successfully launch the Ghost Touch apps on nine smartphone models.  We can inject targeted taps continuously with a standard deviation of as low as 14.6 by 19.2 pixels from the target area, a delay of less than half a second, and a distance of up to 40mm.  We show the real-world impact of the Ghost Touch attacks in a few proof-of-concept scenarios, including answering an eavesdropping phone call, pressing the button, swiping up to unlock, and entering a password.  Finally, we discuss potential hardware and software countermeasures to mitigate the attack."



Okay.  So a little bit of background.  A capacitive touchscreen it turns out is a surprisingly sophisticated and highly sensitive device, which we now pretty much take for granted.  And the technology is known to be quite susceptible to local environmental noise.  It's easy to make a touchscreen malfunction when it's placed near a source of electromagnetic interference.  There are switching regulators used in some phone chargers which are known to generate so much short-range high-frequency electromagnetic interference that nearby touchscreens will not function if they are nearby.



So what these clever researchers have done is turn electromagnetic interference into electromagnetic touch signals to deliberately spoof what are essentially the receivers in capacitive touch systems to generate spoofed touch events.  And they made it work.  At this point it's only of academic interest. But we should not be surprised to learn of it being applied in some way in the future.



Okay.  There's something new that might be coming known as TrustPiD.  If it ever actually happens  and Vodafone and Deutsche Telekom are reportedly both now in pilot testing of this new TrustPiD system in Germany  the technology creates, deliberately, I mean, it's designed to create a persistent, static, super-cookie which, being somehow injected into a user's communications at the carrier-level by the cellular carrier, cannot be seen, managed, or blocked by end users.  The only reason I can see for any cellular carrier to be doing something this clearly privacy invasive is that they've decided that it's more important for them to get in on the Internet advertising revenue boom by arranging to monetize their customer's "anonymous," and that's in air quotes, online identities.



Here's what the TrustPiD website says under the tired banner of "Keeping the Internet Free."  Right?  So that's what they're - this is how we're going to keep the Internet free, kids.  And it anyone's interested, TrustPiD, I think it was dot com.  It reads:  "Consumers appreciate the idea of a free Internet, but this comes with a trade-off.  Publishers need a sustainable revenue model, meaning that it becomes essential to add subscription paywalls or rely on advertising to maintain free access to high-quality content.  With a growing trend of digital information shared in the ecosystem, consumers' concerns about privacy and the amount of information passed into the free Internet have been raised."  No kidding.  By things like this, apparently.



"TrustPiD," they wrote, "is a technology solution that enables consumers to enjoy free content and the benefits of the open Internet whilst retaining control over their privacy."  Uh-huh.  "TrustPiD is a secure" - yeah, like that Digital Driver's License - "unique digital token generated by assigning random numbers to you, which reduces the risk of you being directly identified whilst" - they like that word - "still enabling advertisers and publishers to provide you with a personalized experience across their sites with your consent.  You can find more information on how we generate and manage your TrustPiD in the Privacy Notice."  I went there.  There was none.



LEO:  That's funny.  Good luck finding it.



STEVE:  That's right.  "Your consent for TrustPiD is collected by advertisers and publishers via their Consent Management Platform" - and then they have here in parens (cookie banner) - "when you visit their sites.  The consent will apply only to those websites.  TrustPiD service gives consumers complete control over how their TrustPiD is used by enabling them to manage their consent via a central 'Privacy Portal' at any time.  In the Privacy Portal, consumers are able to track which advertisers or publishers they have allowed to provide them with personalized online marketing in their websites based on their TrustPiD in one central place, the possibility to withdraw consent at single website level, and the ability to turn off the TrustPiD service entirely, preventing any further use of the token.  If you want to understand more about how the Privacy Portal works, or if you want to manage your consent, please visit the dedicated Privacy Portal page."  And I should mention that only customers of Vodafone and Deutsche Telekom who are involved in this have access to the portal.  You've got to validate by using your cellular device.



Okay.  So I have many questions.  Aside from the fact that this will assuredly be an opt-out service where all users will be, by default, opted-in - otherwise it would never get off the ground; right?  Apple and their attempt to require advertisers to get explicit permission showed that people just say no, thank you, I don't want that.  Okay.  And thus enabling tracking without their consent.  And we need to know whether this system will respect the user's GPC, the Global Privacy Control setting.  Presumably the websites at the other end have to.  So we'll see how that goes.



As I said, though, aside from that, how exactly can identifying tags be injected at the carrier level into encrypted HTTPS web sessions?  This is really a concern.  The only way I can see that this can be done in 2022 is by having the carrier, in this case Vodafone and Deutsche Telekom, actively intercepting and proxying all of their customers' encrypted TLS connections.  We saw this years before when some carriers were offering caching and data compression services.  Remember that?  Cellular carriers would offer, like we're going to speed up your Internet connection, you lowly cellular users, by compressing the data for you, sending it to you compressed, and then we'll decompress it on your phone.  But that was back in the pre-HTTPS Everywhere days, where most connections were still unencrypted.



But in 2022, as we know, nothing is unencrypted anymore.  A carrier can see the IP addresses that their users are connecting to, but IP addresses no longer reflect the website properties behind them, since SNI (Server Name Indication) is now being heavily used during TLS handshaking to identify the domain being connected to and thus the TLS certificate which should be sent to the client.  This is allowing many websites to reside at the same IP.  So there's no way to disambiguate them from the outside.  So this means that a persistent identifier is either somehow injected into the user's connections by their smartphone's browser before entering their TLS encrypted connection, or the user's smartphone must accept a Vodafone certificate authority-style root certificate into its root store to allow Vodafone and Deutsche Telekom to function as a sanctioned, active, persistent cookie-injecting man in the middle.



Neither of these seem likely, or possible, or even remotely feasible in 2022.  So I'm stumped.  I mean, I really am.  Without being on the inside, Vodafone and Deutsche Telekom cannot see who their customers are connecting to and cannot alter the content of their data injecting something into the flow.  And cellular carriers are explicitly and deliberately on the outside.  They're carriers of encrypted content.



I followed every link I could find, looking for any technical documentation.  There's none that I could see.  Since I don't read German, my digging was somewhat limited.  Maybe I'm missing something obvious.  But even if there's some way to pull this off technically, why the hell are our carriers getting in on the "identifying their customers to advertisers" game?  My feeling is this is a monster that needs to be strangled in its crib immediately, before it has a chance to grow.  My mind is boggled by the idea that Vodafone and Deutsche could be experimenting with this, with something like this.  And clearly it's to sell the information; right?  I mean, why else?  Wow.



Okay.  A couple of closing-the-loop bits.  MikeO, he said:  "Hi, Steve (@SGgrc) and Leo (@leolaporte)."  He said, "Listed" - and he meant listened, I'm sure -"to SN-872" - so that was last week - "and you said there were no good ad blockers for macOS/iOS."  He said:  "I've been using 1Blocker for years, and it works great with Safari on all of Apple's platforms:  1Blocker.com."



And actually I was reminded of that.  Bill Zegarski, he also tweeted, saying:  "@SGgrc Heard the conversation on ad blockers for Safari.  @1BlockerApp, which you recommended and I love," he says, "on iOS, also has a Mac version.  Might want to look at that.  Best part is that since it's a universal app, if you purchase on one platform, you get both."



LEO:  I guess it depends on what you call "good."



STEVE:  Okay.  Fair enough.



LEO:  If by "good" you mean blocks all ads, it's not.



STEVE:  Then no.



LEO:  So the problem is I don't think it's 1Blocker's.  I also use Firefox Focus.  I use a variety of different, or have attempted to use a variety of different adblockers on iOS.  And the problem is because of the architecture, and this is what we were talking about, it's very hard for an adblocker to work as well as UBlock Origin, Gorhill's.  So I'm looking at iMore.com, great site, using 1Blocker, all of the blocking turned on, and then using Gorhill's UBlock Origin on my desktop.  And on the 1Blocker on iOS I see an ad for Mint, Best Buy, Kay Jewelers - this is with the blocker turned on.  And on and on and on.  There's quite a few ads.  And I don't think that that's 1Blocker's fault.  I just think that it's hard to do with the stricture that Apple places on it.



STEVE:  Yup, good.



LEO:  So that's the problem.



STEVE:  I totally agree.



LEO:  That's what we were talking about.  There isn't any full adblocker available on iOS.



STEVE:  Right.  And as I have said, because I am such a fan of UBlock Origin, when I go to a machine that doesn't have it, I'm like, oh, my lord.



LEO:  You forget.



STEVE:  This is what people put up with on the Internet?



LEO:  Yeah.



STEVE:  Oh, my.  I'd pay not to have that.



LEO:  So if you want a comparison, put 1Blocker on your Mac or your iOS, and then look at it with UBlock Origin on Firefox on your Mac.  I mean, I don't see any of those ads with UBlock Origin.  Any of them.



STEVE:  Perfect, perfect.



LEO:  So again, I don't blame these blockers, although this is not a free blocker.  So you're paying for something that ostensibly blocks ads, but doesn't.



STEVE:  It's not.



LEO:  I mean, Firefox Focus, which is a free ad blocker from the Firefox folks, does, as well.  I think it's more the Safari issue as really anything else.



STEVE:  George Palfi said:  "Just heard your discussion on quantum computing and cryptography.  Perhaps we can adapt new crypto for quantum computers.  But Bitcoin, Ethereum, and many cryptocurrencies are based on very old crypto rules," he says, "and I don't believe the algorithms can be changed.  So people who want to keep their wealth protected for a long time will be at risk as quantum computing evolves.  Better to stick to gold and silver which have worked for thousands of years," says old George.



And, okay, the only issue I would take with that, first of all, there are several aspects of using cryptocurrencies.  One is passwords, and I would never disagree that because passwords may be using public key crypto, they're going to be in trouble.  But, for example, we know Bitcoin well because we talked about, it was 11 years ago or something, when it was worth dust, and there was a bitcoin faucet where you could just get free pieces of a bitcoin by going there.  And so we know that it uses a 256-bit hash.  And hashes are strong even against quantum crypto.  It's specifically the public key aspect of today's crypto that has people worried, not the symmetric crypto or the hashes.  And that's the essence of, you know, hashing is the essence of these various cryptocurrencies.  They're going to be safe.  Their design probably does not need to be changed.  But George makes the point, which is valid, which is they can't be changed.  They are an embedded crypto technology that is here forever.  Okay.  I guess you could just stop mining.  If you stopped all mining, then - I don't know.  I have to think about that.



Anyway, r4nd0m is his handle, with A as a 4 and O as a numeric 0.  He said:  "Hey @SGgrc.  I was thinking about the layer one attack against the unlock function of the various apps using Bluetooth Low Energy, and it occurred to me that a prompt in the app at unlock time should be a feature they add to help mitigate the demonstrated weakness.  Of course the user experience will suffer, but this should defeat the attack."  And I just wanted to share that because he's absolutely right.  So what he's saying is, if you had to touch the lock, and then the use of the app was not autonomous, but you had to then take your phone out of your pocket and tap to acknowledge, that solves this problem; right?  Because it wouldn't be done autonomously.



Unfortunately, on the site that I visited when I was putting the story together last week, the home page shows this lady with her arms full of groceries, like the whole point being she can't do anything except manage to, like, hit the lock with her elbow in order to register her presence, and then, oh, look, the door unlocks, and she's able to get in.  So exactly as he says, the user experience, which is so magical, by just touching the lock and you're able to enter, well, that's a lot less magical if you have to acknowledge it on your phone.  But it would solve the problem.



And Bryant McDiarmid says:  "Hey, Steve.  Quick question.  If I encrypt a file with a 256-bit encryption three times, with three different passwords, what is the resulting bit strength?  Is it 256 plus 256 plus 256?  Or 256 times 256 times 256?"  And Bryant, the answer is plus.  It's the equivalent on the symmetric key side of 768-bit encryption.  But the real strength is the entropy in the keys which are turned into the 256-bit encryption keys.  That's what really limits you.  But to answer your question, you are adding the equivalent bit lengths rather than multiplying them.



LEO:  And of course adding is virtually no value compared to multiplying; right?



STEVE:  Right.



LEO:  Yeah.  Shall I do an ad for you, sir?



STEVE:  Please.



LEO:  Thank you.



STEVE:  I'm going to wet my whistle, and then we're going to talk about whether DuckDuckGo is gone.



LEO:  I just did an informal test using the Firefox Focus plug-in.  And actually that does seem to block all the ads.  So there is a way with Apple's strictures on iOS to still use Safari.



STEVE:  So maybe 1Blocker...



LEO:  So maybe not use 1Blocker, which is not free.  Use Firefox Focus, which is.



STEVE:  And maybe they've sold out.



LEO:  Maybe.  Or, you know, the thing, it's a moving target.  One of the reasons I stopped using 1Blocker is there's literally 10 different blockers you have to turn on, and it's complicated.  Firefox Focus is a browser, but it also works as an extension so you could turn it on as a Safari extension, which is what I do, yeah.



STEVE:  Nice.



LEO:  Our show today, my friends, more importantly...



STEVE:  But wait.  And that's what he wanted.  He wanted a good Safari extension for his Mac.



LEO:  Yes.  That's the issue on the Mac because everything is using WebKit anyway.



STEVE:  But you've solved the problem then.



LEO:  Focus, I think.  That's what I've been using, and I didn't realize how well it did.  See, a problem is I have NextDNS, so I have to turn that off.  I have to go through a lot of hoops to see if I'm going to really get ads or not.



STEVE:  Leo, it's like me trying to get a banking app to run on my phone.



LEO:  Yeah.



STEVE:  To do it I need a week in order to shut down all the security.



LEO:  Really?  You want me to have a banking app on here?  Really?  Actually, you know, if you think about it, what she was really asking for is biometric.  Right?



STEVE:  Well, I unlocked my phone, so she could see the phone.



LEO:  Oh, yeah.  That should be sufficient.  Well, she wants you to unlock the phone and show it's your phone.  I mean, in a way that isn't a bad idea for additional - it's a way of kind of ad hoc biometric security.  Can you unlock your phone and prove that it's yours.



STEVE:  I think I did that.  I do have my credit cards registered with the phone.  And so, you know, it'll be...



LEO:  Well, that should be sufficient.  Yeah, having Apple Pay on there, that's better than banking, yeah.



STEVE:  Yeah, got Apple Pay.



LEO:  Well, anyway, she let you have the car.  That's the important part.



STEVE:  So the winner, as I said at the top of the show, of this week's most tweeted news of concern was the widely reported surprise that the explicitly privacy-centric and privacy-protecting DuckDuckGo enterprise had struck a previously secret backroom deal with Microsoft to enable user tracking from Microsoft-owned domains, including Bing.com and LinkedIn.com, when using the so-called DuckDuckGo Privacy Browser.  It was further revealed by DuckDuckGo's founder Gabriel Weinberg that DuckDuckGo's non-disclosure agreement with Microsoft prevented them from any further disclosure of their agreement's terms.  And I have the show notes, and it's on the screen, two of Gabriel's tweets.



He said:  "For non-search tracker blocking (e.g., in our browser) we block most third-party trackers.  Unfortunately our Microsoft search syndication agreement prevents us from doing more to Microsoft-owned properties.  However, we have been continually pushing and expect to be doing more soon."



This his follow-up tweet was:  "We've been working tirelessly behind the scenes to change these requirements, though our syndication agreement also has a confidentiality provision that prevents disclosing details.  Again, we expect to have an update soon that will include more third-party Microsoft protection."



Okay.  So let's back up a bit and see what happened.  TechCrunch's headline:  "DDG has a tracker blocking carve-out linked to Microsoft contract."  BleepingComputer reported: "DuckDuckGo browser allows Microsoft trackers due to search agreement."  9to5Mac headlined:  "DuckDuckGo caught giving Microsoft permission for trackers despite strong privacy reputation."  An Android Police headline:  "DuckDuckGo's supposedly private browser caught permitting ad tracking."



Okay.  The first thing for us to clear up about all this is that all refers to the use of DuckDuckGo's own browser, not to the use of the DuckDuckGo search engine.  On the other hand, anyone could be forgiven for missing this distinction.  When you go to DuckDuckGo's homepage you're greeted with a bold headline:  "Tired of being tracked online?  We can help."  Then below that are three big topics:  "Privacy for Chrome," "Private Search Engine," and "Privacy Browser App."  And explaining their privacy browser app they say:  "Our private browser for mobile comes equipped with our search engine, tracker blocker, encryption enforcer, and more.  Available on iOS and Android."  And as we've previously mentioned, actually there is one coming for macOS, and apparently one for Windows is also planned.



So all of this brouhaha began when security researcher Zach Edwards took the time to audit the data flows to and from one of DuckDuckGo's mobile browser platforms.  And given a screenshot we'll have later, it looks like he was using the Android browser.  Early last week he tweeted:  "Sometimes you find something so disturbing during an audit, you've got to check/recheck because you assume that something must be broken in the test.  But I'm confident now.  The new @DuckDuckGo browsers for iOS and Android don't block Microsoft data flows for LinkedIn and Bing."



So this is mostly, I think, a story about expectations.  As Leo, you and I were saying up at the top.  Zach was so surprised and disturbed by what he saw specifically because it wasn't what he ever expected to find from DuckDuckGo.  His next tweet he said:  "DuckDuckGo has browser extensions and their own browsers for iOS/Android at DuckDuckGo.com/app."  And then he's got links in his tweet for the iOS version and the Android version.  He said:  "Both versions of the DDG browser claim to use tools which automatically block hidden third-party trackers."  Then he's got two question marks.  And to back that up, Zach attached a screenshot of the clear statements from DuckDuckGo.  And I'll just read them quickly.



It says:  "Tired of being tracked online?  We can help.  DuckDuckGo is the all-in-one privacy app."  Now, the app meaning exactly what we're talking about, the browser app.  "DuckDuckGo is the all-in-one privacy app that helps protect your online activities.  With one download, you get a new everyday browser that offers seamless protections from third-party trackers while you search and browse, and even access to tracking protections when receiving email and using other apps on your device.  With DuckDuckGo, privacy can be your default."



Then it's got five callouts:  Search Privacy, Escape Website Tracking, Enforce Encryption, Block Email Trackers, and Protect  Your Privacy in Other Apps.  I'll just expand on that second one, Escape Website Tracking, where it explains:  "Tracker Radar automatically blocks hidden third-party trackers we can find lurking on websites you visit in DuckDuckGo, which stops the companies behind those trackers from collecting and selling your data."  It doesn't say "Except for Microsoft's advertising trackers, which our contract with them prevents us from blocking or disclosing."  And that's the problem because that's the truth.



Zach's next tweet was:  "I tested the DuckDuckGo so-called private browser for both iOS and Android, yet neither version blocked data transfers to Microsoft's LinkedIn and Bing ads while viewing Facebook's Workplace.com homepage."  He says:  "Look at DDG bragging about stopping Facebook on Workplace.  No mention of Microsoft."



And in the show notes, you've got it onscreen, Zach posted a photo showing his Android phone at Facebook's Workplace.com site with a DuckDuckGo popup, sure enough, bragging about the wonderful job it's doing, saying:  "Google, Facebook were trying to track you here.  I blocked them!  You can check the URL bar to see who is trying to track you when you visit a new site."  And then there's a big High-Five button which the happy user can presumably press.



LEO:  Has to press.



STEVE:  Oh, that's how you get rid of it.



LEO:  I don't want to do a high-five.



STEVE:  Ah.



LEO:  No.



STEVE:  So the fact that this browser specifically singles out other tracking properties while silently permitting Microsoft-owned domains to track feels at best disingenuous.  In an attempt to clarify the mess that arose from this, Gabriel appeared to attempt to explain what's going on over on Reddit.  You'll hear for yourself in a moment why I'm wording this as provisionally as I am because, although I've read what Gabriel wrote several times slowly and carefully, and it sort of sounds like he's explaining something, I wrote I still have no idea what he said, although I think it was on the fourth reading that it finally sank in, and then I explained it.  So we assume that he knows what he's trying to say, but he sure doesn't communicate it very clearly.



Here's what Gabriel wrote in Reddit, in this posting on Reddit.  He says:  "Hi.  I'm the CEO and Founder of DuckDuckGo.  To be clear, since I already see confusion in the comments" - and, boy, confusion in what he's going to write here.  He says:  "When you load our search results, you are anonymous, including ads.  Also on third-party websites we actually do block Microsoft third-party cookies in our browsers, plus more protections including fingerprinting protection.  That is, this article is not about our search engine, but about our browsers.  We have browsers" - and he says, parens, "(really all-in-one privacy apps) for iOS, Android, and now Mac in beta."



He says:  "When most browsers on the market talk about tracking protection, they're usually referring to third-party cookie protection and fingerprinting protection, and our browsers impose these same restrictions on all third-party tracking scripts, including those from Microsoft.  We also have a lot of other above-and-beyond web protections that also apply to Microsoft scripts and everyone else, for example, Global Privacy Control, first-party cookie expiration, referer header trimming, new cookie consent handling in our Mac beta, fire button one-click data clearing, and more.



"What this article is talking about specifically" - he keeps telling us, but he never really gets to it - "is another above-and-beyond protection that most browsers don't even attempt to do for web protection, stopping third-party tracking scripts from even loading on third-party websites, because this can easily cause websites to break.  But we've taken on that challenge because it makes for better privacy and faster downloads.  We wrote a blog post about it.  Because we're doing this above-and-beyond protection where we can, and offer many other unique protections - for example, Google AMP/FLEDGE/Topics protection, automatic HTTPS upgrading, tracking protection for other apps in Android, email protection to block trackers for emails sent to your regular inbox, et cetera - users get way more privacy protection with our app than they would using other browsers."



Okay.  So he's sort of deflected a little bit here.  He's like, oh, look over here at all these other things you get.  He says:  "Our goal has always been to provide the most privacy we can in one download."  So he still hasn't told us, in the third paragraph yet, what it is.  Now the fourth paragraph.



"The issue at hand is, while most of our protections like third-party cookie blocking apply to Microsoft scripts on third-party sites," he says, "again, this is off of DuckDuckGo.com, i.e., not related to search," okay, whatever that meant, he says, "we are currently contractually restricted by Microsoft from completely stopping them from loading," and he says "(the one above-and-beyond protection explained in the last paragraph)," but he didn't really explain it, "on third-party sites.  We still restrict them, though, for example, no third-party cookies allowed.  The original example was Workplace.com loading a LinkedIn.com script.  Nevertheless, we have been and are working with Microsoft as we speak to reduce or remove this limited restriction."  Which maybe he just explained?  Except not really.



He says:  "I understand this is all rather confusing" - uh-huh, still - "because it is a search syndication contract that is preventing us from doing a non-search thing.  That's because our product is a bundle of multiple privacy protections" - okay - "and this is a distribution requirement imposed on us as part of the search syndication agreement that helps us privately use some Bing results to provide you with better private search results overall."  He says:  "While a lot of what you see on our results page privately incorporates content from other sources, including our own indexes - for example, Wikipedia, local listings, sports, et cetera - we source most of our traditional links and images privately from Bing," he says, "though because of other search technology our link and image results still may look different."



He says:  "Really only two companies, Google and Microsoft, have a high-quality global web link index," he says, "(because I believe it costs upwards of a billion dollars a year to do so).  And so literally every other global search engine needs to bootstrap with one or both of them to provide a mainstream search product.  The same is true for maps, by the way.  Only the biggest companies can similarly afford to put satellites up and send ground cars to take street view pictures of every neighborhood.



"Anyway," he says, "I hope this provides some helpful context."  Uh-huh.  "Taking a step back, I know our product is not perfect and will never be.  Nothing can provide 100% protection.  And we face many constraints:  platform constraints (we can't offer all protections on every platform due to limited APIs or other restrictions); limited contractual constraints like this one; breakage constraints (blocking some things totally breaks web experiences); and of course the evolving tracking arms race that we constantly work to keep ahead of.  That's why we've always been extremely careful to never promise anonymity when browsing outside our search engine because that frankly isn't possible.  We're also working on updates to our app store descriptions to make this more clear.  Holistically, though, I believe what we offer is the best thing out there for mainstream users who want simple privacy protection without breaking things, and that is our product vision."



Okay.  So here's what I think Gabriel said.  He said, and I'm paraphrasing, this is written in his voice, so imagine that this is what he said.  We want to provide a robustly privacy-enforcing search engine service.  But to do that we first need to have a search engine service, and no one other than Google and Microsoft can do that on their own.  So if we want to provide search, we need to purchase access to a big search index.  And we chose Microsoft's.  Because we're able to sanitize and purify the link results we provide, we're able to offer tracking-free search, and we do.



Next paragraph, making this up:  But completely aside from and separate from web search, we also wanted to provide a privacy-enhancing web browser.  We wanted to do this because offering clean search results is only part of the problem.  A privacy-centric web browser could do so much more.  And one of the "so much more" things a browser can do is not only block cookies being set and read by third-parties, but also block their third-party JavaScript code from ever being run in the user's browser.  And we're able to do that, to whatever degree we can, for everyone other than when a Microsoft property is that third party.



In that case we must allow their JavaScript to run.  This is not because we want to make an exception for Microsoft.  It's because the completely unrelated agreement we have, which allows us to have access to their Bing search index, explicitly requires that we not block the execution of their scripts in our browser.  Because this has all come to light now, we're planning to amend our DuckDuckGo browser description pages to say something about this.  End of my attempt to say what Gabriel should have said.



LEO:  Sounds fair.  Do you think if they used Google they would have the similar issue?



STEVE:  I have no idea.



LEO:  You don't know.



STEVE:  And I can't imagine why, like, Microsoft's agreement said you can't ever block our third-party JavaScript.  Maybe it's just because Microsoft can set the terms that they wish because they're one of only two games in town.



LEO:  This is a duopoly, yeah.



STEVE:  Yeah.  And so it's like, take it or leave it.  And so that's why he was talking kind of like apologetically about the fact that, well, we can't provide search unless we have search.  And you can only get search from two places.  And so he's very proud of their search.  And so maybe this is just the way it is.



LEO:  It might be.  I mean, if he's right, there's really only two search engines, and this is no way at this point for anybody to catch up.



STEVE:  Oh, my god, no.  It would be interesting to know, like, what it costs Microsoft and Google to maintain...



LEO:  He says a billion a year.  I mean, that doesn't sound unreasonable.  They certainly make much more.



STEVE:  That's a lot of bandwidth.  I mean, I've got spiders crawling all over me all the time.



LEO:  That's right, yeah.  I mean, others have said we are - I think Brave has a search engine.  They say we're trying to create our own search index.  But everybody, I mean, I think now we know, and I always suspected this, that everybody's either using Bing or Google.



STEVE:  Yeah.  So I think their heart's in the right place.  I don't think they would be making an exception to allow Microsoft's domains to run third-party scripts if their search index syndication contract did not require it.  They're not happy about it either.  On the other hand, they didn't disclose it.  And that was the mistake that they made.



LEO:  Well, they couldn't, apparently.  They were enjoined from saying anything.



STEVE:  Oh, right.  You're right.  You're right.



LEO:  Yeah.



STEVE:  You're right, there was...



LEO:  Which really, I mean, this all reflects poorly on Microsoft, to be honest with you.



STEVE:  It's creepy.  It's creepy.



LEO:  Microsoft says you have to allow our trackers, and you can't tell anybody you're doing it.



STEVE:  Yeah.



LEO:  And I have to imagine, although I don't know, and I hope the security researches are now looking, that any other so-called privacy protection browser...



STEVE:  Is offering what it claims.



LEO:  Yeah.  I mean, they must all be doing this.  I imagine Google's got even more draconian terms.  So I think that's the whole point of DuckDuckGo is, well, we're doing the best anybody can do, given the situation.



STEVE:  Right.  Right.



LEO:  Nobody's going to be able to do better.



STEVE:  And just put UBlock Origin on DuckDuckGo's browser, if you can.  I don't know if you're able to run extensions on it.



LEO:  Right.  Oh, that's a good question, yeah.



STEVE:  Yeah.



LEO:  I mean, everybody said, when this came out, everybody said, oh, I'm going to use Startpage.  But I'm pretty sure it's either Google or Bing.  I think it's Google.  They say we anonymize it.  So my question is, maybe Google doesn't have such draconian terms.  Maybe Google says, oh, go ahead, you can anonymize your search queries to us.  I don't know.  I presume people are working on that.



STEVE:  Certainly I would think they will now.  And now our listeners know that DuckDuckGo is not DuckDuckGone unless this really upsets you, in which case, okay, good luck.



LEO:  Right.  But you're not going to be able to use any search engine, then.



STEVE:  No.



LEO:  No browser.  Unless, well, that's again, there are some questions.  Maybe Google is - maybe Google's, like, beneficent and says, oh, go ahead.  Use our stuff.  You don't have to...



STEVE:  And Microsoft was cheaper, and they...



LEO:  Maybe, yeah.



STEVE:  Well, and they no doubt, I mean, we know that the DuckDuckGo search engine, the search service, predates the browser by many years.



LEO:  Right.  Right.



STEVE:  So they probably signed that thinking, hey, that's not a problem.  We don't have any problem with being forced to run third-party JavaScript.  We're not a browser.  We're a service.



LEO:  It's not going to be an issue, yeah.



STEVE:  Yes.



LEO:  So really what...



STEVE:  And then it became one.



LEO:  What their mistake was releasing, and this just happened, a privacy browser, and implying that it's somehow protected.



STEVE:  Yeah.  Yeah.



LEO:  Okay.  As always, Steve opens our eyes to the situation surrounding us.  That's why you listen to this show.  You can get copies of Security Now! for your friends and for you, share it around, please do, at GRC.com.  Steve has a couple of unique versions, a 16Kb audio, the smallest audio version we have.  He also has transcripts, text written by humans, not a computer.  So it's very good.  And then he also has a 64Kb audio version.  All at GRC.com, that's Steve's website.  It's where you'll find all the freebie stuff Steve does besides this show like ShieldsUP!.  But it's also where you'll find his bread and butter, which is the world's finest mass storage maintenance and  recovery utility, SpinRite.  6.0 is current.  He's working hard on 6.1.  As long as he doesn't forget what he did last weekend.



STEVE:  I'm writing it down before I do so I know what I wrote.



LEO:  Document.



STEVE:  Wow.



LEO:  You will, if you buy 6.0 now, get 6.1 when it comes out, and you can also participate in its development:  GRC.com.  He's on Twitter, @SGgrc.  And that's where he tweets the show notes, but also where you can tweet him.  His DMs are open, so if you want to leave a message, give him a hot tip, quibble, all of that, just @SGgrc on Twitter.



STEVE:  You know what you should do?



LEO:  Uh-huh?



STEVE:  Go to GRC.com/dev/SpinRite.



LEO:  Okay.  Look at that.  What is this?



STEVE:  So there are, if you scroll down a bit, there are, if you scroll down a bit, there are the most - toward the bottom there.



LEO:  Oh my god, look at all this.  This is like your clippings.



STEVE:  Well, those are, down there, a little bit lower, those are some of the most recent of the test EXEs along the way.  



LEO:  Oh, look at that.



STEVE:  But go to, up higher, previous releases.  Click on previous releases.  This will give you some idea of these were all test releases that were developed.



LEO:  Holy cow.  Holy cow.  That is a long list.  AHCI, so all these AHCI drives.



STEVE:  Yeah.



LEO:  Holy cow.



STEVE:  And then we get into the read speed work, the init disk work.



LEO:  Oh, geez.  Look at you. 



STEVE:  I mean, this is like...



LEO:  This is heavily tested.  Good for you.  This is how you should do it.



STEVE:  Yes, well...



LEO:  And I love it that you put this online. 



STEVE:  Yup.  Well, that's the way all of our testers get it.



LEO:  Right.



STEVE:  Because it's just right there.



LEO:  Makes sense.  Nice.  



Well, there you have it.  That's one of many wonderful things about GRC.com.  It's kind of a rabbit hole you can't resist going down.  Bring food and water because you'll be there for a while.  We have 64Kb audio versions, of course, on our site.  We also have video at TWiT.tv/sn.  That's all available a couple hours after we finish the show.  You can subscribe in a podcast player.  That way you get it automatically, whenever it's ready, audio or video.  Video versions are also at YouTube.  There's a full YouTube channel devoted to this.  Steve, have a wonderful week.  If you want to watch us do it again next week, it's Wednesdays at...



STEVE:  Tuesdays.



LEO:  Tuesdays.  This is Tuesday.  Right around 1:30 to 2:00 p.m. Pacific time.  Thank you for the correction.  I'll make sure to be here Tuesday.  4:30 to 5:00 p.m. Eastern; 20:30 UTC.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.








GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#874

DATE:		June 7, 2022

TITLE:		Passkeys, Take 2

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-874.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we have a response from ServiceNSW to the news of their insecure Digital Driver's License.  ExpressVPN is the first VPN to pull the plug on India.  Turning off the Internet is becoming a common practice by repressive regimes.  The Windows Follina exploit explodes in the wild.  Another Windows/Word URL scheme can be exploited.  A critical cellular modem chip defect has surfaced.  Named ransomware is being impacted by U.S. sanctions and ransomware is taking aim at our system boot firmware.  We have a bit of errata and closing-the-loop feedback.  Then, in the wake of Apple's big WWDC 2022 keynote, which mentioned Apple's forthcoming adoption of the FIDO2 Passkeys, I want to highlight one glaring concern that everyone seems to have missed.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  As always, well, actually, you know what, Steve said it was kind of a slow week.  There's still, it seems to me, plenty to talk about.  We'll start with a look at ServiceNSW's new driver's license.  Steve talked about how insecure it was last week.  This week, their response.  You won't believe it.  ExpressVPN is the first VPN to pull out of India.  We'll tell you why.  There's a critical cellular modem chip defect.  Does anybody care?  And then Steve is going to look at Passkeys once again in light of Apple's announcement that they're going to support it.  What is the key critical deficiency of Passkeys?  Steve identifies it, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 874, recorded Tuesday, June 7th, 2022:  Passkeys, Take 2.



It's time for Security Now!, the show where we protect you, your loved ones, your privacy, your wallet online with this guy right here, Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo, great to be with you again for, what is this, this is the first, no, yeah.



LEO:  Yeah, June.



STEVE:  The first episode of June.



LEO:  Yeah, yeah.  Hard to believe.  How did we end up here?



STEVE:  First episode of June.  I know.  I know.  So I, along with you and Mikah, watched and enjoyed yesterday's Worldwide Developer Conference Keynote.  And I saw in passing their mention of Passkeys.  And something occurred to me that I hadn't thought about before.  And I initially put it in "Miscellany" because it just - it didn't seem that big a deal.  But the more I thought about it and wrote about it, the more I realized that there might be something significant that we're just all kind of glossing over while we're rejoicing about Apple, Google, and Microsoft adopting this technology.  So, and it was kind of a slow week, which doesn't happen that often in security.



LEO:  Woohoo.



STEVE:  Yeah.



LEO:  This is the only show where we celebrate slow weeks.



STEVE:  And so I was sort of thinking, well, I don't want to talk about that.  I don't want to talk about that.  Don't want to talk about that.  So we have things to talk about.  We're going to talk about the response that Service New South Wales provided about - their response to all of the news which we covered last week of their very insecure, apparently hard-to-spoof, so they said, Digital Driver's License.



ExpressVPN is the first VPN to pull the plug on India.  We're going to talk about that.



LEO:  Our sponsor, yeah.



STEVE:  Yeah.  Also turning off the Internet has turned out to become a common practice among some repressive regimes.



LEO:  Oh, yeah.



STEVE:  For an interesting reason.  The Windows Follina exploit - Follina, Italy, we talked about that.  It has exploded into the wild, not surprisingly.



LEO:  Oh.



STEVE:  Yeah.  And there's another Windows/Word URL scheme which has been uncovered, which can be exploited.  We've got a critical cellular modem chip defect which has surfaced.  And maybe I'm the only one who's not worried about it.  We'll see.  Also we've got an interesting consequence of U.S. sanctions which is that named ransomware is being impacted as a consequence.  And also in a separate story, ransomware is beginning - we have, as a consequence of these Conti leaks, we learned that some new development in the ransomware sphere is taking aim at our systems' boot firmware, the UEFI.



LEO:  Uh-oh.  Uh-oh.



STEVE:  Which is not where we want them going next.  But they're going to.  We've got a bit of errata and a bit of closing-the-loop feedback.  Then we're going to talk about - I titled today's podcast "Passkeys, Take 2" because four weeks ago it was the title of the podcast, and I need a little bit of follow-up.



LEO:  Oh, good.



STEVE:  That I think all of our listeners will find very interesting.



LEO:  Yeah, I'm looking forward to that.  One of our listeners, Redacted in the chatroom, has noted that on your show notes, at least on the Mac, the Mac interprets this number 874 for 06-07-22 as a phone number.  And I don't know what'll happen if I dial it, but I don't think I will.  I just thought that was kind of interesting.



STEVE:  That is interesting. 



LEO:  Yes.  I'd never noticed that before.  Thank you, Redacted.  I don't know what country code that is.



STEVE:  I have noticed, and I've never asked you about this, I've noticed that sometimes, on my Pad at least, Safari doesn't show the symbols that other browsers show.  They've got the little missing icon squares.  Like, why?  Is this hard?



LEO:  It's a font thing, Steve.



STEVE:  Oh, okay.



LEO:  Yeah, I think that whatever font you're using there, it doesn't have those Unicode characters.  I think honestly by now every font should have - but you know there's potentially, I think, 65,000 Unicode characters.



STEVE:  That's true.



LEO:  I mean, it's just such a large number, you don't want to put that all in a font set.  So occasionally some Wingdings get turned into squares.



STEVE:  You'd need to have the add-on ROM if you put that...



LEO:  Yeah, no kidding.  Every font's 100MB.



STEVE:  Yeah.  So this one, this is not high-brow, but it's just a fun little ditty.  So the caption on this picture reads:  "I don't get it.  We keep changing the password, and we still have a leak."  And the picture is two people standing in front of a corporate bulletin board where in 48-point type it says:  "Today's security access code:  55523."  And again, yes, yeah, wonder where that leak could possibly be occurring.  That's a mystery.



Anyway, okay.  So in a follow-up to the quite unflattering coverage the tech press, including this podcast, gave to the New South Wales ridiculously insecure Digital Driver's License, someone over at ServiceNSW was told that they needed to offer a rebuttal.  So here's the official reply from ServiceNSW.  Starts out:  "This issue" - and, like, which issue?  Like take your pick.  "This issue is known and does not pose a risk to customer information."  Okay.  "The blogger has manipulated their own Digital Driver's License information on their local device."



LEO:  Yeah.



STEVE:  Uh-huh.  Right?



LEO:  Yeah.



STEVE:  "No other customer data or data source has been compromised."



LEO:  Oh, they thought it was a breach or something.



STEVE:  Well, okay.



LEO:  Completely misunderstood it, yeah.



STEVE:  Yeah.  Maybe.  "It also does not pose any risk in regard to unauthorized access or changes to backend systems such as DRIVES."  And of course no one ever said it did.  But it's good that it doesn't.  Then they said:  "Importantly, if the tampered license was scanned by police, the real-time check used by NSW Police" - and they said "scanning mobipol" - "would show the correct personal information as it calls on DRIVES."  So DRIVES is apparently the name of their backend database system.



LEO:  Oh, you mean like a physical driver's license.



STEVE:  Yeah.



LEO:  Well, I'm glad we did this.



STEVE:  Oh.  Then they said:  "Upon scanning the license it would be clear to law enforcement that it has been tampered with."  Right, since nothing that's being displayed can be trusted.  So the offline aspect of this is apparently not very useful.



LEO:  So they're basically saying, well, just don't trust it.  Okay.



STEVE:  Oh, but Leo, here's the kicker.



LEO:  Oh.



STEVE:  "Altering the DDL is against the law."



LEO:  Oh, no.  No one would do that.



STEVE:  No one.



LEO:  Lot of law-abiding people there.



STEVE:  And so it's kind of like a fake ID, which I would imagine is also against the law.  But no one's ever used one of those.  Finally, the DDL has been, fortunately, "independently assessed by cyber specialists and is more secure than the plastic card."



LEO:  [Buzzer sound]



STEVE:  Exactly.  Like, okay, were these by chance the same cyber specialists who came up with the system's design in the first place?



LEO:  Yeah, yeah.  Our geeks tell us it's safe.



STEVE:  And really, what does "more secure than the plastic card" mean?  How can the security of a physical card be compared against a software solution?  After all, everyone always says that all software has bugs.  A physical card doesn't have software bugs.  Seems like that might be more secure.  And of course we talked last week about the difficulty of acquiring the paper and the printer, and apparently it's got some multilayer laminated sandwich with a different color so you look at the side of the card, and you see a little orange stripe or something.



Anyway, at least now we know why nothing happened three years ago, back in 2019, when the egregious and completely avoidable problems with this system were first publicly displayed.  ServiceNSW apparently employed the now-famous "These are not the droids you're looking for" diversion by claiming that "The DDL system is working exactly the way we intended, and you all just don't understand why this is what we want."



LEO:  That is such a cover-your-ass response.  Unbelievable.  Unbelievable.



STEVE:  Really.  It really is.  Yeah.  Well, you know, government here to help you.



Okay.  ExpressVPN, as you reminded our listeners, a sponsor of the TWiT network, last Thursday announced that it would be removing all of its India-based VPN servers in response to a new cybersecurity directive issued by the Indian CERT, the Indian Computer Emergency Response Team.  However, that doesn't necessarily mean, they said, that users of ExpressVPN will be out of luck.



ExpressVPN wrote that:  "Rest assured our users will still be able to connect to VPN servers that will give them Indian IP addresses and allow them to access the Internet as if they were located in India."



LEO:  Oh.  How do they - I guess the pool of its - that's interesting.  Yeah.



STEVE:  That's interesting, yeah.  As long as you are able to route those IPs...



LEO:  You could put a router there, not a server.



STEVE:  ...to somewhere else; right.



LEO:  Yeah, there you go, yeah.



STEVE:  Right.  So they said:  "These 'virtual' India servers will instead be physically located in Singapore and in the U.K."



Okay, so what happened?  CERT-India will be enforcing new controversial data retention requirements that are set to come into effect three weeks from today, on June 27th.  These new rules require VPN service providers to store subscribers' real names, contact details, and IP addresses assigned to them for at least five years.  India's CERT stated that the user data being logged will only be requested for the purposes of "cyber incident response" - which of course could be anything - "protective and preventive actions related to cyber incidents."



So India's CERT, after I'm sure a lot of pushback, they clarified that this rule does not apply to corporate and enterprise VPN solutions, and they're only aimed at those operators who provide proxy-like services to "general Internet subscribers/users."  In other words, to any and all VPN service providers.



In their statement, ExpressVPN said:  "The new data law, intended to fight cybercrime, is incompatible with the purpose of VPNs, which are designed to keep users' online activity private.  The law is also overreaching and so broad as to open up the window for potential abuse."



And in addition to the new rules, which are called Cyber Security Directions, they also require firms to report incidents of security lapses such as data breaches and ransomware attacks within six hours of noticing them.  I think in the U.S., what was it, 72?  So six hours in India.  India's move has not only sparked privacy concerns, but has been criticized as ambiguous and overly broad, with many pointing out a lack of clarity on the scope of incidents that come under purview of this upcoming directive.



In a statement, the Internet Freedom Foundation said:  "Such excessive requirements for collecting and handing over data will not just impact VPN service providers but VPN users as well, harming their individual liberty and privacy.  In the absence of sufficient oversight and a data protection framework to protect against misuse, such requirements have the potential to enable mass surveillance."  Which has got to be what's actually going on here.



And unfortunately, this feels more like the future which we're heading toward than the past we are receding from, or that is receding from us.  Governments are increasingly becoming uncomfortable with the idea of having no means to surveil their citizens and others who reside within their borders.  So the great decryption debate is far from over.



And speaking of pulling the plug, what do Algeria, Iraq, Jordan, Sudan, India, and Syria all have in common?  Their governments have reacted to out-of-control cheating on tests by high school students by completely shutting down their national Internet service during the period of time that tests are being taken.



LEO:  What?  Oh, go try that here.  Holy moly.  Oh, my god.



STEVE:  I know.  I was thinking the same thing, Leo.  Can you imagine?  It must be, although I don't think it is really, but you'd have to imagine that those countries' use of the Internet is sort of circumstantial, or not core.  As you said, you could not do that in the United States.



LEO:  No.



STEVE:  But anyway, so the most recent instance of this occurred last week - two, actually - and this week in Syria, which scheduled a series of four planned outages, each lasting 3.5 hours.  The first two occurred, as I said, last week; and the next two, actually one is set for today and one on the 12th.  The outages are performed via BGP by removing Syria's routing from the global Internet.



LEO:  Wow, that's one way to stop cheating.  Holy cow.



STEVE:  Thus cutting it off completely from the rest of the world.



LEO:  That's mind-boggling.



STEVE:  It is.  In this day and age, Leo, I mean, and that's why you think that the only way it could be possible would be that it's just, like, Syria doesn't depend on the Internet to the degree that we do.  But how can that be?  Anyway, prior to Syria implementing these exam blackouts, which began in 2016, it turns out, I mean, this actually was a problem.  Test questions would begin appearing on social media sites 30 to 60 minutes before each exam, thus allowing cheating students to circulate correct answers and compromise the integrity of the tests.  So now, as hundreds of thousands of Syrian high school students sit to take their national exams, Syria is taking the extreme proctoring measure of shutting down national Internet access.  Wow.



The pressure on Syrian students is great since their performance on these standardized tests largely determines what higher education options they will have access to, which in turn largely controls their economic futures.  Doug Madory, the Director of Internet Analysis at Kentik, said:  "The stakes for the exams are so high that there's an assumption that everyone is cheating."  So the exam blackouts operate in Syria both by blocking, well, actually if you pull out BGP, you know, hardwired and mobile Internet access, in the hours before the exams, as paper tests are printed and physically distributed across the country.  So in order to minimize leakage, they already, like, print, I mean, the ink is still wet on these things when the students get them, in order to keep them from escaping.  And they went one step further and just pulled their BGP routing from the Internet.



And as I said, this strategy is not only being used in Syria.  Iraq previously drew criticism from digital human rights groups for ordering local Internet providers to shut down during school exams in the summer of 2015.  The academic-related Internet shutdowns have been reported in India, as well.  And last year more than 25 million people faced a mobile Internet shutdown in the Indian state of Rajasthan during a local teacher eligibility exam.  So I guess even the teachers were cheating.



LEO:  Well, that's what happens when you have these high-stakes exams.  



STEVE:  Yes.



LEO:  I mean, France does this.  A lot of countries do this.  They're all at the same time on the same day countrywide, which is why they can shut down the whole country.



STEVE:  Right.



LEO:  But, I mean, I guess there's an agreement among the citizenry that this is important.  We're going to do this right.



STEVE:  We're just all going to, you know, bite the bullet.



LEO:  That's pretty wild.  Wow.



STEVE:  Wow.  So "Follina," I just love that name, is under active exploitation.  So under the heading of "Well, that didn't take long," we have last week's Microsoft mess which Kevin Beaumont named "Follina," as we know, after the area code of Follina, Italy, which appeared in one of the exploit documents.  And recall that this is the ms-msdt:// protocol vulnerability that was being abused through Office, all versions of Office.  And by using an RTF extension on the file, you were bypassing the protected viewing mode.  So it's now under widespread and quite aggressive attempts of abuse.



A most likely Chinese state-aligned threat actor has been observed attempting to exploit this Follina vulnerability, targeting target government entities in Europe and in the U.S.  The enterprise security firm Proofpoint, I think they're Israeli-based, said it blocked attempts at exploiting this remote code execution flaw being tracked as CVE-2022-30190, with a CVSS of 7.8, no fewer than 1,000 phishing messages containing a lure document which were sent to targets.  Proofpoint said:  "This campaign masqueraded as a salary increase and utilized an RTF with the exploit payload downloaded from the IP 45.76.53.253."



The attack payload is a Base64-encoded PowerShell script which functions as a downloader to retrieve a second PowerShell script from a remote server named "seller-notification.live."  The second expanded script checks for the presence of virtualization  because, you know, that's one of the ways that these scripts are analyzed is they're stuck in a VM in order not to get, you know, in order to create containment.  So now increasingly scripts and malware are checking to see whether they're being run under virtualization, in which case they don't do anything bad.  So it checks for virtualization, steals information from local browsers - and in the write-up of this there was a list of like all of the passwords that this thing attempts to exfiltrate from every browser that we know about - and also mail clients and file services.  It conducts local machine reconnaissance and then zips it all for exfiltration back to IP address 45.77.156.179.



The phishing campaign has not been linked to a previously known group, but Proofpoint said that given the specificity of the targeting and the PowerShell payload, it has wide-ranging reconnaissance capabilities.  They didn't think it was an amateur.  They believed it to be mounted by a nation-state-level actor.  The vulnerability, as we know from Microsoft, remains unpatched, with Microsoft urging customers, I guess who ask, they're not being proactive, to disable the protocol to prevent the attack vector.



And in the absence of a security update, the great guys over at 0patch, you know, numeral 0patch.com, have released one of their unofficial micropatches to block the ongoing attacks against Windows systems.  Let's see.  We're the first Tuesday of the month.  So next Tuesday the 14th hopefully we're going to see that this thing gets updated and fixed.  0patch's founder Mitja Kolsek said:  "It doesn't matter which version of Office you have installed, or if you have Office installed at all.  The vulnerability could also be exploited through other attack vectors."



Proofpoint said that the extensive reconnaissance conducted by the second PowerShell script demonstrates an actor interested in a large variety of software on a target's computer.  So this, coupled with the tight targeting of European and local U.S. governments, led them to suspect a campaign that has a state-aligned source.



And as we sign off from this follow-up, let's all remember that, as I noted last week when this nightmare began, it was the middle of April, about a month and a half ago, when this was responsibly reported by a credible security research group as being at the time, mid-April, under active exploitation.  The report was made to Microsoft's Security Response Center.  The group providing the report provided a copy of the in-the-wild, real-world Microsoft Office document which demonstrated the nature of the exploit.  But because the exploit didn't immediately reproduce itself and fall at the feet of the Microsoft MSRC guy, he just blew it off, saying it wasn't a security problem.  Well, it certainly is now.  And a different Windows URL schema can be abused.  Leo, I'm going to take a sip of water.



LEO:  Okay.



STEVE:  Let's tell our listeners how we're here listening to this.



LEO:  Okay.



STEVE:  Last week, I opened our coverage of the latest Microsoft mess by stating:  "We have a new, head-buried-in-the-sand, quite pervasive Microsoft Office zero-day remote code execution vulnerability which is now being used in attacks."  And of course I was referring to the issue we were just talking about, which has now since then exploded in the wild.  And of course the head buried in the sand I was referring to was Microsoft's decision to ignore the early warning that they were given a month beforehand.  



And last week I also observed that this msdt:// protocol scheme wasn't a bug, it was a feature; and that this would make its remediation all the more difficult because it could not simply be turned off globally since there might well be some users who were dependent upon that feature because, again, it's not a bug.  Unfortunately, it's a feature which has now been revealed to be insecure.  And it's not alone.



We're back here today because, sure enough, another similar feature of Windows has just surfaced.  This time it uses a different scheme.  This one is search-ms:// protocol.  BleepingComputer reported that a security researcher by the name of Matthew Hickey, a co-founder of Hacker House, found a way to combine a newly discovered Microsoft Office OLEObject flaw - because yes, as we've commented, Leo, OLE is still with us.  You could combine this OLEObject flaw with the search-ms: protocol handler to open a Windows search window from a Word document.



And by "search window" we mean that a Windows Explorer search results window will open, showing a list of files to run; but that list of search results, that's search result files, can be  sourced from a hacker-controlled remote server anywhere.  Whoopsie.  The result can be an extremely convincing "Your software must be upgraded to proceed" style attack.  It's convincing because the dialog runs from Windows.  I mean, it is a Windows dialog, and so it's not going to be, you know, like in the browser or stuck within some borders anywhere.  It is coming from Windows.  And although it's triggered by an untrusted Word document which the user can have received through any channel, such as spoofing mail, it also can be zero-click.



Bad guys can use this hack by sending phishing emails claiming to be security updates or patches that need to be installed.  The OLEObject flaw that Matthew Hickey found bypasses what would normally be a confirmation dialog which would pop up, which Word would show, if you were trying to use the search-ms:// protocol scheme.  Now that gets suppressed, making these very convincing-looking, and fasten your seatbelt.



And this might not trick and probably would not trick listeners of this podcast.  But we've made our computers so complex that most users have no idea what's going on.  And legitimate software does open pop-ups telling us that we need to upgrade this or that so often that it's become commonplace.  Microsoft, as I mentioned, did incorporate a warning confirmation dialog in an attempt to prevent the abuse of this confusion, but this most recent hack arranged to bypass those warnings.



And it seems to me that, again, there are a whole bunch of protocol handlers in Word that can be abused.  This creeping featuritis is insidious.  And it appears to be unavoidable as products mature.  In the case of Windows, what has now become an incredibly complex system has been created that no one fully understands.  And I mean that seriously.  I mean, the Office group are the size of a company themselves.  And they're producing stuff, having some interaction over on the Windows side because they need this or that feature added.  And you're just going to get mistakes in this process.  Security, as we know, is unforgiving; and only one mistake is all that's required for a bad guy to get in.



In this particular instance of over-engineered complexity, there are many similar protocol handlers which can be triggered by Microsoft Word documents, often without requiring any user interaction.  And once again, Microsoft apparently just doesn't get it, or at least they don't want to.  When BleepingComputer asked Microsoft how they planned to resolve this most recent foible, Microsoft replied:  "This social engineering technique requires a user to run a malicious document and interact with a list of executables from an attacker-specified network share.  We recommend users practice safe computing habits and to only open files that come from trusted sources."



Uh, duh.  The whole point is that the abuse of these protocol handlers allows for the creation of zero-click exploits against users who merely open documents or for the creation of extremely convincing spoofs which hide what's really going on.  So when Microsoft says:  "We recommend users practice safe computing habits," well, the users think they are.  I mean, they don't want to do anything bad on purpose.  But it happens anyway.  Grumble.



Okay.  What is billed as a critical UNISOC (U-N-I-S-O-C), I'm sure SOC is System on a Chip, vulnerability affecting millions of Android smartphones probably isn't anything to worry about.  If you were worried by headlines that you saw like that, I don't think there's anything to be too concerned about.  Checkpoint went to the trouble of reverse engineering the firmware of a widely used cellular modem chipset.  And they uncovered a buffer overflow which could be exploited to hang the chip.  But there's no suggestion that attacker-provided code could be injected.  And even if it could be, it's unlikely that much damage could be done since the chipset in question here is only running on a tiny subsystem of the entire device.  It's the cellular modem.  And while in theory there could be some damage done, in this case it just creates a denial of service, meaning that the service crashes.



But the headlines were followed by breathless statements such as:  "A critical security flaw has been uncovered in UNISOC's smartphone chipset that could be potentially weaponized to disrupt a smartphone's radio communications through a malformed packet."  Well, yeah.  That's true.  The company in question, UNISOC, is based in Shanghai and is the world's fourth-largest mobile processor manufacturer following MediaTek, Qualcomm, and Apple.  So, I mean, they're significant.  They currently account for around 11% of all System on a Chip components.



The problem has been patched after having received a somewhat surprisingly high CVSS of 9.4.  So that seems high to me for a denial of service on a cellular radio.  But I suppose the fact that an adversary could simply send a malformed packet to crash a handset's radio seemed like a big worry to whomever assigned the CVS.  In any event, Google actually will be pushing an update in their June 2022 release for Android, and I do congratulate Checkpoint for their reverse engineering work.



As I've noted before, it seems wrong to me that widely used proprietary products need to be reverse engineered to have their security verified by a third party, but that's the closed-source world we live in today.  And increasingly it seems clear that having the source in the open, the source for very important significant components of the ecosystem should be a requirement, really, of somebody agreeing to license it and put it in a large number of devices.



So, ransomware sanctions are causing trouble for the ransomware cretins.  Yay.  In an interesting bit of ransomware news, sanctions are turning out to have quite an impact on the ransomware business.  The problem is that even though the attackers are not law abiding, they're as we know anything but, their victims are.  So enterprises which have been hit by ransomware and are now being held at ransom are legally prohibited from making any ransom payments, through any mechanism, even if they wanted to.  



Mandiant's research paper published last Thursday was titled: "To Hades and Back:  UNC2165 Shifts to LockBit to Evade Sanctions."  And that headline requires a bit of explanation, which Mandiant then provides.  They explain:  "The U.S. Treasury Department's Office of Foreign Assets Control (OFAC) sanctioned the entity known as Evil Corp back in December of 2019, citing the group's extensive development and use and control of the Dridex malware ecosystem.  Since the sanctions were announced, Evil Corp-affiliated actors appear to have continuously changed the ransomware they use."



And I have a chart from the Mandiant report showing a block of time when they were using BitPaymer.  Then they shifted to DoppelPaymer.  And then for a while they were using WastedLocker.  Then they used Hades for a while, then Hades PhoenixLocker, and then finally Hades PayloadBin.  So again, continually making these changes over time.  They said:  "Specifically, following an October 2020 OFAC advisory, there was a cessation of WastedLocker activity and the emergence of multiple closely related ransomware variants in relatively quick succession."  In other words, the names were changing, but not much else was changing.  "These developments," they wrote, "suggested that the actors faced challenges in receiving ransom payments following their ransomware's public association with Evil Corp.



"Mandiant," they wrote, "has investigated multiple LockBit ransomware intrusions attributed to UNC2165, a financially motivated threat cluster that shares numerous overlaps with the threat group publicly reported as Evil Corp.  UNC2165 has been active since at least 2019" - and notice when that occurred, so yup, somebody new popped up, perhaps changing their name - "and almost exclusively obtains access to victim networks via the FakeUpdates infection chain, tracked by Mandiant as UNC1543.  Previously," they said, "we've observed UNC2165 deploy Hades ransomware.  Based on the overlaps between UNC2165 and Evil Corp, we assess with high confidence that these actors have shifted away from using exclusive ransomware variants to LockBit, a well-known Ransomware as a Service (RaaS), in their operations, likely to hinder attribution efforts in order to evade sanctions."



So Mandiant's paper then delves into all the details of the intelligence that they collected to track these activities and to draw these conclusions.  But I mostly wanted to make the observation, which I thought was interesting, that just as the Conti gang apparently shut down and disbanded due to the trouble they caused for themselves by siding so clearly with Russia, and then falling under the Russian sanction umbrella, thus being unable to receive ransom payments from the West, similarly, U.S. Treasury Department sanctions on many other ransomware operators prevent them, too, from receiving payment from U.S. resident victims.  So the prior practice of building up a big public reputation no longer serves the financial interests of these ransomware gangs.



The next thing I wanted to talk about is what this is in the middle of, or picks up on, which is the fact that ransomware groups have now been spotted successfully compromising motherboard firmware.  One of the consequences of Conti's boasting about how they side with Russia, as we talked about a couple times, is that somebody within the Conti membership who had good access to what was going on leaked a bunch of the internal chats that was always meant to stay private.



So the following leaked message posted exactly one year ago on June 7th, coincidentally, 2021.  It reads, and this is good English, considering it was probably a native Russian speaker, from someone named Stern.  He wrote:  "Hi.  Things are good.  I apologize for not immediately responding.  I haven't communicated through a toad" - that's what it says, maybe that's, I don't know, a typo or something.  "I haven't communicated through a toad for a long time."  And Leo, for what it's worth, I doubt that you or I have ever communicated through a toad.  I'm just saying.



LEO:  It probably means something in the hacker world.  I don't know.  Yeah.



STEVE:  So Stern said:  "I haven't seen what you wrote.  Now I'm finishing a full report on the mechanism of operation of the Intel ME (Intel Management Engine) controller and the AMT technology based on it.  Recovered a bunch of undocumented commands using reverse" - meaning reverse engineering - "interface dump, and fuzzing.  Unfortunately, the starting theory based on the presentation of Embedi/PositiveTechnologies reporters was not confirmed in the form in which they presented it."



Okay.  So that was probably referring to back at the time a report of vulnerabilities in the Intel Management Engine.  So this guy jumped on those, rubbing his hands together, thinking, oh, cool, I'm going to go pursue this.  So he's saying:  "Unfortunately, the starting theory based on the presentation of Embedi/PositiveTechnologies reporters was not confirmed in the form in which they presented it."  Meaning it wasn't what they said, but he stayed with it.



He said:  "But there is another legal mechanism to activate AMT, but so far it has not reached the working software.  At the moment I make a sniffer buffer that provides the HECI interface because it is all configured in UEFI.  Then the sniffer took a little longer.  After I fully restore the command set, the POC will be prepared.  There are ideas.  If we talk about the topic of UEFI, then this is not just a load dropper, but also perhaps some daemon of the level of SMM processors" (System Management Mode processors).



He said:  "Plus since now I have tightly studied the Management Engine controller, the idea is to test such functionality as rewriting the SPI flash drive through it.  Usually this controller is allowed to write to the flash drive, which cannot be said about the processor, and some commands were found that are responsible for this functionality."



So bottom line is, in this posting, which was leaked from Conti, and this was made, the original posting was from a year ago, this guy is saying, "I have achieved what I needed to as a consequence of this reverse engineering of the Intel Management Engine firmware on the motherboard of pretty much everything."



So the conversations among the Conti members have shed light on the syndicate's attempts to search for vulnerabilities related to the Management Engine firmware and BIOS write protection.  They reverse engineered the system to locate undocumented commands and vulnerabilities in the Management Engine interface, achieved code execution in the Management Engine to access and rewrite the SPI flash memory, and dropped System Management Mode-level implants which could be leveraged to modify the OS kernel.  That is, the operating system running, that is booted on this motherboard.



The leaked chats show that the work ultimately resulted in proof-of-concept code last summer that can obtain System Management Mode code execution by gaining control over the Management Engine after obtaining initial access to the host through traditional vectors like phishing, malware, or supply chain compromise.  Basically, we're always talking about how you get into a machine.  And then often you need to elevate your privilege.  And what you're probably doing is looking to see where you are and then moving laterally.  Now this group, some members somewhere, have developed the technology to go down rather than out and across, that is, they are now able to infiltrate the firmware of motherboards.



Security researchers who have been privy to these chat logs have observed that:  "The shift to ME firmware gives attackers a far larger pool of potential victims to attack, and a new avenue to reaching the most privileged code and execution modes available on modern systems."  And of course as we know, and as I mentioned in my notes that I deleted or somehow didn't make it into the show notes, it's enough trouble getting people to update their operating systems and their application software.  The bad guys know that there is a serious update log, or lag, rather, in getting that to happen.  So imagine how many more systems' firmware is never touched.



And to that add the fact that I'm sure many of us here listening to this podcast, because we tend to have propellers spinning on our heads, have done firmware updates.  I've encountered, and I'm sure everybody has encountered, warnings saying basically leave the firmware that you have alone unless something, like there's some hardware problem that you're coming here to update your firmware for.  That is, if it's not broke, don't fix it.  And so this notion in firmware land is still sort of pre-viral.  It's the only reason to update firmware would be to fix some hardware-level compatibility problem, not because there might be something evil crawling around in there that you want to update.



So as a consequence, I mean, I'm sure the majority of systems that are out and running are using firmware that has never been updated since the system was first installed, despite the fact that this podcast has covered multiple firmware problems, and it was one of those apparently a year ago that got a member of the Conti group, or at least somebody posting in the Conti secure chat channel, that hey, you know, I've figured this out.  We now, if we can run code in the user's OS, we can now infiltrate their UEFI BIOS and get persistence and complete invisibility...



LEO:  Terrible.



STEVE:  ...from traditional antimalware.  Yeah, Leo, it really is.



LEO:  If you have, I mean, don't most PCs now have in the firmware a signing certificate for the firmware?  I know they do for the boot, right, the secure boot; right?



STEVE:  Right.



LEO:  Is the firmware not checked?



STEVE:  There's no way for it to check itself.



LEO:  Oh, of course.  Duh.



STEVE:  Because the root is the ultimate authority.



LEO:  Right.



STEVE:  And so there's no one, there's no third party to say...



LEO:  No higher authority, yeah.



STEVE:  ...yeah, you're fine, go ahead, yeah.



LEO:  Boy, that seems like something you might want to put in BIOS or something, somewhere.



STEVE:  Well, what you really want is to rigorously write-protect this.



LEO:  Right.



STEVE:  And, you know, in the old days you had motherboards with jumpers, and some server motherboards will still have a physical write-protect jumper.



LEO:  Right.



STEVE:  That will just disable the ability for the firmware to be modified.



LEO:  Well, that protects BIOS, but it doesn't protect the UEFI because that's on the hard drive; right?



STEVE:  Well, no.



LEO:  And you write to that all the time, I think.  Yes?



STEVE:  Yeah, well, the UEFI is on the motherboard.  There is some information stored on the hard drive.  But of course you're able to put new hard drives in, and the UEFI survives.  So the UEFI is actually in, well, is in NVRAM.



LEO:  All I know is from installing Linux and stuff, if you put a new hard drive in, it wouldn't boot because you don't have an operating system on it.



STEVE:  Right.



LEO:  If you install a UEFI-aware operating system, it is going to write some stuff on the hard drive.



STEVE:  Ah, right.  But so that's like the boot sector on the partition.  But UEFI is that stuff that you get to by hitting F2 or...



LEO:  It's confusing because there's - some of it's in firmware, but some of it's in software; isn't it.  So if you... 



STEVE:  Well, what's in software is what the UEFI BIOS...



LEO:  Boots to.



STEVE:  ...boots to and executes, yes.



LEO:  But if you deleted your UEFI partition on a hard drive, you wouldn't boot.



STEVE:  Right.



LEO:  Right.  So there is firmware UEFI, but there's also software UEFI.



STEVE:  Right.



LEO:  Are they talking about something that modifies firmware?



STEVE:  Yes.  Yes.  That's where the Intel Management Engine and System Management Mode and all of that is.



LEO:  Oh, dear.



STEVE:  Yeah.  So they're talking about going down and physically modifying the firmware on the motherboard so it doesn't matter if you reformat your drive or if you install a new operating system or you do anything.



LEO:  Wow.  Wow.



STEVE:  Yeah.  And I imagine before long, since the UEFI firmware has known bugs, we've talked about them on the podcast, it's got known bugs in the motherboard firmware, and that is not being updated.  And as I said, it's hard enough to get operating systems updated, and applications.



LEO:  I get BIOS updates all the time, though; right?  I mean, don't I?  I mean...



STEVE:  Well, and that's the problem is that the fact that you get BIOS updates means that the BIOS is not protected against being modified.



LEO:  Right, it can be modified.



STEVE:  By software.



LEO:  That's right, yeah, that's right.



STEVE:  Yup.  And once something bad gets in there, because the BIOS is updating itself, it can disable the self-update so that the BIOS will no longer accept an update after it becomes malicious.  I mean, it's really bad.  



LEO:  Wow.



STEVE:  And, you know, and this is the situation we've created for ourselves.  



LEO:  Yup.



STEVE:  I had a fun piece of errata to share from @anocelot, I think that's how I pronounce his name.  He said:  "@SGgrc:  Security Now! Errata - Grover the Muppet was blue."



LEO:  Okay.



STEVE:  "Not green."



LEO:  Don't mess with that.



STEVE:  He said:  "He's not green.  You're probably thinking of Oscar the Grouch."  And he said:  "Just trying to save you from the Muppet Mafia who will not tolerate inaccuracies."  And I hope I don't disappoint you if I have no idea what color any of them are.  I know the Cookie Monster was blue because I had a Cookie Monster with big goo-goo eyes.  It was an early GRC mascot.



LEO:  Oh, how funny.



STEVE:  As for Grover and...



LEO:  No idea.



STEVE:  ...Grouch or Oscar...



LEO:  Oscar the Grouch, yeah.  



STEVE:  Yeah.



LEO:  Lives in a garbage can, yeah.



STEVE:  Oh, and several ServiceNSW Digital Driver's License owners, that is, we've got listeners, not surprisingly, who have these DDLs; right?  And they assured me they have not tampered with them, so that's good.



LEO:  Good boys, yes.  



STEVE:  They did note that a simple screen capture would fool no one because the screen incorporates a number of animated effects.  Which that's where all the time got spent; right?



LEO:  Right.



STEVE:  Doing animation instead of the crypto.  There's a flower that animates in the upper left-hand corner, and the phone's inertial positioning is used to animate a large multi-colored flower in the background wallpaper.  So you look at it, and then you turn it back and forth.  And, you know...



LEO:  Oh, can't fake that.  Ooh.



STEVE:  Ah, no, Leo.  That's high tech.



LEO:  Yeah.



STEVE:  Yeah.  So if only its developers had given as much thought to the security of the device as they gave to its flash.



LEO:  It's better than plastic.



STEVE:  That's right.  And Larry Wilson tweeted as closing the loop, he said:  "I think you've undersold the benefit of triply encrypting and going from 256 bits to 768 bits.  Because those are logarithms, and adding represents multiplying, the growth in security is immense.  Compare the difference between 64-bit keys and 128-bit keys."



And, okay.  So Larry was referring to Bryant McDiarmid's question I replied to last week about whether triply-encrypting with 256-bit keys each time would be equivalent to taking the sum or the product of those bits.  Now, I answered correctly.  But in re-reading Bryant's question, I could see what Larry meant.  Bryant asked:  "Hey Steve, quick question.  If I encrypt a file with a 256-bit encryption three times with three different passwords, what is the resulting bit strength?  Is it 256 plus 256 plus 256?  Or 256 times 256 times 256?"



I answered Bryant's question correctly in that the resulting bit strength is the sum of the individual separate encryption key lengths.  But as we know, each single additional bit of key strength that we add doubles the number of all possible keys since you have all of the original number of keys when that new bit is off, and all of the original number of keys again when that new bit is on.  So obviously, each bit you add doubles the total number.  So encrypting three times with 256 bits each time would result in 2^768 possible keys, which is a ridiculously large number which I, for the heck of it, put in the show notes just because it's kind of glorious.  I mean, I didn't even count the groupings by three, or the digits.  It is a huge number.



LEO:  There is an Emacs command that will turn that into, you know, whatever it is, 180 gazillion.  I will do that.



STEVE:  Oh, like it'll speak it for you.



LEO:  It doesn't speak it.  It actually spells it out in English.



STEVE:  Oh, my lord.  And has man ever run out of names for those groups of three?



LEO:  Oh, god, yeah; right?  I mean, isn't that why googol, there's a googol?  Because that's a one with a hundred zeroes.



STEVE:  Yeah, this has more than that.



LEO:  Yeah.



STEVE:  So, I mean, at some point someone must have given up naming these decades, these like divisions.



LEO:  Well, it's based on a kind of Latin base, so you can probably deduce it.



STEVE:  Oh.  Great.  Please, nobody tweet me the answer.  Actually, it wouldn't fit in a tweet.  So we're safe.  And Leo, let's take our last break, and we're going to talk about Passkeys.



LEO:  I am going to use the Calculator Soup...



STEVE:  Uh-oh.



LEO:  ...word to number converter.



STEVE:  Good.



LEO:  Since I don't have Emacs on this machine.  Convert this number.  Let's see if it can do it.  It broke it.



STEVE:  Yeah.  Oh, I mean, it's big.



LEO:  It never came up with an answer.  That's ridiculous.



STEVE:  15525180930...



LEO:  No, no, no, no, no.  No need to - no need.  That's funny.  This is too big.  All right.  Oh, it has to be less than 200 characters.  So, yeah, too big.



STEVE:  [Buzzer sounds]



LEO:  I bet Emacs will do it.  It's a bignum, but we'll handle it.  I will check into it.  All right.  Now, Steve has been thinking about Passkeys.  So is this going to eliminate passwords forever?



STEVE:  I originally had these thoughts filed under "Miscellany" because I didn't want to make a big deal about it.  But the more I thought about them, the more they grew.  And as I said, no other news of the week rose to any greater significance.  So I decided to sort of more formally take a second look at Passkeys in the wake of yesterday's Apple WWDC presentation.  So a bit of an @SGgrc tweet storm arose from our listeners yesterday following the keynote, where Apple...



LEO:  Yes, because Apple announced it, yeah.



STEVE:  Yeah.  Where Apple made a point of highlighting their forthcoming adoption of essentially the revised and much more practical FIDO2 public key authentication system under the unofficial designation, which has taken hold of Passkeys.  And as we know, that was the title of our podcast on May 10th, four weeks ago.  And so I wanted to thank and acknowledge all those who took the time to tweet.  What I think set most people off was that this was the first time outside of SQRL that we've seen the very SQRL-like use of a QR code to allow logging onto someone else's machine using an identity stored in the user's smartphone.  We haven't seen that from Google yet; but if Apple does it, then Android will have to follow.  So I imagine it's just a matter of time.



One thing I did not highlight when I first talked about this four weeks ago was the uncomfortable unanswered questions surrounding manufacturer lock-in.  Apple seems quite uninterested in allowing me to send and receive iMessages from my Windows desktop.  Being an avid iPhone and iPad user, this imposes a constant and very real inconvenience for me.  If I was using a Mac?  No problem.  From a Mac I have access to my iMessages.  But not from Windows.  And I've sort of come up with a workaround using iCloud for Windows in order to send things over into the Apple world and then stick them in a posting or an iMessage.  But it's more work than it should be.



So I worry that Apple's use of this Passkeys technology will be similarly and characteristically an Apple-only solution, synchronizing only among Apple's authentication devices and not to Windows and Android.  And that's a huge practical problem for Passkeys adoption which SQRL never had.  So I want to make sure that everyone understands what the difference is because there is a takeaway from this, and why we're almost certainly heading for trouble which, among all the celebration, no one seems to have talked about this yet.



Okay, so why is that important?  Both systems, FIDO and SQRL, share the common property that the authenticating client  a smartphone, a fob, or a PC  creates a public key pair for a website.  The process of registering the user's identity to that website just involves providing the remote website with only the public key of the pair.  And that's the essence of both approaches.  That's FIDO and SQRL.



Subsequently, verifying any user's identity amounts to verifying that the user is holding the matching private key.  So to perform that verification, the website sends a random unique nonce, it's a challenge, to the user's authenticator, which signs it using its private key, which it never lets go of.  The signed blob is returned to the site, which verifies the signature using the public key that it originally received from the client during its registration.  And that's it.



LEO:  This is not so very different from public key crypto in general, how you would verify a signed email, for instance, when I send you an email.



STEVE: Yes, that's all it is.  I mean, that's exactly what it is.  Everybody has your public key, and you sign your email using your private key, and they're able to verify the signature on the email which proves that it came from you, or at least someone holding the private key.



LEO:  And because Apple has widespread biometric identification, they have a really good way of verifying you're you.  I don't know if you caught this.  I thought this was very SQRL-like.  They said, well, what if you log into a site at a library, a computer you don't own?  And did you catch this?  They showed a QR code.



STEVE:  Well, that is SQRL-like.



LEO:  Yeah.



STEVE:  In fact, that's what SQRL allows, and it was part of my original demos of SQRL.  And that's why so many people sent tweets is they said, hey, Apple's using QR codes.



LEO:  Right.



STEVE:  With Passkeys.  And it's like, well, good, because that's a use case which is important.  I mean, I'm not - don't get me wrong.  I'm not imagining that SQRL is going to get adopted, at least not yet.  We've clearly - we're going FIDO2.  But there are a couple differences which are significant.



Okay.  So we have the private key.  Websites have the public key.  They send us something that they've never sent before, which we sign.  They verify the signature.  Thus they know we have the private key, exactly as you said, Leo, like email.  Where the two systems, FIDO and SQRL, that is to say Passkeys and SQRL, crucially and importantly differ is how those original key pairs are created.  The FIDO2 Passkeys system creates key pairs randomly, whereas SQRL calculates them deterministically.



Okay, so remember that SQRL's primary design feature, the core concept behind SQRL from the start, was the idea of using a single grand master key and hashing each website's logon domain to automatically create per-domain public key pairs.  That simple innovation eliminates all need for dynamic synchronization of randomly generated Passkeys among devices because each separated device will derive the same per-site private key from one grand master key that they share.  So you load that one master key into each of your various authenticating devices under SQRL, and that master key generates all of the per-domain subsidiary key pairs forever.



Okay, but SQRL is not the system we're going to get.  And that almost might be deliberate since the system it appears we're going to get really will create lock-in.  I've read the glowing celebratory announcements about how Apple, Google, and Microsoft are going to be implementing Passkeys.  But the tech press really needs to start asking, what about cross-platform Passkey sharing and interoperability?  You know, awkward though a username and password are, the one thing they have going for them is that they are platform agnostic.



Okay.  In the near future, when you use an iPhone, an iPad, or a Mac to register your identity as a Passkey on a website, that iPhone, iPad, or Mac creates a random public key pair, and the private key is held close and never released.  Which of course is important because releasing it would defeat the system's security.  Apple will back it up securely to iCloud.  And I'm sure they will freely synchronize all of a user's Passkeys among the devices Apple controls and that are the user's.  But is Apple going to dynamically synchronize those private keys among Android and Windows authentication devices?  When pigs fly.



If they keep their keys to themselves, and if Google and Microsoft each keep the private keys that they create to themselves, then we have a fragmentation disaster on our hands.  You register a Passkey on a Windows device, but none of your Apple devices will know that secret private key.  So you can only log on with the device family which originally registered at that website.  Well, that's a mess.  In today's highly heterogeneous computing environment, the single most compelling benefit of password managers is that they are cross-platform.  Would anyone be comfortable using a password manager that was not?



Now, I suppose if you are 100% all-in on Apple, which of course is what they want, then an Apple-only solution would be okay.  But it's not clear how you could ever change your mind.  I'm all-in on iPhone and iPad, but the things I need to do can only be accomplished with Windows, and I dare say DOS.  So as nice as Apple's Passkey system may be, and as much as I'm a devoted iPhone and iPad user, I can't use Apple's Passkey system until I'm sure there will be a means for also using its Passkey registrations which I created on my iPhone or an iPad, under Windows because we should be able to.



LEO:  Would it be sufficient, yeah, would it be sufficient to say here's your private key?  In other words, if I could take my private key, it's on my iPhone obviously, and just send it to my Android phone, and register it with Passkeys on my Android phone, isn't that all I need is that private key?



STEVE:  Yes.  And, for example, SQRL does that because that's the solution.



LEO:  You have to be portable.  So it wouldn't be a hard thing for Apple to make it visible somehow; right?



STEVE:  All they have to do is put it up on a QR code and allow...



LEO:  What they're going to say is, oh, no, because people don't understand the difference between public and private keys.



STEVE:  Exactly.



LEO:  In fact, we know that because that's how NFTs keep getting stolen.



STEVE:  Exactly. 



LEO:  And so they're going to give out their private key inadvertently.  And then of course the jig is up.



STEVE:  Apple is going to hold this close, they are going to synchronize their devices, and they are never going to allow those keys to be exported.



LEO:  Do you have but one private key?  You wouldn't have - you'd have a private key per person; right?



STEVE:  Per site.  No, that's what - that's the difference.



LEO:  Oh.  So it is a large amount of data then.



STEVE:  Yes.



LEO:  You don't - oh.  Why don't they just - is that what SQRL does?  I have my key that ultimately...



STEVE:  SQRL was one key.



LEO:  Yeah.  That's what we should do.



STEVE:  I know.



LEO:  In the long run it could be tattooed on your inner thigh or something.  But everybody should have that one private key.  That's what I do with PGP.



STEVE:  Wouldn't want it tattooed.  That is the SQRL gift...



LEO:  Yeah, one key.



STEVE:  ...was one key, and it allowed you to have anonymous, secure, unhackable, anonymous logon everywhere.



LEO:  Because I'm me.  Only I have access to that.



STEVE:  That's right.  That's not what FIDO did.  FIDO generates them at random.



LEO:  Oh, that's a mistake.



STEVE:  I know.  It's bad.



LEO:  That's a mistake.  Because now you have a - this is just another password system.



STEVE:  That's all it is.



LEO:  Because now you have a password, which is just...



STEVE:  It is different.  The thing that's different is that the dynamic authentication, that the website sends you a challenge which you sign and return.  So what that means is all they have is your public key.



LEO:  Right.



STEVE:  It doesn't matter if it escapes from them.  That's the only thing we've achieved with FIDO is that websites can now - remember how many times I said SQRL gives websites no secrets to keep.



LEO:  Right, right. 



STEVE:  That's this.



LEO:  It eliminates breach problems.  But this still is a terrible...



STEVE:  It is, it's awful.



LEO:  Why would they solve it that way?  It seems so obvious that everybody should have - and then you solve it by if I have multiple accounts, for instance, for tax preparation software, I have multiple accounts because I do my mother's thing; right?  My mother's taxes.  So I could generate a private key for her that I would keep and a private key for me that I would keep.



STEVE:  Yup.



LEO:  And I would have then multiple logins to a single site.



STEVE:  Yup.



LEO:  But I still...



STEVE:  You're describing SQRL.



LEO:  Yes.



STEVE:  You're describing SQRL.



LEO:  One key per person.  I don't understand why that's not obvious.



STEVE:  That's the right solution, and that's not what we got.



LEO:  I didn't realize that - I should have listened more carefully when you were describing this.  I didn't realize Passkeys generated a unique private key for every site and app.  That's...



STEVE:  Well, and if you think about it, they have to.  Otherwise they'd be giving every site the same public key.



LEO:  Right.



STEVE:  And that can't happen because then you would have no - you would have no security.



LEO:  Something unique, yeah.  So how do you solve that with FIDO?



STEVE:  Well, the only way to do it is cloud synch.  You need cloud synchronization.  And so my takeaway from this whole thing is, until Apple, Google, and Microsoft figure out like...



LEO:  Interoperability.



STEVE:  ...interoperability, do not do this.  Wait for a password manager to support this.  Because what we need is the same cross-platform capability that we have now with passwords.  We need that with...



LEO:  Yes.  Portability.  You need portability.



STEVE:  We need portability, yes.



LEO:  And by the way, don't blame Apple.  This is the FIDO2 spec.



STEVE:  Right, right.  I'm not blaming Apple.



LEO:  No, no, no, some people in the chatroom are.  This is not Apple saying - the one thing you could blame Apple is if they decided not to make it portable.  Because they could make it portable; right?



STEVE:  Well, we could also blame Apple for not adopting SQRL.



LEO:  Well, obviously.



STEVE:  Which solves this problem.



LEO:  They should have done that.  Wow.  Wow.  So I wonder, though, if you're going to use the Passkeys name, which presumably FIDO trademarked...



STEVE:  No, it's not official.  It's not an actual name.



LEO:  It's not official?  Oh, it's just...



STEVE:  No.  There's nothing called Passkeys in FIDO.



LEO:  Okay.



STEVE:  The marketing people said, oh, instead of passwords it's Passkeys.



LEO:  Passkeys.  But there's no requirement.



STEVE:  There's no proprietary ownership of that term.



LEO:  There's nothing you could say that, oh, if you want - see, they could have trademarked it and said, well, if you want to use the word Passkeys trademark, you've got to have portability.  They didn't do it.



STEVE:  Well, remember FIDO began with a dongle; right?



LEO:  Yeah, yeah.



STEVE:  I mean, it was never meant to be like mass, the way it has become.  And it's because they backed away from that and said, okay, well, we're not happy about it, but we'll let you use your phone.  We don't think it's as good as a token; but we tried to do tokens, and no one bought them.



LEO:  Nobody's going to buy tokens.  We all have phones with strong, I mean, here's the good news, we all have phones with strong biometrics.



STEVE:  Yes, yes, yes.



LEO:  And Secure Enclave.  So they're pretty good.  In fact, I would argue, if you just have a YubiKey without biometrics, they make them with biometrics, but if you have one without biometrics, this isn't tied to me.  If I lost it, anybody could use it.



STEVE:  No.  If the valet got a hold of it by mistake when he was parking your car.



LEO:  Exactly.



STEVE:  Yup.



LEO:  Whereas my phone, you know, you could get it, but you'd have to unlock it.  And you can't.



STEVE:  Yup.  And so the beauty is this leverages all of the work that Apple has done on security and biometrics and privacy.  Unfortunately, they adopted FIDO, which is a separate key pair per site.  And so you have to synchronize it.  You've got to use the cloud.  And you don't end up with one key per person, which is what SQRL is.



LEO:  Yeah.  How does SQRL solve this issue of unique public keys?



STEVE:  So I actually have in the notes, I said:  "The entire SQRL idea hit me after I had visited one of Dan Bernstein's cryptography site pages."  And I have the link:  cr.yp.to/ecdh.html.  That's elliptic curve Diffie-Hellman.  So on that page Dan mentioned that to create - if you scroll down a little bit, you'll see some C code, four lines, or three lines of C code, right there, those three lines.



So he explains, he mentions that to create a private key using his Curve25519, you took any random 256 bits of entropy, turned two specific bits off and one bit on, and you had a valid private key.  And from that you could derive its matching public key.  What I realized that morning at IHOP when I was working on SpinRite 6.1, it just hit me like a flash of lightning.  I realized that, rather than starting with a completely random 256 bits of entropy, we could instead start with a 256-bit hash of a domain name.



LEO:  Oh, smart.



STEVE:  And turn that into a private key.



LEO:  Smart.



STEVE:  And that was it.



LEO:  Yeah, that's brilliant.  So each domain has a unique private key.



STEVE:  It automatically gets calculated.



LEO:  Now, I save that on my end; yes?



STEVE:  Yes.



LEO:  So we could do the handshake.



STEVE:  Yes.  Well, so we hash the domain name.  But it's a keyed hash; right?  Remember the HMAC is a cryptographic - so it's a keyed hash.  So that key is your one thing.  Your identity as Leo is the key for the hash.  That means that when you hash Amazon.com, and I hash Amazon.com, with our different keys, we get different private keys.  So your identity keys the hash of the domain name to create a private key which - and you don't have to store it because it creates it every time you need it.  You say, what domain do you want to log onto?  And it says, oh, in that case, this is your private key.



LEO:  It's very simple and fast, yeah.



STEVE:  It's so, yes, I mean, it's just the right answer.



LEO:  Right.  Ah, sigh.



STEVE:  So I guess, if I was guilty of anything, it was of not evangelizing this thing and spending a lot of time pounding on people and being like Stina Ehrensvard.



LEO:  It would have been a pretty uphill battle.



STEVE:  That was my worry.  FIDO already was rolling and had steam.  And I was, I mean, I spent seven years on this.  And I dropped SpinRite 6.1.  And I'm so happy I'm back to SpinRite 6.1 now.  But anyway, this is the problem with FIDO2 and with Passkeys.  And so my advice would be wait until a third-party independent solution exists.  If you start creating Passkeys with Apple, unless, I mean, maybe they're going to show the Passkey on a QR code.  I don't think so.  There's just no way.  See, they like this lock-in.



LEO:  Yeah.  Of course.  Yeah.  They don't want to change a thing.



STEVE:  No.  And so it means that when you create, when you register with an account on your iPad, Apple will synchronize it in the same way they do our messages right now.  And then your phone will know the Passkey that the iPad generated on the fly and add it to your collection of Passkeys.



LEO:  Wow.



STEVE:  But you end up with hundreds of them.



LEO:  Okay, Steve.  You're right.  The world is wrong, dammit.  But this is the world people like you have to live in, I'm sorry to say.



STEVE:  Oh, I love this world.



LEO:  I'm just going to quickly see if I can convert that long number.



STEVE:  Oh, my goodness.  Oh, my goodness.



LEO:  Number to - install an Emacs package here.  Oh, can't find it.  Oh, well.  Next time.



STEVE:  Next time.



LEO:  Next time.



STEVE:  Perfect.



LEO:  I bet Emacs would do it happily.



STEVE:  Wow.  I want to hear that.  I want to see this.



LEO:  Lisp has an integer that's called bignum that is not tied to the register size.  It can just go and go and go and go.



STEVE:  Wow.



LEO:  It's one thing Lisp does very well.  Mr. Gibson, you are the man, and they should have just damn well listened to you about SQRL, gosh darn it.  But that's the world we live in.



STEVE:  Yup.



LEO:  If you wish to hear this show at a 16Kb version...



STEVE:  Sounding like a SQRL.



LEO:  Sounds like Thomas Edison on a cylinder, or you wish to read along, those are quite nice.  Elaine Farris writes those out as we speak.  Or the 64Kb audio.  Steve's got all of that at his website, GRC.com.  Now, while you're at GRC.com, first of all, set aside a few hours because it's a rabbit hole you're going to want to go down.  There's all sorts of good stuff, free stuff, things like ShieldsUP!.  But there's also his bread and butter, which is called SpinRite, the world's best mass storage maintenance and recovery utility.  If you have an SSD or a hard drive, you need SpinRite.  6.0's the current version.  Imminently, Steve will be 6.1.  If you buy 6.0 now, of course you'll get a free upgrade to 6.1.  You'll also be able to participate in the ongoing development, which I say that, it's pretty much done, I think; right?  I mean, just a few little I's to dot.



STEVE:  All the hard stuff is behind us, and now it's a matter of sort of regluing the front end to the new back end.  And that's what I'm in the process of doing. 



LEO:  Do not use wallpaper paste.  That's all I'm saying.  You should go to GRC.com, get SpinRite, support Steve and, by the way, get a great tool that will be very useful to you down the road.  We have a show at our website, shows, 64Kb audio and video actually at TWiT.tv/sn.  There's also a YouTube channel.  So if you want to share it, for instance, if you heard something, and you said, you know, somebody's got to hear this, you can go to that YouTube channel and just get that little snippet.



There's also, of course, it's a podcast so you can just go to any podcast player and subscribe.  Search for Security Now!, you should be able to find it.  And that way you'll get it automatically every Tuesday, the minute we're done here.  We record the show 1:30 Pacific, that's 4:30 Eastern, 20:30 UTC.  You can watch us do it, live.twit.tv.  There's audio and video streams there.  People who watch live often like to chat live.  We have a wonderful chatroom where they're all talking about what you're talking about.



STEVE:  And just to follow up, the one reason you can't use one key for all your sites is then you lose anonymity.



LEO:  Oh, yes.  Good point.



STEVE:  It would be trackable totally.



LEO:  Excellent point.



STEVE:  From one website to the next.



LEO:  Yes.



STEVE:  That's why they create them randomly.



LEO:  Yeah.  That makes perfect sense.  But you solved it.  You do have, though, the burden of keeping a vault with all of those numbers in it.  And that's why you have cloud.  But those things can be leaked without harm, right, because no one else has the private key.



STEVE:  Correct.



LEO:  Yeah.  Well, that's it, Steve.  Are you watching any good TV?



STEVE:  You know, we're currently watching "The Good Doctor."



LEO:  Yeah.  That goes a long time because it was the original one, and then they made "The Good Doctor," so you've got many, many episodes, yeah.



STEVE:  And it's by the creator of "House," and of course "House" was a classic series.



LEO:  Good, good.



STEVE:  So, yeah, we're watching that, and we've got a backlog of things.  Everyone is saying that "The Lincoln Lawyer" on Netflix is really good.



LEO:  Ah.  I loved the movie.



STEVE:  I did, too.



LEO:  Yeah.  I'll have to look at that, okay.



STEVE:  And so apparently the series is worth doing.



LEO:  Excellent.  Excellent.  I always ask Steve.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#875

DATE:		June 14, 2022

TITLE:		The PACMAN Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-875.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week will, I expect, be the last time we talk about Passkeys for a while.  But our listeners are still buzzing about it, and some widespread confusion about what Apple presented during their WWDC developers session needs a bit of clarification.  While doing that, I realized and will share how to best characterize FIDO (which we're going to get) with respect to SQRL (which we're not).  I also want to turn our listeners onto a free streaming penetration testing security course which begins Wednesday after next.  Then we have a ton of listener feedback which I've wrapped in additional news.  One listener's question in particular was so intriguing that I'm going to repeat it, but not answer it yet, so that all of our listeners can have a week to contemplate its correct answer.  And although I wasn't looking for it, I also stumbled upon a surprising demonstration proof that we are, indeed, living in a simulation.  When I share it, I think you'll be as convinced as I am.  And finally, as suggested by this podcast's title, we're going to take a very deep dive into the past week's headline-capturing news that Apple's famous M1 ARM chips all contain a critical bug that cannot be fixed.  Just how bad is it?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here. Some final thoughts about Passkeys and the potential problem with Passkeys.  Also a free streaming pen test security course which begins next week.  A lot of listener feedback.  And then, finally, what is that flaw, the so-called unfixable flaw with Apple's M1?  Steve explains the attack and why you probably don't need to worry about it.  It's all coming up next on  Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 875, recorded Tuesday, June 14th, 2022:  The PACMAN Attack.



It's time for Security Now!, the show where we cover the latest security with this guy right here, the man in charge.  Get ready, folks, this show is going to blow your mind.  Steve Gibson, you promised me that; right?



STEVE GIBSON:  Oh, we've got a good one today, Leo, for 875.  I think that's sort of a good number to have an amazing podcast.  Okay.  So this week will, I expect, be the last time we talk about Passkeys for a while.  But not quite yet.  Our listeners are still buzzing about it, and some widespread confusion about what Apple presented during their WWDC developers session does need a bit of clarification.  But while I was writing that up, I realized, and I'll share, how to best characterize what FIDO is, which we're going to get, with respect to SQRL, which we're not.  But more importantly, what the WebAuthn issue is.  I mean, that's really the big transformative deal that having FIDO on the front end is going to drive, and it's where all the inertia is going to be.



Anyway, I also want to turn our listeners on to a free streaming penetration testing security course which begins Wednesday after next.  Then we have a ton of listener feedback which I've wrapped in additional news.  Oh, and one listener's question in particular was so intriguing that I'm going to repeat it, but not answer it yet so that all of our listeners can have a week to contemplate its correct answer.  



LEO:  I love those.  All right.  My pencil's poised over my note pad.  All right.



STEVE:  It just needed it.  I mean, it's like a, hmm.  And although I wasn't looking for it, I also stumbled upon a surprising demonstration proof that we are indeed living in a simulation.  And it's broken.  That's the point of the proof.



LEO:  Wait a minute.  The simulation's broken?



STEVE:  Well, reality is.



LEO:  Oh, okay.



STEVE:  And reality can't be broken unless there's a bug in the simulation.  Anyway, when I share it, I think you'll be as convinced as I am.  So kind of maybe we're going to rock everyone's world today.



LEO:  Let's do it.



STEVE:  Finally, as suggested by this podcast's title - oh, which I forgot to mention is "The PACMAN Attack" - we're going to take a very deep dive into the past week's headline-capturing news that Apple's famous M1 ARM chips, and presumably the M2s coming soon, all contain a critical bug that cannot be fixed.  We'll find out just how bad it is.



LEO:  Yikes.



STEVE:  And Leo, I've got the weirdest Picture of the Week.  You can't argue with it.  You know?  But it's - who printed this on a cable?



LEO:  That's a good question, and I'm seeing it, so I know what you're saying.  Well, we'll find out in a moment.



STEVE:  Okay.  This is not my fault.  I didn't do this.



LEO:  Not your fault.



STEVE:  Okay.  So what we've got here is an Ethernet cable.  You know, CAT 5 or CAT 6.  And it says, it's printed, very authentic-looking, I mean, I believe that this cable actually has this printed on it.  It says "Cut here to activate firewall."  And then it's got an arrow pointing to a little line, and you can see an arrow pointing from the other direction where it probably says the same thing, "Cut here to activate firewall," telling you that, yeah, I mean, it just must be somebody who is making cable with a great sense of humor, who said, okay, we'll just put something funny on it.



LEO:  He's not wrong.  He's not wrong.



STEVE:  No.  And the person who sent this to me used a big red rectangle to highlight that phrase.  Unfortunately, it overlays the top-level domain.  I can see www...



LEO:  We want to know where you get this, don't we.



STEVE:  Yes, yes.  I was like, okay, where, what?  So I can't see what the top-level domain is.  It's www.bit dot something.  Looks like maybe an "n" something.  I don't know.  I mean, I think the shortest TLD is two characters, so I think we're missing...



LEO:  Maybe an "l"?  Bit.nl, that's Netherlands; right?



STEVE:  That feels like, yes.



LEO:  This feels like a Dutch joke.



STEVE:  It does.



LEO:  Cut here to activate firewall.



STEVE:  Yeah, yeah.



LEO:  I love it because they give you a line and arrows pointing at the line so just in case you're concerned about where you're cutting, you know exactly.



STEVE:  Yeah, well, because you wouldn't want to cut like in the middle of one of those words.



LEO:  No, no.  Cut the right spot.



STEVE:  Because, you know, who knows what would happen?



LEO:  It's got to be made up.  Don't you think it's made up?



STEVE:  You mean the whole picture was, like, faked?



LEO:  Yes.



STEVE:  It's pretty good, Leo.



LEO:  I'm going to go to bit.nl, and I'll find out for you.  If I can order them, I'm getting you a whole roll.  We'll call it the Security Now! - oh, yeah.  You know what?  It's a datacenter, bit.nl.  It's in Dutch, but a datacenter's network managed hosting in the cloud.  So I think they did this as a joke.  I love it.  I love it.



STEVE:  Very cool.



LEO:  Yeah.



STEVE:  Okay.  So I heard from many different listeners that during the WWDC developer presentation on Passkeys, Apple talked about synchronizing keys.  So I listened carefully to the entire presentation.  For anyone who's interested, the 34-minute presentation video is this week's GRC shortcut of the week.  So that means it's https://grc.sc/875.  As I said, it's about a 34-minute presentation, 33 or 34.  And I believe that what these people thought they heard was Apple addressing the need for - or thought that what they heard being Apple addressing the need for the type of synchronization we talked about last week, syncing apps across non-Apple ecosystems like Android and Windows, is not what happened.  I found no mention of anything like that anywhere in the presentation, nor is it anywhere in the developer docs, which I've also linked to in the show notes, for anybody who wants to jump right to it.



The types of Passkey sharing Apple supports is first and foremost using the iCloud Keychain, of course, to dynamically synchronize keys, these Passkeys, across an individual user's Apple ecosystem.  So as we know, all Apple devices thus will remain synchronized.  The other form of sharing Apple described uses AirDrop to pass logon credentials to another user.  And AirDrop can also be used to permanently pass a Passkey to someone else for some site or service, permanently adding it to their Keychain to use from then on.  So that's sort of like explicit, here's my Passkey, you can now use it to log on as me.  But so far, from everything I've seen, Apple has in no way suggested that they will ever be synchronizing Passkeys with external non-Apple platforms.  Nothing has been said so far either way.  They haven't said they're not going to, but nobody seems to have asked that question, and it was not part of the developer presentation.



But Apple's example solution of using Airdrop to send a Passkey to another person's iDevice - like a friend of yours, a spouse, a child, a sibling, whatever - for their subsequent use, highlighted something that I think is important to understand.  And this is where, in thinking about it, I realized what really makes SQRL different from FIDO and why.  SQRL is a complete solution for secure remote logon; whereas FIDO and technically FIDO2 with its Passkeys, is a replacement for usernames and passwords.  The two are not the same.



For example, take the case of giving someone else access to a site.  If you give them your Passkey, which is Apple's solution, demonstrated during the developer presentation, then they are now "you" on that site in every meaningful way.  When they authenticate, it's you authenticating because they're using your Passkey.  It's the exact equivalent of you giving them your username and password.  And since they are you, they can see your settings, your private details, everything that you can see and do when you log in using that same Passkey.  And since they are you, they're presumably also able to change the Passkey to lock you out.  And they can presumably pass it along to others, unless Apple has realized that secondary Passkey sharing is a really bad idea and should be blocked, which would technically be possible.  I don't know either way.



LEO:  Well, this is the situation we're in right now.  When Lisa needs the login to our Comcast account, I just send her my password and login.



STEVE:  Right.



LEO:  She doesn't have a separate one because it's the same account.



STEVE:  Right.



LEO:  So that's the standard.  Does SQRL solve that with some sort of shared access?  No.



STEVE:  Uh-huh.  Of course.  Of course.  



LEO:  How could it?  Because Comcast only has one login for my account.



STEVE:  Okay.  So I don't know either way whether they're going to block secondary Passkey sharing.  In any event, when you volunteer to give your Passkey to someone else, your site access has now escaped your control.  And I agree with you, Leo.  It's exactly the same thing.



We solved this with SQRL.  If you want to allow someone to share some access to an account as a guest  for example, sharing a Netflix account  you obtain a one-time invitation token from the site and provide it to them.  When they attempt to log into the site with their own SQRL ID, the site doesn't know them, so it prompts them to create a new account or to use an outstanding invitation if they have one.  Their use of the invitation identifies them to the site as a guest of yours, enabling them to subsequently log into your account using their SQRL ID.  Since they're using their SQRL ID - and we just sold a copy of SpinRite 6.  Thank you, if you're a listener.



LEO:  Woohoo.



STEVE:  Since they're using their SQRL ID, and guests are unable to request invitations for others, you, as the account owner, retain control.  And you're able to rescind their guest access status at any time, which isn't possible otherwise, I mean in traditional username and password sharing.  And this all scales seamlessly to enterprise use when hundreds of users might need to share access to common resources.  It's called "Managed Shared Access," and it's part of the SQRL solution.  It's already there.  We have an online demo with the entire solution working, and its operation is fully worked out and specified.  And needless to say, there's a lot more to SQRL.



So as it stands, the FIDO2 Passkeys system is without question more secure than usernames and passwords.  No doubt about it.  It's definitely superior.  But the FIDO designers were crypto people working to solve one small part of the much larger problem of practical real-world user authentication.  They didn't think the whole problem through because that was never their charter.  They could credibly say it wasn't their job.  It wasn't.  But even in a FIDO2 Passkeys world, that job still needs to be done.  SQRL does it; but unfortunately, FIDO and Passkeys does not do it.



Unfortunately, this means that instead of being as revolutionary as it could have been, we get another half-baked solution.  It's way better than what came before, but it missed the opportunity, which comes along so rarely, to address the practical needs of, and really solve, the network authentication problem.  Rather than the true breakthrough that SQRL's adoption would have meant, we're going to get incremental progress.  It's definitely progress.  But because it wasn't really thought through as an entire solution, FIDO is basically a crypto hack.  It also brings a whole new set of problems.  If FIDO is to be our solution, we really do need some form of centralized Passkey storage and synchronization, not only within a vendor, but also across vendors.



Last week, someone calling themselves Captain Jack Xno mentioned @SGgrc in a tweet to someone else, so it appeared in my Twitter feed.  Captain Jack wrote to this other person:  "You may be excited about Passkeys, but SQRL was carefully developed over seven years by @SGgrc and solves problems you may not even realize you'd have."  And he mentioned potentially cross-platform portability.  And yeah, as we know, SQRL does that.  And it does so much more.



Someone tweeting as @drnathanpgibson, I mean, that's his Twitter handle, said:  "Hi Steve.  Loved your detailed breakdown of Passkey.  You mentioned waiting for password managers to start providing sync services for these FIDO2 private keys.  I see that LastPass seems to be promising something 'later this year.'"  And then he has a link to the blog.  He says:  "Do you know anything more about when this syncing might be coming to a password manager near me?"



And so I saw LastPass's blog post last Monday the 6th.  It was a bit confusing.  And, I mean, I spent some time trying to figure out what they were saying because they were at the same time also promoting the new use of what they called "No more passwords today."  And what I understand is that apparently that's by the use of their own LastPass authenticator which would use the biometrics present on a handset.  Thus you could unlock your LastPass vault without using a master password.  Okay, so not that big an announcement.



But separate from that immediate announcement was indeed a forward-looking statement of their intention to support FIDO2 Passkeys.  So that's not today, but at least one major password manager is taking aim at this problem.  And if one does, they'll all need to.  So I suspect that the biggest effect of Apple's, Google's, and Microsoft's support may be to induce websites to bring up their own support for WebAuthn, which is what's necessary on the back end.  And so let's talk for a minute about...



LEO:  By the way, that's what 1Password says they're going to do is support Authn.



STEVE:  Good.  Good.



LEO:  Yeah.  So that's the way to do it; right?



STEVE:  Yes.



LEO:  And then presumably there'd be an export/import feature from 1Password Manager or iCloud, I would hope.



STEVE:  I would - well, yes.  Now, it'll be really interesting to see whether Apple allows a wholesale export.



LEO:  Well, they say they are.  They say there's an export feature.  Yeah?



STEVE:  No.  No.



LEO:  They didn't say that.



STEVE:  No.  They said there's a Passkey sharing, which is different.  You use Airdrop to send one key to a phone...



LEO:  No, no, I know that's not the same.  But somebody told me.  You've watched the presentation.



STEVE:  I watched the presentation.  I read the developer docs.  There's not a word about...



LEO:  Oh, okay.  There's no export.



STEVE:  About export.  And if I'm wrong, listeners, please correct me.  I'd much rather it be true that they're going to allow export than just be a curmudgeon and say, wow, they didn't get it right.  So if anyone finds that there's an export of Passkeys from i-land, I want to know.



Okay.  So WebAuthn.  The heavy lift that FIDO will face and that SQRL would have faced was the need for backend web server support.  As we know, it's surprising, always surprising how slowly things change.  No question about it, it's going to take years.  It's going to take all of the major web server platforms to build it in, then for all of the existing web servers to be upgraded to support it and then to do whatever they need to do on their end to actually give it a database and enable it and have the lights on.  And since it's not clear that there's a huge benefit to them, since things are sort of working as-is, I think we can expect it's going to take a while.  Think about how long it took for the "Logon with Facebook" and "Logon with Google" OAuth kludge to finally become popular.  And it's still far from universal.  It's around, you encounter it, but you certainly can't use it everywhere.



What you can use everywhere is the original fill-out-the-form username and password.  The reason password managers were an overnight hit was that they provided an entirely client-side solution which did not require any backend change to websites. Web servers didn't know or care how a user was arranging to provide their username and password.  And they didn't need to care. But make no mistake, automated form-fill of username and password is and has always been a horrific kludge.  The fact that kludges mostly work doesn't make them any less kludgy.



WebAuthn finally, and at long last, changes that.  The adoption of WebAuthn, which was approved and formalized by the W3C consortium three years ago, back in 2019, represents a massive and long-needed update to username and password authentication.  As I mentioned last week, FIDO and SQRL work essentially the same way.  They both give a web server a public key.  The web server generates a random nonce which it sends to the browser.  The browser, holding the matching private key, which it never releases, signs and returns that nonce to the web server, and the web server uses the public key that it has on file to verify the returned signature.  What WebAuthn is and does is provide all of the mechanics, definitions, protocols, specifications, and implementations of that new form of interchange between a web server and a web client.



Now, we're likely going to be facing a chicken-and-egg situation for some time.  What kind of got lost amid the ballyhoo of Apple's announcement last Monday is the fact that you can't use it anywhere.  I mean, it's like having SQRL, which we've had for years now.  But you can't actually log in anywhere but GRC and a couple of our...



LEO:  TWiT.community also.  You can use your SQRL there.  I think you can.  Well, I set it up.  And then remember one of your guys in the forums was providing a backend server because you need a backend server for SQRL; right?  Some sort of authentication thing.



STEVE:  Well, you can - I don't have one on any of my implementations.  But you certainly could.  You could federate the authentication to a backend server.



LEO:  There was something going on I couldn't do that he was doing.  And then he let that slide, and so people were using SQRL to log into TWiT.community, couldn't use it, and then he said, oh, yeah, let me see if I can get - I don't know.  I haven't kept up on that.



STEVE:  Then it just sort of died.



LEO:  Yeah, just sort of died out, yeah.  But I needed something, some piece that he was providing.



STEVE:  Well, thank you for trying, Leo.  It will be good that Apple and Google and Microsoft will all be supporting Passkeys, which is to say FIDO2 on the client side and WebAuthn on the protocol backend server side.  That's the glue that makes this possible.  But when iOS 16 arrives with its built-in Passkey support, you'll probably only be able to login to Apple.com, Google.com, and maybe Microsoft.com, due to the heavy lift of change that will be required on the back end.



But WebAuthn is the key.  Period.  It provides a complete replacement for the insecure mess of usernames and passwords that we've been living since the dawn of man.  And interestingly, WebAuthn optionally supports SQRL's chosen 25519 elliptic curve.



LEO:  Yay.



STEVE:  Yeah, with its special properties that allow for non-random deterministic private key synthesis.  That's the key behind SQRL, as I've mentioned, if you'll pardon the pun.  So it might be possible, someday in the future - I'm not going to do it, we've got a bunch of SQRL developers now, they'll do this - to transparently run a modified SQRL solution to use SQRL-style deterministic Passkeys on the server infrastructure that FIDO built.  So that would be cool.  And perhaps indeed the only kind of progress that can practically be made in today's world is incremental.



So the fact that everyone's excited, and it's going to be available in our clients, and when web servers start adding Authn support, their UI, their login UI will show, hey, if you've got a Passkey, use it.  And so you'll give them, the first time you login as you, using old-school login, then you'll have your client generate a passkey which they'll hold onto, and you can then use that to log in in the future.



But the other point I wanted to make was that what I really did with SQRL, and this is exactly to the point you were making with you and Lisa, is I didn't just stop at replacing usernames and passwords.  I solved the whole problem.  I mean, as you heard me say back then, every single question anybody could ask about what if this, what if that, blah blah blah blah, I had SQRL solve the problem and provided all kinds of forward-looking solutions that would be handy to have.  We're not going to get it yet.  Maybe we'll get it eventually.



LEO:  Yeah.  I realize that Discourse, which is the forum software I use - XenForo you don't have to do anything.  I think it's built in.  But Discourse has to use an OAuth2 provider.  So Jose C. Gomez, who I think is probably still around in your forums, had set up a SQRL OAuth2 provider.



STEVE:  That's very clever.  Basically an OAuth gateway for SQRL.



STEVE:  Gateway, exactly.



LEO:  But it's gone, and so it's a 502 Bad Gateway.



STEVE:  Bad gateway.  Bad gateway.



LEO:  Bad gateway.  So unfortunately everybody who used SQRL that set up an account on TWiT.community I guess is out of luck.



STEVE:  [Buzzer sounds]



LEO:  So, sorry, I apologize.  I don't know enough to run an OAuth2 server myself.



STEVE:  Well, and as we know, I've moved on.  So I did what I could, and maybe we'll get little pieces of SQRL over time.



LEO:  Admittedly, this is not perfect.  But, you know, the perfect can be the enemy of the good.  And Passkey is much better than what we're doing right now.  



STEVE:  Yes.



LEO:  So, yes, there could be something better.



STEVE:  I guess my only argument is that making a change on the back end requires everybody to make the change.



LEO:  Right.



STEVE:  And it's such a heavy lift that, if we had just - if we were going to make a change, let's make it a good one.  I mean, let's make it one that actually solves all these other use cases for which people are like, you know, sharing passwords, and what if a bad guy did get a hold of your key and then were able to use it to lock you out of an account.  That doesn't work with SQRL.  Some person who gets your SQRL identity cannot lock you out of accounts where you are registered with SQRL.  It's just that there's so much there.  And that's where the time went.  But I got it out of my system, and now I'm back to SpinRite.



LEO:  Do you know what the lift is, well, you must because you've watched this video now, how hard it would be - let's say I wanted to implement Passkeys on a site.



STEVE:  So no mortal will do that.



LEO:  It's that hard.



STEVE:  Yeah.  And so but it'll be a library.  For example, I googled "FIDO2 Passkeys IIS."  Obviously Microsoft server.  Zero hits.  There's nothing for Microsoft's web server.



LEO:  So just it's like with SQRL.  XenForo supported SQRL, so it was easy for you to implement.  Discourse does not.  It requires an OAuth2 backend.



STEVE:  Actually, XenForo did not support SQRL.



LEO:  Oh, it didn't.  But somebody wrote something.



STEVE:  That was Rasmus Vin.



LEO:  Rasmus wrote that.



STEVE:  Our wonderful PHP guy who built -



LEO:  So he hacked XenForo to do that.



STEVE:  Yeah, well, actually XenForo has a beautiful add-ons architecture.  So he was literally able to create an add-on that any XenForo user, any XenForo site could just do it.  And of course none have because what's a SQRL?



LEO:  Well, you're reliant, just as I am with Discourse, I would be reliant on Discourse adding that capability.  And somebody who's using WordPress, they would need a plugin.  The good news is that the web is mostly dominated by a handful of servers.



STEVE:  Yes.  Very few servers, and they will all get WebAuthn  plugins, or modules in the Apache case, for example.  And so you just, you know, add the WebAuthn module and configure it, and you're probably good to go.



LEO:  Yeah, right.  Well, that day I hope comes soon.



STEVE:  It'll be interesting to see.  It's not, and this is the problem, there was a benefit to Google and Facebook doing the login with Google Login with Facebook OAuth hack because, as we know, they track you as you pass by.  They know who you are and where you're logging in.  So this federated OAuth login is, you know, they had a benefit.  They had a reason for doing it.  It's not clear to me what the benefit will be to web servers and websites adding Passkeys.  Things are working the way they are now.  So we'll see.



LEO:  Yeah, yeah.



STEVE:  And as I said, not everybody is doing the logon with Google and Facebook.  You certainly can't use it universally.  You see it here and there.  And it's kind of like being able to buy something with PayPal.  It's like, oh, good, I can use that here.  But other places, oh, you need my credit card number [grumble].



LEO:  WordPress does have a SQRL plugin.  Did that get a lot of uptake?



STEVE:  Yeah. 



LEO:  It did.



STEVE:  Yeah, yeah.



LEO:  I think that's got to be key.  You've got to have those easy switches.



STEVE:  Yes, got to be a drop-in solution.  So last Wednesday a company known as Offensive Security, who are the people behind Kali Linux, announced that they'd be live streaming their "Penetration Testing with Kali Linux" course sessions on Twitch later this month.  I mention this because a subset of the course's material  all of the streamed content  will be open to the public on Twitch at no charge.  And I felt sure that some of our listeners would find that interesting.  For those who don't know, Kali Linux is a Debian-based Linux distro which is targeted toward facilitating information security tasks such as Penetration Testing, Security Research, Computer Forensics, and Reverse Engineering.



The course in question, PEN-200, is a paid course which helps students prepare for the Offensive Security Certified Professional (OSCP) certification exam.  Before the pandemic it was only available in person.  But during the COVID response live training was suspended and Offensive Security moved their courseware online for remote instruction.  So today a 26-part, 13-week, twice-weekly hour-long per event online course will be offered to prepare students for the OSCP certification.  Non-enrolled viewers are welcome to audit the course.  You of course won't get any credit for it, but there's the information.  There's an FAQ in the show notes, or a link to the FAQ in the show notes, which details everything you'll need to get going.



I grabbed the two most relevant Q&As from that page.  First one is where and when will it be happening?  And their answer is the OffSec Live: streaming sessions are currently planned to start June 22nd, 2022, so that's Wednesday after next at from 12:00 to 1:00 p.m. Eastern, so that's currently 9:00 to 10:00 West Coast, and run through December 7, 2022, so pretty much the rest of the year. 



The OffSec Live: PEN-200 streaming sessions will consist of twice-weekly, hour-long interactive presentations.  They said:  "We'll also be streaming a weekly office hour where we will address any questions a student may have regarding the PEN-200 course and prior streaming sessions."  And those, I would imagine, will be only for enrolled people.



And how much will it cost?  And they said the OffSec Live: PEN-200 Twitch streaming sessions will be free and open to the public.  So just a heads-up for our listeners.



Before I introduce the proof that we are living in a simulation - just wait.  You laugh now.  Just wait.  I did want to mention that Surfshark has followed ExpressVPN in pulling out of India.  I heard Andy mention on MacBreak Weekly that he logs into a VPN in India.  



LEO:  Alex, yes.



STEVE:  Oh, Alex, yeah.



LEO:  So he can watch the Pittsburgh Steelers football game.



STEVE:  Yes.  And the good news is, as I mentioned last week, at least in the case of ExpressVPN, you can still get an IP from India, even though it's being hosted in some other country.



LEO:  Right.  Some very clever little shenanigans they must be pulling there.



STEVE:  Yup, a hack.



LEO:  Yeah.



STEVE:  And Leo, let's take our second break, and then the proof that we're living in a simulated reality.



LEO:  Okay, officially the best tease in the history of all time.



STEVE:  You cannot un-know this, so be careful.  This is going to change things.



LEO:  I remember I went to see the movie "The Matrix" without any prior knowledge; right?  And I go in, I watch it, I come out, my eyes are like this because I think, oh my god, I'm in a simulation.  Because that's the whole premise.  So I'd have to say my life was never the same since.  So I'm ready.  



STEVE:  And once upon a time...



LEO:  Are we going to want the blue pill here, or the red pill?  It's up to you.



STEVE:  Once upon a time the fact that Elon agreed with us had some value.  Not so much anymore.



LEO:  No.



STEVE:  No one really cares what he thinks.



LEO:  No.  Get ready.  Your red pill moment is coming up.  Okay.  I'm ready to have my mind blown.  I'm taking the red pill, Steve.  Go ahead.



STEVE:  So I stumbled upon a proof that we are, indeed, living in a simulation.  My proof that we are in a simulation is that I have identified a clear and incontrovertible bug that exists within the simulation itself.



LEO:  A glitch.



STEVE:  Yes.  It's a bug because it defies reality, which is after all what the simulation is intended to simulate.



LEO:  Yeah.



STEVE:  And what makes this so compelling is that all of us are aware of this bug, but we all just shrug it off without it ever occurring to us that that is what it is.  So here's the bug.



LEO:  I'm ready to have my mind blown.  Okay.



STEVE:  It's a clear failure in the rules of probability.  If the rules of probability were being properly simulated, when you attempt to plug in one of those original USB-A style rectangular plugs, the plug's orientation would be wrong only half the time.



LEO:  Yeah?



STEVE:  But we all know that it's not 50/50.  Everyone's experience is that the first time we attempt to plug in one of those, it is almost always wrong.



LEO:  Always.  Always.



STEVE:  Always.  Isn't it?  It is.  Leo, it is always wrong.  It cannot be always wrong.  But it is.



LEO:  I have the Leo Laporte corollary to that.  The toast always falls butter side down.



STEVE:  Well, see?  I think the more we think about this, the more we're going to realize things are not all as they seem.



LEO:  You got me.



STEVE:  I mean, right?  I mean, isn't one of those G-D USBs always wrong?



LEO:  Always upside down, yup.



STEVE:  Yes.  It should be 50-50.  It must be 50-50.  And it is not.  No one's experience is that it's 50-50.  So we've been given a clue that we've not been paying attention to.



LEO:  It's a glitch in the matrix.  You found it.



STEVE:  It's a bug.  It's a bug in probability.  And that must mean that there's a failure in the simulation.



LEO:  That's why cats always land on their feet.



STEVE:  Now, Leo, I have something that may be of interest to you.  I have found, I stumbled upon, a valid use for facial recognition.



LEO:  Okay.



STEVE:  It's in the show notes.  It's the Smart Pet Door.  The other day I encountered an absolutely valid and very clever application for facial recognition which I doubt that anyone would have a problem with, unlike recognizing people.  The bad news is that it's called "Petvation," which has just got to be about the worst name that anyone has ever come up with.  Petvation.  My god.  I'm sure there must be a wonderful name for such a product.  Hopefully some listener will think of it and tell these poor Kickstarter people.



So what is it?  Though you'd never know it from the name, it's a little automated doggie door, or cat door, which employs facial recognition of the household canine or feline.



LEO:  Now, see, you're not a pet owner, so I know that you don't know how serious the need is for this.  We have a pet door.  And we actually had to chip our cats so that the pet door, when they get close, it uses RFID, and it goes chink and opens.  Because otherwise raccoons, turkeys, vultures, wolves...



STEVE:  Apparently ducks.



LEO:  Ducks.



STEVE:  I heard about apparently ducks.



LEO:  Anything could use that portal.



STEVE:  Yup.  So, I mean...



LEO:  But I would love the face recognition.  That'd be great.



STEVE:  Isn't that great?



LEO:  Yeah.



STEVE:  So it's an automated doggie door or cat door, employs facial recognition.  As they approach the door, their identities are verified.  And only if they are known to it will it raise the barrier and allow them to pass.  It's brilliant.



The text which accompanies this horribly named but otherwise wonderful-looking innovation notes that:  "It can also recognize the most common unwanted guests  raccoon, wolf/coyote, bear, deer..."



LEO:  Bear?  How big is your dog door?



STEVE:  How big is that dog door, yeah.



LEO:  "...squirrel, rabbit, duck, boar, skunk, fox, rats, boar, snake, chickens, and more  and deny them entry to your home."



LEO:  Well, well, now.



STEVE:  So it's unclear to me, in reading that description, why explicit recognition of unwanted pets is important if it's truly being discriminating about the identity of the family's pet.



LEO:  Sure.  Well, you don't want to let the neighbor's cat in.  I mean...



STEVE:  Well, what if you did have a pet duck?  I mean, that's probably happened.



LEO:  That's a good point, yeah.



STEVE:  So you'd probably have to turn off the duck eliminator.  But in any event, perhaps that's just to make sure that nothing unwanted can get in, even if it's like some clever animal wearing a dog mask to spoof the identity of your pooch.



LEO:  But really I think dogs - maybe I'm biased, but I think a lot of dogs look alike, and a lot of cats look alike.  



STEVE:  Yeah.



LEO:  Can you really do this accurately?



STEVE:  I don't know.  I don't know.



LEO:  I'm sending this to Lisa.



STEVE:  And you're right that chipping animals is a solution.



LEO:  That's perfect, yeah.



STEVE:  Get them a little short-range RFID.



LEO:  Exactly.



STEVE:  I should note that I stumbled upon this over on Kickstarter when I was following up on a recent communication from Ed Cano.  That was Ed's 48th communication.  Ed is the originator of the SandSara, which you and I are still waiting for, Leo.



LEO:  Oh.



STEVE:  It may finally actually ship.  Apparently, it has shipped, and they're working their way through customs at the moment.



LEO:  What is it?  It's been so long I forgot.



STEVE:  I know.  The SandSara, for those who don't recall, is a nifty-looking tabletop bed of fine-grained sand through which a steel ball bearing is rolled by a magnet located underneath the bed.



LEO:  Oh, yeah, I remember this.  Yeah, yeah, yeah.



STEVE:  And I've got the link to it in the show notes.  If you want to click on it, you'll see some animated reminders.  The magnet is moved by a pair of stepper motors using a quite ingenious mechanism.  This podcast discovered this Kickstarter project back in March of 2020, just as COVID was descending upon the world.  Ed's original timeline anticipated fulfillment during August that same year, so it appears that it'll finally be happening just shy of two years late.



LEO:  Yeah, that's pretty par for the course, unfortunately, with this stuff.



STEVE:  But I have no complaint with Ed's approach.  His communication has been good and consistent; and, boy, is he a man after my own heart.  He's proven himself to be an absolute perfectionist.



LEO:  He should be.  He raised 18 million Mexican dollars.



STEVE:  Yes.  He has 2,000 supporters, 2,000 backers.  Though it appears that we'll be receiving this nifty-looking gadget nearly two years late, it really does appear that what we'll be receiving will have been well worth the wait.  As I mentioned, his project has just shy of 2,000 backers, and I know that many of them are among our listeners since Ed told me so.  When we discovered this, we got quite excited at the time.  So our collective wait may finally be nearing an end.



LEO:  What is 18 million pesos in American money?



STEVE:  I don't know.  It was a couple hundred bucks, I think.



LEO:  Yeah.  I think it's, yeah, yeah, it was.  It's expensive.  He says 19 pesos to the dollar.  So, yeah, that's right, they're about a nickel each.  So he raised a lot of money.  What is 20 million nickels worth to you?



STEVE:  He didn't run out of money.  Apparently he pulled this thing off.  I thought of it because Lorrie, just think she will love this.  It's just, you know, it's like the perfect - and he's worked on making it quiet because it was like you could hear the stepping motors buzzing.  And so no, not anymore. 



LEO:  Yeah, it's got to be quiet, yeah.



STEVE:  And like these beautiful round wood, I mean, he just went through like fire in order to get this thing done.  But they're shipping.  So I just wanted to mention that.  And that's how I found Petvation, the worst-named thing ever.  The engineering looks good.  But, boy, they're not much for marketing.



LEO:  By the way, it's almost - it's $874,000.  It's a lot of money he raised.



STEVE:  Yeah.



LEO:  So I'm glad he didn't run with it.



STEVE:  No, he did not.  He is a good guy.  And if he were to do something else, I wouldn't put it on my calendar, but I'd probably give him more money because he does come through.



LEO:  Yeah.



STEVE:  Okay.  We've got some closing-the-loop feedback from our listeners, a bunch of interesting stuff.  Robert Wicks, who tweeted from @bertwicks, he said:  "You called it, Steve.  This just popped up on my ancient i7-4770 system that has no TPM."  And he sent me a screenshot that says:  "Windows Update.  Windows 11, version 22H2 is ready, and it's free.  Get the latest version of" - yeah.  Pinch your nose.  "Get the latest version of Windows with," and it says, and it's a link you can click, "a new look, new features, and enhanced security."



LEO:  Wow.



STEVE:  Because we always say that.



LEO:  That's a fourth-generation i7, so it's just the TPM.  This is definitely ineligible.



STEVE:  Yeah.  It's got like smoke signals coming off of it.  So Peter G. Chase also tweeted from @PchaseG, he said:  "Ha ha.  My only slightly ineligible-for-WIN-11-upgrade PC processor is now magically eligible, and Windows is pestering me to do it. Nothing changed on my end.  Must be them.  You said they might miraculously change the requirements, but I still don't want Windows 11."



Okay.  So what happened?  Leo, under the category of "You just can't make this stuff up," we have the explanation for what Robert and Peter and many others experienced last week.  The Verge covered the story last Thursday under the unbelievable headline - well, if it weren't Microsoft it'd be unbelievable:  "Microsoft has accidentally released Windows 11 for unsupported PCs."



LEO:  Well, thank you, Microsoft.



STEVE:  And it runs just fine.



LEO:  Thanks so very much.



STEVE:  Paraphrasing from what The Verge wrote, they said:  "Microsoft released the final version of its next big Windows 11 update (22H2) to Release Preview testers on Tuesday, and accidentally made it available to PCs that are not officially supported.  Oops."  They actually wrote "oops," not me.



Twitter and Reddit users were quick to spot the mistake, with hundreds of Windows Insiders able to upgrade their Windows 10 machines on older CPUs.  Microsoft, as we know, has strict minimum hardware requirements for Windows 11, leaving millions of PCs behind, so the mistake will once again highlight the company's controversial upgrade policy.  Which is to say that it runs just fine everywhere, even though we don't want to give it to you.



So they said:  "Windows 11 officially requires Intel 8th Gen Coffee Lake or Zen 2 CPUs and later, with very few exceptions.  While there are easy ways to install Windows 11 on unsupported CPUs, Microsoft doesn't even let its Windows Insiders officially install beta builds of the operating system on unsupported PCs, so this mistaken release is rather unusual.  Microsoft is aware of the mistake, and it's investigating."  Oh, well that's comforting.



Says Microsoft:  "It's a bug, and the right team is investigating it."  As opposed to having the wrong team investigate it, which would just add insult to injury, presumably.  And that is what Microsoft's official Windows Insider Twitter account tweeted.  "So if you managed to install Windows 11 on an unsupported PC and were expecting only Release Preview updates for Windows 10, you should be able to roll back the unexpected upgrade in the settings section of Windows 11," said The Verge.  So anyway, got a kick out of that one.



And speaking of large numbers, Jeff Parrish, tweeting from @kb9gxk - sounds like he might be a ham, don't you think?  Is that a ham designation, kb9gxk?  Or does it have to be k9 something?



LEO:  I don't know.



STEVE:  I don't know.  He says you can go to math.tools/calculator/num... - and I didn't get the rest of the link - to convert that number.  Remember the number was, I don't even remember now, 2^768 from last week.



LEO:  Yes.



STEVE:  It was three sets of 256-bit encrypted.



LEO:  It was big, yes.



STEVE:  Well, he says, it's large, just the first couple of numbers.  And then he lists them.  But somebody else sent me the entire thing, of course.  We have that kind of listener.



LEO:  I am not reading this.  Are you going to read this?



STEVE:  No, no.  But I'm going to go up from the bottom.



LEO:  Okay.



STEVE:  Because I stumbled on something that was sort of interesting.  So it ends in 38.  Then we've got - we have 52,000.  Then we have 139 million, 611 billion, 696 trillion, 116 quadrillion.  Okay.  Then we have 17 quintillion.  Yeah, I guess I am reading it.  886 sextillion.



LEO:  He's so good, he's reading it backwards, folks.



STEVE:  In high heels.  256 septillion.  555 octillion.



LEO:  This is all Latin.  You know, sept, oct, non.  Go ahead.



STEVE:  Yeah.  Well, and what happened was later I got to sept, oct, and nov.  And it's, wait a minute, those are months:  September, October, November.



LEO:  That's right, yeah.



STEVE:  Yeah.  So anyway, we got nonillion.  Then we've got decillion.  Then we have undecillion.  Then duodecillion, tredecillion, quattuordecillion - these are not very imaginative at this point - quindecillion, sexdecillion...



LEO:  That's why.  It's orderly so you can figure it out.



STEVE:  Septendecillion.  Then we have octodecillion and finally - oh, no, novemdecillion.



LEO:  Novemdecillion, yeah.



STEVE:  And finally one vigintillion.



LEO:  Vigintillion.



STEVE:  Vigintillion.



LEO:  That's because, yeah, that's 20, yeah.



STEVE:  Wow.  Anyway, thank you, those of you who followed up on the 2^768.  We will never forget ye.



LEO:  And Gumby I think told us he tried to do it in Emacs, and it quit at quadrillion.  What a wimp.  So never mind, Emacs.



STEVE:  So Roy Ben-Yosef tweeting at @WizardLizardRoy is his Twitter handle.  That one wasn't free when he went for it.  He said:  "Hi Steve.  Just heard your piece on the New South Wales driver's license.  And maybe I'm missing something, but isn't it all client-side?  They can do all sorts of fancy crypto and whatnot.  Can't I just write my own app that looks the same" - with a dancing flower - "and shows whatever I want?  Is there no, or shouldn't be, server-side variation somehow?"  He says:  "Long-time listener since 2011.  You got me into the cyber security business since 2012.  Thank you."



LEO:  Nice.  Wonderful.



STEVE:  And I'll follow that immediately with Ryan's tweet because he's his company's PKI expert.  Ryan tweeted, actually he sent via email, GRC.com/feedback, it was in the mailbag:  "Hello Steve.  Regarding the New South Wales Digital Driver's License, I work as my company's PKI expert."  You know, that's Public Key Infrastructure.



LEO:  There's a job called PKI Expert.



STEVE:  Oh, yeah.  And he knows all about WebAuthn, I'm sure.  He says:  "I liked your solution to the DDL problem."  You know, I talked about how you could just solve this with a certificate.  He said:  "I wanted to let you know that your proposed PKI-based solution can also prevent spoofing of the driver's license QR code.  Dancing flowers are all well and good, but could be spoofed if you were motivated enough," as our previous writer noted.



He said:  "The way to prevent copying the QR code and passing it off as your own lies in exactly the type of certificate that you issue.  If you issued a signed DDL certificate like you suggested, with the Key Usage attribute of digitalSignature," he said, "(and the Enhanced Key Usage attribute of emailProtection), then your DDL can sign arbitrary messages.  This is how encrypted S/MIME email works.  In our case, the DDL [Digital Driver's License] could sign your name and age and include the signature in the QR code."



In other words, he came up with a very clever extension.  The certificate that is the DDL itself could be used for signing.  So that QR code could constantly be changing locally on the screen, each of those changes dynamically refreshed and signed by the certificate which is the DDL because it's a cert.  So the point is that's a very clever extension of the idea.



He said:  "But couldn't this QR code be copied?"  He says:  "Well, the next step is to have the person verifying the ID check the timestamp to see exactly when the QR code was signed.  You could have the user push a button or enter a PIN in the app."  And I had it just doing it automatically because you could do that, too.  "This also lets you customize your trust.  A police officer's scanning device could trust a QR code for 30 seconds, a bouncer's device could trust a QR code for 1 to 2 minutes, so you can pull up your QR code while you're in line and keep the line moving."  He really thought this through.



"The chances of being able to predict the exact 30, 60, or 120-second window that you'll need to produce the QR code means that setting the clock ahead on your friend's phone and pre-generating a QR code are vanishingly small.  Live long and prosper.  Ryan."  So Ryan, very nice piece of work on that.



LEO:  Nice, yeah.



STEVE:  Tom Davies was a little grumbly.  He said:  "It always grates me a bit when you dunk on GDPR for 'ruining browsing.'  Please have a look here."  And he sent me to www.coventry.ac.uk.  He says:  "As you can see, it's perfectly possible to be GDPR-compliant only forcing a single interaction on the user.  What has 'ruined browsing'" - he has again in quotes - "is not GDPR, but web hosts' reliance on tracking users to the point where they would rather ruin your browsing experience than allow you to opt out."  He says:  "I've not dug into it that much, but I'm sure we've all seen examples where it's clear that sites are deliberately making it extremely difficult to do anything other than accept the default 'please track me' option.  It really seems that if you don't want to be tracked, these sites do not want you to visit."



So I was curious to see what Tom was referring to.  And Leo, you brought it up.  So I went over there.  What Tom means - because we have a bar at the bottom with three options.  What Tom means is that it's possible for a website to request to set a cookie in order to no longer need to notify about setting cookies.  Okay.  And just for the record, since Tom says this is grating on him, we've previously made it clear that the strict privacy regulations that had been in place long before the GDPR, but that everyone was happily ignoring  thus no annoying cookie warning banners  until the GDPR was enacted to give those regulations some teeth.  I certainly don't disagree with the bias toward tracking that Tom has, but it really was the enacting of the GDPR that's what is making our lives miserable because now it's not just a regulation everyone ignores.  It's something that everyone has to pay attention to.



LEO:  Well, what really is the problem in my opinion is this notion that all cookies are bad.  My site uses one cookie, a cookie to remember your preferences between light and dark mode.  But because it uses one cookie, it's not tracking you, it's just you're saving on your computer your setting...



STEVE:  Right, your preferences.



LEO:  That's all it's doing.  Because it's doing that, I have to put up a stupid-ass cookie thing.  And I think it's wrong.  And it's absurd.  And it has the exact opposite effect by inuring people to these messages.  It's making them forget about - it's just like, well, yeah, click.  How many quadrillion vigintillion useless clicks have there been?



STEVE:  Oh, yes, clicks, clicks.



LEO:  Waste of resources because that has to be transmitted.  No, it's a perfect example of a misguided law that accomplishes exactly the opposite of its intent.  By the way, pat on the back because today Firefox announced what should have been the case all along, that third-party cookies are entirely blocked.  They're going to sandbox all cookies unless the originating site is asking for it.



STEVE:  By default, yes.  Nice.



LEO:  Yeah, and that's the problem is third-party cookies.  First-party cookies are not problematic, I would submit.  In any event, certainly my light/dark mode cookie is not problematic.



STEVE:  No.



LEO:  It's incredibly infuriating.  And it may be that our mentions of this grates on you, but that ain't nothing compared to how these cookie warnings grate on every other person in the globe because we have to do it because the EU decided to do it.  It damaged the EU's credibility beyond belief.  It's a terrible idea.  It had all sorts of negative effects.  It makes the EU a laughingstock.  Thank you.



STEVE:  Yeah.  Philip Le Riche, he said regarding last week's podcast, SN-874:  "The only essential diff between Passkeys and SQRL is that SQRL uses computed private keys.  So could SQRL be made to register its private key with a FIDO2 website and respond to a FIDO2 auth challenge?  Then SQRL would work on any FIDO2 site."  He said:  "(But please finish SpinRite first.)"



LEO:  Yes.



STEVE:  So he anticipated what I talked about at the top of this episode.  The use of deterministic private keys may be possible under WebAuthn.  If so, then yes, SQRL's single-key approach could transparently replace the big-bucket-of-keys approach that is used by FIDO without sacrificing any security.  And as we now more know, there is in fact much more to SQRL than just the difference between one master key that synthesizes Passkeys on the fly versus randomly generated Passkeys, all of which you have to store.  As I demonstrated at the top of the podcast, SQRL really goes all the way to solving authentication.  What we have with Passkeys and FIDO2 is a very robust replacement of usernames and passwords.  So definite improvement there.  And as for SpinRite, everyone should know that I'm done, done with SQRL.



LEO:  What?



STEVE:  Except for talking about it here.  I've handed it off and over to a very competent group of developers who know it now just as well as I do.  And they hang out over at sqrl.grc.com.  So no one need worry for a nanosecond that I will again become distracted by SQRL.  That's not going to happen.



And somebody with a clever handle, lyntux, L-Y-N-T-U-X, lyntux, he said:  "I wanted to remind you, @SGgrc, that a YubiKey can only hold up to 25 credentials each.  That's not a problem, since there aren't that many FIDO2 websites out there.  But that could change.  That's another reason why SQRL is better."



Yes.  And the fact that YubiKeys have a 25-credential limit also beautifully demonstrates that the FIDO project's work was never aimed at mass use and adoption.  They were originally focused upon using super-secure hardware dongles for authentication to just a few crucial sites.  That system was never able to scale. So when it failed to get off the ground, they relaxed their requirements and produced FIDO2.



I'm going to skip one because we're...



LEO:  Yeah, it's my fault we started late.  I apologize.



STEVE:  No, no, no, it's okay.  Oh, I guess I pronounce his name Juho Viitasalo.  He said:  "Hi.  I tried the latest SpinRite..."



LEO:  I'm pretty sure that's not how you pronounce it, but it's going to have to do.



STEVE:  Yeah.  Sorry about that.  I'll just say that his Twitter handle is @juhov.



LEO:  Yeah, that's easier, yeah.



STEVE:  "I tried the latest SpinRite EXE from your dev directory, but could not run it in Windows 7.  I was hoping to burn an ISO that way.  I have a failing SSD I need to massage.  Can you tell how to use these executables you have on your server?  Thanks."



LEO:  Oh, boy.



STEVE:  Yeah.  And Greg got a bunch of email from our listeners.  And so I'm sure that he's referring to the reference we recently made to grc.com/dev/SpinRite.  And I asked you, and you did, to pull up the directory listing just to sort of show how much work we did on this first phase of SpinRite.  That directory contains a gazillion incremental SpinRite development releases.  But they are DOS executables.



LEO:  Oh, boy.



STEVE:  Not Windows EXEs.  So they definitely will not run under Windows.  But moreover, even if you did run them under DOS, they do not yet move the user past the system inventory stage, where SpinRite is figuring out what you've got and how it can best work with it.  That's where all of our collective work thus far has focused.  Once I do have something that's operational, all existing SpinRite licensees will be able to obtain their own copies from GRC's server.  But we're not quite at that stage yet.  So I apologize for the confusion.



LEO:  I'm guessing Juho thought he'd found a source of free SpinRite, which this is not.



STEVE:  Maybe, although he was expecting to burn an ISO, and the normal SpinRite that you run on Windows will do that.  So it sounds like he has some understanding of the way it normally works.



LEO:  He just wanted to upgrade.  Right, right.



STEVE:  Okay.



LEO:  Could you use the old SpinRite for an SSD?  Yeah; right?



STEVE:  Yes.



LEO:  Yeah, okay.



STEVE:  Okay.  If anyone listening to this podcast has only been paying half attention, although this has been so great, I don't know how that's possible, now is the time to bring your entire focus to bear on this next question, our final question from a listener.  Because it wins the award for the best question maybe ever.  And I'm going to leave it hanging and unanswered until next week.  So everyone will have the opportunity this next week to ponder the question and its answer.  And Leo and everyone in the Discord and chat rooms, please resist the temptation to blurt out any spoilers for everyone else who wants to ponder this very intriguing question.  Here it is.



Erik Osterholm tweeted:  "Hi Steve.  Long-time listener.  I'm a little confused on the key length discussion.  I always thought that encrypting twice merely doubled the effective strength.  Here's my reasoning:  Imagine you have an algorithm and a key that you can fully brute force in one day.  If you add one bit to the key, you double the key space, and therefore it takes twice as long to brute force (or two days).



"If you instead encrypt twice, then it takes one day to decrypt the outer ciphertext, at which point you get back to the first output of ciphertext.  Then in one more day you can brute force the first/inner ciphertext.  Like in the first example, this takes two days.  It seems to me that these are equivalent.  Is there something I'm missing?"  So there it is.  Everybody think about that.  And we'll answer the question at the top of next week's show.



LEO:  Hmm.  His logic seems sound.



STEVE:  It seems very sound.



LEO:  Hmm.



STEVE:  So no spoilers, anybody.  Don't blurt it out.  Just smile if you think you've got it.



LEO:  Ain't gonna blurt it out 'cause I don't know it.  You're safe with me, Steve.



STEVE:  So let's take our last break, and we're going to delve into the so-called "PACMAN Attack," which leverages an unpatchable and unfixable flaw which exists across Apple's custom M1 chipset family.



LEO:  [Sigh].  Okay, Steve.  Let's move on.



STEVE:  So by far the biggest headline-grabbing news this past week was the teaser that Apple's much-vaunted M1 chips contain a critical bug, or bugs, that cannot be patched or repaired.  First of all, only part of that is true.  The reason Apple is being singled out is that their M1 ARM architecture chips are the first to use a cool new feature of the ARMv8.3 architecture known as Pointer Authentication.  Everyone else plans to use it, too; they just haven't gotten their newest chips out yet.



The heading of the 14-page research paper published by four researchers at the MIT Computer Science & Artificial Intelligence Laboratory reads:  "PACMAN Attacking ARM Pointer Authentication with Speculative Execution."  So once again we have the spectre - pun intended - of speculative execution raising its ever ugly head, this time in a new context, a new feature of the ARMv8.3 architecture initially represented by Apple's proprietary M1 chip.  Okay?  So what is it?



PACMAN is a novel hardware attack that can bypass the benefits of an emerging feature known as Pointer Authentication (P-A-C, PAC, so that's where the PAC comes from PACMAN).  The authors of this work present the following three contributions, they feel:  First, a new way of thinking about compounding threat models in the Spectre age, meaning hardware and software, not only one or the other.  Second, reverse engineering details of the M1 memory hierarchy.  There's almost nothing available publicly from Apple, so they had to just figure it out the hard way to pull this off in practice.  And three, a hardware attack to forge kernel PACs, that is, kernel pointer authentications, from user space on M1.



They wrote:  "PACMAN is what you get when you mix a hardware mitigation for software attacks with microarchitectural side channels.  We believe the core idea of PACMAN will be applicable to much more than just PAC."  So these guys believe that they've uncovered another entire class of exploitable microarchitectural flaws which are introduced by attempts to mitigate software attacks.  We'll talk about Pointer Authentication and what it is in detail in a second.



But here's what the researchers wrote for the summary Abstract at the top of their paper.  They said:  "This paper studies the synergies between memory corruption vulnerabilities and speculative execution vulnerabilities.  We leverage speculative execution attacks to bypass an important memory protection mechanism, ARM Pointer Authentication, a security feature that is used to enforce pointer integrity.  We present PACMAN, a novel attack methodology that speculatively leaks PAC verification results via" - and I'll be explaining all this in a second - "via microarchitectural side channels without causing any crashes.  Our attack removes the primary barrier to conducting control-flow hijacking attacks on a platform protected using Pointer Authentication."



In other words, pointers are what the system uses to, well, obviously to point to things; but, for example, to control its flow of control.  So if you're able to change the pointers and not have that change detected, then you're back where you were being able to, without this protection, obviously, being able to, if you're able to get down into the kernel, to do things.



So they said:  "We demonstrate multiple proof-of-concept attacks of PACMAN on the Apple M1 SoC [System on a Chip], the first desktop processor that supports ARM Pointer Authentication.  We reverse engineer the TLB" - the Translation Lookaside Buffer, which is the way memory is organized these days - "on the Apple M1 SoC and expand microarchitectural side-channel attacks to Apple processors."  Meaning before it had been Intel stuff; right?  So now Apple.  "Moreover, we show that the PACMAN attack works across privilege levels, meaning that we can attack the operating system kernel as an unprivileged user in user space."



And I loved how they set this up and framed this work in their paper's short introduction, so I want to share it, too, just two paragraphs.  They said:  "Modern systems are becoming increasingly complex, exposing a large attack surface with vulnerabilities in both software and hardware.  In the software layer, memory corruption vulnerabilities such as buffer overflows can be exploited by attackers to alter the behavior or take full control of a victim program.  In the hardware layer, microarchitectural side-channel vulnerabilities such as Spectre and Meltdown can be exploited to leak arbitrary data within the victim program's address space.



"Today it is common for security researchers to explore software and hardware vulnerabilities separately, considering the two vulnerabilities in two disjoint threat models.  In this paper, we study the synergies between memory corruption vulnerabilities and microarchitectural side-channel vulnerabilities.  We show how a hardware attack can be used to assist a software attack to bypass a strong security defense mechanism.  Specifically, we demonstrate that by leveraging speculative execution attacks, an attacker can bypass an important software security primitive called ARM Pointer Authentication to conduct a control-flow hijacking attack."



Okay.  So what is ARM Pointer Authentication?  We've talked, and Leo was just talking about canaries; right?  We've talked about related attack mitigation measures in the past.  Remember stack cookies?  They're a form of canary.  The idea with stack cookies is that the system places little bits of unpredictable crypto hashes on the stack as a means for detecting when a buffer overrun has just occurred.  So before executing a return from a subroutine, which is when the overwritten attack code would get executed, the bit of code performing the return first checks the stack for the proper cookie, the proper canary, and only executes the return if the value found at the location matches what's expected.



The idea is that attackers don't have nearly sufficient visibility into the system to know what the cookie should be, so their attempt to overrun a buffer on the stack is inherently somewhat brute force.  You know, kind of a blunderbuss approach.  The cookie allows their overwriting to be proactively detected before the contents of the stack is trusted and acted upon.



What the ARMv8.3 architecture introduces is a clever means of adding a tiny authentication hash - actually it can be too tiny, as it turns out, we'll get there in a second - adding a tiny authentication hash into ARM pointers as a means for detecting any unauthorized changes to that pointer.  As we know, the ARM architecture is 64-bits wide.  That means that it will have 64-bit pointers.  But 64-bits is - I can't even pronounce this number.  We now know we could.



LEO:  It's a vigintillion.



STEVE:  It's a virgintillion, yes.  It's 18,446,744,073,709,551,616.



LEO:  It's a big number.



STEVE:  That's how many 64 bits gives you.  Said another way it's, get this, 18.446 billion gigabytes of pointer space. 18.4456 billion gigabytes of pointer space in 64 bits.  Since no one has nearly that much RAM in 2022, we'll see what happens 10 years from now, that means that a large number of the high-order bits of 64-bit ARM memory pointers are always going to be zero.  So the clever ARM engineers realized that they could set a boundary point in their pointers.  To the right of the boundary is a valid pointer which is able to address as much memory as a system has.  And to the left of the boundary, using all of the bits, the high order bits, which are always going to be zero because they can't ever, because the system doesn't have enough memory for them to ever come into use, those can now be cleverly used to validate the bits on the right.



Here's how the MIT guys describe this.  They said:  "Memory corruption vulnerabilities pose a significant security threat to modern systems."  Right.  We're talking about that pretty much all the time on this podcast.  They said:  "These vulnerabilities are caused by software bugs which allow an attacker to corrupt the content of a memory location.  The corrupted memory content, containing important data structures such as code and data pointers, can then be used by the attacker to hijack the control flow of the victim program.  Well-studied control-flow hijacking techniques include return-oriented programming (ROP)" - which we've discussed at length in the past - "and jump-oriented programming (JOP).



"In 2017, ARM introduced Pointer Authentication (PA for short) into ARMv8.3 as a security feature to protect pointer integrity.  Since 2018, Pointer Authentication has been supported in Apple processors, including multiple generations of mobile processors and the recent M1, M1 Pro, and M1 Max chips.  Multiple chip manufacturers, including ARM, Qualcomm, and Samsung, have either announced or are expected to ship new processors supporting Pointer Authentication.  In a nutshell," they wrote, "Pointer Authentication is currently being used to protect many systems  and is projected to be even more widely adopted in the upcoming years.  Pointer Authentication makes it significantly more difficult for an attacker to modify protected pointers in memory without detection.  Pointer Authentication protects a pointer with a cryptographic hash.  This hash verifies that the pointer has not been modified, and is called a Pointer Authentication Code, or PAC for short."



Okay, so this is really clever.  It essentially makes 64-bit pointers self-authenticating such that any change to any of the pointer's 64 bits will trigger a protection fault and cause the operating system to immediately terminate the offending program.  The Achilles heel of this approach is that there are not a large number of authentication bits available.  In the case of macOS v12.2.1, which extravagantly uses 48 of the total 64 bits for valid pointers, only 16 bits remain available for the authentication role.  As we know, 16 bits is a famous number.  It's 64K.  That's the total number of possible combinations that can be taken by any 16-bit value.  Again, here's what the researcher's describe.



They said:  "Considering that the actual address space in a 64-bit architecture is usually less than 64 bits" - yeah, less than that 18 billion gigabytes - "for example, 48 bits on macOS 12.2.1 on M1," they said, "Pointer Authentication stores the Pointer Authentication Code (PAC) together with the pointer in these unused bits.  Whenever a protected pointer is used, the integrity of the pointer is verified by validating the PAC using the pointer value.  Use of a pointer with an incorrect PAC will cause the program to crash.



"With Pointer Authentication in place, an attacker who wants to modify a pointer must correctly guess or infer the matching PAC of the pointer after they've modified it.  Depending on the system configuration, the size of the PAC, which ranges from 11 to 31 bits, may be small enough to be brute forced.  However, simple brute forcing approaches cannot break Pointer Authentication.  The reason is that every time an incorrect PAC is used, the event results in a victim program crash.  Restarting a program after a crash results in changed PACs, as the PACs are computed from renewed secret keys every time the program runs.  Moreover, frequent crashes can be easily captured by anomaly detection tools."



The breakthrough that these guys achieved was finding a way to successfully brute force probe for the proper authentication code given a modified pointer.  By doing that, they're able to make up their own PAC for whatever pointer value they need.  So, the final piece of description I'll share from their paper is the PACMAN Attack.



"In this paper, we propose the PACMAN attack, which extends speculative execution attacks to bypass Pointer Authentication by constructing a PAC oracle.  Given a pointer in a victim execution context, a PAC oracle could be used to precisely distinguish between a correct PAC and an incorrect one without causing any crashes."  In other words, exactly what's needed for brute forcing.  They said:  "We further show that with such a PAC oracle, the attacker can brute force the correct PAC value while suppressing crashes and construct a control-flow hijacking attack on a Pointer Authentication-enabled victim program or operating system.



"The key insight," they said, "of our PACMAN attack is to use speculative execution to stealthily leak PAC verification results via microarchitectural side channels.  Our attack works relying on PACMAN gadgets.  A PACMAN gadget consists of two operations:  First, a pointer verification operation that speculatively verifies the correctness" - or not, typically - "of a guessed PAC; and, two, a transmission operation that speculatively transmits the verification result via a microarchitectural side channel."  So these guys have gotten very clever.  But then bad guys are, too.



"The pointer verification operation is performed by an authentication instruction, one of the new instructions introduced in ARMv8.3, which outputs a valid pointer if the verification succeeds, and an invalid pointer otherwise.  The transmission operation can be performed by a memory load/store instruction or a branch instruction taking the output of the pointer as an address.  If a correct PAC is guessed, the transmission operation will speculatively access a valid pointer, resulting in observable microarchitectural side effects.  Otherwise, the transmission step will cause a speculative execution due to accessing an invalid pointer."  They said:  "Note that we execute both operations on a mis-speculated path.  Thus, the two operations will not trigger architecture-visible events" - god, that's clever - "avoiding the use where valid guesses result in crashes."



So they've basically taken what we've learned in the last few years about microarchitectural side effects, the idea that subtle variations in branches taken and not taken can be sensed across process because they're left in the hardware that everyone shares, and you're not going to cause a crash because they're deliberately arranging that these tests occur in the speculation path which the processor never executes.  So the side effects are generated by the speculation portion of the engine which is executing ahead of where the processor is actually going.  The processor doesn't take that path, so no mistake, no crash occurs.  No exception fault is triggered.  Yet it still leaves a trace, a footprint in the microarchitecture which they're able to sense.  This is just so cool.



Okay.  So now we bottom-line it.  What does all this actually mean for those relying upon the security of Apple's M1-based devices?  In their own FAQ, the researchers provide some reality-check answers.  Does this attack require physical access, they ask themselves?  "No.  We actually did all our experiments over the network on a machine in another room.  PACMAN works just fine remotely, even if you have unprivileged code execution."  Should I be worried?  "As long as you keep your software up to date, no.  PACMAN is an exploitation technique.  On its own it cannot compromise your system.  While the hardware mechanisms used by PACMAN cannot be patched with software features, memory corruption bugs can be."



Can I tell if someone is using PACMAN against me?  They replied:  "Much like the Spectre attack our work is based on, PACMAN executes entirely in the speculative regime and leaves no logs.  So probably not."  They ask, why do you need a software bug to do the PACMAN attack?  And they answer:  "PACMAN takes an existing software bug (memory read/write) and turns it into a more powerful primitive (pointer authentication bypass).  For our demo, we add our own bug to the kernel using our own kernel module.  In a real-world attack, you would just find an existing kernel bug.  Our demo only uses the kernel extension for adding a software bug.  The entire attack runs in user space.  So an attacker with an existing bug could do the same attack without the extension that their proof of concept provides."  Finally, Is PACMAN being used in the wild?  And they replied:  "To our knowledge, no."



Now, Apple's response didn't offer much.  They officially said:  "We want to thank the researchers for their collaboration as this proof of concept advances our understanding of these techniques."  Yeah, uh-huh, I bet.  "Based on our analysis, as well as the details shared with us by the researchers, we have concluded this issue does not pose an immediate risk to our users and is insufficient to bypass device protections on its own."



Okay.  That was carefully worded to be exactly factually correct.  And it is.  Generally speaking, the common M1 chip user does not have anything to worry about more after this than they did before.  But the authors never suggested that this could be used in any form of standalone attack.  What this does mean is that successful attacks against current and future ARM chips equipped with Pointer Authentication will need to be more complex in order to also defeat this new extra layer of protection.  Because the protection is there, and it's good.  But the attacks are going to have to be more complex to defeat it, and they can.  As is, Apple's M1 chips based on the ARMv8.3 architecture are thus significantly more difficult to successfully attack than other systems which lack this additional, though imperfect, layer of protection.



And I'll close with this observation:  Apple's allocation of 48 bits for their pointers, which affords access to more than a quarter million gigabytes of RAM, is obviously excessive in the extreme.  Before this paper was published, Apple's engineers probably thought, what the hell, we have 64 bits of pointer space, so we'll use the lower 48 for pointing and the upper 16 for pointer authentication.  Assuming at the time that it was not possible to brute force probe for the correct value of those 16 spare bits, Apple's engineers likely thought that there would only be one chance in 65,536 of an attacker correctly guessing the proper hash for a modified pointer; and a wrong guess, the first wrong guess, any wrong guess, would cause the victim program to be immediately terminated.



But now we know that 16 bits - now we know that 16 bits is not sufficient.  So Apple could choose to significantly strengthen their Pointer Authentication by tightening down on the active pointer space, maybe even setting it up dynamically based upon the amount of RAM a system actually contains.  A large system, even a large system with 64 gigabytes of RAM requires pointers to consume only 36 bits.  36 bits lets you address 64GB of RAM.  That leaves 28 bits for authentication, which is 4,096 times more difficult and time consuming to brute force, with no other overhead or cost to the system.  In any event, the sky is once again not falling, and all is well.



LEO:  Good.  Could it fall in the future?  



STEVE:  Well, when you consider that every time we attempt to plug in a USB plug it fails, it could be that the attacker will guess right the first time.



LEO:  Yes, it could be.



STEVE:  Even though they've got one chance in 65,536.  I mean, I've never plugged in a USB plug correctly the first time.  It just it never happens.



LEO:  Never happens.



STEVE:  Which demonstrates, like that black cat, remember, that kind of flickered.  Two cats walked by the matrix, and that represented a glitch in the matrix?



LEO:  Yeah.



STEVE:  We've got a glitch here.  You know, the best thing about USB Type C is that this may actually be an attempt to cover up this bug.



LEO:  Hiding the glitch.  I think you're right.



STEVE:  Now it doesn't matter.  You will get it right the first time, every time.  And with time those old Type A connectors, they'll fade away, and this bug in reality that made it very possible, I mean, somewhere someone is thinking, boy, those humans are dumb.  Every time they try to plug in those USB things...  



LEO:  They never notice.



STEVE:  No.  We can't change the code in the simulation now.  It's running.  We cannot patch it on the fly.  So we just have to put up with this bug in USB Type A connectors.  But now those are going to fade into the past.  We're going to get Type C, and nobody will even know.  



LEO:  Ah.  See?  Brilliant.  Brilliant.  That matrix, it knows what it's doing.  Well, thanks for joining us on the red pill edition of Security Now!.  Mr. Steve Gibson lives at @SGgrc on the Twitter if you want to leave him a DM.  He also has a little website he calls GRC.com, the Gibson Research Corporation.  What he's researching right now, SpinRite 6.1.  SpinRite 6 is current, world's best mass storage maintenance and recovery utility.  6.1 is imminent.  And if you buy 6.0 now you'll get 6.1 when it comes out.  You can also participate in the development of 6.1.  You can also find this show there.  He has a couple of unique versions, a 16Kb audio version.  Why don't you do 8-bit?  Why not just go all the way, do 4-bit audio?



STEVE:  We would call that the Minecraft edition.



LEO:  Yeah, 8-bit audio.  16-bit audio, which is smaller, obviously, than the normal 64Kb audio.  He also has transcripts which are actually very handy for searching and reading.  And of course the 64Kb audio, the standard version.  That's all at GRC.com.  While you're there, check out all the wonderful goodies Steve makes available to you for free.  And of course his bread and butter, SpinRite.



We have audio and video on our website, TWiT.tv/sn.  There's a YouTube channel with a video dedicated to Security Now!.  Just search YouTube for Security Now!.  You'll also be able to subscribe because it's a podcast.  As soon as we're done editing this thing, cleaning it all up, polishing it up, we'll put it out on the Internets, and you can get it just by subscribing in your favorite podcast player.  If your podcast player has ratings, leave us a five-star, would you?  Let the world know.  Everybody needs to know.



STEVE:  I mean, this was clearly a five-star episode.



LEO:  I don't know how you can get any better than this one.  But we'll find out next week, that's for sure.  We do this show every Tuesday.  We try to do it around 1:30 Pacific, if Leo isn't a laggard, a slug.  1:30 to 2:00, something like that, Pacific.  1:30 is 4:30 Eastern, 20:30 UTC.  The livestreams are at live.twit.tv.  There are chatrooms.  The IRC chatroom is free and open to all at irc.twit.tv.  I will be back here next week.  I know you will, too, Steve, for Episode 876.  More revelations.



STEVE:  And the answer to the big question, why is double encrypting not only twice as strong, but exponentially as strong?



LEO:  Takes one day to do the outer shell, one day to do the inner shell.  It should take the same amount...



STEVE:  Seems right to me.



LEO:  Should be one plus one; right?



STEVE:  Seems right to me.



LEO:  What do the boffins say?  One times one?



STEVE:  No, it would be 2^256 times stronger.



LEO:  A lot better.



STEVE:  Way better.



LEO:  Why is that?



STEVE:  Be like summing the keys, like having a 512-bit key instead of a 257-bit key.  I know.



LEO:  That's a head scratcher.



STEVE:  I know.



LEO:  Get to work on that.  Do you want people to DM with you their thoughts?



STEVE:  They can certainly DM me.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#876

DATE:		June 21, 2022

TITLE:		Microsoft's Patchy Patches

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-876.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  We begin this week by answering last week's double-decryption strength puzzler.  I then take a look at what's currently known about FIDO2 support in LastPass and Bitwarden.  We look at last week's Mozilla announcement of Total Cookie Protection for Firefox, which doesn't appear to be working for me, and invite everyone to test their browsers.  DDoS attacks have broken yet another record, another NTLM relay attack has been uncovered in Windows, Apple messed up Safari five years ago, more than a million WordPress sites were recently force-updated, and another high-severity flaw was fixed in a popular JAVA library.  Then after sharing a bit of miscellany and some fun closing-the-loop feedback, we look at the awareness the rest of the security industry is sharing regarding the deteriorating quality of Microsoft's security management.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's raring to go.  We're going to talk about Passkeys and what the password managers LastPass and Bitwarden have to say about it.  We'll also talk about Firefox's new Total Cookie Protection, which seems to be something less than total.  And Microsoft, its attitude toward security is, shall we say, a little casual?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 876, recorded Tuesday, June 21st, 2022:  Microsoft's Patchy Patches.



It's time for Security Now!, the show where we protect you, your loved ones, your privacy online with this man right here, Mr. Steven "Tiberius" Gibson, the host of our show.  Hello, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again, once again on a Tuesday.  I don't think we ever miss our podcast.  Someone was saying about holidays, talking about that and days off.  Oh, I know what it was, it was our neighbors talking about how if a holiday occurs, then the trash pickup gets skewed by a day.



LEO:  Right, yes.



STEVE:  And what would normally be like on a Saturday when we get our leaves picked up, that becomes Monday.  And I said we once were doing the podcast on Mondays, and it used to really bug me when we'd have like a holiday.  And now we're protected, we're on protected Tuesday because you might have your day on holiday as we did yesterday, it was a federal holiday, of course.  It was Juneteenth.  But the podcast goes on.



LEO:  I do have to warn you, let's see, no, I guess Independence Day is a Monday this year.  So we're all right.  Remember last year.  Was it last year?  There was one year you hated it.  We said there's no show, and you hated that.  So no, no day off for you in July.  The fifth of July we will be here.



STEVE:  Thank you.  That's good.  So we're on Episode 876, which I titled "Microsoft's Patchy Patches" for reasons that will be made clear.  It turns out that for just sort of the alignment of the planets there were several completely unrelated stories in this past week of other security firms saying, you know, Microsoft's really not doing the job like they used to.  And of course that's not news to anybody who has listened to me rant about that over and over and over.  But like, okay, it's not just me.  So I thought that was interesting.  And there wasn't really that much else that happened this week.  Lots of other sort of interesting tidbits, but nothing that wanted to grab the title away from that.  So that's where we ended.



We're going to begin this week by answering last week's double decryption strength puzzler.  Then I take a look at what's currently known about FIDO2's upcoming support in both the LastPass and the Bitwarden password managers, which I know pretty much covers our listener base.  We look at last week's Mozilla announcement of Total Cookie Protection in Firefox and wonder about why it doesn't appear to be working, at least for me.  And I'll invite everyone to test their own browsers because we have a simple way to do that.



DDoS attacks have broken yet another stunning record.  We have another NTLM (NT LAN Manager) relay attack uncovered in Windows.  Apple messed up Safari five years ago, which sat that way until it was just found at the beginning of the year by Google's Project Zero.  Interesting story there.  More than a million WordPress sites were recently force-updated to resolve a very bad problem they had.  And we have another high-severity flaw which was fixed in a popular Java library.  Of course Log4j was famous earlier this year.  Then after sharing a bit of miscellany and a bunch of fun closing-the-loop feedback, we look at the awareness the rest of the security industry is sharing regarding the apparent deteriorating quality of Microsoft's security management.  So I think another interesting podcast for our listeners and a very apropos Picture of the Week.



LEO:  And of course you probably heard us talk on MacBreak Weekly about the wonderful Microsoft Defender which is being added to iOS, even though all it can do is make recommendations because Apple won't let it do anything else.



STEVE:  Right.



LEO:  I love it.



STEVE:  Our Picture of the Week.



LEO:  I love it.



STEVE:  Now, you know, in this day and age of virtual reality modeling, you see a picture like this, and you just - you can't tell if it was something someone whipped up algorithmically or if it's like an actual photo of something that exists in the physical world.  It looks absolutely authentic.  And in fact there's even a reflection of the person who appears to be taking the picture in the polished black headstone.  You can sort of see him holding, with his left hand, holding his phone, taking this picture.  But regardless, this is a headstone, or a memorial, I guess maybe, for Internet Explorer.  We've got the big modernized eGlobe sort of logo, and then a little Japanese something character, and then Internet Explorer.  And we get the date of birth and the date of death.  1995.8.17, so August 17, 1995 through June 15th, 2022.



And the best part of this whole thing, this beautiful stone headstone sitting on a granite slab, is sort of the - often on headstones there's a little slogan or something about who this person was or what they meant to people.  This one says "He was a good tool to download other browsers."  And indeed, many people launched it exactly once.



LEO:  That's the only reason, yup.



STEVE:  To go get a copy of Firefox or go get a copy of Chrome.



LEO:  I love it.



STEVE:  And, yes, so...



LEO:  Good epitaph for Internet Explorer.



STEVE:  Put it to rest.



LEO:  Yes.



STEVE:  Finally.  Okay.  So last week's key-strength puzzler.  The question that we left our listeners with, and boy did it get a lot of response through Twitter and our Security Now! feedback, it reduces to whether a divide-and-conquer attack can succeed.  Way back in 2011, the WPS, we talked about this at the time, the WPS, the so-called WiFi Protected Setup protocol, was found to be vulnerable to this style of attack.  Remember that the way it was supposed to work was that a user would press a button on their WiFi access point to enable this feature.  Then they would enter a preset 8-digit PIN into a device they wished to connect to their router.



And technically, since the eighth digit was a check digit, which was so dumb, like what a dumb way to waste a digit because if it didn't work you could just enter it again, you didn't need a check digit.  Anyway, the eighth digit was a check digit.  So only really seven digits were important because the eighth could always be calculated from the first seven.  Okay.  Since seven digits can have 10 million combinations, brute forcing those 10 million combinations was deemed impractical in 2011, within the timeframe that WPS would be enabled and so forth.  So it was thought, okay, great.  A 10-digit guess is strong enough.



But two researchers, Stefan Viehbock and Craig Heffner, who we talked about at the time, discovered a flaw in that WiFi protocol because all eight digits were not sent to the access point at once.  The first four digits were sent before the second four; and, worse, the router's behavior would change if the first four were not correct.  Now, how this ever got past the almighty Wi-Fi Alliance will forever be a mystery.  But then again, it was only one of many mistakes that made it past the Wi-Fi Alliance through the years.



In any event, the fact that the router's behavior would change if the first four were wrong, meant that it wasn't necessary to guess all seven or eight digits at a time.  It was possible to divide and conquer.  It was possible to guess just the first four, of which there are only 10,000 combinations; then, having found the first half, separately brute force the final three, since the last digit can be calculated from the preceding seven.  Since that's 1,000 maximum for the second three, we have a grand total maximum of only 11,000 possible guesses, reduced from a previously believed 10 million.



Okay.  So Erik Osterholm's puzzler from last week amounts to the same question.  A ciphertext is encrypted with an effective 512-bit key length by first encrypting the original plaintext with a 256-bit key, the first half of the whole 512-bit key; then by encrypting it again with another 256-bit key, that is, the second half of the 512-bit key.  And of course he was asking, why is that not only twice as strong, rather than exponentially stronger?



So if we can brute force decrypt that second encryption in X-time, then brute force decrypt the first encryption also in X-time, what Erik asked is why isn't this strength just 2X rather than X times X?  And the correct answer, which all of our many listeners who wrote in answered correctly, amounts to whether it's possible to perform the same sort of divide-and-conquer attack as was possible, which broke the WPS setup protocol.  Remember that the weakness that was exploited by the WPS attack was that there was some affirmative feedback after the first half of the guess was made about whether or not that first half guess was correct.  And that's what's missing from Erik's double-encryption thought experiment.



Here's how I would phrase it formally:  The result of any encryption by a high-quality cipher such as AES's Rijndael is indistinguishable from entropy.  Therefore, the result of the first encryption will be indistinguishable from entropy.  So when following Erik's suggestion and question, and performing the first decryption, how can the attacker, who is using brute force key guessing, know when their decrypted guess is correct, when both a correct guess and all other incorrect guesses appear equally random? In other words, it is not possible to divide and conquer.  The only way to decrypt the double-encrypted plaintext would be to make a first guess at the outer key.  Then the attacker would need to try all possible inner keys, that is to say, all 2^256 of them, to see whether any of them worked.  Assuming that none did, all of that work would then be discarded; the next outer key would be chosen; and again, all 2^256 possible inner keys would need to be tried.



In other words, only when both the first 256-bit key and the second 256-bit key were simultaneously correctly applied would the correct plaintext be restored.  So this is, indeed, 2^256 times 2^256, which is 2^512 maximum possible brute force guesses needed.  And thus, Erik, the answer to your question, and the answer to the teaser that I got a lot of great feedback about.  And a lot of people enjoyed the idea of us having that fun.



Okay.  I just turned the AC up.  It was getting a little warm in here.  So third-party authenticators.  In the aftermath of the Apple's, Google's, and Microsoft's announcements of their forthcoming support for FIDO2 and passkeys authentication, we've been talking about what all this means.  And I believe that we've settled into exactly the right understanding.  It's relatively quick and easy for those three major publishers to add this support to their clients, as they've all announced they're going to.  And when they've done so, everyone will be just one software update away from having that client-side technology in their hands.  But it's a bit like creating the first shortwave radio.  There's no one else to talk to yet.  So the existence of all of those clients won't be very useful initially.  The heavy lift will be getting the millions of individual web servers updated to support the WebAuthn standard at their end, since any use of Apple's, Google's and Microsoft's clients will require that, too.



And I believe that we've also identified that the biggest usability hurdle for the practical use of FIDO2's private passkeys is the need for their dynamic synchronization.  And now that the world - it's been interesting to watch.  Now that the world has sobered up after the intoxicating passkeys announcement parties, others are realizing what we immediately saw as a problem.  A story in Fast Company is titled:  "There's a big problem with Apple and Google's plans to nix passwords."  And 9to5Mac's headline read:  "A world without passwords could further lock users into Apple and Google ecosystems."  Yeah.  Like we've been saying.



Those stories note that:  "FIDO's current proposal has no mechanism for bulk-transferring passkeys between ecosystems.  If you want to switch from an Android phone to an iPhone, or vice versa, you won't be able to easily move all your passkeys over."  And they didn't mention Windows, but we know the same problem will exist there.  "We don't really have a batch export method right now," says FIDO Alliance executive director Andrew Shikiar.  He said:  "I think that's probably a future iteration."



Wow.  So those FIDO guys were really not thinking through the usability angle of all this.  You know, saying, "We'd like you all to adopt this half-baked solution today, and we'll worry about exporting your locked-in keys later."  The reports that have been published also explain:  "The fear is that, if users can easily move all their passkeys between providers, hackers may try to exploit this capability.  For now, it's unclear when or how FIDO might address that problem."



And then they quote the president, the president of the FIDO Alliance.  Sam Srinivas, Google's product management director for secure authentication, who's also the president of the FIDO Alliance, says:  "It's very hard to do it safely from the get-go because, if we give a mechanism without great care for someone to export all these keys, you know who's going to show up first for that."



LEO:  That's a good point.  That's actually a really good point I hadn't thought about.  Because it would have to export that in the clear; right?  



STEVE:  It's got to do it, well, I mean...



LEO:  No.



STEVE:  No.



LEO:  Because your input's going to be a secondary FIDO2 server, so...



STEVE:  Right.  I mean, it's clear there are ways this could be done.  But again, because it was like the FIDO concept was never meant to scale this way.  It was scaled by force because it didn't go as FIDO1.  It just didn't - it never got off the ground.  So they did one without really thinking it through.  Anyway, so in other words, we're going to be quite happy, they're saying, with lock-in.  And we're going to tell users that it's too dangerous to allow them to move their keys around themselves.



LEO:  That's a huge problem.



STEVE:  Of course it is.  As a cross-vendor user myself, I need Apple and Windows to sync.  And I don't see that happening without either a third-party synchronization vendor, which is a thing that could exist, or third-party FIDO2 passkeys being supported by a password manager.  Which brings us to two password managers which I've been looking into and want to briefly discuss: LastPass and Bitwarden.



As everyone knows, LastPass was previously a many-years sponsor of this podcast and of the TWiT Network.  And Bitwarden is currently a sponsor and offers a compelling array of solutions.  So the question I had was where do those two fit within this new and evolving era?  As I believe I mentioned last week, I was annoyed with LastPass because their most recent blog posting, from Monday before last, which I was hoping would provide some clarification, left me feeling more confused than I was before.  Everything they say feels sort of coy and blurry. Nothing they say just tells us what is going on.  Here's an example direct quote from an announcement of an upcoming webinar that they'll be hosting two days from now, this coming Thursday.



They wrote:  "It's time to envision a world without passwords, a world that removes the password-related friction that prevents users from securing and managing their passwords easily and automatically.  True FIDO2-compliant passwordless access to every device, browser, website, and app will take years to develop" - okay, right - "but LastPass can get you there sooner."  What?



LEO:  Huh?



STEVE:  Yeah, right.  Okay.  How?  "Join us to learn how LastPass is enabling an end-to-end passwordless experience for the LastPass vault and all sites stored within."  Okay.  "What will this enable you to do?  Reduce password-related friction for employees, increase usage and adoption, set stronger policies and increase security, fewer lockouts for employees and resets for IT.  Hear from LastPass CTO Christofer Hoff as he demonstrates a passwordless login experience, and discusses future plans for FIDO2 authenticators like biometrics and security keys."  Okay, well, first of all, FIDO2 authenticators.



LEO:  It's not passkeys; right?



STEVE:  It's like it's just - it's all a big blur.  So as I said, this is like their recent blog posting which doesn't actually say anything.  And you could - I guess you could sort of forgive them here because they're teasing their webinar in two days.  But the blog posting was the same.  It was like, it made up new terms and used them in weird ways.  Like what does an authenticator have to do with biometrics?  Those are two different things.  But they used them together.  So anyway, for what it's worth, I did want to let our listeners know that there will be a webinar in two days.  I've got the link to it in the show notes.  I made it this week's shortcut, so grc.sc/876 will bounce you over to a signup page.  I'll be watching to see what we learn from Christofer.  We know Christofer.  You and I have met him, Leo.  We were onstage together a couple years ago, just before COVID.



LEO:  Oh, yeah.  No, that's not the guy we were on with.



STEVE:  Oh, it isn't.



LEO:  No.



STEVE:  Oh, okay.



LEO:  I think this is a new guy.  They've gone through some ownership changes.



STEVE:  Well, yeah.  They're owned now by an equity...



LEO:  No.  No, they were.



STEVE:  Oh.



LEO:  And now they're spun off.



STEVE:  From the private equity firm?



LEO:  Yeah.



STEVE:  That didn't take long.



LEO:  No.  So it's unclear, it's really unclear what's going on with them.  And the people that we know are gone, pretty much.



STEVE:  Okay.  Well, we know that Joe has long since gone.



LEO:  Yeah, Joe Siegrist, the creator, is gone.  His niece was there.  I think she's gone.  And then we knew, we were with the CISO, I think, or the CTO, on that panel.  But I think he left, as well.



STEVE:  In Boston, yeah.



LEO:  Yeah, I think he left, as well.  So I don't know who these people are.  But we should watch.



STEVE:  So I wanted to watch because I know that just for inertia's sake a lot of our listeners are still there.  The good news is Bitwarden is a member of the FIDO Alliance.



LEO:  Well, that's a good sign.



STEVE:  Yes, it is.  And so of course Bitwarden is our current password manager sponsor.  So I think the problem that any third-party logon system reasonably has had is the chicken-and-egg problem.  Which makes it difficult for them to invest in any system which cannot actually be used until it's supported by the world's servers.  So the flipside of that is that the clear and obvious need for cross-vendor passkey synchronization  which it's more and more clear every week, it's now very clear - FIDO and Google both just throw up their hands saying, "Yeah, that's a problem."  And that creates the biggest need and push for third-party passkey managers that there's ever been.



So I wanted to understand where Bitwarden stood.  I did some digging and found some dialogue in their community forum under the title "Bitwarden Passkey (How does Bitwarden fit into the new Microsoft/Google/Apple Passkey initiative?)"  So the person posting this wrote the question:  "Microsoft, Google, and Apple have announced support for the FIDO2 passwordless initiative that media are calling 'Passkeys.'  Because Passkeys creates a new key pair for each website login, there is the issue of moving all these key pairs among devices."  He says:  "I'm sure that Google will do that for Android and Chrome, and Apple will do it for their iPhones and Macs.  But what about between Android and Apple or Linux?"  And not to mention Windows.



"Would," he asks, "Bitwarden be able to support the new Passkeys cross-platform, like it does with current passwords?"  He says:  "I want to sync Android to Linux desktop, and I will wait for Bitwarden to support this, if the feature will be added."  And that's how I think a lot of us feel.  You do not want to get stuck with non-exportable passkeys.



LEO:  Could a third party like Bitwarden do passkeys and become, I mean, it's still not exportable.  So if you decided you didn't like Bitwarden, you'd be stuck there.  But at least it would let you use an Android phone or an iPhone or Windows or Mac.



STEVE:  Yeah, yeah.



LEO:  I mean, that would be a big advantage.  So could they do this?  They'd keep the database?  They'd have to have some sort of biometric, ideally some sort of biometric login; right?  Or you could use a YubiKey.  I mean, I can use a YubiKey.  I do in fact use a YubiKey with my Bitwarden.



STEVE:  Well, as we know, it's now possible for any apps on those platforms like Android and iOS to leverage the built-in biometrics on the device.



LEO:  True.  So in fact they do that.  When I open Bitwarden on those devices, it does a face recognition, and we're in.  On my computers when I set up a new account I have to use the YubiKey the first time on a new system.  



STEVE:  Right.  And so for security they might want to enforce the use of some affirmative device in order to protect.



LEO:  Right.



STEVE:  But there is - but they could synchronize passkeys in the cloud exactly as they synchronize usernames and passwords right now.  And so I think that's going to be the solution.  I don't think Apple is going to address this.  I don't think Google are going to address it.  They're both saying, I mean, it turns out the president of FIDO is the Google guy.  And he's saying, oh, I know.



LEO:  I thought and I guess I was wrong, I was told that Apple had said there is a way to get these out.  But maybe...



STEVE:  A key at a time. 



LEO:  Oh, one key at a time.  Oh, well, that's...



STEVE:  Yes, in order to share a passkey.  And so the problem is when you authenticate to a new site, you want all of your ecosystem to be brought up to speed so that you can then go somewhere else, like to a different computer, and log into that site.  You don't want to have to manually send that passkey to each, like, cross ecosystem into the other world.  Or those two are never going to be synchronized.  And that's why I've been using the term "dynamic passkey synchronization."  It needs to be done for you on the fly.  And that is exactly what Bitwarden supporting this passkeys, FIDO2-style passkeys, would mean.  Anyway, the answer...



LEO:  I think maybe, and this is a complete conspiracy theory, this will open the idea to maybe there's a better way.  And maybe somebody's just going to come across SQRL and say, actually there is a better way, and let's just do this.  Because it is better in every respect.



STEVE:  Yes.  It would be good.  It would be good if that happened.  So the answer in the forum is:  "Bitwarden does currently support FIDO2 WebAuthn for multifactor authentication in addition to your master password for vault unlocking."  In other words, when you use - and this has been in there for a while.  And again, they are already a member of the FIDO Alliance.  So they're actually being a WebAuthn server to accept a FIDO2 client's authentication as a very strong factor when you log in to unlock your Bitwarden vault.  And I have a picture in the show notes, but there is a Bitwarden blob, says:  "Two-step Login via FIDO2 WebAuthn | Bitwarden Help & Support."  So that's been in Bitwarden for some time. 



And so this guy finishes:  "Bitwarden does not support using these passkeys to log in in lieu of the password manager yet, but there is a current similar feature request for this to be supported."  And you've got to know that the wizards at Bitwarden understand they've got an opportunity here to get going on this.



LEO:  This is one advantage you have as an open source project.  Somebody could issue a pull request and implement it.  I mean, if the community wants to support it, they can add it.  I mean, it sounds like in addition to your master password for vault unlocking, that's just for - basically that's one password.  It supports WebAuthn for one password, your master password.



STEVE:  Exactly.  Exactly.



LEO:  Yeah, yeah.  Which is nice.  In fact, you could use a FIDO2, there are FIDO2 YubiKeys you could use for that purpose.



STEVE:  Right, right.  And so what this does mean is that somewhere in Bitwarden there are already people who are fully FIDO-aware.  And what we need is for them to reverse roles.  Right now they're being a WebAuthn provider for a FIDO2 authenticator.  We need them to become a FIDO2 authenticator  talking to WebAuthn providers at websites.



LEO:  That would be awesome, yeah.



STEVE:  And that doesn't seem like that big a reach to me.  So anyway, the roadmap page has a great deal of discussion of this.  So I'm sure it's something that they're aware of now.  And again, you really can't fault any password manager for not doing it preemptively.  I mean, I did.  But I just did it as a proof of concept to demonstrate this is the way we can solve this problem.  And I knew that only two sites or three or four in the  world were going to be able to use SQRL.  But my hope was that by showing how it could be done, that would get the world going.



And it may be that showing that there's a better solution, as you said, Leo, may still get the world to say, hey, you know, why don't we just do this?  And it's not that big a reach because the WebAuthn protocol optionally supports the crypto that is key to SQRL's operation, that is, it allows it to use deterministic keys rather than keys that are completely random.  So anyway, we'll see.  And wow.  We're at 36 minutes in.  Let's take our first break.



LEO:  Holy cow.  How does that happen?  It's amazing.  Thank goodness we have advertisers so we can pause and give you a pause that refreshes.



STEVE:  Yeah, to rehydrate.



LEO:  Yes.  You're going to get that thing that Alex Lindsay was talking about?



STEVE:  I did go do some shopping.  Except that...



LEO:  A little tempting, eh?



STEVE:  Except that Lorrie and I both use SodaStreams that carbonate, and it looks like it doesn't like to have a carbonated beverage.



LEO:  Oh, no, I'm sure it doesn't.  So you're drinking fizzy water when you drink water.



STEVE:  I am.  Yup.  In fact...



LEO:  You prefer that.



STEVE:  I just cracked the seal, and it went "pshhhhhhh."



LEO:  Yeah.  You prefer that.



STEVE:  Yeah, I like the taste.  I like the taste a lot.



LEO:  I guess it's slightly acidic; right?  So...



STEVE:  It's slightly acidic.



LEO:  Remember I bought the tank, I was going to do that whole SodaStream thing.



STEVE:  Yeah.



LEO:  Bought the tank, and we couldn't find anybody to fill it.  So I basically pushed it - I palmed it off on Mikah.  The tank is still under his desk.  I said, "Here's the tank.  Here's the nozzle.  You figure it out."  So he's going to find - because he said, well, let's share.  That was his mistake.  He said, "Let's share.  We'll leave the tank at work, and then we can just bring in our bottles."



STEVE:  Yeah.



LEO:  So that's a good idea if we can just find somebody to fill it.



STEVE:  So for me it's a home brewer.  We have a...



LEO:  Yeah, we have plenty of those, but they don't - they want to use their bottles.



STEVE:  Oh.



LEO:  Which would be fine, I guess.



STEVE:  So you just...



LEO:  It has to be one with a siphon in it.



STEVE:  It's got to have the siphon tube because it needs to be - you don't want the gas off the top.



LEO:  Get the liquid.  Yeah, yeah.



STEVE:  You want the actual liquid off the bottom.



LEO:  Yeah, we'll have to figure it out.  So anyway...



STEVE:  So you want a non-cranky home brewer.



LEO:  Yes.



STEVE:  Is what you want.



LEO:  One who doesn't mind refilling my bottle.



STEVE:  Yeah, who's had some of his own brew and is relaxed about it now.



LEO:  Yeah, that's it.



STEVE:  Yeah.



LEO:  We're surrounded by breweries.  Everybody here makes wine and beer.  I mean, that's what you do in this town.  



STEVE:  Yeah, you've got grapes, yeah.



LEO:  But we couldn't find anybody.  But we'll keep looking.  We'll find it.  Anyway.



STEVE:  Okay.  So last Tuesday, Mozilla's headline read:  "Firefox rolls out Total Cookie Protection by default to all users worldwide."



LEO:  Woohoo!



STEVE:  I know.  And the big word that's so easily missed, as we know, is "default."  That's what's changed.  Until last Tuesday, "sequestered third-party cookies" were optionally available.  I had them enabled in my Firefox instances, or I guess I should say disabled, which I had to do manually, as I imagine many of this podcast's Firefox users also did after we talked about the option quite a while ago.  But until now it's been an option. Which means, of course, that the majority of Firefox users would not have had this enabled since it wasn't the browser's default setting.  Now it is.



What's most shocking to me is that it took us this long to get here because it is such an easy place to get to.  This change does not disable third-party cookies.  That's the secret.  It merely divides the single massive global cookie jar into individual per-domain or, as web engineers would say, same-origin cookie jars.  In that manner, any third party is welcome to set a cookie in anyone's browser.  But when that user goes somewhere else, the cookie jar will be switched to a new jar for that new domain.



And again, any third party will be welcome to set their cookie into that jar.  But what they will not be able to do is to see the cookie that they had previously set into the same user's same browser when they were visiting that previous domain.  And that simple measure kills cookies whose primary purpose had been cross-domain tracking.  You just do per-origin cookie jars.  Again, such a simple measure.  The idea that it took this long for it to happen is to me astonishing, but it finally happened.



LEO:  This is the number, this is the problem with cookies; right?  I mean, all of these cookie banners and all this missed the point.



STEVE:  Yes.



LEO:  Cookies are fine.  They're necessary.  It's third-party cookies that are the problem.



STEVE:  Actually, it's third-party cookies that cross domains.



LEO:  That could be read on a first-party site.



STEVE:  Yes.



LEO:  It's the Facebook Like button which gives Facebook a view into that site and who's visiting it.



STEVE:  Right.



LEO:  And even in the original Mozilla or Netscape specification for cookies, they said only the site that created the cookie can read it.  But they didn't anticipate this loophole that people would embed little bits of other people's sites on their web.



STEVE:  Exactly.  Exactly.  And so this is, I mean, this is such an easy change.  So, okay.  Now, all that said, it's not working for me under Firefox v101.0.1.



LEO:  Oh, no.



STEVE:  Which appears to be the latest.



LEO:  Oh, no.



STEVE:  Chrome is wonderful as I have it set currently.  And I'm sure I went in and tweaked something.  But Firefox under its so-called Standard Privacy & Security / Enhanced Tracking  Protection is doing nothing.  I set it to Strict, and still nothing.  I set it to Custom.  And then I had to tell it to block - I told it to block cross-site tracking cookies.  It still wasn't.  It was necessary for me to turn off all third-party cookies in order to get cross-domain third-party cookie blocking to work.



Now the question is, how do I know?  A piece of technology I spent a great deal of time developing many years ago, in fact I think it was in '08.  The pages have dates on them.  So, I mean, this is - I've been focused on this third-party cookie problem for a long time.  Any, it's GRC's Cookie Forensics.  If you google "GRC cookie forensics," it's the first link that comes up because it's been there since the dawn of the Internet.  And if you click it, it does an instant test of your browser's current cookie handling.  And we'll show you green for good and red for bad, and like blank if there's no cookie transaction going on.  My Firefox, unless I turn off, as I said, all third-party cookies, it's not blocking third-party cookies.



I actually maintain a separate domain, GRCtech.com, just for this purpose.  I created a third-party so that I could experiment and then automate that testing in order to show people what their browser was doing.  So anyway, I just - I wanted to say that, great news, that Firefox says they're doing this.  But it doesn't seem to be working.  And so anyway, GRC Cookie Forensics, for anyone who is interested.  And hopefully they'll get it working at some point.  And they did say they're rolling it out.  So maybe that's what's going on is that I've not been rolled on.  It hasn't been rolled out to me yet, or maybe they're tiptoeing.  I don't know.



But boy, Chrome looks great.  The way I've got it set up it just comes back completely happy.  And in fact I have a different page, it's GRC.com/cookies/stats.htm.  And that shows a series of bar graphs.  Again, you can see it's been a long time since I've been there since I have IEv5 and IEv6 and IEv7 that I'm tracking, and a bunch of others.  But I also have Chrome.  And boy, it used to be - the bars used to be all the way at the top.  And these are GRC's visitors.  GRC's visitors all have Chrome, their Chrome, blocking third-party cookies.  Maybe that's the default now in Chrome, and I once talked about that, and I've forgotten it.  I don't know.



Yup, so now you're showing the cookie forensics on the site.  And see all those red, if you scroll down into that second group of red, that's all bad.  Those are third-party session and persistent cookies that were just - you can see it says oldest cookie was one second old.  So they were just sent.



LEO:  Is this my browser?



STEVE:  Yes.



LEO:  Oh, funny.  I'm on Firefox.



STEVE:  Yeah, I know.  And it's not good.



LEO:  Oh.  Hmm.



STEVE:  Uh-huh.  And if you were to go under - if you go in the hamburger menu to Settings and, let's see, what is it, Settings, Privacy & Security, and then Enhanced - then you get there, and then, yeah, Privacy & Security.



LEO:  Oh, see, I'm on Standard.  Usually I'll run under Custom.



STEVE:  Okay.  Now do that.  Now, and normally it tells you you need to refresh your page.  It didn't tell you that you have to do that.



LEO:  So all third-party cookies, they don't want to block those.  And I know why.  Sometimes third-party cookies are from image servers that are really first-party, but they're on a different URL or different IP address.  So which would I want on this?



STEVE:  Well, try just instead of doing Custom, try doing - so first of all...



LEO:  Strict should do it; right?  Yeah, let's see.



STEVE:  Strict should do it.  Oh, and it does say refresh your tabs, yeah.



LEO:  Reload tabs, yeah.



STEVE:  Okay.  Now go back to GRC.



LEO:  So social media trackers, cross-site cookies in all windows.  Okay, good.  All right.



STEVE:  Yeah.  Exactly.  Should work.



LEO:  Now let's go back and refresh.



STEVE:  Now if you scroll down there is a button that I have, or you can do that, yup.



LEO:  Uh-oh.



STEVE:  Still bad.



LEO:  Worse.



STEVE:  Did not - I know.  It did not...



LEO:  There's one more red dot than there was last time.



STEVE:  Yeah, the icons, exactly.



LEO:  Yeah.



STEVE:  And so now go back over, and if you go to Strict...



LEO:  I'll go back to - I'll make it even stronger.  Custom is more than Strict; right?  If I do Custom...



STEVE:  Yes.



LEO:  Then I can say let's block a lot of crap here.  So what should I block?



STEVE:  I had to turn them all off.  And see...



LEO:  All cookies?  No.



STEVE:  I had to.



LEO:  Well...



STEVE:  No, no, no, third-party.



LEO:  Third-party cookies.  All right.



STEVE:  But that's extreme.  And the point is that should no longer be necessary.



LEO:  Yeah.  Let me refresh.  Icons are going to always be a problem, I guess.  Icon, icon.  And now these are empty.  What does that mean?  No third-party cookies.



STEVE:  Yes, that means no third-party session cookies were received from your browser.



LEO:  Okay.  So it's not green, it's empty.  That means there was no cookie at all.



STEVE:  Correct.



LEO:  And green would be okay; right?  First-party.



STEVE:  And orange, the orange meant that an older cookie was received, but that's not that big a problem.



LEO:  Right.



STEVE:  But it was necessary for you to go all the way to locking all third-party, not the default that they say they're going to be supporting, and not even only blocking tracking cookies.  But as you said, Mozilla was maintaining a list of sites that they were blocking which I thought was unfortunate.  But they're still not doing the right thing.



LEO:  Generally at home, you know, I think I turned this to Standard only because of the show because generally at home I run Custom.  But I don't turn - I just do cross-site.  I do, like, this setting.  But okay.  So now cross-site tracking cookies blocked, cross-site tracking cookies and isolate other cross-site cookies.



STEVE:  Yeah.  You would think that would be enough.



LEO:  That should be enough.



STEVE:  Yes



LEO:  But it's not.



STEVE:  No.  So refresh the page.



LEO:  Okay.  All right.  Let's see what we've got here.  [Buzzer sound].



STEVE:  Yeah. 



LEO:  [Buzzer sounds].  Okay.  I guess I'm going to - because this is the problem is that I think everybody's scared off when they do this because it says this is going to break stuff.  This may cause websites to break.  And as I my experience is, it does sometimes.



STEVE:  Well, yeah.  Yes.  And the other problem is that when Mozilla announces this, we just assume, oh, good, it's all good now.  Well, you know, as they say...



LEO:  No, it's not by default.



STEVE:  Trust but verify.



LEO:  So, and GRC.com/cookies.  That's very valuable.



STEVE:  Yeah.



LEO:  Will that get me there, or do I have to type in "forensics," as well?



STEVE:  No, I think GRC.com/cookies probably takes you - I think the first page has a cookie monster saying "Delete cookies?"



LEO:  Delete cookies, grrr.  And then this is good because it's an explainer, which is a good thing.



STEVE:  Yes.  You know me.  Back in those days I was doing a lot of explaining.



LEO:  Yeah, yeah.  Now we keep you busy with the show, you don't get to.



STEVE:  That's right.  That and SpinRite. 



LEO:  And then this is it, the bottom link is the web cookie operations.



STEVE:  Actually it's the third link there in that block of links.



LEO:  Cookie forensics, there it is.



STEVE:  Cookie forensics, yeah.



LEO:  Yeah, nice.  This is a great tool.  Thank you for - I knew about it, but I forgot.  So thanks for reminding me, yeah.  Very nice.



STEVE:  So we will see if they're going to roll it out and eventually get it right.  And the cookie forensics page lets us find out real quickly.



Okay.  So DDoS in the news.  We keep breaking DDoS attack records.  We're pretty much at the point where our eyes just glaze over now at the size of these attacks, gigawatt bits per second and all that.  It's like, whoa, okay.  My wires melted.  I guess I should have cut the wire where it says "Engage firewall."



So once again, Cloudflare has reported that last week it stopped and mitigated the largest HTTPS DDoS attack on record.  That attack weighed in at 26 million requests per second and was aimed at a website of a protected Cloudflare customer.  So attacks of this size no longer originate from individual compromised hardware devices because they just don't have the speed or connectivity.  Instead, most of the attacking IPs, it turns out, were owned - and this was in Cloudflare's analysis - by other cloud service providers whose virtual machines and their powerful servers had been hijacked to generate the attack.  Since HTTPS query attacks cannot be spoofed, Cloudflare was able to trace the attack back to a powerful botnet of 5,067 - that's not approximately, that's exactly - 5,067 IPs, each of which generated approximately 5,200 requests per second at peak.



A Cloudflare spokesperson said that to put the size of this attack in perspective, they had been tracking another much larger in agent count, but less powerful in query rate botnet consisting of over 730,000 devices.  Now, rather than letting that number just wash over us, let's stop and think about that for a second.  A single identified botnet consisting of more than 730,000 compromised devices.  Nearly three quarters of a million "somethings," all participating in coordinated attacks.



Now, however, the devices are apparently things like light switches and $5 outlet plugs, since that botnet, while large in number, generated fewer than one million requests per second overall, thus roughly 1.3 requests per second on average per device.  1.3 requests per second per device pretty much classifies the device as a $5 light switch or plug, but individually they're still able to generate some Internet traffic.  The ones I have at home all phone home to China.  Since they live behind a SoHo NAT router  actually two series-connected NAT routers  it's exceedingly unlikely that anyone got into them from the outside.  It's far more likely that, if they have been up to some mischief lately, they've been sold into slavery by their original producer.



I'll say again that no one should have IoT gadgets attached to their primary home network.  It takes deliberate work to set up and maintain a secondary IoT network, but I cannot think of anything more important for residential security.  The entity they are phoning home to may not be friendly.  And how would you ever know?  Now, the good news is that most recent IoT routers, WiFi routers, support one or more isolated guest networks.  Thank goodness.  That's what you want to use for those unknowable IoT widgets.  This makes the establishment of a secure perimeter far more easy and stable.



Anyway, as for Cloudflare's latest finding, that recent attack was, on average, 4,000 times stronger due to its use of virtual machines and powerful infrastructure servers.  Cloudflare also notes that HTTPS DDoS attacks are more costly to produce than others because these days they require more computational resources, needing as they do to bring up a secure TLS encrypted connection for every attacking query.  The bottom line is, DDoS attacks are no longer survivable unless the target is isolated behind an attack-mitigating bandwidth provider.  If you're not, and your organization is being attacked, just declare a holiday and send everyone home until the attack has passed.  That's another reality of today's Internet.



Okay.  As for another reality of today's Internet, we have MS-DFSNM.  As we've all learned, complexity is the sworn enemy of security.  In any complex environment consisting of complex interacting components, the addition of another component requires an understanding of that new component's potential interaction with all other existing components.  In the same way that adding each bit to a key doubles that key's total complexity, adding components to a system creates exponential complexity growth.  And this is one of the biggest dilemmas which we keep seeing that Microsoft has stumbled into.  Now we have a newly uncovered type of Windows NTLM (NT LAN Manager) relay attack which has been named DFSCoerce.  DFSCoerce leverages the Distributed File System (DFS) Namespace Management Protocol (MS-DFSNMP), and that allows the attacker to seize control of a domain.



When we hear the phrase "a new form of NTLM relay attack" at this point our eyes roll because you need to wonder just how many different forms of NTLM relay attacks there can be if new forms are still being discovered and uncovered in the year 2022.  NTLM relay attacks are a well-known method that exploits Microsoft's original half-baked and never sufficiently robust challenge-response mechanism.  We've talked about this so many times before.  Rather than simply discarding it decades ago as being too broken to fix, they have kept this sickly patient on life support by continually attempting to patch it and wrap it in additional layers of gauze.  As a result, today's NTLM relay attacks work the same way today as they did back then.  A malicious party sits between clients and servers, intercepts and relays validated authentication requests to gain unauthorized access to network resources, effectively allowing it to gain an initial foothold in Active Directory environments.



Filip Dragovic, who has been a prolific discoverer of problems, we've mentioned him before, also discovered this latest wrinkle in the rich NTLM attack surface.  He tweeted:  "Spooler service disabled, RPC filters installed to prevent PetitPotam, and File Server VSS Agent Service not installed, but you still want to relay."  And he says "Domain Controller authentication to Active Directory Certificate Services?  Don't worry.  MS-DFSNM has your back."  Meaning yes, there's a way you can still do that.



What is MS-DFSNM?  Well, it appears to be another of those things that some random Microsoft engineer added in one of those "Oh, here's what we need to add to get that done.  Won't it be neat?"  One of those fits of protocol design 15 years ago back in 2007.  That's when it first appeared.  It went into Windows, and now it can never be removed.  For anyone who's curious, it provides yet another remote procedure call interface for administering distributed file system configurations.



To give everyone a taste for this, here's what Microsoft's first paragraph of their overview of this protocol explains.  Microsoft says:  "The DFS Namespace Management Protocol (DFSNMP) is one of a collection of protocols that" - now listen to this.  It's one of a collection of protocols.  Not the only one.  This is a part of a collection - "that group shares that are located on different servers by combining various storage media into a single logical namespace.  The DFS namespace is a virtual view of the share.  When a user views the namespace, the directories and files in it appear to reside on a single share.  Users can navigate the namespace without needing to know the server names or shares hosting the data.  DFS also provides redundancy of namespace service."



Yeah.  And DFSNMP is one of a collection of protocols for doing that.  So okay, sure.  That sounds neat.  And I guess 15 years ago, when there was nothing better to do, then okay, let's add that.  So this is a perfect example of what appears to be complexity for its own sake, and nothing could be more antithetical to security.  Fifteen years ago the danger of this should have been appreciated, but apparently it wasn't.  In any event, we're stuck with it now.



The discovery of this particular DFSCoerce attack follows the related PetitPotam attack which is an abuse of Microsoft's Encrypting File System Remote Protocol (MS-EFSRPC) to coerce Windows servers, including domain controllers, into authenticating with a relay under an attacker's control, letting threat actors potentially take over an entire domain.  Sound familiar?  Yeah.  Same exact attack using an entirely different protocol.  But don't worry, there's lots more of those protocols  where those came from.



The CERT Coordination Center noted in detailing this attack:  "By relaying an NTLM authentication request from a domain controller to a Certificate Authority Web Enrollment or Certificate Enrollment Web Service on an Active Directory Certificate Server System, an attacker can obtain a certificate that can be used to obtain a Ticket Granting Ticket (TGT) from the domain controller."  And yes, if that makes your head spin, it probably should.



So here's my advice:  If any of our listeners are offered a job, given responsibility for managing and securing any significant Windows enterprise installation, first, you should start off being single because you will wind up being single.  And you should be sure to get a lot of money because you're going to be trading your life and your sanity for that thankless job.



To mitigate NTLM relay attacks, Microsoft recommends enabling protections like Extended Protection for Authentication (EPA), SMB signing, and turning off HTTP on Active Directory servers.  Again, those are all mitigations, not cures or solutions.  Just wrap it up in some more gauze.  Wow.



Although I didn't set out with this goal, this did wind up being a pile-on-Microsoft episode.  It's not that it's not deserved, but it does feel somewhat redundant.  So I'm happy to be able to report that Apple screwed something up, too.



LEO:  Oh, what a relief.



STEVE:  In a somewhat predictable way.



LEO:  And WordPress, I'm sure, at some point.  But okay, keep listening, yeah.



STEVE:  Yes.  We're about to get to them, as a matter of fact.



LEO:  Okay, good.



STEVE:  Yes.  Google's Project Zero discovered that a security flaw in Apple's Safari was found being exploited in the wild earlier this year.  What was interesting about this particular flaw was that it was originally fixed back in 2013, then inadvertently reintroduced in December of 2016, and only just fixed last month.  The issue, tracked now today as CVE-2022-22620 and bearing a hefty CVSS of 8.8, is another use-after-free vulnerability in WebKit that was being exploited by a piece of specially crafted web content to give its exploiter arbitrary code execution on the machine.  That's never good.



Early in February of 2022, Apple shipped patches for the bug across Safari, iOS, iPadOS, and macOS, while acknowledging that "it may have been actively exploited."  Uh huh.  And the sun may rise in the morning.  Google's Project Zero's Maddie Stone wrote:  "In this case, the variant was completely patched when the vulnerability was initially reported in 2013.  However, the variant was reintroduced three years later during large refactoring efforts.  The vulnerability then continued to exist for five years until it was finally fixed when it was discovered as an in-the-wild" - meaning yes, actually being exploited - "zero-day in January of this year," 2022.



Maddie explained that both the October 2016 and the December 2016 commits, one of which reintroduced the original bug from 2013, were very large.  The commit in October changed 40, four zero, files with 900 additions and 1,225 deletions.  The commit in December changed 95 files with 1,336 additions and 1,325 deletions.  It seems untenable for any developers or reviewers, that is, code reviewers, to understand the security implications of each change in those commits in sufficient detail, especially since they're related to long-lived semantics.



So whatever it was that was happening at the end of 2016, it was apparently a major revamp of some core Safari system.  And it appears that you get bit either way.  On the one hand, if you leave crappy old code alone under the theory of "if it's not broken don't fix it," you wind up eventually with even older crappy old code.  Right?



LEO:  Yup.



STEVE:  But if you bite the bullet to make huge sweeping revamping code-modernizing changes, you get bit by the introduction of brand new bugs which might be the same as some very much older bugs that were previously found and fixed.  Now, given a choice, I think I would do what Apple apparently did.  Code, as we know, really doesn't evolve very well.  If enough time passes, the assumptions that were once baked into the original code base no longer hold true, and they really can begin to chafe, and things can begin to crumble.  If the problem  code was rewritten to solve the problems of a strong new future, it can be best to scrap a large previous investment, no matter how solid it now is, and just start over with all the benefit of the knowledge acquired through the intervening years.  That seems to be what Apple did.  Okay, so not perfectly.  But this got fixed.  So, yeah, they did mess up, but I would say probably for the right reasons.



And Leo, speaking of WordPress, one million WordPress...



LEO:  I confess I looked ahead.  I had a feeling.  Go ahead.



STEVE:  One million WordPress sites were just force-updated.  The WordPress security guys at WordFence identified an exploited in-the-wild zero-day bug in a widely used, more than one million installations, WordPress plug-in called Ninja Forms, which is a customizable contact form builder.  The severity of the bug earned it a CVSS of, yes, 9.8; and it affected many versions of Ninja Forms starting with v3.0.  WordFence explained that the bug made it possible for unauthenticated attackers to call a limited number of methods in various Ninja Forms classes, including a method that deserialized user-supplied content.  And that resulted in an Object Injection which could allow attackers to execute arbitrary code or delete arbitrary files.  That's never good.



Although the update was supposed to be automatic and forced, any Ninja Forms users listening to this are advised to definitely check their WordPress installations to verify that they are now running the latest release of Ninja Forms.  So in other words, if you know you're a Ninja Forms user, and one million plus installations are, hopefully they all got fixed.  But if you know you're such a user, make sure that you're running an updated version.



And in sort of a related deserialization problem, there was another vulnerability and attacks on a very popular JSON library  known as FastJson.  It has a CVSS of 8.1 and was recently found and fixed.  It was patched at the end of May and affected all Java applications that rely on the FastJson versions 1.2.80 or earlier.  And I'm not going to go into detail.  It was an issue where a user-provided content could be deserialized into a Java class.  Now, that's obviously bad.  A user, if a user can provide a blob, then deserializing it turns it back into Java code, which then gets instantiated and run.  So obviously that's a remote code execution vulnerability.



The guys who had this realized that this was dangerous.  So unfortunately they created a block list of objects that they did not want to be allowed to be deserialized.  Well, that's obviously a bad idea.  It's like having a firewall where you close the ports for known attacks.  Okay, we tried that in the beginning.  That didn't work.  So they should have done a deny-all and then an allow whitelist.  Anyway, it's been fixed.  The function that was enabled by default called AutoType is no longer enabled by default.  Things have been cleaned up.  If you have any connection to anything known as FastJson, then you'll want to make sure that you're running the latest version.



Okay.  Finally, some bits of Miscellany.  A few weeks ago I mentioned that I had appeared as a guest of one of our own listeners on his own Trek Profiles podcast.  Last Wednesday John tweeted.  He said:  "Now comes Episode 66 of Trek Profiles!  In this one, we speak with Steve Gibson about his..."



LEO:  I know him.  He's famous.



STEVE:  "...about his Star Trek fandom, his history with the show, and we endeavor to discover why we all love Trek so much."  He says:  "Plus, you'll get to meet the mysterious Bunn-Ons.  Get it wherever you get your audio."  And so it's called Trek Profiles, for anyone who's interested.  And one thing that John did, and I did know he was going to, he took the recording he had made of me saying "We are the Bunnons!  Surrender your ship or be destroyed!"



LEO:  Yeah.



STEVE:  And our listeners will remember that that is, backwards, that's yo-sha ba-di-dro, pa-shor-yor-nar-ros, sna-na ba-na-ni.  I said that on the podcast.  Well, he reversed the audio.



LEO:  Oh, good for him.



STEVE:  And it worked.  Now, the timing on the "We are the Bunnons" part I got a little bit off.  But after all it was 50 years ago, and I still remember those words.  But the rest of it was intelligible as "Surrender your ship or be destroyed."



LEO:  Oh, my god.  I really have to listen to this.  It's on, for some reason, it's on Amazon Music.  Maybe that's - it can't be the only place it is.



STEVE:  I think he said get it wherever you listen to podcasts.



LEO:  Oh, good.  Okay.  Okay.  So the link you put in your show notes goes to Amazon Music.



STEVE:  Yeah.  That's the link that he had provided, and so I just put it in the show notes for anybody who has it.



LEO:  Nice.



STEVE:  Okay.  Closing the Loop.  First was from jvstech.  He said:  "Hey Steve.  I'm sure you've had several others already mention this, but USB type A connectors" - remember, which we talked about last week as how they demonstrate that we're in a simulation because they're always wrong the first time you try to plug them in.  "USB type A does not break probability.  It is, in fact" - and this does explain it, actually - "in a state of quantum superposition.  The connector exists in both the correct and incorrect orientations simultaneously until observed, at which point the wave-function collapses and a single orientation manifests.  This is why it often takes three tries to plug it in correctly."  And he says:  "Thanks for the amazing podcast, by the way."



And there was an unrelated tweet that I actually showed, saying "It's a well-known fact that you must spin a USB three times before it will fit.  From this we can gather that a USB has three states:  up position, down position, and superposition."  So apparently a lot of people have had fun with this sort of thing in the past.



Brian Tillman tweeted:  "Here's a question I have.  Suppose I have an account with some service, and they decide to support SQRL.  How do I get my SQRL identity associated with an existing account?"  And so I wanted to share Brian's question since I'm sure that FIDO's passkeys will be working the same way.  So it actually has a future practical application.  In all cases, an additional method of authenticating is simply added to an account.  So you would logon first through your traditional means, presumably a username and password.  Then under your account settings you would choose to add a SQRL, or more likely  a FIDO passkey.  That would trigger an authentication transaction from the website.  You would authenticate with your client, which would synthesize a pair of FIDO keys, and the client would provide the website with your specific public key for it to keep on record.



In the case of FIDO's passkeys, that's as far as it goes.  But with SQRL, since we thought through the entire paradigm, we realized that since a system's security is limited by the security of the weakest link, adding a super-secure means of authenticating doesn't actually increase security unless and until you also eliminate the possibility of using all weaker links.  In other words, if the site still allows you to log on with username and password, it can still get hacked, and you can still lose control of your password just like you always have been able to.



That's why SQRL goes a step further and is able to request that a website disable all non-SQRL authentication.  Once a user has become familiar with SQRL and is confident in its use, they're able to set a switch in their SQRL client, which when they then visit any websites, causes their SQRL client to request that sites no longer honor their traditional username and password, or any other non-SQRL means of authentication.  Which at that point actually does elevate your security to the level of a true WebAuthn-style SQRL or FIDO-style authentication.  Again, another example of what SQRL does that the FIDO people just didn't bother with.



Jose C. Gomez tweeted:  "Hi Steve. I am the owner/host of SQRL OAuth that Leo uses on the forum.  I had wandered off onto other things.  I got alerted by Jeff Arthur" - Jeff of course is the author of the iOS SQRL client.  He says:  "Thank you, Jeff.  And everything is back up and running.  I have added some monitoring tools to keep a better eye on it.  I honestly thought nobody was using it."  Which may have been the case.  He says:  "I wish it would get more adoption," he says, "SQRL in general.  But alas, with all this new FIDO, et cetera, I suspect it isn't going to happen.  Nevertheless, I have fixed the issues, brought the site back up, and even did some bug fix so the TWiT community should be back up and running for those that want to use it.  Thanks for the heads up, and thanks again to Jeff for reaching out.  Cheers."



And David Lemire said:  "Hi Steve.  Listen every week and always enjoy the show.  But I'd like to suggest you use as careful a definition for IoT as you do for zero-day.  You often point out how Microsoft's use of the term" - meaning zero-day - "isn't really appropriate."  Indeed.



He said:  "Well, NIST has a definition for an IoT device in NIST IR 8259, which has also been adopted into U.S. public law in the Cybersecurity Improvement Act of 2020.  It says such devices" - that is, IoT devices - "have a network interface for interacting with the digital world and a sensor or actuator for interacting with the physical world.  I bring this up because you often use routers as examples when you talk about IoT; but by the NIST definition, which is pretty well accepted, a router isn't IoT because it lacks an interface to the physical world.  I know you like to be precise about things so I thought you'd want to know about the NIST definition."



So David, thank you, yes.  And I think that's interesting.  And I think that we do have a problem here with a weak definition.  IoT is weakly defined.  My original feeling about the use and definition of IoT devices was actually more along the lines of what David cites as NIST's formal definition, things like light switches and plugs and Internet-connected thermostats, all of which qualify under that definition.  But my original definition has been broadened, I think usefully and appropriately, to now include what I would call "unattended devices" as the primary distinctive feature which determines their IoT-ness for the purposes of concerns about security and their abuse.



Is a router an IoT device?  Increasingly, the Internet security community is adopting that definition.  So I don't know what to do.  If something like a router is not an IoT device, then it would be nice to have some broadly agreed upon term for the class of Internet-connected and often quite powerful devices that can have firmware flaws allowing them to be remotely subverted.  I think we're stuck with IoT, and we need to call those things something.  So I think it's probably IoT.



And finally, chuck3000 said:  "Re SN-875.  Here is a reason for an electronic pet door."  He said:  "Apparently this is common in Florida."  And I've got a photo in the show notes that shows an alligator entering someone's home through the swinging flap which was insufficient at repelling the alligator.  I imagine he's looking for the pet cat.



LEO:  I admire the Floridian, the Florida man who had the intestinal fortitude to stand there and take that picture.  Because I wouldn't have.



STEVE:  Good point.  Someone took that picture.



LEO:  I would not have stood there.  The alligator's mouth is wide open, and he's a foot or two away.  And those things move fast.



STEVE:  Apparently they don't turn rapidly.



LEO:  Oh, okay.



STEVE:  Lorrie spent a lot of time in Florida, and she told me that what you learn is to zigzag.



LEO:  Ah.



STEVE:  Because they can run in a straight line, but they're not good at turning.



LEO:  See, you've learned something on Security Now! today, how to run away from an alligator.  That's wild.  What a picture.  Oy.



STEVE:  Yeah.



LEO:  Okay.  That's a good reason to have a chipped cat door.



STEVE:  Yeah.  And here comes my water.



LEO:  By the way, here's a weird one.  So I went to your Cookie Forensics site in Microsoft Edge with default settings.



STEVE:  Yeah?



LEO:  Nothing.



STEVE:  Whoa.



LEO:  It's actually better than Firefox.  No icon issues.  I find that hard to believe.



STEVE:  It's Chromium based, and Chrome did the same thing for me.  It behaved itself, although I had assumed it's because I'd turned on a bunch of stuff.



LEO:  This is default, I think.  I didn't mess with this.



STEVE:  Yeah.



LEO:  So, wow.



STEVE:  That's cool.



LEO:  Yeah.  Well, yeah, it's on basic tracking protection.



STEVE:  Wow.  And if you turn off tracking protection?



LEO:  I don't even have it turned on.  I don't even have it turned on.  Which of course it's Microsoft, so it's off by default.  Send do not track request?  No.  Yeah, wow.  It seems to me maybe they've got sneaky ways of getting cookies out of there.



STEVE:  That's interesting.  Well, I don't doubt that our listeners are going to dig into that, given this tool, and they'll let me know what's going on.  



LEO:  Yeah.  Weird.  All right.  We are going to pile on Microsoft in just a second.  All right.  On we go with the show.



STEVE:  Okay.  So today's podcast title, "Microsoft's Patchy Patches," was chosen after encountering a number of separate and independent pieces in the tech press, all decrying Microsoft's recent vulnerability and patch handling.  Since this has been something that we know I've been observing and repeatedly noting here, it was somewhat comforting to get a bit of a reality check that my perceptions are not coming out of left field.  Dan Goodin, writing for Ars Technica, published a piece headlined:  "Botched and silent patches from Microsoft put customers at risk, critics say.  Case in point:  It took five months and three patches to fix a critical Azure threat."



And Jonathan Greig, writing for The Record, separately published:  "Debate rages over Microsoft vulnerability practices after Follina and Azure issues."  Since Jonathan's reporting contained some new information from interviews he conducted across the industry, some from veteran Microsofties, I'll start by sharing some of what Jonathan found.



Microsoft finally released a patch for the much-discussed Follina vulnerability - we've talked about it a number of times, that was CVE-2022-30190 - amid fixes for 55 other issues last Tuesday.  But Microsoft's initial response to the issue, as we know, and several others, has stirred debate among security experts who question Microsoft's recent handling of vulnerabilities.  Microsoft initially claimed Follina "wasn't a security issue" after being sent evidence by the head of advanced persistent threat hunting organization Shadow Chaser Group.  They eventually acknowledged the issue, but several security experts have aired concerns about Microsoft's responses to a number of vulnerability reports.



Last Monday, Amit Yoran, the CEO of the cybersecurity firm Tenable, published a lengthy blog post criticizing Microsoft for its recent response to two disclosed vulnerabilities affecting the Azure Synapse service.  In his blog posting, Amit wrote:  "After evaluating the situation, Microsoft decided to silently patch one of the problems, downplaying the risk.  It was only after being told that we (Tenable) were going to go public, that their story changed, 89 days after we initially notified them of the vulnerability, when they privately acknowledged the severity of the security issue.  To date, Microsoft customers have not been notified.  This is a repeated pattern of behavior.  Several security companies have written about their vulnerability notification interactions with Microsoft, and Microsoft's dismissive attitude about the risk that vulnerabilities present to their customers."



Yoran went on to say that Microsoft's frequent reticence to notify customers of issues was "a grossly irresponsible policy."  In response to questions about Yoran's comments, Microsoft told Jonathan Grieg, reporting for The Record, that it only assigns CVEs to issues that require customers to take action.  What?  So now they're not vulnerabilities if Microsoft handles them secretly?



The Microsoft spokesman said:  "We addressed the issue that Tenable reported to us, and no customer action is required."  This apparently is Microsoft's new "sweep it under the rug" policy.  And really, when you think about it, isn't this exactly what a behemoth that's unanswerable would do if it's unable to act responsibly?  "Nothing to see here.  What CVE?"



Aaron Turner, CTO at the security company Vectra, said he understood both sides of the debate as a longtime former Microsoft security team member.  Microsoft wants to have the freedom to manage their cloud services the way they see fit, Turner said.  He said:  "I was at Microsoft in the worst of times, from 1999 through 2006, when the company had to go from some of the worst security management policies to eventually leading the industry in predictability, transparency, and one of the best supporters of responsible disclosure."



Turner explained that he knows and respects Yoran personally, but did not think that Tenable's blog post was constructive.  The rules around responsible disclosure do indeed need to be updated, according to Turner.  But he noted that both sides have room for improvement.  Well, I'd like to hear more, but he didn't offer it, at least for this article.



And I'll note that, if Turner left Microsoft in 2006, when things were going great, according to him, then he will have missed the events of the past 16 years, when things have definitely taken a turn for the worse.  In a few minutes I'll be sharing some thoughts from someone who once tested Windows, before Microsoft decided that actual testing was no longer needed.



Anyway, Turner said there needs to be clearer rules around research into core Platform as a Service and Infrastructure as a Service technologies - in other words, how to deal with Cloud stuff - as well as easier ways for cloud platform operators to provide testing capabilities to researchers and clear responses to responsibly disclosed vulnerability information.  And that's what Tenable wants.



Several other researchers were less forgiving of Microsoft, pointing out that more than 33% of the vulnerabilities added to the CISA Cybersecurity and Infrastructure Security Agency's list of known exploitable bugs, 33%, more than, came solely from Microsoft.  One third of all.  Microsoft had the most vulnerabilities added to the list in every month this year.  And those are known exploited bugs.



Andrew Grotto, former White House Director for Cybersecurity Policy, who is now a cybersecurity professor at Stanford University, argued that Microsoft's market dominance was part of the problem.  Yeah, no kidding.  As we know, one of my theories has been that they just don't need to do anything since there are no consequences when they do not.



Anyway, Grotto explained:  "The data speaks to an outsized representation of Microsoft products having the most critical vulnerabilities.  On some level it may reflect the sheer prevalence of Microsoft products, but it's not like there aren't other vendors whose products are constantly being poked and prodded and tested.  No other vendor appears with the same frequency and level of severity in terms of vulnerabilities that Microsoft's products seem to," says Grotto.  "Does the market force Microsoft to remedy this problem or not?  What worries me," he says, "is right now there is not a ton of competition, so I'm a bit pessimistic about this trend changing."



Steven Weber, professor of the Graduate School of Information at UC Berkeley, said procurement is the best way to drive positive changes in security practices.  Government procurement practices right now are making the government less secure, but also hurting the private markets as well, Weber explained, because it is not creating greater demand for better security.  He said:  "It's important to keep in context that the widespread market penetration of a company's products is no explanation for why its products are also the most vulnerable."  Amen to that.  "What we ought to be asking is, given that we know and are shown again and again that Microsoft's products are highly vulnerable, why do they remain so prevalent in the market?"  Well, we know.



And Dan Goodin's reporting added some additional depth to this topic.  Reporting in Ars Technica, Dan observed:  "Blame is mounting on Microsoft for what critics say is a lack of transparency and adequate speed when responding to reports of vulnerabilities threatening its customers.  Microsoft's latest failing came to light on Tuesday in a post" - that's last Tuesday - "in a post that showed Microsoft taking five months and three patches before successfully fixing a critical vulnerability in Azure."  And I'll just say that part of the problem here is that everyone wants, those people finding these problems want to behave responsibly.  They want Microsoft to fix this.  Yet Microsoft is under no pressure to fix it because these companies will wait until they do.  Maybe that's the mistake that they're making.



Anyway, Orca Security first informed Microsoft in early January of this year of this very critical flaw, which resided in the Synapse Analytics component of the cloud service Azure and also affected the Azure Data Factory.  It gave anyone with an Azure account the ability to access the resources of other customers.  "From there," Orca explained, "an attacker could gain authorization inside other customer accounts while acting as their Synapse workspace."  They wrote:  "We could have accessed even more resources inside a customer's account depending on the configuration.  It would leak customers' credentials stored in their Synapse workspace; communicate with other customers' integration runtimes saying we could leverage this to run remote code execution on any customer's integration runtimes; and take control of the Azure batch tool managing all of the shared integration runtimes.  We could run code on every instance."



In other words, this was a horrible, critical vulnerability.  Yet Orca said that despite the urgency of the vulnerability, Microsoft responders were slow to grasp its severity.  Microsoft botched the first two patches.  And it wasn't until just this past Tuesday in June, June's Patch Tuesday, that Microsoft issued an update that entirely fixed the flaw, and Orca finally felt it was safe to talk about this.  A timeline provided by Orca shows just how much time and work it took them to shepherd and coax Microsoft through the remediation process.



On January 4th the Orca Security research team disclosed the vulnerability to the Microsoft Security Response Center (MSRC), along with keys and certificates they were able to extract.  So that was part of the vulnerability was the ability to extract keys and certificates which then empowered to do this.  A month goes by, February 19th.  Another month, March 4th, MSRC requested additional details to aid its investigation.  Each of those two times, on February 19th and March 4th, Orca responded the next day.



Now we're in late March.  MSRC deployed the initial patch.  March 30, Orca was able to bypass the patch.  Synapse remained vulnerable.  March 31st, Azure awards Orca $60,000 for their discovery.  Oh, joy.  But Azure remains vulnerable.  April 4th, 90 days after disclosure, Orca Security notifies Microsoft that keys and certificates are still valid.  Orca still had Synapse management server access.  Three more days, April 7th.  Orca met with MSRC to clarify the implications of the vulnerability and the required steps to fix it in its entirety.  Three more days, April 10th, MSRC patches the first bypass, and finally revokes the Synapse management server certificate.  Orca was able to bypass the patch yet again.  Synapse remained vulnerable.



Five more days.  Now we're at April 15th.  MSRC deploys the third patch, fixing the RCE and reported attack vectors.  May 9th, both Orca Security and MSRC publish blogs outlining the vulnerability, mitigations, and recommendations for customers.  End of May, Microsoft deploys more comprehensive tenant isolation including ephemeral instances and scoped tokens for the shared Azure Integration Runtimes.  In the end the fix was silent, with no notification ever provided to Microsoft customers that they had ever been in this danger and had been for five months.



And this account from Orca followed 24 hours after the previously mentioned security firm, Tenable, related a similar tale of woe of Microsoft's failing to transparently fix vulnerabilities that also involved Azure Synapse.  And this is wonderful.  In a statement, Microsoft officials wrote:  "We are deeply committed to protecting our customers, and we believe security is a team sport."  What?  What are you guys smoking up there in Redmond?



LEO:  The Tigers bought uniforms.



STEVE:  Yeah.  The security of your proprietary software is a team sport?  They said:  "We appreciate our partnerships with the security community" - although of course we don't act as if we do - "which enables our work to protect customers."  Right, because we're not doing security in-house anymore.  Apparently we're relying on outsiders.  Whoa.  "The release," they said, "the release of a security update is a balance between quality and timeliness."  Yeah, right, we wouldn't want to rush it out and get it wrong.  Oops, we did.  Three times.  Oh, well, never mind.  "And we consider the need to minimize customer disruptions while improving protection."  Oh, so we don't want to notify them.  That might be disruptive.  Right.  Wow.



So I recalled that a few years ago Microsoft laid off their large and expensive testing teams.  So I went looking for, and found, a description of exactly what changed and how, and why today's system has become so badly broken.  At least in part why.  Martin Brinkman, writing for Ghacks.net, had a piece a few years ago titled "Former Microsoft Employee explains why bugs in Windows updates increased."



Have the number of bugs in Windows updates increased in the past couple of years?  If so, what's the reason for the increase in bugs?  What might it be?  That's the question that former Senior SDET, that stands for Senior Software Development Engineer in Test, Jerry Berg, recently answered.  Berg worked for 15 years at Microsoft, where one of his roles was to design and develop tools and processes to automate testing for the Windows OS.  He left the company after Windows 8.1 shipped to the public.  Microsoft changed testing processes significantly in the past couple of years.  Berg described how testing was done in the late 2014 and early 2015 period, and how Microsoft's testing processes changed since.



Back in 2014 and '15, Microsoft employed many teams that were dedicated to testing the operating system, builds, updates, drivers, and other code.  The teams consisted of multiple groups that would run tests and discuss bugs and issues in large daily meetings.  Tests were conducted manually by the teams and through automated testing and, if tests passed, would give the okay to integrate the code into Windows.  The teams ran the tests on "real" hardware in labs through automated testing.  The machines had different hardware components - processors, hard drives, video and sound cards and so forth - and other components to cover a wide range of system configurations.



Then Microsoft laid off almost the entire Windows Test teams, and it moved the focus from three different systems - Windows, Windows Mobile, and Xbox - to a single system.  The company moved most of the testing to virtual machines which meant, according to Berg, that tests were no longer conducted on real and diverse hardware configurations for the most part. They were tested on generic VM's.



Microsoft employees could self-host test releases of Windows, which would mean that their machines would also be used for testing purposes.  The main idea behind that was to get feedback from in-house Microsoft employees when they encountered issues during their work days.  Berg notes that self-hosting is not as widely used anymore as it was before.  Right, because who wants to be a tester when that means that one's main machine will be crashing during their day.



So now today the primary source of testing data, apart from the automated test systems that are in place, running on VMs, comes from outside Microsoft, from Microsoft's users, through Telemetry and Windows Insiders.  Windows Insider builds are installed on millions of devices, and Microsoft collects Telemetry from all of these devices.  If something crashes, Microsoft gets information about it.  Unfortunately, one of the problems associated with the collecting of Telemetry is that most bugs are not caught by it.  If something does not work right, Microsoft may not be able to discern the relevant bits from Telemetry data.  While it is in theory possible that users report issues, many don't.  Additionally, while Insiders may report bugs, it is often the case that necessary information is not supplied to Microsoft, which poses huge issues for the engineers tasked with resolving these problems.



Back in 2014 and '15, Microsoft's Testing team would be tasked with analyzing bugs and issues, and supplying engineers with the data they required, patches, to resolve these.  Nowadays, Berg notes, all of that is gone, and it's Telemetry that the engineers look at to figure out how to fix these issues, and fixes are then pushed to customer devices running Insider Builds again to see if the issue got fixed or if it created new bugs.



One of the main reasons why Microsoft stopped pushing out new feature updates to everyone at once was that issues that were not detected by that previous process could potentially negatively affect a large number of customers.  Yeah, no kidding.  To avoid total disasters like the Windows 10 version 1809 launch, gradual rollouts were introduced that would prevent feature updates from being delivered via a Windows update to the majority of machines in the early days of the release.



So Microsoft exchanged the dedicated in-house testing team for remote Telemetry data that it gathers from Insider Builds that it pushes to consumer and business devices, and replaced much of the PCs that it once used for testing with virtual environments.  All of that has led to an increased number of issues and bugs that customers face on production machines when installing Windows updates or feature updates.  And it appears to have created disconnection throughout various major arteries of Microsoft.



I have no doubt that individual Microsoft employees are doing the best they can with what they have.  But management appears to have royally screwed the pooch when they decided to disband serious prerelease testing in favor of collecting outside telemetry from Windows Insiders.  And we've already talked about what Microsoft plans to do next by further automating the Windows Update system to dynamically back out when things go wrong and to learn from its mistakes.  This essentially broadens its Telemetry collection from Windows Insiders to include all other commercial Windows users.



Stepping back to look at the big picture, it becomes clear that what Microsoft has essentially done by disbanding serious internal testing on real hardware, switching to limited testing on virtual machines and outsourcing its software testing first to Windows enthusiasts and then to all Windows users, is to turn us all into their unpaid beta testers.  And as we've seen in example after example, the external security community becomes their thankless security research arm.



So someone hearing this says to me:  "Okay, Gibson.  What OS are you using?"  And we all know the answer to that.  I'm sitting in front of Windows.  I've always been sitting in front of Windows.  And before that I was sitting in front of DOS.  I also often sit in front of FreeBSD UNIX, which runs several of my most important servers.  And I spent a lot of time in front of Debian Linux when I was working to recover that Lenovo laptop's weird-acting BitLocker-encrypted NVMe drive that had died in a mysterious way and would no longer boot.  That's the one that I managed to get finally working by hooking it to an external PCIe Thunderbolt port.  I used Debian then thanks to the powerful command-line tools that Linux offers.



But the fact remains I am most comfortable sitting in front of Windows.  The tools I prefer using are hosted only there.  And the tools I'll be using for SpinRite 7 when I move away from DOS are only hosted on Windows.  My failure to preemptively show Microsoft the folly of shipping a consumer OS with userland access to raw sockets taught me that Microsoft had become a blind behemoth.  So I have no illusions that we can change Microsoft.  But understanding the path and the trajectory that Microsoft's policies have put it on remains valuable.  And so I think we can predict the future.



LEO:  Terrible.



STEVE:  It's not good.



LEO:  Yeah.  It's why I do not use Windows.  I know you need to, but...



STEVE:  I hear Paul and Mary Jo on Wednesdays just saying, "We don't understand what's going on."  I mean, they don't.  They're as inside as you can be, and they're just like, yeah, we don't know what happened.



LEO:  Yeah.  I have a Windows machine sitting in front of me.  This is the only one I have.  Just because I guess I have to help people figure out what to do.



STEVE:  Yes, you do.  Yup.



LEO:  That's no fun.



STEVE:  It's a love/hate relationship.



LEO:  You know, I mean, most operating systems are kind of the same and do the same thing.



STEVE:  There certainly has been sort of an aggregation around...



LEO:  Yeah, they're very similar.



STEVE:  ...the right way to do things.



LEO:  Yeah.  And I think anybody who uses Windows, if they picked up, I wouldn't say Debian, but maybe Ubuntu, which is based on Debian, or Pop!_OS, or Manjaro Linux, which is what I use, would feel pretty much right at home almost right away using GNOME or KDE.  It still baffles me.  But, you know, nobody ever got fired for buying a Microsoft Windows machine.



STEVE:  That's exactly right.  Microsoft has become the IBM of yesteryear.



LEO:  Yeah, yeah.  Thank you, my friend, as always.  You'll find Steve Gibson at GRC.com.  That's the Gibson Research Corporation.  But it's of course the home of SpinRite, the world's best hard drive or mass storage maintenance and recovery utility.  You'll also find a lot of other good things there, including this show.  He has a unique, couple of unique versions:  a 16Kb audio version, which sounds a little scratchy but is small, smaller than that bottle.  And, wow.  That's a lot of water.  And he also has transcripts, which are great for reading along.  And I guess you said the transcripts are not as popular as the show notes.  But I think for searching, they may not be downloading them, but for searching they're very, very valuable.



STEVE:  Oh, no comparison, yes, super valuable.



LEO:  Show notes are the kind of thing you almost would want to subscribe to, like it's a newsletter every week with the contents and the images and the links from the show, which I think is fantastic.  That's also at GRC.com.  We have video, oddly enough, of the show.  If you want to see Steve's giant bottle, that's at TWiT.tv/sn.  There's also of course a YouTube channel dedicated to it.  We also have audio versions at TWiT.tv/sn.  Or you can subscribe in your favorite podcast player and get it automatically.



You can watch us record this show Tuesdays at about 1:30 to 2:00 p.m. Pacific, that's 4:30 Eastern, 20:30 UTC.  The stream is at live.twit.tv.  The chatroom is going, it's hot and heavy, irc.twit.tv.  That's open to all.  And of course the Discord is also chatting away.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.











	















71










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#877

DATE:		June 28, 2022

TITLE:		The Hertzbleed Attack

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-877.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, after dealing with a major piece of errata from last week, we look at Germany's reaction to the EU's proposed "Let's monitor everyone and privacy be damned" legislation.  The Conti gang finally pulls the last plug.  We have an update on the status of Log4j and Log4Shell and a weird proposal for a "311" cyberattack reporting number.  A sweeping 56 new vulnerabilities were found and reported across the proprietary technologies of major industrial control technology providers.  And this week we have a piece of miscellany, followed by 10 interesting items of closing-the-loop feedback to share from our listeners.  We will then take a deep dive into the latest "Hertzbleed Attack" which leverages the dynamic speed scaling present in today's modern processors.  We'll examine another effective side-channel attack, which is even effective against carefully written post-quantum crypto and can be used to reveal its secret keys.



SHOW TEASE:  Coming up on Security Now! it's me, Jason Howell, filling in for Leo just for this week, joining Steve Gibson, of course, who does the majority of the talking because he is the security pro.  We say farewell to the Conti gang, at least I think it's a farewell.  Also we look at the 311 cyberattack reporting number.  Don't worry, it has nothing to do with a rock band.  Also we check in on the still-going Log4j and Log4Shell vulnerability.  Steve reminds us of, you remember Heartbleed?  Yeah, we take a look back in time, eight years ago, to Heartbleed.  And then dives deep into the latest Hertzbleed attack.  You're not going to want to miss that.  Steve Gibson explains it all next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 877, recorded Tuesday, June 28th, 2022:  The Hertzbleed attack.



It's time for Security Now!.  This is the TWiT show where we talk about all the latest security news, the security happenings, happening all around the Internet right now and beyond.  Of course I am not Leo Laporte.  I am Jason Howell, filling in for Leo this week.  And I'm so excited anytime I get the chance to sit in with Steve Gibson.  Welcome to the show, Steve.



STEVE GIBSON:  Hey, Jason.  Great to have you back.  You are our fill-in when Leo is on a cruise.



JASON:  Or not on a cruise, but somewhere else.



STEVE:  Or off traipsing around the East Coast.



JASON:  Yes, exactly.



STEVE:  Wherever.



JASON:  Yeah, wherever he happens to be.  I'm happy to be here, thank you.



STEVE:  It's always good and comfortable to have you.  So I'm glad for that.  So this is our last episode of the middle of the year, 877 for June 28th.  This is a topic, our main topic has been on my radar for a couple weeks.  In fact, I sort of forgot about it.  It fell through a crack last week and what we had to sort of talk about.  This one is titled "The Hertzbleed Attack."  And our longtime listeners, I think it was from 2008 - no, I know, that's too far away.  But maybe '13.  Anyway, I've got it in the show notes.  We'll talk about it when we get there.  There was the famous Heartbleed attack.  And this is clearly a play on that.  Which, and this is really interesting.  I think that our listeners are going to get a kick out of it.



But we've got to first deal with a major piece of errata from last week.  Then we're going to look at Germany's reaction to the EU's proposed - basically it amounts to "let's monitor everyone and privacy be damned legislation."  The Germans are not too happy about that.  The Conti gang has finally pulled its last plug.  We've got an update on the status of the Log4j and Log4Shell exploits.  Also a weird proposal that seems to be actually gaining some traction for a "dial 311" cyberattack, like, incidence report number.  It's like, okay.  You know, I dial 911 for problems.  611 I think is for your phone to be repaired.  And now we've got 311.  So it's okay.



There's also a sweeping 56 new vulnerabilities that were found and reported across the proprietary technologies of a handful of major industrial control technology providers, and a little bit of philosophy of what's wrong with the way we're doing things there.  We have a piece of Miscellany.  And then there was just a lot of great feedback from our listeners since last week.  So we have 10 interesting items of closing-the-loop feedback from our listeners.



And then we're going to take a gratifyingly deep dive into this latest and interesting new Hertzbleed attack, which manages to leverage the dynamic speed scaling present in today's modern processors.  It's yet another way of leaking information, believe it or not.  It's a side-channel attack.  And it's even effective against post-quantum crypto.  So nowhere to hide there.  And after you tell us about our first sponsor, I have to say we've got a really fun Picture of the Week.  We'll save it till then.



JASON:  We'll expand when it's time to expand.



STEVE:  So I think another great podcast for our listeners.



JASON:  Absolutely.  And it took me back, hearing Hertzbleed and thinking of Heartbleed, because I totally remember when that was a big deal.  That was, I'm just looking on Wikipedia right now, released 10 years ago, discovered eight years ago.  So that sounds about right.



STEVE:  Okay.



JASON:  Yeah, I remember that being a pretty darn big deal, so I'm super curious to hear how that ties into now.  All right.  Picture of the Week time.  This is how we start the show.  Tell us what I'm looking at here.  This is actually - this is a big picture.  It like scales three pages.



STEVE:  Well, that's the point, exactly.  This is the first time in 17 years that the Picture of the Week did not fit on one page.  A clever blogger created this, sort of a joke, but also to prove a point.  We've often talked about how OpenSSL has become a, well, a mixed blessing.  It is literally the Swiss army knife of secure communications.  It's also the Swiss army knife of secure communications even if that's not what you want.  If you just want to connect two endpoints using TLS, well, it's not clear that OpenSSL is the best thing to use.  And it's a coincidence that it was a bug in OpenSSL that caused the Heartbleed attacks and that whole vulnerability. 



So, okay.  What people are looking at, if you have the show notes, is if OpenSSL's command line options had a GUI.  That is, if you - because, I mean, the command line options are just nuts for OpenSSL.  So the person who did this blog posting, basically to prove a point, he gave OpenSSL a GUI.  And so, for example, it starts of course with just, let's see, one, two, three, four, five, six lines of tabs.  So you have a tabbed user interface.



And for example, there's CA for Certificate Authority.  Ciphers.  I see CRL, that'll be Certificate Revocation List.  I see dhparam, that'll be Diffie-Hellman parameters for signing.  DSA, that's Digital Signature Algorithm.  Dsaparams.  There's EC params, so Elliptic Curve parameters.  There's encode, engine.  What else do I see?  Generate RSA keys, OCSP, that's Online Certificate Status Protocol.  Password, who knows what that is.  Generic pkcs7.  Public key, private key, pkeyutil, something about prime numbers, a random number generator, rehash, there's RSA, I mean, then there's s_client and s_server to create a secure client or secure server connection.  Okay.  Those are just the tabs.



JASON:  Right.



STEVE:  Now, the last tab in all of that, just alphabetically sorted, is x509, which is the standard that's been defined for the format of certificates.  So then the panel of this GUI begins with input format, is it a PEM or a DER.  Output format, PEM or DER.  And a button to open the input file, a button to open the output file.  And I'm not going to enumerate all these.  But anyway, the point is here we have then buttons and sliders and forms to fill out, and checkboxes, do you want this, like text output options, print serial number value, print subject hash value, print issuer hash value, print subject DN, issuer DN, email address, certificate purpose, blah blah blah blah blah.  I mean, endlessly.  And these are all reflected in the command line of this insane OpenSSL that has just been growing and growing and growing since it was first created.



So anyway, I knew that we've often talked about OpenSSL.  And I loved it that after three pages of scrolling through this, you get down to the bottom, where the guy who created this added a note.  He said:  "Note that even this is incomplete.  It covers about 80% of one corner of OpenSSL's functionality.  The certificate policy options have a lot more knobs that were not included."  It's like, oh, my god.  Anyway, just a perfect Picture of the Week for our podcast, and I wanted to share it with our listeners who would get a kick out of it.



JASON:  I love it.  And I love that what you're seeing, because the first thing that my eyes go to, right, is just the sheer length of it.  And then when you go back up to the top and you realize, that's just one tab of, I don't know, if I had to guess, like 30.  It's pretty remarkable.



STEVE:  Well, and it is an accurate reflection of this great, I mean, you can do anything with OpenSSL.  It'll create certificates.  It'll verify certificates and validate them.  You can create, I mean, it is literally it's like the one place for doing everything. And the problem is that it is - oh, the other thing is that it's where this stuff is first tested.  So if someone's creating like a new, well, for example, TLS.  OpenSSL went through a series of versions, went to 3.0, and then SSL 3.0 became TLS 1.0, and then 1.1 and 1.2 and 1.3.  It supports all of that, and it's where these things were first tested.  And after they were tested, they may get spun off into something else, but they also stay there.



So anyway, it is a remarkable utility, but I would not recommend that anyone actually use it these days.  There are a number of very lightweight, purpose-specific TLS libraries.  If all you want to do, and that is what most people simply want to do, is to create a solid robust TLS connection between two points, the last thing you want to do is load OpenSSL into your server process in order to make that happen.



Okay.  I have to start with a piece of errata from last week.  I don't know how to explain or apologize or what, the brain fart that I had.  Last week I talked about Firefox's Total Cookie Protection.  And then with Leo we went to GRC's Cookie Forensics page, and there I expressed my puzzlement over the fact that it didn't seem to be working.  Well, it was working perfectly.  That is, everything was working perfectly.  Firefox's Total Cookie Protection was turned on and working perfectly and just the way it should be.  And my page was working correctly, even though I thought something was broken.  So I just got tangled up, and I want to fix that.



The brilliance of Mozilla's approach is that third-party cookies do still work just as they always did.  So the fact that I was getting red, like, warnings of oh my god, this site GRC has active third-party cookies from GRCtech.com, is yeah, of course.  That's third-party cookies.  Total Cookie Protection doesn't stop third-party cookies, it creates the context.  It binds the third-party cookie, a specific third-party cookie, to the domain where it was sent.  So this is the way it's going to look.



Basically the short version is my Cookie Forensics page was never designed to test for third-party cookie sequestration or context.  In order to do that, I would need a third domain.  I don't know what I would call it, but I don't have one.  I mean, I have one, but I'm not going to take the time to go do that right now.  I'm working on SpinRite, and nothing will distract me from that.  But I just wanted to clarify that everything's working.  Mozilla's got it going.  Firefox is protecting people.  And the point is that third-party cookies have not been suppressed.



It is the case now, although my Cookie Forensics page does not illustrate this, if GRC exchanged third-party cookies with, as it does GRCtech.com, were you to go to GRD.com and try to get GRCtech.com's cookie, your browser, which does have a GRCtech.com cookie from GRC.com, would go, huh?, if you were at GRD.com, and it would say, no, you don't have a cookie for that.  And if GRD.com set a third-party cookie for GRCtech.com, then the browser at GRD.com would say, yeah, okay, fine.  We've got a third-party cookie now for GRCtech.  But it would be completely disconnected from and distinct from the third-party cookie of the same name, which you transacted over at GRC.com.  And that's Total Cookie Protection.  It means that third-party cookies work as they always did, but they cannot be used to track.  They are, I mean, just elegantly and beautifully blocked from tracking.



And as I said last week, the only thing that I don't understand, except I obviously didn't even understand how my own Cookie Forensics test worked, I don't understand how it took us this long to get such an elegant and simple solution.  It must be that there is just tremendous lobbying pressure, like there's got to be tremendous pressure on the browsers.  Well, we know that Google is about tracking, so they're not going to fix this in Chrome any sooner than they have to.  But here's Mozilla off to the side, wanting to be the alternative browser that we all love because it and Safari and now everything else is Chromium.  So anyway, it's working great.  I just wanted to correct the record.  I just got myself tangled up last week.



Okay.  Also last week, we talked on Tuesday that there was going to be a LastPass webinar two days after that on Thursday, where LastPass's gurus, the CTO, the chief tech guy, and what we found was a bunch of marketing people, of course.  That's how you pay for getting the technical information is putting up with that.  We're all going to hold a webinar.  And they did.  The hope was that they would clarify the very confusing blog post they'd made the previous week where they talked about passwordless logon coming soon to LastPass, and then said, oh, and kind of FIDO2, but blah blah blah.  Like, what?



Anyway, so I sat through this mostly marketing spiel, and here's what we learned.  I snapped two slides from it.  The first slide was titled "Also, it's a Journey..."  And under that it says "Real talk:  truly removing passwords from the login equation could take years.  Why?"  And then it says:  "Universal passwordless access requires FIDO2 support from devices, operating systems, browsers, and authenticators, but also from each website individuals need to access."



So, yeah, that's what we've been talking about.  The last couple weeks, ever since Apple's announcement of passwordless and passkeys and their support for FIDO2 coming in iOS 16, it's like, okay.  And Google has said they're going to do it, as has Microsoft.  And we've identified two issues.  One is it's very clear that each of these three providers is kind of rubbing their hands together saying, oh, goodie, we're going to sync the passkeys you create in our little ecosystem among the devices that share that ecosystem, but we're not talking yet about whether we're going to let them go, other than one at a time if you move it to a different device.



And in order for this whole passkeys thing to work, every device that a user uses has to be dynamically, that is, in near real-time, synchronized with every other so that if you create a new passkey with a site on one of your devices, you can later go to a different device and log on as you, and that passkey will be known with the client that you're logging in from.  And that requires, because they didn't use SQRL, which doesn't have any of this problem, but fine, that requires that in real time you cross-synchronize all the devices.



There's been, last week we saw, we learned first-hand from the president of FIDO and the chief something or other, both of them said, oh, no, we don't know how to do that.  FIDO2 doesn't do that.  We haven't thought about that.  So use it now, and we'll worry about that if it becomes a problem.  Well, we already know it's going to be a problem.  So that's the first thing.



And the second thing is, yes, you have to have web server support.  I mean, when we get iOS 16 and whatever - you're the Android expert, Jason, whenever Google is doing that in Android.  Well, we'll be - I loved my analogy last week.  I said, it's like the person who invented the first shortwave radio.  Hello?  Can anyone hear me?  Wait, there's only one radio.  So no.



JASON:  It's going to take a little bit more than just that.  Nice thought, but...



STEVE:  Yes, exactly.  So anyway.  So yes.  So basically LastPass is saying, okay, you know, like cool your jets.  We know everybody wants FIDO2.  But when you get it, there's nobody to talk to.  So we're going to move in that direction.  We're heading there.  But we don't have it yet.  And in fact that brings us to the second slide which showed their three phases.  Phase 1 is Available Now! and it has an exclamation point because it's like, Now!  And that is "Passwordless login to the vault using LastPass Authenticator."  Which, okay, that's good.  So you put your thumb on the thumbprint reader, or you smile at the camera on your smartphone.  And ooh, look, I'm in.  So, okay.  That's good.  Doesn't seem that hard.  Everybody has that.  Oh, yeah, and it's available now in LastPass. 



Then later this year, no exclamation point on that one because not yet.  They say:  "Adding FIDO2-supported Authentication [which is to say] security keys and biometrics, for additional security and flexibility when using passwordless login to your vault."  And I'll remind everyone from my digging around in Bitwarden, as I mentioned last week, Bitwarden already offers the use of FIDO2 login.  Bitwarden, by the way, a sponsor of the podcast and TWiT Network.  They've already got it.  So LastPass will be later this year, catching up with Bitwarden, basically the idea being more protection using the FIDO2 passkey technology to more strongly protect access to your own vault of old-school usernames and passwords.  This has nothing to do with logging onto websites using FIDO2.  It's the other way around.  You're logging onto your vault using FIDO2 with some sort of Authenticator.



And finally Phase 3, which is what nobody has yet because, again, we've got the chicken-and-egg problem.  With Phase 3 they said:  "Coming in 2023."  In other words, next year sometime.  And they said, and this is from their slide:  "Adding secure storage of passkeys, in addition to passwords" - meaning LastPass will do both, it'll do old-school usernames and passwords, and you can also put passkeys in there, which it'll unlock and treat the same way and allow you to then use your passkeys with FIDO2-compliant websites when and to the degree that they start to emerge.



And of course that's what Apple, Google, and Microsoft have all just announced their support for.  Neither LastPass nor Bitwarden, nor as far as I know any other password manager offers a solution for that today.  And why would they?  As I said, one short-wave radio.  Can't talk to anybody.  So we don't know when in 2023.  Probably they'll set the priority based on what traction these passkeys and FIDO2 is appearing to get over time.  And so I think that timing probably works well.



The point, the reason we're spending so much time focusing on passkey support for FIDO2 is that it isn't - "it" meaning LastPass or Bitwarden or 1Password or whatever - isn't Apple, Google, or Microsoft, where their intention seems to be to use the fact that there is some inherent lock-in to their own competitive advantage.  The password managers will inherently be cross-platform.  And so I've been cautioning our users, our listeners of the podcast for a couple weeks now, careful.  Before you invest heavily in one of these isolated platforms, see about exportation of passkeys because you may end up wishing that you were on some platform that was dynamically synchronizing.



And again, the dynamics is the key.  Even if Apple were to - if Apple, Google, and Microsoft or even the FIDO were to get together and come up with a passkey blob export/import, that doesn't sound dynamic.  That is, for this thing to be practical, all of these ecosystems would need to be dynamically sharing passkeys so that if you create one on your iPhone, when you sit down at your Linux desktop, the Authenticator there has received the new passkey from Apple somehow.  That's just never going to happen.  It's like, they're not going to talk to each other on the fly.  That's just - no.  So it's going to be a third-party password manager that solves this problem.



Okay.  Politico's headline on this was Germany forces EU into damage control over encryption fears.  And then they added that Berlin has criticized the EU's plan to fight child sexual abuse material, CSAM, as a threat to privacy and fundamental rights.  And, you know, no kidding.  We talked about this earlier this year when the EU announced what they were going to do.  I guess, well, yeah, earlier this year.  I guess it was about a month and a half ago.  That's when the European Commission proposed a new law to crack down on sexual abuse of children online.



And as our listeners will recall, that proposal was controversial, to say the least.  It is clearly facing some serious headwind from the bloc's largest member country, Germany.  Since the proposal first aired, the Germany government has repeatedly slammed the proposed legislation as an attack on privacy and fundamental rights until finally, just last week, Germany's Digital Minister Volker Wissing warned that the draft law, as he put it, "crosses a line."



Berlin's opposition prompted the EU's Home Affairs Commissioner to step in last week in what seemed like an effort to limit the damage with some sort of appeasement.  Although, if you look at exactly what the guy said, it's not clear that worked.  This was some guy named Johansson.  He defended the proposal during an impromptu press conference, so I guess you could forgive him for maybe not being prepared, which occurred in Luxembourg on last Friday afternoon, saying that the legislation "is much more targeted" than the current regime to scan for illegal images, which is not what the legislation said.  He said:  "Will only allow only to do detection after a court decision or another independent authority have decided so after consultation with data protection authorities and with specific technologies that have been approved."  What?



Okay.  Those were direct quotes.  Again, off the cuff.  I'm not sure what he was meaning or meant to say.  But if we were to take it as gospel, whoever wrote that is apparently describing some very different legislation than the text we carefully examined more than a month ago.  The intent of that legislation was very clear, as was the breach of end-to-end encryption that would be needed to make it work.



And this is devolving into the sort of mess that it was bound to.  Sweden's EU commissioner spoke alongside Germany's Interior Minister Nancy Faeser, insisting upon Faeser's "strong support" as a mother, an adult and a politician.  Okay.  Not clear what she's strongly supporting.  For her part, Faeser said that "the initiative, from the German point of view," she said, "we support this," but added that "for us, it's important to find the balance," meaning between the right to confidential communication and cracking down on child sexual abuse material.  The problem is it's just not possible to provide both at the same time.  It's not.



So to remind everyone, as it was presented, the revised rulebook wants to force all tech companies, including messaging app providers WhatsApp, Apple's iMessage, Instagram and Telegram, and I'm sure Signal is there, too, anybody, to scan, remove, and report illegal photos and videos of this CSAM material.  Courts could also order digital companies to hunt down manipulative conversations between potential sex offenders and children, which is referred to as "grooming."



So no one wants our technology to be used for criminal child sexual abuse.  But there's no way around the fact that examining photo and video content, and scanning and interpreting text messaging proactively while looking for suggestive "grooming language patterns," there's no way around the fact that it can only be accomplished by defeating both the spirit and the act of end-to-end communications privacy.  You can't have it both ways.



So while welcoming stronger action to protect victims of online abuse, a big collection of German government ministers has nevertheless piled on to lament that the EU proposal would effectively result in mass surveillance of people's private messages and thus undermine encryption.  Of course.  Again, you can't do it both ways.  So we know that crypto technology is obedient.  It'll do anything we want it to.  But what we want, apparently, is to invade everyone's privacy on the off-chance of detecting that their conduct might be criminal, while at the same time invading no one's privacy.  Well, you can't.



JASON:  It doesn't sound likely.



STEVE:  It's just no.  And as we've said more broadly, not just the EU particular legislation, more broadly, this is the big dilemma.  Governments want the ability to see what's going on.  And users - and the government says they don't want to violate end-to-end encryption.  Oh, no, no, no, you can still encrypt things.  But also not.  What?  Okay.  So the great debate, the decryption debate of this century is how do we settle this.



And Jason, you may still be alive by the time that happens.  I'm feeling great, I'm healthy and happy, but I don't know how we're going to resolve this.



JASON:  I know.  Artificial intelligence.



STEVE:  Ah.  That's right.



JASON:  That's the blanket that we throw on every problem nowadays and hope.



STEVE:  Send all that to Skynet.  And what's weird is that, of all the companies, I would argue that Apple's suggestion was the least privacy intrusive.  What Apple wanted to do was to take, although it wouldn't be completely effective, they wanted to take hashes of known objectionable material and put all those hashes on people's phones and check any image being sent against the hash, meaning hash the image, see if it's in the list, and only if so, then flag it for close scrutiny by a human to decide if it's a false positive or not.



And everyone, oh, my god, no.  They objected.  And I get it.  They objected to the idea of even hashes of images being on their own handset.  Yet it was a clever way of solving this problem of addressing the need to in some way deal with the problem of CSAM while not actively monitoring everyone's content.  Basically, and I'm reminded of this because of your comment about AI, it's sort of a local AI-ish sort of solution.  But no.  I mean, that thing was just dead on arrival.



JASON:  Well, and not only that, I mean, it was also an understanding problem, to a certain degree; right?  Like there are a lot of people that - like in our circles, hash makes sense.



STEVE:  We know what a hash is.  



JASON:  We know what a hash is.  The majority of people, when the read the article about what's changing on their iPhone, and they see the word "hash," like they're not going to understand the actual reason that it's a hash or the difference between storing a hash of images on your phone and storing actual image data of some way, shape, or form.  You know what I mean?  So I hate to call it a marketing problem, but ultimately, probably for the wide majority of people, that's exactly what it was.



STEVE:  Yeah, I think you're right.  It was a marketing problem. I think that's the right term.



JASON:  Yeah.



STEVE:  Okay.  So the Conti gang have finally pulled the last plug.  Recall that we previously noted the clear indications that the very prolific, actually too prolific for its own good, Conti ransomware gang had boxed itself into a corner earlier this year after Russia invaded Ukraine by clearly and forcibly siding with Russia in Russia's very unpopular, unprovoked war.  The sanctions that the West and most of the rest of the free world leveled against Russia choked off Conti's victims' ability to pay ransom to Conti in Russia even if they wanted to.



So in reaction, Conti set up a shell game.  All the members of Conti abandoned ship except for one last guy who remained behind to keep the fires burning.  This member continued leaking data and taunting Costa Rica, which was nominally their last victim, which created a faade of Conti still being a going concern and a running operation, while all of the rest of Conti's members quietly moved on to other ransomware groups.



Last month's report by Advanced Intel stated that the only goal Conti had for this final attack, that is, the Costa Rica attacks, was to use the platform as a tool for publicity, to keep the spotlight on Conti as they performed and faked their own death, keeping their multiple rebirths in other names and under other names off anyone's radar.  Even though they were pretending to still be active as Conti, the ransomware operation was not performing any further attacks, and the data being leaked by this one remaining Conti member turned out to be from earlier attacks.  To confuse researchers and law enforcement even more, this sole Conti member released the same victim's data on both their site and Hive's data leak site, where he is also an affiliate.  But this was just all a charade with the rest of the Conti ransomware crew infiltrating or even taking over other ransomware operations.



Well, as of last week, finally, the masquerade is over, and the Conti ransomware operation has finally shut down its last public-facing infrastructure, which consisted of two Tor servers which were used to leak data and to negotiate with victims.  And of course no victims remain.  So they're gone.  The Conti name has been retired, and its crew has spread out and around and have taken up operations under other names.  These guys know what they're doing.  So they formed a potent crew.  And it would be good if they just ended up being less powerful in the future.



JASON:  All right, Steve.  What do we have up next?  Oh, the Log4j, Log4Shell.  This seems to just like never go away, as well.  It's kind of like the Conti gang. 



STEVE:  And in fact that's exactly the point.



JASON:  Yes.



STEVE:  Last Thursday CISA posted a useful reminder report that I thought was interesting on several levels.  Its title was "Malicious Cyber Actors Continue to Exploit Log4Shell in VMware Horizon Systems."  I didn't have it in the show notes, but VMware Horizon basically is a remote desktop system, the idea being that in the cloud you spin up instances of Windows, and you run Windows remotely in a remote desktop mode, or also apps.  So it is inherently public facing in order to function.  And VMware knew right off the bat they had a problem.  They had it patched immediately.  Of course, as we know, it's one thing to patch it.  It's a different thing for those patches to be deployed.



One of the reasons I thought this was important is because it's easy for us to become complacent.  For a few weeks late last year, Log4j and Log4Shell were big news and everyone was scurrying around fearing that this might result in the meltdown of the Internet, or another big worm, or the end of civilization as we know it.  Okay, probably no one really believed that.  But it was initially grabbing all of the tech press headlines, and it certainly had our attention on this podcast.



Then we learned an important lesson.  Because it did not enable super-simple drop-and-go exploitation, despite the fact that the presence of the Log4j vulnerability remained quite widespread  remember we talked about how long it would take for all of the Java packages that were dependent upon it to be updated, if ever, I mean, we're talking years  the mass of the world's script kiddies did not pick it up and run with it.  Potentially widespread though it was, actually exploiting the Log4j vulnerability turned out to require significant per-instance work.  It wasn't the lowest hanging fruit, so lower hanging fruit continued to be preferentially exploited.



But as we also said several months ago, since the Internet is still here, neither did the Log4j threat go away completely.  It was quietly adopted and added to the toolkits of the world's most sophisticated threat actors for judicious deployment when and where its presence had been overlooked on a system where it might offer a way in.  And that's what CISA's posting last Thursday was intended to remind us of, and to document.



CISA wrote:  "The Cybersecurity and Infrastructure Security Agency (CISA) and United States Coast Guard Cyber Command (CGCYBER) are releasing this joint Cybersecurity Advisory (CSA) to warn network defenders that cyber threat actors, including state-sponsored advanced persistent threat actors" - meaning the big guys - "have continued to exploit CVE-2021-44228 (Log4Shell) in VMware Horizon and Unified Access Gateway servers to obtain initial access into organizations that did not apply available patches or workarounds."



Okay, now here we are at the end of June.  Log4Shell was December of 2021.  So we're now six-plus months downstream, and attacks, effective attacks are still happening, not just against random stuff that nobody knew there was a problem with.  No, against VMware's Horizon systems, which are known to be vulnerable, which VMware immediately created patches for  And here we are six-plus months later, and those systems have not been updated.  Thus they're still vulnerable, and bad guys are getting in and setting up shop.



They said:  "Since December 2021, multiple threat actor groups have exploited Log4Shell on unpatched, public-facing VMware Horizon and Unified Access Gateway servers.  As part of this exploitation, suspected advanced persistent threat actors implanted loader malware on compromised systems with embedded executables enabling remote command and control.  In one confirmed compromise, these APT actors were able to move laterally inside the network, gain access to a disaster recovery network, and collect and exfiltrate sensitive data.



"This Cybersecurity Advisory provides the suspected APT actors' tactics, techniques, and procedures" - a new term, TTPs - "information on the loader malware, and indicators of compromise (IOCs).  The information is derived from two related incident response engagements and malware analysis of samples discovered on the victims' networks."



So I have a link in the show notes to the entire Advisory, which it does have a lot of really interesting details, I mean, specific executables, DDLs, scripts, the IP addresses of these command-and-control servers, where all of this stuff is linked to and communicating with.  For anyone who wants more detail, as I said, a link in the show notes.  But the moral of the story is an important reminder, which is that exploits which invariably fall off the radar don't cease to exist.  Just because old problems are no longer being actively discussed should not be any source of comfort for anyone.  So definitely worth keeping systems up to date.  And, boy, the lesson is you just have to wonder who these machines belong to.  They were anonymized, that is, the CISA advisory does not tell us who got themselves hacked by waiting at least six months to update their VMware Horizon system.  So certainly not any of our listeners.  We know that.



Okay.  Last Wednesday, in their third meeting, a group of cybersecurity company experts met in Austin, Texas - that's last Wednesday - to provide industry recommendations to CISA.  So they have sort of an industry meeting group, government-facing interface.  The group was founded in June of last year, and its first meeting was held in December.  This one last week was its third.  The group is split into six subcommittees, each focused upon different issues which cover cyber workforce; information dissemination; cyber hygiene, whatever that is; technical advisories; critical infrastructure; and misinformation.  



The cyber hygiene subcommittee, led by Apple's VP of Corporate Information Security, suggested that CISA "launch a '311' national campaign, to provide an emergency call line and clinics for assistance following cyber incidents for small and medium businesses."  The measure was also floated by the communications subcommittee, which was led by a member of Tenable's board.



A spokesperson for Checkpoint, the security firm Checkpoint Software, said that the idea for a "311" emergency line is "smart and timely."  The Checkpoint executive noted, he said:  "Right now we're seeing on average organizations in the United States being attacked 868 times per week."  868 known attacks per week.  "The emergency line," he said, "can make for a faster path towards incident response."  During this month of June, which is almost over, Checkpoint said that they were seeing an average of at least 27 cyberattacks against small and medium-size businesses per week, and that this was an increase of 72% compared to last year.  So the attacks are on the rise, and there's lots of them.  And when you think about it, there are far more small and medium-size businesses than big behemoth businesses.  And while the small/medium-size businesses can't fork up so much ransom as a biggie, there's lots more of them.



Okay.  So meanwhile, CISA executives and others continue to push for more robust and reliable incident reporting.  As we covered at the time, cyber incident reporting legislation was passed and signed into law earlier this year, but it only covers critical infrastructure organizations.  All qualifying organizations are now required by law to report breaches to CISA within three days, 72 hours, and report ransomware payments within one day, 24 hours.



Three weeks ago, during the annual RSA security conference, Eric Goldstein, CISA's executive assistant director for cybersecurity, spoke at length about how damaging the lack of data on ransomware attacks in the U.S. is for organizations like his.  And I assume that more attacks might mean more funding, although CISA has recently been getting a great deal of funding.  Eric told attendees of RSA that, he said:  "Only a small fraction of ransomware victims are reported to the government, and the problem is getting worse."  He said:  "We have no idea what the actual number is.  We have no idea what level of ransomware instructions are occurring across the country on any given day."  



And I hadn't really stopped to think about it, but that would really be true; right?  I mean, if you aren't required by law to report a breach of your organization, and if you don't have cyberattack insurance, as I imagine most small-to medium-size businesses probably don't, so you're able to keep the fact of an attack closely held, then why would small businesses do anything more than arrange payment and hope to obtain a recovery key?  I can see how that lack of information flow to CISA would be frustrating for them.



But even so, it's unclear what good calling some 311 cyberattack line would do.  It's not as if the government is going to pay the ransom for you.  And you can imagine that the bad guys are going to take the standard action of anyone doing extortion of saying "Don't get law enforcement involved, or you'll never see your data again.  We'll know if you do."  So I suppose it's unsurprising that small- to medium-size organizations that are attacked are not reaching out, and that CISA is annoyed by this because they'd just like to know.



Some reports are just demoralizing and depressing.  One such is a recently published report from Forescout which details a collection of 56 vulnerabilities which they have found in the so-called "OT" or what's known as Operational Technology category of devices.  OT is the newer term which we once used for SCADA systems.  So they're the technologies used to monitor and control oil and gas refining, chemical and nuclear power generation and distribution, manufacturing, water treatment and distribution, mining, and building automation.  You know, the industrial control sorts of things.



What's interesting is that these products are sold under the rubric "secure by design," and many even carry, I think three quarters even carry certifications for OT operational technology, like having met operational technology security standards. But Forescout's report was titled "OT:ICEFALL," which is the name they gave to this collection, they said "A Decade of Insecure-by-Design Practices in OT."



The vulnerabilities affect Siemens, Motorola, Honeywell, Yokogawa, ProConOS, Emerson, Phoenix Contract, Bentley Nevada, Omron, and JTEKT.  And the disclosure of this mess was coordinated with CISA and other relevant government agencies around the world, other governments' equivalents of CISA.  Summarizing what they found, the vulnerabilities could be divided into four categories:  Insecure engineering protocols, so like they made up their own protocol, and it was not secure.  Shock.  Because we know how difficult it is to create secure protocols.  Or weak cryptography or broken authentication schemes.  Again, if you roll your own good, good luck.  Third, insecure firmware updates.  And fourth, remote code execution through native functionality.  In other words, remote code execution was supported natively by some of these things.



38% of the 56 vulnerabilities allowed for compromise of credentials.  One in five, 21%, allowed for firmware manipulation; and 14% allowed remote code execution.  And despite that, three quarters, as I noted, of the affected product families carry some form of feel good, yet apparently worthless, security certification.  Forescout explained their intentions and why they felt this work was important.



They wrote:  "With OT:ICEFALL, we wanted to disclose and provide a quantitative overview of Operational Technology insecure-by-design vulnerabilities rather than rely on the periodic bursts of CVEs for a single product or a small set of public, real-world incidents that are often brushed off as a particular vendor or asset owner being at fault.  These issues range from persistent insecure-by-design practices in security-certified products to subpar attempts to move away from them.  The goal is to illustrate how the opaque and proprietary nature" - and you can almost hear my voice in this - "the opaque and proprietary nature of these systems, the suboptimal vulnerability management surrounding them, and the often-false sense of security offered by certifications significantly complicate OT risk management efforts."



So this report highlights another of the recurring themes of this podcast.  And that is that something is very wrong with the model we currently have for proprietary technology being blindly applied in critical infrastructure such as power generation and water treatment without any truly effective oversight over the security of these proprietary systems.  We need to move away from this model and away from a culture where the security status of products whose security affects vast numbers of people is allowed to be obscured by their proprietary intellectual content.  This is the classic voting machine problem.  Until this happens, until the culture and I guess it's going to have to be government who says, you know, if we're allowing you to manage our water, then we're not going to allow you to use systems that have not been opened and scrutinized.  And again, a voting machine.  You can sell the hardware, but you can't keep the software proprietary.  It's just too important how it works.  And until that happens, our infrastructure's going to be fragile, essentially by design.



JASON:  I don't understand why that's not the default.



STEVE:  I know.  Isn't it dumb?



JASON:  Doesn't make any sense, yeah.



STEVE:  And Jason, when you think about it, though, it is the culture.  Every software license that we click on says that the manufacturer is not responsible no matter what it does.  The Windows license, well, you know, we don't know if it even works.  We don't know if it's good for anything.  And you're going to click this to eliminate all responsibility from us.  You're responsible for what it does.  I mean, that's just like saying, oh, well, gee, I want that, like to use it, so what choice do I have?  Okay, fine.



JASON:  Yeah, exactly.  And all it is is a single click away; right?



STEVE:  Yeah.  And has anyone ever successfully read that crap?  You can't.  I mean, your mind goes numb.  



JASON:  Yeah, for sure.



STEVE:  Trying to read and parse all that boilerplate.  Okay.  I have something completely off the reservation for Miscellany, totally random and off-topic.  I tweeted about it.  I just wanted to share it with our listeners.  And it would be completely off-topic if it didn't have a little bit of time-travel in it, even though it's not sci-fi.  It is something that my wife and I discovered completely by chance Sunday evening.  It's a low-budget, independent, 90-minute, well-written romantic comedy.  And believe me, I'm not a customer normally of romantic comedies.  It just doesn't get me.  But this one on Netflix did.  It's called "Long Story Short."  It was released a year ago on July 2nd of 2021.



It isn't any sort of blockbuster.  And interestingly, it only scored a 6.5 on IMDB, which is below my normal 7.0 threshold for IMDB.  But it's odd because all the reviews that I saw were like raving about it, with 8's, 9's, and 10's.  So in any event, this little gem carries and I think beautifully delivers an important message about the way we conduct our lives, and not in a preachy way.  So anyway, it's called "Long Story Short."  Since I know how widely people's tastes vary, I'm not offering a guarantee.  But I know that I would have been glad to have been told about it.  So I didn't want to pass up the opportunity to share this little discovery.  Again, "Long Story Short" on Netflix. And if you have someone to watch it with, so much the better.



JASON:  I'll check that out.  Yeah, you had mentioned that in email, and I'm trying to know enough about it to pique my interest, but not so much about it that I'm going to spoil the movie because it seems like a movie you probably don't want to spoil very much.



STEVE:  You don't want to spoil it.  I'll just say that I am a fan of dialogue.



JASON:  Yeah, me, too.



STEVE:  I loved back in the day "The West Wing" because of the Aaron Sorkin, like real people don't actually talk this way kind of dialogue.  The lead male actor, you would almost think he's a standup comic because his lines are delivered with such timing.  And, I mean, you have to - don't watch this if you're like falling asleep because it really requires your attention.  But it's very clever.  And again, the point of it is, not only was it fun, but it's a worthy message is bound into this thing.  So I think you and wifey would like it as much as I and wifey liked it.



JASON:  Right on.  Thank you, Steve.  I'll check it out.



STEVE:  Okay.  So 10, a surprising number of closing-the-loop bits of feedback.  Lawn_dart, he tweeted:  "Just listened to this week's SN, and the encryption problem still isn't quite complete.  If an attacker knows a significant amount about the plaintext, then encrypting plaintext guesses might be more efficient."  So he's saying going forward encrypting plaintext guesses.  He said:  "The puzzle only truly works if the plaintext is also indistinguishable from entropy."



Now, I'm sharing this tweet because what you always do with public key encryption - although this was symmetric encryption, so not public key.  So I guess what he's saying is that if you were guessing what the plaintext was, then you could use it to guess the keys.  The problem is, well, I guess the problem is the plaintext is likely far longer than the private key that you're trying to guess.  So there are, like, there are already 2^256 times 2^256 guesses in double encrypting with two symmetric keys.  So 512 bits.  But plaintext is going to be way longer than that, typically.  It's going to be a book or a movie or something, megabits in length.  So I don't think that's a practical solution.



Tiemo Kieft, he said:  "Routers are IOT devices.  They interact with the physical world through LEDs.  Either that, or smart light bulbs are not IOT."  And of course he's referring to what we talked about, the NIST formal definition of IoT as being something that had a sensor or was able to affect the environment in some fashion.  And, you know, yeah.  And routers have light bulbs on them, or LEDs, as well.  So okay.



Arvind, wow, his last name is Narayanan.  His Twitter handle is easier, it's @random_walker.  He said:  "I mis-clicked on one of my 150 open tabs" - meaning he has a browser like I do, he says - "and it happened to be a tab that's been open since 2019 with a paper that has a solution to the exact research problem I've been puzzling over today."  He says:  "This is the moment I've been waiting for, and I've decided to never close a tab again."  So anyway, just a bit of humor on the topic of, yes, keeping a gazillion tabs open and arranging to make that practical.



LDizzy tweeted:  "Hello, Steve.  I've been listening to a lot of the talks lately about passkeys and how it locks you into the ecosystem you start with more or less.  But I think there's a big chunk we're missing here.  While the public/private key pair does function much like a more sophisticated password, where things will get better in sites is when they accept more than one passkey.  Instead of having a single password that's reset via email when lost, sites will most likely allow for the addition of several passkeys.  If I am an iOS mobile user that also runs Windows on my desktops, which I am" - much as I am - he says, "then I could add an iOS-based passkey on my phone that's synced via iCloud.  Then, whenever I get around to logging into the site from my computer, I could use my iPhone once to authenticate and add a second passkey from the desktop that would then be synced within Microsoft's ecosystem."



He says:  "This makes sense to me and seems like a new paradigm.  There's no real reason why sites do not allow multiple passkeys, although this wouldn't have been logical with the old username/password combo."  Okay, and I don't understand why that's true, but okay.  He says:  "The downside is that if you use multiple ecosystems, then you'd have to add a passkey within each.  But they don't have to be synced between ecosystems, and it's a one-time thing for each.  I doubt many people use more than two.  I hope I'm not being overly optimistic, and I'd appreciate your take on this.  Love your work."



So I guess my main take is everything about what LDizzy tweeted presumes that sites will allow multiple passkeys per account.  And I see no reason for that to be true.  Granted, that would be cool.  But then, if you do that, I guess if you were to change a passkey, it would change the passkey that you used when you logged on, so the site would have to know which of multiple passkeys you used when you logged on.  And then how would you control access to the other passkeys?  It sounds like a little bit of a logistics problem there.  I'm not sure, but we'll find out once sites begin to support them.  I don't know, I've not looked yet in depth into WebAuthn.  And I am thinking that we're going to have to do a podcast, a deep dive into the operation of WebAuthn since it will before long, I think it's inevitable, become the way websites authenticate.  And so we certainly need to cover that on this podcast.



Joseph Fienberg tweeted:  "My wife and I each received an email about the Facebook tracking class action settlement.  When I went to click on the link to the file containing the class action claim, UBlock Origin blocked lzzgcc5d.r.us-east-1.awstrack.me, which is not secure and is a tracking link for a tracking settlement where we'll each probably get 50 cents.  I thought Security Now! listeners would get a laugh out of that and the irony of this."  Indeed.  Thank you, Joseph.



Thomas Martin said:  "Hi, Steve.  I wanted to weigh in on your solution to the double encryption dilemma in SN-876.  Your position was that an attacker cannot know the intermediate text, and therefore the two encryptions cannot be attacked separately.  The conclusion was then that double encryption was substantially more secure than single encryption.  If you can brute force a single 256-bit cipher using AES-256 in one day, then it does not take two days to brute force double AES-256, it would take on the order of 2^256 days."  Agreed.



He says:  "Unfortunately, this line of reasoning is incorrect" - oh - "as it does not take into consideration another type of attack, a meet-in-the-middle attack.  The attacker needs one known ciphertext-plaintext pair.  Meaning one instance where the ciphertext and the plaintext were encrypted.  They encrypt the plaintext with every possible key and store the key and the text pairs in a lookup table."  And then he notes that it requires 2^256 operations and on the order of 2^256 storage.  "They also decrypt the ciphertext with every possible key.  Each resulting text is checked against the lookup table.  If it matches a stored text, then the attacker has a pair of keys that decrypt the known ciphertext to the known plaintext.  This would need to be confirmed with other plaintext/ciphertext pairs, and there is a possibility of a coincidental match."



Okay.  Thomas is correct as far as it goes.  And so the point was that a meet-in-the-middle attack is absolutely worth discussing.  It is a means of producing a computational and storage-based shortcut to the brute forcing approach which is what we've talked about.  That is, where you would need to try the first decryption of which there's 2^256 possible keys, then try all of the 2^256 possible second encryption keys for each of the first, thus making the total effort 2^512.



There is a better way, and Thomas highlights it.  It's a famous meet-in-the-middle attack where if you can get a ciphertext and plaintext pair, and if you have sufficient storage, and this is where this falls into impracticality, unfortunately, meet-in-the-middle attacks were famous back when key lengths were 2^64 or thereabouts.  Then you could kind of say, okay, well, it might be feasible to have that much storage.  This requires 2^256 sets of keys and matching text which can then be compared after the first guess.  But each of those guesses needs to be compared against all of the 2^256 sets of storage.  And at that point I think that this sort of brute forcing shortcut, while theoretically possible and worth reminding ourselves it exists, now that keys have gotten as long as they have, you just - yes. If it were possible, it's a shortcut.  So Thomas, thank you for it.



Douglas Nichols says:  "Double encrypting can be divide-and-conquer if you are using an authenticating encryption like AES-GCM or CCM modes on at least the outer encryption since the integrity check will fail."  And I thought that was a great observation.  All of the assumption that we've been making is that authenticating encryption is not there, specifically because it isn't, like, hard to do.  If you immediately get confirmation when you've guessed the proper first half of the key, then in fact it would take you two days, if it took you one day to solve the outer.  So Douglas, thanks for pointing it out.  It's not the problem we're solving, but it is also definitely worth noting.



Sean OBrien said:  "Many years ago you gave almost the opposite answer to the divide-and-conquer attack.  Back then you said the encryptions add because of authentication.  If the first encryption is signed, then the first decryption is obvious.  It's in the case without authentication that the first decryption results in noise."  And so Sean, thank you for reminding me that I did mention basically exactly what Douglas had tweeted, and that is that if you have any sort of authentication of the first encryption, then it's trivial to determine when you have guessed right with the first one.



Rando said:  "A better proof that we live in a simulation is Murphy's Law.  Because statistically it is unlikely for everything possible to go wrong all the time without exception," he says, "but in my experience that's exactly what happens."



JASON:  Awww.



STEVE:  So I got a kick out of that.



JASON:  I'm sorry, Rando.



STEVE:  The fact that you have Murphy's Law means that, okay, yeah, you're right.  That's a failure of stochastic randomness.  Can't happen in a completely random environment.  So we must be living in a simulation with a bad random number generator.



And the guy who we mentioned last week who once was testing Windows in their hardware-based Windows software testing lab, sent me a series of tweets.  The first one said:  "Hey, Steve.  It was really cool hearing you talk about my Microsoft career as a Senior SDET."  And that was a Device Engineer of Testing.  That's the DET.  He says:  "I love sharing my experiences with the Windows Hardware Testing Lab and ESC teams responsible for testing the WINMAIN branch and talking about why the current system post-2015 layoffs is so bad."  So it was nice to have the loop closed with that.



JASON:  All right.  So why does it Hertz so bad, or so good?  The Hertzbleed Attack, what have we got going on here?



STEVE:  So this has everything you want.  It's got a snappy name.



JASON:  Yeah.



STEVE:  It's got its own website.  It's got a memorable logo, you know, everything the modern vulnerability needs.  And its name is an obvious play on 2014's Heartbleed attack, for good reason.  Remember that eight years ago Heartbleed was one of those rare vulnerabilities, like Dan Kaminsky's realization about DNS spoofing vulnerabilities, that truly moved the industry to action.  That one was CVE-2014-0160.  Ah, those good old days of four-digit CVEs.  Yeah, we don't have that anymore.



Heartbleed, that is, eight years ago the original Heartbleed attack, its summary reminds us, says:  "The Heartbleed Bug is a serious vulnerability in the popular OpenSSL" - as I mentioned before about OpenSSL - "cryptographic software library.  This weakness allows stealing the information protected, under normal conditions, by the SSL/TLS encryption used to secure the Internet.  SSL/TLS provides communication security and privacy over the Internet for applications such as web, email, instant messaging, and some virtual private networks.



"The Heartbleed bug allows anyone on the Internet to read the memory of the systems protected by the vulnerable versions of the OpenSSL software.  This compromises the secret keys used to identify the service providers and to encrypt the traffic, the names and passwords of the users, and the actual content.  This allows attackers to eavesdrop on communications, steal data directly from the services and users, and to impersonate services and users."



And finally they said:  "We have tested some of our own services from attackers' perspective.  We attacked ourselves from outside without leaving a trace.  Without using any privileged information or credentials, we were able to steal from ourselves the secret keys used for X.509 certificates, usernames and passwords, instant messages, emails, business-critical documents, and communications."



Okay, so that was eight years ago with Heartbleed.  Today, of course, we have Hertzbleed.  And anyone who is interested, it's Hertzbleed.com - H-E-R-T-Z-B-L-E-E-D dot com.  Okay.  So here's how Hertzbleed, today's problem, describes itself:  "Hertzbleed is a new family of side-channel attacks:  frequency side channels. In the worst case, these attacks can allow an attacker to extract cryptographic keys from remote servers that were previously believed to be secure."  And I'll note that whereas Heartbleed leveraged a bug in the OpenSSL library at a certain version level until it was patched, this is not a bug.  Not a bug.



"Hertzbleed," they said, "takes advantage of our experiments showing that, under certain circumstances, the dynamic frequency scaling of modern x86 processors depends upon the data being processed.  This means that, on modern processors, the same program can run at a different CPU frequency and therefore take a different," they called it "wall time," you know, like real-world time, "when computing, for example, 2022 plus 23823 versus 2022 plus 24436."



And they finish:  "Hertzbleed is a real and practical threat to the security of cryptographic software.  We have demonstrated how a clever attacker can use a novel chosen-ciphertext attack against SIKE" - S-I-K-E, which I'll describe in a minute - "to perform full key extraction via remote timing, despite SIKE being implemented as 'constant time.'"



Okay.  So SIKE stands for Supersingular Isogeny Key Encapsulation.  And all we really need to know is that it's a state-of-the-art post-quantum crypto.  In other words, meant to solve the quantum computer problem.  This is like next-generation.  It's been applied for NIST certification.  But what it is is not really that important.  The point is that, regardless of how secure something is, and this SIKE, S-I-K-E, is like post-quantum super secure, its keys can still be extracted.  And if that's the case, it's game over.  Doesn't matter how secure and how far post-quantum you are.  If you've got the keys, doesn't matter.



Okay.  So I should first say a little bit about power consumption in modern state-of-the-art semiconductor systems.  Today's CPUs still use transistors.  But whereas the first transistors that were originally invented were current amplifiers, which are known as "bipolar transistors," today's transistors use electrostatic charge rather than active current to open and close their switches.  They are metal oxide semiconductor field effect transistors.  So that's the abbreviation M-O-S-F-E-T, or MOSFET.  



If a MOSFET transistor is either on or off, no power is consumed.  None.  In order to "flip the switch," the controlling gate of a field effect transistor must either be charged up with electrons or drained of its electron charge.  But once that's done, the gate will remain charged up or drained out, and the switch will remain open or closed without consuming any power.



So the key thing to appreciate is that in a large array of interconnected MOSFET transistor switches, which is any modern CPU, power is only consumed when the states of those MOSFET switches are changed.  And the amount of power consumed is proportional to how many switches are changed and how often they're changed.



This explains why we can hear our laptop fans spin up when our machines get busy, and why they eventually spin back down some time after our computers have been idle.  When a CPU is idling, it is doing less work because it's got less work to do.  Since switching transistors on and off is what consumes power, batteries can be made to last longer and systems can be made to be more "green" by slowing down the clock speed of CPUs when they don't have much work to do.  A reduced clock speed means fewer transistors switching per second, which means less power needed and consumed, which means less power lost as heat, and fans don't need to spin as fast since there's less heat needing to be removed from the CPU.



The dynamic speed scaling of today's CPUs has become a virtual art form, with the CPU tightly and instantly changing its speed based upon the instantaneous demands that are placed upon it.  And if you've guessed that this dynamic speed changing results in a new form of side-channel information leakage, give yourself an "A" and move to the head of the class.



Looking again at what the researchers said in their summary, they said:  "Hertzbleed takes advantage of our experiments showing that, under certain circumstances, the dynamic frequency scaling of modern x86 processors depends upon the data being processed."  In other words, and they went on to talk about how adding two different numbers would actually result in the CPU running at a slightly different speed.



So this work was done through a collaboration of a team of six researchers from universities in Illinois, Texas, and Washington.  Their paper, which is titled "Hertzbleed:  Turning Power Side-Channel Attacks Into Remote Timing Attacks on x86," will appear in the 31st USENIX Security Symposium being held in Boston from August 10th through the 12th in about a month and a half, this summer.



So what does this mean for us?  They did a fun Q&A, which I will share bits of and also add some commentary to.  First question:  "Am I affected by Hertzbleed?"  Answer:  "Likely, yes."  They said:  "Intel's security advisory states that all Intel processors are affected."  They said:  "We experimentally confirmed that several Intel processors are affected, including desktop and laptop models from the 8th through the 11th generation Core microarchitecture."  Although Intel was even  broader, saying, yup, got them all.



"AMD's security advisory states that several of their desktop, mobile, and server processors are affected."  They said:  "We experimentally confirmed that AMD Ryzen processors are affected, including desktop and laptop models from the Zen 2 and Zen 3 microarchitectures."  They said:  "Other processor vendors, for example ARM, also implement frequency scaling in their products and were made aware of Hertzbleed.  However, we have not confirmed if they are, or are not, affected by Hertzbleed."



So next question:  "What is the impact of Hertzbleed?"  They answered:  "First, Hertzbleed shows that on modern x86 CPUs, power side-channel attacks can be turned into even remote timing attacks, lifting the need for any power measurement interface.  The cause is that, under certain circumstances, periodic CPU frequency adjustments depend upon the current CPU power consumption, and these adjustments directly translate into execution time variations.  Second, Hertzbleed shows that, even when implemented correctly as constant time, cryptographic code can still leak via remote timing analysis.  The result is that current industry guidelines for how to write constant-time code, such as Intel's, are insufficient to guarantee constant-time execution on modern processors."  And that is the brilliance of what these guys have done.



We talked a long time ago about the need for constant-time execution of cryptographic algorithms.  Or being a bit more specific, never having any secret information, like the cipher's keying material, directly controlling the execution path through the cipher algorithm.  As we know, in modern processors, all execution paths leave breadcrumb trails in the underlying microarchitecture, things like the processor modifying its future branch predictions based upon past actual branches taken and not.  So constant-time code is carefully designed now to take the same path and to execute in the same number of CPU cycles specifically so as to give attackers who may be carefully watching from the outside, or even in an adjacent VM, or sharing hardware, living in a different VM on the same chip, keeping them from seeing anything about what specific data was processed.



What these guys brilliantly realized was that there's actually  another form of information leakage occurring from Intel's x86 and AMD's Ryzen processors.  Even though the number of CPU cycles may be constant, and the execution path may be invariant, the exact number of individual transistors whose on and off states were changed during the computations will almost necessarily differ depending upon the data that was actually processed.  And since CPUs dynamically scale their speed based on power consumption, the actual time required to perform the computation, not in CPU cycles but in actual passage of world time, will be subtly altered.  And believe it or not, that can leak secret key information, and they proved it.  Their code is open source and available for people to play with.



So they ask themselves:  "Should I be worried?"  And they answer:  "If you're an ordinary user and not a cryptography engineer, probably not.  You don't need to apply a patch or change any configurations right now.  If you are a cryptography engineer, read on."  They said:  "Also, if you're running a SIKE decapsulation server, make sure to deploy the mitigation described below."



They ask:  "Is there an assigned CVE for Hertzbleed?  Yes.  Hertzbleed is tracked under CVE-2022-23823 and CVE-2022-24436 in the Common Vulnerabilities and Exposures (CVE) system."



"Is Hertzbleed a bug?  No.  The root cause of Hertzbleed is dynamic frequency scaling, a feature of all modern processors, used to reduce power consumption during low CPU loads and to ensure that the system stays below power and thermal limits during high CPU loads."



"When did you disclose Hertzbleed?"  And they said:  "We disclosed our findings, together with proof-of-concept code, to Intel, Cloudflare, and Microsoft in the third quarter of last year, , and to AMD in the first quarter of this year, 2022.  Intel" - I love this.  "Intel originally requested our findings be held under embargo until May 10, 2022."  So just last month.  "Later, Intel requested a significant extension of that embargo, and we coordinated with them on publicly disclosing our findings on June 14, 2022."  In other words, we said no.  You already had six months.  What are you going to do?



And the question:  "Do Intel and AMD plan to release microcode patches to mitigate Hertzbleed?  No.  To our knowledge, Intel and AMD do not plan to deploy any microcode patches to mitigate Hertzbleed.  However, Intel provides guidance to mitigate Hertzbleed in software.  Cryptographic developers may choose to follow Intel's guidance to harden their libraries and applications against Hertzbleed. For more information, we refer to the official security advisories, Intel and AMD."



"Why did Intel ask for a long embargo, considering they're not deploying patches?"  Answer:  "Ask Intel."



"Is there a workaround?  Technically, yes.  However, it has a significant system-wide performance impact.  In most cases, a workload-independent workaround to mitigate Hertzbleed is to disable frequency boost.  Intel calls this feature 'Turbo Boost,' and AMD calls it 'Turbo Core' or 'Precision Boost.' Disabling frequency boost," they write, "can be done either through the BIOS or at runtime via the frequency scaling driver.  In our experiments, when frequency boost was disabled, the frequency stayed fixed at the base frequency during workload execution, preventing leakage via Hertzbleed.  However, it is not a recommended mitigation strategy as it will significantly impact performance.  Moreover, on some custom system configurations with reduced power limits, data-dependent frequency updates may occur even when frequency boost is disabled.  In other words, as with Spectre and Meltdown, this is another instance where a longstanding CPU optimization must be discarded if its exploitation is to be completely eliminated. Nothing is safe."



And then:  "What is SIKE?  SIKE," they answer, "Supersingular Isogeny Key Encapsulation, is a decade-old, widely studied key encapsulation mechanism."  In other words it's a public key sharing system.  They said:  "It's currently a finalist in NIST's Post-Quantum Cryptography competition.  It has multiple industrial implementations and was the subject of an in-the-wild deployment experiment.  Among its claimed advantages are a well-understood side-channel posture.  You can find author names, implementations, talks, studies, articles, security analyses and more about SIKE on its official website."



Then they ask:  "What's a key encapsulation mechanism?"  Then they say:  "A key encapsulation mechanism is a protocol used to securely exchange a symmetric key using asymmetric public key crypto."



They ask:  "Is my constant-time cryptographic library affected?"  And they answer:  "Affected?  Likely yes.  Vulnerable?  Maybe."  They answer:  "Your constant-time cryptographic library might be vulnerable if is susceptible to secret-dependent power leakage, and this leakage extends to enough operations to induce secret-dependent changes in CPU frequency. Future work is needed to systematically study what cryptosystems can be exploited via the new Hertzbleed side channel."



And finally:  "Do you release the source code of the Hertzbleed attack?  Yes, for full reproducibility.  You can find the source code of all the experiments from our paper at the link https://github.com," and then the rest of the link is /FPSG-UIUC/hertzbleed.  It's all there.



So we have a new attack.  Much as with Spectre and Meltdown, there is no good solution.  Even in the instance, and in fact it was Intel's, Intel provided a constant time crypto library, but it was constant in cycle count, not in actual real-time.  Whoops.  So that's going to be a problem.  What you would have to do, you know, you could say, well, you could disable CPU scaling, speed scaling, during the cryptographic operation and then reenable it.  That won't work because the cryptographic operation itself is what consumes a variable amount of power.  So then if after the operation is complete you reenable scaling, the CPU will still immediately scale based on the power that was previously consumed.



What you would have to do is literally develop another technology, much as we now have constant time crypto, you would literally need constant power crypto so that the power being consumed by the architecture of the CPU was not affected based on the secrets that you're trying to keep in the crypto.  And I would imagine that's what the ultimate solution will be is we'll get post-quantum that is not only constant in time, but constant in power consumption.  And I'm glad I don't have to write that code.



JASON:  If you had to choose your favorite, Heartbleed or Hertzbleed, what would it be, Steve?



STEVE:  Well, Heartbleed's solved.



JASON:  Right, yeah, right.



STEVE:  Hertzbleed is an open problem now, which makes it really more interesting.



JASON:  This is the challenge of the work that you do.  There's always a new thing.  So even when these things happen, and it seems like the end of the world, like I guess maybe that's the upside, is as bad as some of these things seem, we still get a year, two, three years down the line, and we realize things are still moving.



STEVE:  The world did not end.



JASON:  The world did not end.  Although when it's happening, man, it seems like the biggest deal in the world.  So at least there's that.



STEVE:  Well, and there is a real point there, and that is one could argue that the world did not end because we did update the servers for Heartbleed.  The world did not end because we did secretly on one day announce DNS spoofability when all the servers were ready to be updated.  So and the best example is the Y2K problem.  I mean, I literally sat here awake, waiting to see what was going to happen because we know, if nothing had been done, a chunk of the world would have gone insane.  But we knew about it in time to mitigate a huge distributed mitigation effort across the globe.  And so nothing happened.  And there were people who said, see, it was all a big con.  It was nothing.  It was a tempest in a teapot.  It's like, yes, fortunately that's what we turned it into.



JASON:  Exactly.



STEVE:  By fixing the code. 



JASON:  Right.  But I want to see it with my own eyes.  It's like, no, not good enough, even though the solution is the real story.



STEVE:  We've got another one coming up in 2038.  That's when Unix time wraps.  There is a Unix time is the number of seconds which have elapsed since some date, I don't remember when it is, but it goes back to zero in 2038.  And that's not long from now.



JASON:  No.



STEVE:  And there's a lot of systems in closets and a lot of old Unix kernels that are going to get very confused in 2038.  And that's going to be tough.



JASON:  I suppose we have plenty of time to signal that.



STEVE:  I don't know.  I don't know if anybody is paying attention.



JASON:  Well, yeah, exactly.  It's almost too much time to signal.



STEVE:  Sixteen years; you know?  I guess we can hope that every...



JASON:  Right.  Let's make that a priority right now.  We'll see what happens.  Well, Steve, thank you so much.  Always appreciate the work that you do.  I know everybody that watches and listens echoes that sentiment completely.  You're a master and a pro at all things security, so we appreciate you sharing your knowledge with us every single week.



STEVE:  Glad to be here.



JASON:  If you want to follow what Steve is doing, all you have to do is head over to his site, GRC.com.  You can find all sorts of Steve goodness on the site.  Of course SpinRite.  You can find information about the best mass storage recovery and maintenance tool and get your copy there.  You can find audio and video of this show there, as well as I believe that's the only place where you can get transcripts of this show found there, so go to GRC.com for that.  We of course have a show page on the web for this show as well, TWiT.tv/sn, so you can find our audio, our video, the ways to subscribe to the podcast, jump out to YouTube.  Anything you need between those two sites you're going to find it.



This show records live every Tuesday at 4:30 p.m. Eastern, 1:30 p.m. Pacific, 20:30 UTC.  So if you're around at that time, and you want to watch us do the show in real-time, usually it's Leo with Steve, but today it's of course me sitting in for Leo.  You can do that at TWiT.tv/live, and then of course participate in the chatroom and all that fun stuff.  And finally, don't forget Club TWiT, TWiT.tv/clubtwit, that's our subscription tier for all things we do with no ads and a whole lot of other perks and features.  So TWiT.tv/clubtwit.



Thank you, Steve.  Thanks for letting me crash your party this week.  I always enjoy it.



STEVE:  Thanks for standing in for Leo, Jason.  It's always a pleasure.  And I think we've got, I don't know when - oh, no, I do know when.  Leo does have another trip coming up.  I actually have it...



JASON:  He's got, well, he's got the cruise, that's right, the cruise is coming up, what is that, a month from now?  A month-ish?  Somewhere?  I don't have the dates.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#878

DATE:		July 5, 2022

TITLE:		The ZuoRAT

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-878.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Chrome's fourth zero-day of the year and at another welcome privacy-enhancing bump from Firefox.  We share the disclosure and forensic investigation of the bug bounty clearinghouse HackerOne's discovery of a malicious, now ex-, employee among their ranks.  Some listener feedback draws us into a discussion of the nature of the vulnerabilities of connecting Operation Technology systems to the Internet, and also some hope for the future amalgamation of the currently fragmented smart home IoT industry.  And before we start into our deep dive into some new and worrisomely prolific malware, we're going to consider whether we'd rather have one 9" pizza or two 5" pizzas.  As always, another gripping episode of Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Another zero-day exploit for Chrome.  Some new settings you might want to turn on in Firefox.  We'll walk you through it.  And HackerOne has a malicious, now ex-, employee they want you to know about.  Plus the latest in router exploitation with ZuoRAT.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 878, recorded Tuesday, July 5th, 2022:  The ZuoRAT.



It's time for Security Now!, the show where we cover the latest security advances, things you should know to protect yourself, your network, your company, privacy information, and a little bit of how computers work and science fiction thrown in for good measure.  All thanks to this guy.  It's really the mind of Steve Gibson on display.  Hello, Steve.



STEVE GIBSON:  Oh, that's a sad statement, but it is true.



LEO:  Thanks to Jason Howell.  Did Jason do the show last week, or Mikah?  Jason did it.



STEVE:  Yup, he did it last week, and we're going to see him two weeks when you're off sunning yourself, cruising in the Pacific.



LEO:  No, it's Alaska.  Well, it is the Pacific, but I don't think there'll be a lot of sun.  I mean, there will be sunshine.



STEVE:  Not a lot of sun?



LEO:  It's Alaska, so there will be glaciers, that's for sure.



STEVE:  Anyway, it was great working with Jason.



LEO:  Yeah, he's wonderful.



STEVE:  He's comfortable with the podcast since I guess he's the producer of the podcast normally.



LEO:  Yes.



STEVE:  And he also listens to it, so he was sort of up to speed on things.  I didn't have to say, okay, now, we talked about this three weeks ago, and here's what that is.  So he had the background.  That was great.



So we are at our first podcast for July, as we start into the second half of the year, 878.  And no one knows how to pronounce this, unfortunately.  We're going with ZuoRAT?



LEO:  Zuo, ZuoRAT.  It's a Chinese word.



STEVE:  Yes.  For those who are as phonetically impaired as I am, it's spelled Z-U-O-R-A-T, ZuoRAT.  Of course RAT is Remote Access Trojan.  



LEO:  Oh.



STEVE:  And it's on our radar because a state-sponsored actor, apparently in China, has managed to install this bad remote access trojan in many SOHO routers, taking advantage of the fact that COVID drove people home.  And so somebody realized, hey, we've got all - and this is U.S. and European targeted routers have been infected with this as a means of getting into the corporate network through a remotely located employee.  So the world in which we live today.



But first we're going to take a look at Chrome's fourth zero-day of the year and at another welcome privacy enhancing bump that Firefox just received from Mozilla.  We're also going to share the disclosure and forensic investigation of the bug bounty clearinghouse we've talked about often, HackerOne, and their discovery of a malicious, now ex-employee among their ranks.  Can happen to everyone.  And we don't talk about insider threats as much as we probably should.  So I thought this would be a good opportunity to sort of take a look at that.



We also have some listener feedback which has drawn us into a discussion, or will, of the nature of the vulnerabilities of connecting, and this is a term that we discussed last week, Leo.  We used to talk about SCADA devices.  Now they're sort of more generically being called OT, or Operational Technology systems.



LEO:  Ah, yeah.



STEVE:  So the nature of the vulnerabilities of these OT Operational Technology systems when they're merged onto the Internet.  We also have some hope for the future amalgamation of the currently still fragmented smart home IoT industry.  And before we start into a deep dive into some of this ZuoRAT prolific malware, we're going to consider whether we'd rather have one 9" or two 5" pizzas.  So as always, another gripping episode of Security Now!.



LEO:  All I know is pi r squared.  So if you just would make the pizza square, we'd know.  Right?  It'd be easier.



STEVE:  Yeah, it'd be much, much easier to deal with.



LEO:  Much easier.  Picture of the Week time.



STEVE:  So we have the debut of a good friend of the show by the name of Evan Katz.



LEO:  Know him very well.  Yeah, he's a great guy.



STEVE:  Yeah, he is.  He sent me this picture that I thought was really apropos of what we've been talking about with all these cookie notices.  And what struck me was that for a retailer to put this sign up in their store demonstrates just how pervasive this issue is.



LEO:  Everybody knows about cookies now, yeah.



STEVE:  Yes.  And I can't identify the store.  It looks like of like a Pottery Barn kind of place.  It looks like it's in New York City, which is I know where Evan hangs out because it says NYC Tote for 20 bucks that's not related to this.  But anyway,  Evan's pointing to this big plaque that says - and it's a display of plates.  And so the plaque says "These plates accept all cookies."  Uh-huh.  And then there's two buttons down below, also printed on this plaque.  On the left it says "Manage Cookies," and the right has an old-style sort of Macintosh-like pointy finger saying "Accept all cookies."  And that's what's being pressed here.  So anyway, sort of fun for the retailer to do this, but also look at the degree to which this has permeated just common experience in the world.  It's like, yeah.



LEO:  And that smiling face there is Evan.



STEVE:  Indeed.



LEO:  Pointing at that sign.  We love Evan, yeah.  Thank you, Evan.



STEVE:  Yup.  So for our zero-day watch we have Chrome having moved to v103.0.5060.114.  Yesterday for Windows users Chrome jumped to that version.  It was an emergency update to address a high-severity zero-day vulnerability which was being exploited by attackers in the wild.  It's also the fourth Chrome zero-day patch so far this year.  But really, four is not bad.  They're like, this is better than they were at this time last year.  Because here we are already in the second half of the year, and this is only the fourth problem that they've had.



And as usual, Chrome/Google is saying very little at this point.  And there's really no reason for them to say more except fix it.  The only thing we know is that Google was made aware of this by the Avast Threat Intelligence team last Friday, July 1st.  So I have no problem with the idea that they were told of this on Friday before the long U.S. holiday Fourth of July weekend, and that my Chrome browser was already updated, had already updated itself as I'm writing this Monday evening of July 4th.  And I just thank goodness Microsoft didn't win the browser wars with their track record of slow or nonexistent responses to known critical problems.



We do know that the trouble was a high-severity heap-based buffer overflow weakness that was found within Chrome's WebRTC.  That's the Web Real-Time Communications component.  We also know that this reported problem was promptly assigned a CVE, which is nice to know, 2022-2294, even though it didn't require any user involvement.  And of course we recently learned that Microsoft doesn't bother assigning CVEs to their problems if no user action is required.  You know, is that a rug?  Oh, good.  Just lift the corner, and we'll sweep it under.  So anyway, Google did the right thing, and they're also cruising along well with only having, you know, this being the fourth zero-day so far, and they made it to the second half of the year.



Mozilla released Firefox 102 last week which includes a new privacy-enhancing feature which I have mixed feelings about.  It strips some of the URL parameter crap from the ends of URLs that are used to track us around the web.  Certainly it's good for any of that to be stripped.  The problem is that it's going to be kind of problematic.  We talk a lot about tracking cookies.  But another more blatant way of tracking us is to customize each individual URL to embed in the link a unique tracking tag which will just get passed along.  Many companies, including Facebook, Marketo, Olytics, Drip, Vero, and HubSpot - and more, that's just some - utilize, they embed these custom URL query parameters because it's going to go wherever you click it.  So when the click comes in, the URL essentially closes the tracking loop to let them know where, when, and whom it was issued to.



And for example, Facebook appends a big long hairy query parameter named "fbclid" that looks, you know, I put an example of it.  I mean, it's like, after the question mark, "fbclid," and it's, god knows, looks like it's about 50 characters of gibberish.



LEO:  It's a GUID; right?  Or something like a GUID; right?



STEVE:  It's very much like that.  This looks like about two GUIDs worth.



LEO:  Yeah, it's long, yeah.



STEVE:  I mean, it's this big.



LEO:  But they have three billion users.  They've got to have long GUIDs.



STEVE:  That's right.  You've got a lot of those people to differentiate among.



LEO:  Yeah.



STEVE:  So, now, though it's reportedly not as complete and as thorough as the Brave browser's current URL stripping, at least Firefox v102, which I already had, so it was updated last week, it's added this new Query Parameter Stripping feature.  It will automatically strip various known and recognized query parameters, and this is part of the problem, which are used for tracking from the URLs where they appear.



And for example, we could generically refer to third-party cookies as being bad, regardless of what the cookie is.  I mean, like there's no good reason for a third-party cookie to follow you across domains, whereas it's necessary to specifically recognize fbclid in order to strip it from a URL.  And it also wasn't clear that it's specifically Facebook fbclids, that is, on Facebook.com links.  It looks like it's maybe any domain that has the misfortune of naming part of their URL tail fbclid.



So maybe this explains why Firefox is reportedly not doing as thorough a job as Brave is.  They're not wanting to break things because this is going to tend to be a little bit flaky.  This Olytics company embeds something called oly_enc_id= and then a big blob of crap.  And then there's also oly_anon_id=.  Drip, the company Drip has __s= and then some blob.  Vero is vero_id.  HubSpot uses _hsenc= and then something.  Marketo, mkt_tok=.  And Facebook actually has two, fbclid and also mc_eid.  Anyway,  okay.  It's good, as I said, that Firefox is doing this.  But stripping stuff from URLs is error prone, and to me it feels a bit messy.  And if Facebook were to change the name of their fbclid parameter to fbclid2 or something, presumably that wouldn't match, and Firefox would let those go until they updated themselves.  So it doesn't really grab me.



Okay.  In any event, I'm still using Firefox, and I'm glad to have that incremental improvement.  Since my Enhanced Tracking Protection is already set to "Strict," as I would imagine is the case for most of our Firefox-using listeners, this was already enabled for me when Firefox updated itself to v102 last week.  What's weird, however, is that until and unless an additional tweak is made, this tracking parameter stripping will not be enabled when using Firefox in its private browsing mode.  It's like, what?  Why would you not want it there?  I don't get it.  And that's even when Strict mode is enabled.



So Firefox-using listeners go to about:config.  And about:config has a gazillion individual little things in there now.  So search for the string "strip," S-T-R-I-P.  And that'll reduce the number to something manageable, I don't know, looked like about eight, as I recall.  You'll see one that's already enabled and set to "true," and that's privacy.query_stripping.enabled.  And that was true probably because I'm in Strict browsing mode.  And so that was probably - that's something that the Strict browsing mode setting turns on.  But the one directly below it is privacy.query_stripping.enabled.pbmode, which is doubtless private browsing mode.  It was not enabled.  And it should be.  So you want to double-click on that to flip it to true.  It'll get bold, and then you should be good.  And did you not find it?  Oh, yeah, there it is, yeah.  It's right above the cursor.  So there's enabled, and the one below that, Leo, is enabled dot and then pbmode over - and now it's true?  Yes, good.  And that's what you want.  



LEO:  Just that one.  I don't need to do the stripping enabler  [crosstalk]?



STEVE:  Yeah.  I wonder why - I had the one above it, stripping.enabled, the non-pbmode.  Mine was already set to true because probably, I assumed, because I was in enhanced tracking protection as a main feature.  You definitely want that, you do want that turned on.  



LEO:  They should all be true, in other words.



STEVE:  They should all be true.  And just as an aside, because this is sort of a problematic thing, on the other hand I don't know what else we're going to do if we don't have our browsers stripping tracking crap from URLs because that's a thing.  It could tend to break something.  So if you turn this stuff on, and a website doesn't work the way you expect it to, then you might try turning it off briefly to see if that fixes the problem.  Again, knowing Mozilla, they are tiptoeing very cautiously into this.  I'm sure they didn't turn it on for everybody.  I had it turned on for me because I was already in the highest level of strict browsing protection.  I don't think they would turn it on if it was causing problems.  So I'm sure it's been well vetted, and it's safe to enable.  You just also want to enable that private browsing mode, as well.



Okay.  One thing this podcast probably doesn't spend enough time focusing on, probably because we don't get enough information about it, is the very real threat from insiders gone bad.  Naturally, nobody wants to advertise, no enterprise wants to advertise when they find someone being malicious inside their organization.  They're quickly escorted out, and that's the end of it.  So consequently it doesn't make the news.



Of course, it doesn't matter how strong one's perimeter defenses may be if someone with malicious intent walks right through the front door, smiles at the security guard, swipes their badge, and then settles down in their cubicle, now with full internal access to the corporate network.



We've briefly touched on the notion that our government's own three-letter agencies might bring an existing employee of some other company or two who have critical access into their fold under the auspices of patriotism and national security.  And certainly it's the case that private company employees were aware and stayed silent when the NSA was establishing those massive Internet listening post taps at the major domestic network exchange points.  You know, that had to be known by the employees who worked in those facilities.



And we have deep suspicions that some of the older crypto designs - remember the DRB pseudo PRNG, that pseudorandom number generator that was like the weakest of all possible versions that could be chosen, and for some reason that was the RSA default, which always sort of raised some eyebrows.  Which made us wonder whether maybe there was some influence under the guise of "This will be good for Uncle Sam."  In these instances, people knew and were keeping quiet, it seems.



And then there's the insidious possibility that a truly malicious foreign agency might bribe someone, or perhaps extort them if there's some embarrassing weakness and say, okay, just wipe all of your fingerprints off of this little USB device that we're going to give you, then plug it into the back of the break room's printer and just walk away.  We'll know when you have, and a parcel with $50,000 in crisp $100 bills will be delivered to you privately.



LEO:  Awesome.  Where do I get that job?  I'd like to do that.



STEVE:  Just make sure you wipe your fingerprints off quite thoroughly.



LEO:  Oh, yeah.  Absolutely.



STEVE:  That's right.  So yet another possibility is the self-corruption of a trusted employee who's able to subvert their employer's own business model to make money on the side.  And that's what was revealed last Friday by the famous bug bounty program HackerOne.  Because it's possible, and often appears to happen, that the same bugs are found nearly at the same time by different researchers who just happen to be looking at the same place, bug collision reports and bounty collisions  are not uncommon.  So it's not unusual for a hacker to complain when a bug they confidentially submit for consideration for a bounty is denied and paid instead to someone else.  It can happen when no one is at fault.  But to HackerOne's credit, they investigate all such reports.  And in this case, once clear evidence of an insider gone bad was found, they went public with the details.



So here's what HackerOne's disclosure started out explaining.  They said:  "On June 22nd, 2022, a customer" - now, but when they say "customer," they mean someone who is using them to manage their company's bug bounty payments, payouts and so forth.  "A customer asked us to investigate a suspicious vulnerability disclosure made outside of the HackerOne platform."  Okay.  So that meant that this customer was informed of a vulnerability, not through HackerOne, but through some outside agency.



"The submitter of this off-platform disclosure reportedly used intimidating language, as in like extorting this customer, in communication,"  they said, "with our customer.  Additionally, the submitter's disclosure was similar to an existing disclosure previously submitted through HackerOne."  They said:  "Bug collisions and duplicates, where multiple security researchers independently discover a single vulnerability, commonly occur in bug bounty programs.  However, this customer expressed skepticism that this was a genuine collision and provided detailed reasoning.  The HackerOne security team took these claims seriously and immediately began an investigation.



"Upon investigation by the HackerOne security team, we discovered a then-employee had improperly accessed security reports for personal gain.  The person anonymously disclosed this vulnerability information outside the HackerOne platform with the goal of claiming additional bounties.  This is a clear violation of our values, our culture, our policies, and our employment contracts.  In under 24 hours, we worked quickly to contain the incident by identifying the then-employee and cutting off his access to data.  We have since terminated the employee, and further bolstered our defenses to avoid similar situations in the future.  Subject to our review with counsel, we will also decide whether criminal referral of this matter is appropriate.  Our full discussion of the incident is below."



Then they lay out a  detailed nine-day blow-by-blow timeline starting June 22nd and finishing last Friday, the 1st of July.  And I'm going to skip that.  But I think that the discussion of their investigation would be interesting to our listeners.  So they said:  "Our investigation has concluded that a now former HackerOne employee improperly accessed vulnerability data of customers to resubmit duplicate vulnerabilities to those same customers for their personal gain.  The investigation began after a customer notified us of reportedly receiving a threatening communication outside the HackerOne platform about a vulnerability disclosure.



"We immediately launched an investigation.  Within 30 minutes of the investigation, additional evidence surfaced that caused us to escalate the priority of the incident.  We began to run down every scenario of a possible exposure to disclosure data, including potential exploitation of our application, a remote compromise of the hacker, customer, or analyst; a leak by misconfiguration; and others.  There was information to support only one of our hypotheses, an internal threat actor.



"Upon this discovery, we began a separate investigation into the insider threat with [what they described as] a contained group.  These steps were necessary as we worked to investigate and eliminate the prospect of multiple insiders."  Thus they immediately went to a constrained investigation, with only a few people being on the inside.  They said:  "We are now confident that this incident was limited to a single employee who improperly accessed information in clear violation of our values, culture, policies, and contracts."  And then they repeated:  "Within 24 hours of the tip from our customer, we took steps to terminate that employee's system access and remotely locked their laptop out pending further investigation.



"We were able to reach our conclusion quickly using the following methods:  Our internal logging monitors employee access to customer disclosures for regular business operations, namely vulnerability intake and triage.  Analysis of this log data suggested a likely actor soon after our internal investigation kicked off.  Only a single employee had accessed each disclosure that our customers suspected of being re-disclosed by the threat actor.  The threat actor had created a HackerOne [what they described as a] 'sockpuppet account'" - meaning an alternative identity as if they were an external vulnerability researcher - "and had received bounties in a handful of disclosures.



"After identifying these bounties as likely improper, HackerOne reached out to the relevant payment providers, who worked cooperatively with us to provide additional information.  Following the money trail, we received confirmation that the threat actor's bounty was linked to an account that financially benefited a then-HackerOne employee.  Analysis of the threat actor's network traffic provided supplemental evidence connecting the threat actor's primary and their sockpuppet accounts.



"We identified seven customers who received direct communication from the threat actor.  We notified each of the customers of our investigation and asked for information related to their interactions.  We are grateful for their involvement in our investigation, and we thank them for their willing cooperation.  Facts shared from their own investigations corroborated the conclusion from our investigation and allowed us to act more quickly.



"We've issued platform bans for the employee's known HackerOne accounts.  We've terminated the employee for violating our values, culture, policies, and contracts.  Subject to review with counsel, we will decide whether criminal referral of this matter is appropriate.  We continue forensic analysis on the logs produced and devices used by the former employee.  We are reaching out to other bug bounty platforms to share details in case their customers received similar communications from 'rzlr.'"  That was the handle used by this guy, the sockpuppet account, "rzlr."



"The threat actor's motives appear to be purely financial in nature.  The threat actor had access to HackerOne systems between April 4th and June 23rd of 2022."  So not a long-term employee, somebody that they hired at the beginning of April who went south or sour pretty quickly.  Maybe that was the entire intent of the employment was to get on the inside and then grab these things and try to get additional bounty payments.



So insider threats are always a real possibility.  The best outcome is to prevent them from happening in the first place.  But if they do anyway, it's clear that having extensive previous access and traffic logging becomes invaluable for post-incident forensics.  And casually disclosing through water cooler conversation that such logging is always being done can go a long way toward preventing such abuse in the first place.  We've talked a lot about the fact that while no one likes the idea of being watched and their activities being logged, an enterprise's network and the traffic it carries is not the sovereign property of its employees.  It belongs to the corporation.  So a casual reminder of the fact that what's done on the enterprise's network is being monitored can go a long way toward preventing such abuse from occurring.



And Leo, let's take a break.  I'm going to prevent further abuse of my vocal cords by giving them a bit of hydration.



LEO:  No hydration abuse here.  No.



STEVE:  We're going to use some of our closing-the-loop feedback from our listeners to bring us into some interesting discussions.  



LEO:  Great.  Awesome.



STEVE:  So someone on Twitter calling himself ReliefTwitcher, he said:  "@SGgrc I thought of you today when I reached around to the back of my computer tower," he says, "yes, I still have one, to plug in a USB-A cable. There is a USB 3.0 expansion card back there, and I know the orientation of the ports from years of experience.  I thought, I know for sure this port is upside down.  Time to prove Gibson wrong.  But then, as I turned the plug to the correct orientation as I knew it to be, I noticed that the cable seemed to want to lay in an unnatural way..."



LEO:  Ah, didn't like it.



STEVE:  "...that I had not noticed when it had been connected last.  This caused me to doubt my own certainty, so I turned it over.  The cable lay more naturally this way.  Shaking my head, I thought, maybe it's time to prove Gibson right."  He said:  "I moved to plug it in; and, sure enough, it did not go in.  I turned it over again, the way I had it the first time, and in it went.  It seems that even when I know the correct orientation, the bug in the simulation introduces a chaotic variable to make me wrong way the first time anyway."



LEO:  Wow.



STEVE:  I'm just sayin'.



LEO:  There you go.  Or it's Schrodinger's USB key.  It could be that, as well.



STEVE:  That's right.  Yes, actually we did have, I think I shared it a couple weeks ago...



LEO:  Yeah, a couple weeks ago, yeah.



STEVE:  The notion of it existing in both states until you observe it.



LEO:  Exactly.



STEVE:  And attempt to plug it in.  And then it says, oh, no, no, no, no, not so fast.



So Stefan Bang, he said:  "I rarely have the chance to bring something useful to the table, but I have some insights to the OT (Operational Technology) subject of last week, which I hope will not waste your time.  And I also think I can fix reality for you," he said.  "So regarding Operational Technology," he said, "I work as a developer in the OT business in Denmark, and I think it makes a lot of sense that OT protocols are insecure.  They should be.  Most protocols like Modbus over TCP or BACnet over IP are really old, from the '70s and '80s," he says, "RS-485 protocols that have been made to work over Ethernet.



"These protocols don't use any authentication, and BACnet/IP is even a UDP protocol.  But they're fast and reliable.  And if I need to close a valve in order for a heat exchanger to not explode, I'd rather have it be insecure on a local isolated network than have people hurt because a certificate had expired and no one had noticed.



"The control units are also probably insecure because they are rarely updated.  Most OT controllers are installed and forgotten in kindergartens and homes and such.  They don't need to be secure if they're not accessible from the Internet because if you have physical access to the device, then it's easier to pull the plug on the pump than to break into the system.



"When an OT system is accessible from the Internet, it is often through a supervisory system on a server.  The server is likely to be located in the cloud or on-prem at a service partner like the company I work for."  He said:  "The security of the supervisory system is easier to maintain as the server can auto update, and the firewall only needs to allow HTTPS traffic.  A site-to-site server VPN will eliminate any protocol insecurities and is much easier to maintain, instead of securing and maintaining the security of every SCADA protocol - renewing certificates, securing private keys and so on."



He says:  "I'll quote a wise man:  'Security is hard,' and 'Complexity is the enemy of security.'"  He said:  "By using a VPN, the only things that need to be secure is the VPN and the web server of the supervisory system.  This is easier and much less complex than securing every single OT protocol, as there are a lot of them."



And then he just finished on his fixing reality for me.  He said:  "Regarding reality, I don't think the simulation is broken.  I think the person who writes and maintains the simulation listens to Security Now!."



LEO:  Oh, that would be a nightmare.



STEVE:  Oh, my god.  "The reason why you need to turn the USB-A plug three to four times for it to be allowed to enter the socket is because you need to brute force the physical port-knocking sequence of the USB-A socket."



LEO:  Yeah?



STEVE:  So apparently, if that's the case, it is a Security Now! listening simulating overlord.



LEO:  It's all about port-knocking.



STEVE:  Yeah, that's right.  Okay.  So the trouble with Operational Technology over IP is that it's a classic case of convenience trumping security.  I completely agree with Stefan that worrying about things like expiring certificates is a nightmare.  But I think that the proper way to think about security is that it's the cost that's incurred when any non-IP technology wants to become an IP technology that shares the Internet's inherently routable IP network.  Now, I heard what Stefan said.  I get it that in his mind sequestering a local or a private network behind a firewall or behind a VPN, or behind an HTTP-only, an HTTPS server is the solution.  But those packets want to get out, Leo.  They want to be free.



LEO:  Data wants to be free, yeah.



STEVE:  Yes.  And they're going to find a way, just like those Jurassic Park dinosaurs that were all female, they somehow - they reproduced anyway.  Nature will find a way.  So the problem is that the global insecure network is IP.  While that pressure regulating valve was using the RS-485 protocol for its signaling, it was speaking an entirely different language and was fundamentally inaccessible to hackers in Russia.  Computer users are more familiar with RS-232.  RS-485 is closely related. It used differential signaling, meaning that two wires rather than one were used, where the data was contained in the difference in the voltage on the two wires, rather than their absolute voltages.



This creates a great deal of environmental electrical and magnetic noise immunity which can be a problem in heavy industrial environments.  And whereas RS-232 is a simple point-to-point system, RS-485 uses device addressing to allow multiple devices to share a so-called "multi-drop" connection.  So it was a very early form of a serial bus architecture.  So it existed.  It was secure.  There's no way any, I mean, it didn't use packets.  So they couldn't have escaped if they wanted to because there weren't any.  



But the world back then was suddenly flooded with Ethernet cabling and Ethernet switches and routers whose cost dropped to near nothing as high manufacturing volumes and massive inter-vendor competition fought for market share.  Wired Ethernet is also a highly noise-immune differential signaling system, so it only made sense to move to it.  However, it wasn't absolutely necessary to also move to IP.  Other non-routable IP protocols, remember, what was it...



LEO:  UDP?  No.



STEVE:  No, I'm thinking that other - Novell.  Novell had, you know, Novell Networking was on Ethernet.  It just wasn't IP.  So you could have had all of the advantages of all of those economies of scale without using a protocol that could escape.  So other non-routable, non-IP protocols have been carried over Ethernet, and they would have been a far safer choice.  But ultimately, being non-IP protocols, they too would have suffered the same fate as non-Ethernet RS-485 by ultimately being more expensive to deploy because they were in any way different from the IP technology that has completely overtaken the world.  There was, I mean, I agree there was probably no way to resist the allure of that near zero cost.



Okay.  But now our mission-critical, pressure-regulating valves are actually connected to, and potentially reachable from, Russia.  As remember, as Khan was famously heard to say in the second Star Trek movie, when he was explaining why Kirk didn't raise his own ship's shields, he said, "We're all one big happy Federation," just before he blew the crap out of Kirk's ship.  



LEO:  Khan!!!  Khan!!!



STEVE:  Now, with Operational Technology systems supporting IP protocols, we're all one big happy global network.  What could possibly go wrong?



LEO:  Yeah.



STEVE:  So there is a simple and elegant solution to this.  I've mentioned this a few times in the past when its application has been appropriate.  But it's just sort of like deriving one, like deriving all websites' passkeys from a single master key rather than just using random number generators.  Yeah, okay.  The same IP technology that creates this problem in the first place offers a solution.  Operational Technology equipment, or for that matter any equipment that doesn't want or need to be reachable from across the globe, that is, that is designed for local network access only, simply needs to set the TTL value of the packets they emit to a very low number.



The absolutely universally honored fact of IP packet routing is that every router which encounters any IP packet first decrements the packet's incoming TTL, its Time To Live value, which is contained in the header of that packet.  If that TTL decrements to zero, meaning if it was one when it arrived at the router, the router decrements it, it hits zero, no router on the Internet will forward that packet.  Period.  Nothing would kill the Internet faster than packets which refused to die.  So every router honors that requirement before any other.  Therefore, the simple expedient of emitting packets with a TTL of, say, two or three, just to be safe, will never affect any local networking use of such equipment, while at the same time flatly, elegantly, and completely preventing them from ever reaching foreign soil.



This use of deliberately short TTLs is something, as I said, that never seems to gain any traction, but it's a beautiful solution when adding another security failsafe to a system might be useful.  Especially when you've got a system designed for local-only use, and its packets should never be allowed to escape.  Right?  In fact, we talked about this a long time ago because there was a time when TTLs were set to 32.  Just sort of arbitrarily early on.



LEO:  Is that seconds?



STEVE:  No.  It's hops, 32 hops.



LEO:  32 hops.



STEVE:  So, for example, when we do a trace route, the way the trace route works is it deliberately sends out a packet with a TTL of 1, which is returned by the router that rejects it, and thus we get that router's IP.  Then it sends a packet with a TTL of 2, so that first router says okay, fine, it's got some time left, and forwards it to the second router, which decrements the TTL from 1, which the first router set it to, to 0, which the second router sent it to.  Now it dies at the second router, so that second router sends back a sorry, your packet died.  So by successively emitting packets with greater and greater TTLs, we're able to trace or trace route the router-to-router hops that packets take.



And so it wasn't - there was a point during this podcast, because I remember we talked about it, where packets were just, you know, our operating systems were - the TCP/IP stacks set the IP TTLs to 32.  And because there was just, you know, it was a nice number that engineers tend to use, 2^5.  And that should be plenty; right?  Then the Internet kept getting bigger and bigger.  And more and more ISPs got sort of like shimmed in between others.  And what happened was the so-called, the transit diameter of the Internet in some locations became greater than 32.  There were two points on the Internet greater than 32 hops apart.  And they were unreachable to each other.



And so it was necessary to go, oops, and just simply we doubled the TTL to 64, or in some cases it went to 127 or 255 because it's an eight-bit count.  It doesn't take up very much space at the front of the packet, just a little eight-bit header, or eight-bit field, and it gets decremented.  And so if these systems that never wanted to allow anyone in Russia or China to get a hold of them, even if the firewall fails, even if a flaw is found in the web server that they're hiding behind, then just emit packets with a TTL of 2 or 3, and they can't get out.  They can't go far.  But as far as I know it's never been done for security.  And it would have been, you know, I'm a belt and suspenders person.  And so it would make sense to do that.



Dr. Nathan P. Gibson, he sent another tweet that I thought was - actually it gave me a perfect segue.  He said:  "I haven't gotten into smart home devices, among other reasons because of all the security issues you talk about with them.  I was wondering about two things."  And I quoted the first of those two.  He said:  "When you talk about isolating your smart home network from your Internet network, it sounds like a lot of work.  Are there are any secure smart home hubs that can do this for you?  For example, you talk about," he says, "you talk to the hub on your regular network, but it sets up another air-gapped network and talks to the other devices on that."  He says:  "My Fritz!Box router actually has a setting for smart home devices, but I'm not really sure what it does."



Well, my feeling is that we're still on the "don't have it yet" side of the smart home compatibility revolution.  So I haven't yet invested in any single vendor's hub technology because they've all been hoping to own the market for themselves, reminiscent of not synching passkeys across vendors.  So I have, currently, I have individual hubless IoT WiFi devices running on their own isolated guest IoT network, courtesy of my ASUS router, which offers up to four individual isolated guest networks.  No matter how the IoT market sorts itself out, I would strongly recommend that anyone who purchases from this point forward a WiFi access point or router in the future be certain that it supports isolated guest WiFi networks.  Just it's a terrific technology.  And it's just trivial to set up.  You just turn on a guest network, give it its own SSID and password, and then click a checkbox, typically, where you affirm that you do not want this network to have any access to the other LAN networks, WiFi or wired, and it becomes isolated.



Now, all that said, there is encouraging news on the IoT home front.  Last summer, Ben Patterson for TechHive wrote:  "We've been eagerly awaiting the arrival of Matter, the new open-source and platform-unifying standard that promises to make our various smart home devices play nicer with each other.  And now comes word that we'll have to wait a little longer.  Initially, the Connected Standards Alliance" - that's CSA, formerly the Zigbee Alliance - "had announced that we might see the first Matter-enabled smart products by the end of the year.  But as Stacey Higginbotham at Stacey on IoT reports..."



LEO:  I know her.



STEVE:  Yes, we do, "the CSA now says that a software development kit for Matter won't be finalized until the first half of 2022, which means the first Matter devices won't arrive until sometime next year."  He was writing this in August of 2021, so he's saying this year, 2022.  "In a release announcing the delay, CSA (Connected Standards Alliance) CEO Tobin Richardson cited the 'need to get it right'" - words dear to me - "in terms of ensuring the upcoming Matter specification and SDK are 'stable, deployable at scale, and meet market expectations for quality and interoperability.'"  Amen to all of that.



"According to Stacey, CSA's CEO Tobin Richardson also blamed the resurgence of COVID, as well as the addition of nearly 30 [three zero] more companies to the Matter group.  Formerly known as Project CHIP, which stood for Connected Home over IP, Matter is an IP-based protocol that's compatible with WiFi, Ethernet, and Thread.  Matter has the backing of some of the biggest names in the smart home market, including Amazon, Google, Signify" - I didn't see Apple, but Apple's name is somewhere - "Signify is the owner of the Philips Hue smart lighting brand, and Samsung's SmartThings.



"Last month, Amazon announced that all of its current, Alexa-enabled Echo speakers and displays will support Matter, while Google had previously said its Nest speakers and displays will support Matter also.  Apple's HomePod mini comes with its own integrated Matter radio."  So it looks like Apple's onboard, too.  "Matter promises to unify the thicket of competing smart home platforms, as Matter-certified devices will be able to recognize each other and work seamlessly together across different ecosystems, including Apple's HomeKit, Amazon's Alexa, and Google's Assistant-powered Nest platform.



"In other words, if you buy a Matter-certified smart gadget, you ideally will be able to control it with Alexa, Google Assistant, and Siri; and it should also work with any other Matter-enabled devices you own.  That's a welcome prospect," he finishes, "for anyone who's pulled their hair out trying to make different makes and models of smart home devices work well together.  For now, however, it looks like we'll have to keep coddling" - coddling?  Oh, yeah, coddling - "our stubborn smart gadgets..."



LEO:  You wrote it, dude.



STEVE:  "...through the end of the year, and probably even longer."  Oh, I did also note that DigiCert and Wemo's names were also associated with Matter.  And Wikipedia just had this to say.



They said:  "Matter, formerly Project Connected Home over IP (CHIP), is a royalty-free home automation connectivity standard, with manufacturers only incurring certification costs."  And so using it costs nothing, thank god.  And that gives it a chance to be the chosen solution.  "Announced on December 18th, 2019, Matter aims to reduce fragmentation across different vendors and achieve interoperability among smart home devices and Internet of Things platforms from different providers."



Wikipedia says:  "The project group was launched and introduced by Amazon, Apple, Google, Comcast, and the Zigbee Alliance, now called the Connectivity Standards Alliance (CSA)."  And then they said:  "Subsequent members include IKEA, Huawei, and Schneider.  Matter-compatible products and software updates for existing products are expected to be released in 2022.  Although the Matter code repository is open source under the Apache license, the Matter specification is licensed by the CSA."



So the Connectivity Standards Alliance, for anyone who is interested, is at csa-iot.org.  Again, csa-iot.org.  And this couldn't be better news.  It sounds like we'll be needing to do a podcast on this soon.  And of course I won't be able to resist titling it "What's the Matter?"



LEO:  Ohhh.  We've been talking about Matter for quite some time.  Of course Stacey is all over it.



STEVE:  Yup.



LEO:  But this is good.  I'm glad they're starting to make some progress instead of just being empty suits.



STEVE:  And a hope, yes.



LEO:  Yeah.



STEVE:  So that's really great.  I mean, basically that's what we've been waiting for.  I would not move to a single, I mean, a hub-based solution is going to be better because it's going to integrate much more tightly with your other devices, and potentially solve the security problem.  I mean, it is nerve wracking, knowing that my individual light switches and smart plugs each establish a persistent connection to China.  That's just like, you know, that's not what you want.  So having them instead connect to a centralized hub from a security-conscious company like Apple, Google, or Amazon, that's a way better solution.  So looks like that's going to happen soon.  And couldn't happen soon enough.  Yay.



Ivan Neeson, he tweeted.  He said:  "Hey, Steve.  I have a question about passkeys.  Why would I want my passkeys on my PC at all?  Isn't the whole point to keep the keys on the phone with its high security chip and use that to log on?  So," he says, "so if someone hacks the PC they won't get any passkeys?"



And Ivan, that's a good point, and it's potentially true.  I'm not sure how I'll come down on this issue.  At the moment, for example, I have all of my TOTP second-factor time-varying keys on an iPhone that I have sitting next to me.  I use the iOS app OTP Auth, which I like a lot.  So obtaining a token for me takes little time.  We're going to need to see how the flow works with smartphone cross-device authentication.  This was, of course, the original mode for SQRL.  When I first described SQRL to this podcast's audience, SQRL stood for Secure QR-code Login, which was back then a novel concept.  And of course we later renamed it Secure Quick Reliable Login as its application space expanded.  And we saw something like that in Apple's presentation, though all the mechanics were not exactly clear.



So if it's super easy to present a website's FIDO2 QR code, whatever it is that it's showing, to a smartphone, to then have the smartphone authenticate the user, that indeed might be sufficient and, for many people, ideal.  And perhaps we, for example, only have our PCs storing passkeys that are less critical, and keep like our banking passkeys on a non-shared environment.  So anyway, we don't know yet.  We'll need to see how this all shakes out.  And the good news is we'll probably have a couple years to wait for all that to happen.



Jonathan tweeted.  He notes that math geeks make difficult customers, and he pointed me to a tweet thread.  Tweet thread read:  "I ordered a 9" personal pizza.  After a while, the waiter brought two 5" pizzas and said the 9" pizza was not available, so he was giving me two 5" pizzas instead, and that I was getting 1" more for free.



LEO:  Five plus five equals 10.



STEVE:  That's right, baby.  Of course we know where this is going.



LEO:  Yes.



STEVE:  The person tweeting this said:  "I requested the waiter to call the owner over.  I showed the owner the mathematical formula to calculate the area of a circle."  As we know, the area of a circle is pi times r squared.  And you don't actually need to know about pi.  You could sort of throw that out; right?  You could just use r squared.  But this tweet thread says:  "So the area of a 9" pizza is 63.62 square inches, whereas a 5" pizza has only 19.63 square inches.  The two 5" pizzas together add up to 39.26 square inches.  I said to the owner that even if he gave me three 5" pizzas I would still be losing out.



LEO:  Ah, this guy is absolutely math literate.  I love it.  Not the owner, obviously.



STEVE:  Right.  Or the server.  "How can you say you're giving me an extra inch for free?"



LEO:  I bet it fools everybody except this guy.  Absolutely.



STEVE:  Yeah.  "The owner was speechless," he says, "and gave me four 5" pizzas."



LEO:  Happy ending.



STEVE:  Happy ending indeed.



LEO:  That's hysterical.  That's great.



STEVE:  And really, as we know, you can do the math easily in your head because you could ignore the pi and just do r squared.  Because that's all you really care about.



LEO:  That's right.



STEVE:  Exactly.  Although you could say that pizza is a pi.



LEO:  Unh-huh.



STEVE:  So there is that.



LEO:  And, well, yeah.  You know where we're headed.



STEVE:  Okay.  We're heading to our final sponsor, and then we're going to talk about the hard-to-pronounce...



LEO:  ZuoRAT.



STEVE:  ZuoRAT.  Last Tuesday, Black Lotus Labs, which is the threat intelligence arm of Lumen Technologies, which was formerly known as CenturyLink, they've been tracking elements of what they say appears to be a sophisticated campaign leveraging infected SOHO (small office/home office) routers to target predominantly North American and European networks of interest.  So generically RATs (Remote Access Trojans) are a dime a dozen and wouldn't usually command much of this podcast's prolonged attention.



But in one of those kind of "It's obvious after the fact that this would happen" disclosures, Black Lotus Labs revealed that they had uncovered a complex campaign that had gone undetected for nearly two years, and that the trigger for the campaign was apparently the COVID-driven shift to working from home.  How do remote well-financed nation-state actor bad guys get into well-protected corporate networks?  They enter through the avenue of least protection and resistance, less secured remote employee networks.  And from there they pivot onto the corporate LAN.  



Or, as Black Lotus explained:  "The rapid shift to remote work in spring of 2020 presented a fresh opportunity for threat actors to subvert traditional defense-in-depth protections by targeting the weakest points of the new network perimeter, devices which are routinely purchased by consumers but rarely monitored or patched, small office/home office (SOHO) routers.  Actors can leverage SOHO router access to maintain a low-detection presence on the target network and exploit sensitive information transiting the LAN.



"Black Lotus Labs is currently tracking elements of what appears to be a sophisticated campaign leveraging infected SOHO routers to target predominantly North American and European networks of interest.  We identified a multistage remote access trojan (RAT) developed for SOHO devices that grants the actor the ability to pivot into the local network and gain access to additional systems on the LAN by hijacking network communications to maintain an undetected foothold.  While we currently have a narrow view of the full extent of the actor's capabilities due to the limited state of SOHO device monitoring in general, using proprietary telemetry from the Lumen global IP backbone we've enumerated some of the command-and-control infrastructure associated with this activity and identified some of the targets.  We assess with high confidence the elements we're tracking are part of a broader campaign."



Okay.  So some powers of darkness realized early in the COVID lockdown that large numbers of employees of North American and European enterprises would begin working from home, and that many of those employees would be inherently weakening their employers' security by effectively extending their previously well-curated and secured network out into the periphery.  Of course this also occurred to many of those enterprises whose employees suddenly needed to have access to corporate resources which were once protected by virtue of having everyone physically located within the same internal LAN.  And we talked about this at the time, that this sudden workforce migration to home was going to be straining enterprise security.  Obviously, this occurred to others, as well.



ZuoRAT may not have been explicitly and expressly created to fulfill this agenda, but it appears to have been perfectly designed for this role.  The campaign consists of several components.  There's the first stage RAT that was developed specifically for SOHO routers.  We'll come back to take a closer look at that in a minute.  There's a simple loader for Windows machines which was compiled in C++.  And then, third, there are three separate fully functional agents, which the RAT and the Windows loader, which is just a simple remote file retriever, downloads.  There are three fully functional agents, two of which were custom developed, and the one that isn't is the well-known Cobalt Strike Beacon which we previously covered in some detail.  Together and independently, these full-function agents allow for full enumeration of the infected device, downloading and uploading of files, network communication hijacking, process injection, and more.  And I'll go into those also in some detail in a minute.



So this ZuoRAT is a MIPS chip file compiled for the MIPS chip which will run on routers from ASUS, Cisco, DrayTek, NETGEAR, and others.  That executable can enumerate a host and the internal LAN, capture packets being transmitted over the infected device, and perform man-in-the-middle attacks, including DNS and HTTPS hijacking driven by predefined and actually softly defined rules.



We've often talked about the threat and power of deliberate DNS corruption.  As we know, DNS still transits over unsecured and unencrypted UDP packets.  The reason Dan Kaminsky was able to get the entire Internet to update DNS overnight, when he realized and quietly shared how vulnerable it was to spoofing, was because DNS really has no other protection than relying upon the goodwill and good behavior of all the interconnecting networks.  Therefore, if malware is able to set up shop on the router that's linking a residential LAN to the Internet, a great deal of mischief and damage can be done.



Black Lotus notes that what they found surprised them because attacks as severe as these, true DNS and HTTPS hijacking, have mostly been theoretical and remain rare.  This is what they wrote.  They said:  "While compromising SOHO routers as an access vector to gain access to an adjacent LAN is not a novel technique, it has seldom been reported.  Similarly, reports of man-in-the-middle style attacks, such as DNS and HTTP hijacking, are even rarer and a mark of a complex and targeted operation.  The use of these two techniques congruently demonstrated a high level of sophistication by a threat actor, indicating that this campaign was possibly performed by a state-sponsored organization."



Now, notice that when we talk about certificate spoofing, one of the concerns is that our computers now trust so many certificate authorities, including foreign actor, like foreign state certificate authorities.  So the reason we're not that worried is that the IP, we assume that the IP address we are getting from DNS is correct.  So even though a foreign actor, a foreign state could produce a certificate for some high-profile website, like say Facebook, or a bank, our traffic will go to the bank.  So it doesn't matter, I mean, we're not going to believe, we're still going to get the bank's certificate.



But if you combine the ability of a high-level state actor to create certificates, with the ability for them to change the IP address that your DNS lookups return, which is exactly what this thing does, now you think you're connecting to your bank.  You're instead connecting to a server elsewhere which has a certificate not issued for the bank, but issued under the bank's name for a foreign actor.  And so your browser's happy, no alerts are shown, and you're completely compromised.  Everything you do on that site is decrypted at that location and can be relayed anywhere.  So it's a serious attack, when you get somebody who has the ability, either directly or indirectly, to get certificates, who's also able to redirect traffic.



They said the Windows loader that was analyzed reached out to obtain a remote resource and then ran it on the host machine.  They assessed that it was used to load one of those three functionally second-stage agents, and the one that was chosen depended upon the environment.  There was something known as CBeacon, which was a customized developed RAT written in C++, which had the ability to upload and download files, run arbitrary commands on the machine where it was installed, and persist on the infected machine through a component object model (COM) hijacking method.  Well, the fact that it's COM means that it's Windows only, and CBeacon was Windows only.



However, GoBeacon, the second of those three, is a custom-developed RAT written in Go.  This trojan had almost the same functionality as CBeacon, but also allowed for cross-compiling on Linux and macOS devices.  So that one was used to infect non-Windows Linux and macOS.



And then, finally, the third one, Cobalt Strike.  They said:  "We observed that in some cases this readily available remote access framework was used in lieu of either CBeacon or GoBeacon."  They said:  "Analysis of multiple Windows samples revealed the consistent use of the same program database" - PDB is an internal Microsoft development term - "some of which contained Chinese characters, while others referenced a possible name or Chinese locality."  So strong evidence that this was Chinese in origin.  Although you could have a false-flag operation, too.



"Additionally, there was a second set of actor-controlled command-and-control infrastructure used to interact with the Windows RATs that was hosted on Internet services from China-based organizations Alibaba and Tencent.  Given the age of the first observed router sample, which was first submitted to VirusTotal in December of 2020, as well as a sampling from Black Lotus Labs telemetry over a period of nine months," they said, "we estimate this multiyear campaign has impacted at least 80 [eight zero] specific targets, likely many more."



And that was one of the problems was they recognized that due to the structure of the system, they didn't have visibility into everything that was going on.  These are individual SOHO routers, so how are they ever going to see what's going on?  Well, one of the things they could do because they are Lumen, which is CenturyLink, thus a Tier 1 backbone provider, is they can look at the traffic that at least their own backbone is carrying, which will certainly - it's far short of the global Internet, but it's a big chunk of smaller ISPs who buy their bandwidth from CenturyLink, now Lumen.  So, for example, if they see one compromised SOHO router connecting to a specific command-and-control server at a given IP, they can then look for all other traffic connecting to that IP and see what they can learn from it.  So they are in a privileged location by being such a large Tier 1 provider.



They said, or actually I said, that during their investigation of ZuoRAT's activity, they observed telemetry indicating infections stemming from numerous SOHO router manufacturers, including, as I noted, ASUS, Cisco, DrayTek, and NETGEAR.  But they were less lucky in capturing any actual running code on those.  They were only able to obtain the exploit script for a model Q20 of a router manufactured by a company JCG.  So it was the JCG-Q20.  In that case, the actor was found to be exploiting known CVEs 2020-26878 and 26879 by using a Python-compiled Windows EXE that referenced a proof of concept called ruckus151021.py, a Python script.  So I looked, and I found that Python proof of concept over on GitHub from before it had been weaponized and it obtained the credentials and - well, it was weaponized to then obtain credentials and to load ZuoRAT.



The weaponized script first performed a command line injection to obtain authentication material.  Then it used the output from the command line injection, namely that authentication material,  to authenticate itself in order to get the access that it was looking for.  So this chain of vulnerabilities allowed the actor to download a binary onto that router, then execute it in order to gain the access they wanted.  The script that they were able to obtain contained four functions.  There was one called "getpasswd" which sent a specifically formatted request to the remote host requesting its password.  There was "getloginsysauth," "execCmd," and "telnet."  Basically a little toolkit providing everything that they needed.



The final stage of the exploit script downloaded the ZuoRAT agent framework.  That framework enables in-depth reconnaissance of target networks, traffic collection, and network communication hijacking.  And that can be divided into two components.  The first contains functions that would auto-run upon execution of the file.  The second component contains functions, and 2,500 of them believe it or not, that were embedded into the file, but were not explicitly called.  Black Lotus believes that these additional functions were intended to be called by additional commands.  And they wondered why are some active and some appear to just be along for the ride.  It appears that this ZuoRAT was a heavily modified version of the Mirai malware, which of course was a strain of ransomware that we've talked about before.



So the core functionality of that first component.  It was designed to obtain information about the router and its locally connected LAN, to enable packet capture of network traffic, and to send the information back to the command-and-control servers.  Black Lotus believed that its purpose was to acclimate the threat actor to the targeted router they'd landed on, as well as the contents of the router-adjacent LAN to determine whether or not there's anything worthy of further exploitation.  They don't know anything about the router that they've managed to get into.  They've got to look around and decide whether they want to spend any more time there.



The capabilities included functions to ensure only a single instance of the agent was present, and to perform a memory dump that could yield useful data in memory such as credentials, routing tables, IP tables, and so forth.  The file was initially executed by the threat actor via the command line, specifying an IP address and port for the command-and-control node.  So it would execute the command and say, send this package of information to this IP and port.  If the IP and port was not provided in the exploit script, the ZuoRAT code contained a default command-and-control hostname listed as cs.memthree.com, which was a domain originally purchased in October of 2020.  So coincident with the first development of all this.



Upon execution, the agent would launch a new process with a randomly generated, 32-character string using the characters A-Z and 0-9.  Next, it gathered host-based information by running the uname command to send to the command-and-control server.  It also attempted to gather the router's public IP address by querying the following web services.  It would query whatismyip.akamai.com, ident.me, myipdnsomatic.com, and ipecho.net.  If ZuoRAT was unable to obtain a public IP address, it would immediately delete itself and terminate under the assumption that it was being run in an isolated sandbox, probably being analyzed.



Next, ZuoRAT would connect to the command-and-control server and attempt to bind a "listen" on port 48101.  If the bind failed because the port was already in use, it would also immediately terminate the current process to ensure that only a single instance of the Trojan was running on the compromised device because you're not allowed to locally bind two different applications to be listening on the same port because it's the binding that decides when a packet comes in which process will receive a notification and access to that packet.



ZuoRAT then used a scan function designed to survey the adjacent LAN's internal IP addresses.  So it would do an internal scan of the entire behind-the-router LAN.  It scanned for a hardcoded list of open listening ports including 21, 22, 23, 80, 135, 139, 443, 445, 808, 902, 912, 1723, 2323, 3306, 5222, 5269, 5280, 5357, 8080, 8443, and 9001.  And I know that many of those ports are familiar to our listeners as they are to me.  It then performs an internal LAN scan of all machines on the LAN across all the IPs.  So in doing so, it's on the trusted internal network.  It's inherently trusted by Windows firewall on those machines and able to bridge traffic from the external command-and-control to anywhere within the network, giving this thing a lot of power.



And once that's done, ZuoRAT would send the reconnaissance information it had obtained to the previously supplied command-and-control.  If the connection was being established for the first time, it would occur over port 55556.  If the connection was being refreshed, communication switched to port 39500.  And I have no idea why.  If the connection was successful, data would be transmitted.  If errors were returned, the program would sleep and then repeat the attempt in a loop.



And finally, in preparation for establishing network capture capabilities, ZuoRAT allocated memory to itself for increased performance in order to have room to capture packets, and also allocated a mutex semaphore to ensure that only one instance would be running at a time.  Then, if initiated by subsequent commands, an array of network functions would allow the remote threat actor to collect network traffic on UDP, DNS, and some TCP connections where data might be sent in the clear.



So anyway, we have a breathtakingly sophisticated and serious remote access trojan which is designed to avoid detection.  In their write-up they also talked about something that we had talked about before, how rather than connecting directly to command-and-control servers, they also often bounced through innocent routers that were being used as simple IP proxies.  Routers that have universal plug-and-play ports open are able to be asked to serve as public routers.  And so the people behind this know where those routers are, and they're able to set them up to bounce traffic through the router, making it much more difficult to track down the location of the command-and-control servers.  So they are being very deliberately and carefully protected.



They said, based on their telemetry, they observed 23 devices maintaining a persistent connection to a single command-and-control server during September and October of 2021.  All 23 devices were located in the U.S. and Canada.  The majority of IP addresses communicated with the command-and-control server over TCP port 9000, but a few communicated over other ports, including 55556, 55558, and 39500.  The device types which were infected included, but were not limited to, Cisco's RV 320, Cisco's RV 325, and Cisco's RV 420; the Asus RT-AC68U, the Asus RT-AC530, the RT-AC68P, and the RT-AC1900U; also a DrayTek Vigor 3900 and an unspecified bunch of NETGEAR devices.  Based upon their analysis of the router malware and their telemetry, the Trojan first attempted to establish a TCP connection over port 55556 and, as I noted above, subsequent connections were made to port 39500.



They said more recently they've seen activity from a separate command-and-control located at 103.140.187.131 on port 6666.  That was occurring from February 22nd of 2022 through May 16th of 2022, and that's been acting similarly.  So anyway, the Black Lotus report contained much more detail, but I'm sure that everyone now has a good idea how much work someone, apparently in China, has put into building, deploying, and maintaining a powerful network intrusion capability.  And it's not just theoretical.  It is in active use.



In summarizing what they had done, Black Lotus wrote:  "Though advanced actors have long demonstrated the capability and intent to target sensitive networks, the industry has uncovered only a handful of router-based malware specifically designed to covertly target them.  The sudden shift to remote work spurred by the pandemic allowed a sophisticated adversary to seize this opportunity to subvert the traditional defense-in-depth posture of many well-established organizations.  The capabilities demonstrated in this campaign - gaining access to SOHO devices of different makes and models; collecting host and LAN information to inform targeting, sampling and hijacking network communications to gain potentially persistent access to in-land devices; and intentionally stealth command-and-control infrastructure leveraging multistage siloed router-to-router communications - points to a highly sophisticated actor that we hypothesize has been living undetected on the edge of targeted networks for years."



LEO:  What?  Like the Unabomber?  I mean, not physically living on the edge.  He's just - he's out there somewhere.  Probably China; right?



STEVE:  Well, they're on the edge of targeted networks.



LEO:  Right.



STEVE:  So they are in the SOHO routers of employees of major networks, and they are using that location to get into major enterprise networks.



LEO:  I get it.  I get it, yeah.



STEVE:  So it's a stepping-stone.



LEO:  Yeah.



STEVE:  And it actually exists.



LEO:  Just like a sewer rat.



STEVE:  Just like a sewer rat, Leo.



LEO:  Living on the edge.  Oh, wow.  All right.  I guess what can you do?  What can we do?  We can update our routers, I guess.



STEVE:  Yes.  I would say make sure that your routers are running the most recent firmware, and reflashing the firmware when you know you've got good firmware.  That will clear them out of the file system.  And then just, you know, make sure that your perimeter security is up.  Turn off any, you know, Universal Plug and Play should never be bound to the WAN port, and remote management should never be turned on.  Just find some other way to do it.



LEO:  Yeah.  Well, it's a never-ending litany of threats, and that's why we love him.  Steve Gibson, GRC.com.  If it weren't for him, you might not even know about it; right?  So that's why you've got to listen every single week.  You can get a copy of this show at Steve's site.  In fact, he's got some unique copies, a 16Kb version for the bandwidth impaired.  He also has the transcripts, handcrafted by Elaine Farris.  Those are great for not only searching, but reading along while you listen.  All of that plus 64Kb audio available at GRC.com.



While you're there, pick up SpinRite.  Not too late to get v6 of the world's best mass storage maintenance and recovery utility.  If you have drives, you need SpinRite.  Works on SSDs as well as hard drives.  6.1 is coming soon.  And anybody who buys 6.0 now will get 6.1 as a free upgrade when that happens.



STEVE:  Yup.



LEO:  There's also lots of other stuff there that Steve puts up for free.  It's a really good site to browse around:  GRC.com.



STEVE:  Does rathole, does that term work?



LEO:  Yeah, yeah.  It's a rathole.



STEVE:  It's a rathole.



LEO:  We even have a rathole jingle somewhere which I will not take your time up.



STEVE:  Thank you.



LEO:  Digging up.  You can also get copies of this show at our website, TWiT.tv/sn for Security Now!.  There's a YouTube channel dedicated to Security Now! so you can watch it there, share it from there, which is a good way to share the video if you want to share it with somebody.  And I know a lot of times there's stuff in here you want to tell people about, raise the alarum, as it were.  You can also subscribe in your favorite podcast player.  That's probably the best thing to do.  That way you'll get it automatically the minute it's available.  You won't miss an episode at all, ever, that way.  And you can also leave us a five-star review that way, which is very much appreciated.



We record Security Now! on Tuesdays at about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  You can watch us do it live, if you wish, or listen live at live.twit.tv if you want the very freshest version of Security Now!.  If you're watching or listening live, chat live at irc.twit.tv, or in our Discord chatroom open to Club TWiT members.  Actually, Club TWiT's a great thing to get because you get ad-free versions of all the shows.  You get the Discord.  You get the TWiT+ feed.  A mere seven bucks a month.  Just go to TWiT.tv/clubtwit if you're interested.  Thanks in advance.  We really appreciate your support.	



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#879

DATE:		July 12, 2022

TITLE:		The Rolling Pwn

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-879.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a recently made and corrected mistake in the super-important OpenSSL crypto library.  The NIST has settled upon the first four of eight post-quantum crypto algorithms.  Yubico stepped-up to help Ukraine.  Apple has added an extreme "Lockdown Mode" to their devices.  Microsoft unbelievably re-enables Office VBA macros received from the Internet.  The FBI creates a successful encrypted message app for a major sting operation.  We close the loop with some of our listeners.  Then we examine an even more egregious case of remote automotive wireless unlocking and engine starting. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about NIST.  They have finally settled on some algorithms, the first four of their post-quantum crypto algorithms.  Steve will talk about them, including one called CRYSTALS-Dilithium.  And Apple's extreme lockdown mode.  Will Steve start using it?  Plus, why you may not want to own a Honda automobile.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 879, recorded Tuesday, July 12th, 2022, "The Rolling Pwn."



It's time for Security Now!.  Here he is, ladies and gentlemen, the star of our show, Bob Barker.  No, no, that's the wrong show.  Steve Gibson.  Hello, Steve.



STEVE GIBSON:  And I've got my lighting proper this week.



LEO:  Oh, yeah.  Yesterday.  Because you had the skylight open.



STEVE:  I have, like, one, two, three, four, five, six, seven, eight, nine.  I have many high-wattage LEDs, and they point at the ceiling, which is white.



LEO:  Oh, nice, soft, diffuse.



STEVE:  Which is why I get this really nice diffused - but it lit the room so much that my face was dark in the camera.



LEO:  Right.



STEVE:  So I thought - man, look at this.  It's just amazing.  It looks perfect now.



LEO:  Yeah.  Good lighting makes a big difference.  Even with a bad camera it makes a difference; right?



STEVE:  That's right.  Well, and of course we all have that Logitech HD 720 or 722 or whatever it is.



LEO:  Yup, there you go, yup.



STEVE:  So we're at Security Now! Episode 879 for the 12th of July.  And I was a little ambivalent about the title "The Rolling Pwn," P-W-N.



LEO:  It's funny.  It's funny.



STEVE:  Yeah.  That's the official name of the hack/attack.  But I wanted to do - I was toying with Rolling Your Pwn.  And I thought, well, no, okay.  Or The Rolling PIN.  But Pwn is what it's supposed to be.  So anyway, I just left it alone.  But first we're going to look at a recently made and corrected mistake in the super-important OpenSSL crypto library.  What you missed last week, oh, no, week before last, I forgot to mention when I was on with Jason, somebody - it was really pretty funny - somebody took the OpenSSL command line - and remember, OpenSSL is like the Swiss army knife times a thousand - took the OpenSSL command line and said, what if it was a GUI?  And so our Picture of the Week was that, and it took up four pages of our show notes.



LEO:  Wow.



STEVE:  And that was for one tab of one subset of the OpenSSL  commands.  Those were the options for I think it was like making a cert.  Anyway, so it was just a...



LEO:  That's a GUI for you, yeah.



STEVE:  It was a fun Picture of the Week.  But anyway, we're going to talk about a problem that was found and corrected in OpenSSL.  The NIST, and many of our listeners tweeted me to make sure I knew about this and wanted to hear about it, has settled upon the first four of the total of eight post-quantum crypto algorithms which will become the next standard, much as Rijndael was decided as the AES standard, and like the SHA-256 hashes.  I mean, we need standards.  Otherwise it's bad enough that our USB plugs won't go in the right way.  Fortunately, they're all the proper shape.  And at least we have that, you know.  Some are not triangular, and some are not hexagonal.  So anyway.  Also Yubico stepped up to help Ukraine in a little blurb that passed by.  And I thought, oh, I've just got to give them a shout-out and a thank you.  



LEO:  Good.



STEVE:  Apple of course has added the extreme lockdown mode that we'll talk about, or it's forthcoming.  Microsoft unbelievably, and the whole security industry has just gone in meltdown over their announcement that they are re-enabling Office's VBA macros, which are received over the Internet, after telling us in February, to everyone's great relief and many sighs, that they were going to finally disable them by default.  Now they're saying, oh.



LEO:  You're out of date.  They're decided not to enable them.



STEVE:  Oh, you're kidding.



LEO:  No.  This just in.



STEVE:  Wow.



LEO:  Yeah.  So they put out a press release yesterday because there was a lot of upset over this.



STEVE:  Cool.



LEO:  And macros from the Internet will be blocked by default in Office, according to Microsoft, as of yesterday.



STEVE:  Still be blocked by default.



LEO:  Still, yes.



STEVE:  Okay.



LEO:  And I think, yeah, there's some nuance into the whole thing, but...



STEVE:  Well, we'll have some fun at their expense anyway because, you know, they're Microsoft.



LEO:  I will show the flowchart that decides whether macros are to run.  You've showed it before.



STEVE:  Oh, you mean there's still a way for it to happen.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  Oh, boy.  Wonderful.  The FBI has created a successful encrypted messaging app which participated in a major sting operation that we're going to talk about.  We're also going to close the loop with some of our listeners.  Then we're going to examine an even more egregious case of remote automotive wireless unlocking and engine starting.  Thus the Rolling Pwn.



LEO:  Ah.



STEVE:  Cars roll.



LEO:  I get it.



STEVE:  After they've been pwned.  And we have a really clever wonderful Picture of the Week.  So I think another good podcast for our listeners.



LEO:  An OR gate.  Yes, a little bit of a logic lesson.  It's exciting, yes.  Good.  Good show, as always, coming up.  I look forward to Tuesday all week long to hear the latest.  In fact, and I'm sure I'm not alone in this, I see articles, and I go, I can't wait to hear what Steve has to say about that.  Can't wait to hear what Steve says.  Without you, I wouldn't know what to think about any of this stuff.  Picture of the Week time, Steverino.



STEVE:  So this is a visual feast.  Unfortunately, it would be difficult for me to explain it in words.



LEO:  Well, that's why there's a picture.



STEVE:  Yeah, it's a very good point.  For those who do have the show notes in front of them, there's a, as I described at the beginning, and I gave it a title, a wonderful mechanical OR gate.  If you - oh, Leo, it's just so good.  If you - imagine that six different people had six different keys to six different padlocks.  Now, if you wanted to allow any one of them to unlock, for example, a gate that was secured with a chain, well, you could interlace, you could interlock six padlocks, the hasps of six padlocks through each other, right, to sort of create one long padlock-y thing such that any one of the keys could unlock its one padlock.  And because it was a chain of padlocks, that would unlock the whole chain.  The problem we have here, though, is that this thing that's being secured, there's a big steel arm coming in from the left.



LEO:  Looks like a gate or something, yeah.



STEVE:  Yeah.  And it wants to come, it wants to slide outwards so there isn't really a way you could do that with interlinking hasps of padlocks.  So some mechanical engineer, I mean, this is just so cool.  The more I looked at it for a while, the better I liked it.



LEO:  This is brilliant.  And unlocking any one of these padlocks opens the gate.



STEVE:  Yes. Well, I guess it would be an AND, wouldn't it.  Because they all have to be closed for it to work.  Any one of them being opened...



LEO:  It just depends whether the true value is unlocked or locked; right?



STEVE:  Yes, exactly.  Exactly.



LEO:  So it's OR if it's unlocked.  It's an AND if it's locked.



STEVE:  Right.  But what's so cool is that, again, there's no way to describe it except to say that it's this wonderful mechanism where any one of these locks, like one at the bottom, you would unlock one at the bottom.



LEO:  18 or 15.



STEVE:  Yes, exactly.  That would allow the shim to be removed from a pin which then allows the pin to slide out, the little pin at the bottom, which then allows the big pin, the big vertical pin to be lifted out of the way of the lever that wants to come out.  So it's this multistage lock trick.



LEO:  It's very clever.



STEVE:  It's really good.



LEO:  I'm not sure I understand how 15/18, so if I take 15 or 18 out, then that means, oh, I can slide this bigger bar out and then slide this little pin out.



STEVE:  Right.



LEO:  Yeah, yeah, you just kind of have to be there.  And you certainly can't describe it.



STEVE:  And then the big ones comes - as I said, there's really no way to describe it except to, you know, it's analogous to a chain of six locks that are interlinked, but in a way that works with this particular mechanism.  Anyway, it's just wonderful, wonderful, wonderful.  So whoever that was, a listener of ours who said, what do you think about this for a Picture of the Week?  It's like, right on.



LEO:  Wow, wow, wow.



STEVE:  That is dead on the money.  That's our kind of picture.  Okay.  So OpenSSL v3.0.4 introduced a serious vulnerability which v3.0.5 just repaired.  It is a potential remote code execution flaw which was recently discovered in an update, that is, this 3.0.4, to the v3 branch of OpenSSL.  The issue was found, as I said, in 3.0.4.  The good news is it was just released late last month, on June 21st.  It impacts x64, so 64-bit Intel architecture systems having the AVX-512 instruction set extension.  The good news is OpenSSL v1.1.1, as well as the two forks, BoringSSL and LibreSSL, they're not affected.  So only the 3 branch.  The trouble stems from a remote memory corruption vulnerability.  This AVX are the Advanced Vector Extensions which add instructions to the x86 instruction set architecture from processors both from Intel and AMD.



There was some interesting back and forth about this in the GitHub issue thread where I think that the OpenSSL Foundation's guy Tomas Mraz said:  "I do not think this is a security vulnerability."  Of course he didn't want it to be.  He said:  "It is just a serious bug making the v3.0.4 release unusable on AVX-512 capable machines."  Okay.  So I guess he's saying that it will crash.  So like the certificates, it involved RSA certificates.  So they won't work.  So that's a problem.  A security researcher, Guido Vranken, he said it can be triggered trivially by an attacker.



Another person participating in the thread, Alex Gaynor, wrote:  "I'm not sure I understand how it's not a security vulnerability.  It's a heap buffer overflow that's triggerable by things like RSA signatures, which can easily happen in remote contexts like," he says, "a TLS handshake."  And the post-grad Ph.D. student who originally discovered and reported the bug chimed in to the thread, stating, he said, "Although I think we shouldn't [as in should not] mark a bug as 'security vulnerability' unless we have some evidence showing it can, or at least may, be exploited," he says, "it's necessary to release version 3.0.5 as soon as possible given the severity of the issue."



Which is what did in fact soon happen.  The issue has been assigned CVE-2022-2274, described in that CVE as a case of a heap memory corruption within RSA private key operations.  The advisory notes that "SSL/TLS servers or other servers using 2048-bit RSA private keys running on machines supporting AVX-512-IFMA instructions of the x86_64 architecture are affected by the issue."  And calling it a "serious bug in the RSA implementation," but still apparently not wishing to call it a vulnerability, the maintainers of OpenSSL said that the flaw could lead to memory corruption during computation that could be weaponized - sounds like a vulnerability to me - by an attacker to trigger remote code execution on the machine performing the computation.  So anyway, as I said, smacks of a security vulnerability.



Well, anyway, the flaw has been patched, and all users of OpenSSL v3 should move to 3.0.5, especially if you had diligently moved to 3.0.4, which is the buggy release.  On the other hand, the window was a couple weeks.  So the chances are nobody even had a chance to get the buggy one before you got the good one.  So anyway, just FYI.



Okay.  Last Tuesday, the U.S. NIST, right, our National Institute of Standards and Technology, announced that the results of a six-year - these things do take a while.  And I'm glad.  This is not something you want to rush because we're going to be living with this for a long time.  The six-year competition among a set of post-quantum algorithms had resulted in the selection of four initial algorithms.  That is, because there are going to be a total of eight.  So the first half have now been chosen.  After editing out the various self-congratulatory statements from various bureaucrats who have no clue what this is all about and who certainly didn't even write what they are quoted as saying in this official announcement - I read through this.  It's like, oh, come on.  You have no idea what you're talking about.  Anyway...



LEO:  What are you talking about?  I've always been a fan of elliptic curve cryptography.



STEVE:  I use it to clean my sheets every week.  So here's what the people who actually wrote something about this and were involved in the choosing had to say.  They said:  "The U.S. Department of Commerce's National Institute of Standards and Technology (NIST) has chosen the first group of encryption tools that are designed to withstand the assault of a future quantum computer, which could potentially crack the security used to protect privacy in the digital systems we rely on every day"  as in today - "such as online banking and email software.  The four selected encryption algorithms will become part of NIST's post-quantum cryptographic standard, expected to be finalized in about two years.



"The announcement follows a six-year effort managed by NIST" - but not in any way poisoned by them, this has all been done open on GitHub in full public view - "managed by NIST which in 2016 called upon the world's cryptographers to devise and then vet encryption methods that could resist an attack from a future quantum computer" - that is, you know, one with more than four  Qubits - "that is more powerful than the comparatively limited machines available today.  Today's selection constitutes the beginning of the finale of the agency's post-quantum cryptography standardization project."  And this is clearly a good thing.  I don't remember, Leo, whether they were able to do the factorization of, what was it, 33 or something?



LEO:  It was some ridiculous number, yeah.



STEVE:  Yeah, it's like, oh, okay.



LEO:  How hard is that, huh?



STEVE:  We don't have to worry about it right now.



LEO:  I'd say we are making progress, though.  This was a story a couple of days ago that scientists in Germany have showed spooky action at a distance of 20 miles.  Two atoms.  So that's when the...



STEVE:  Quantum entanglement.



LEO:  Yeah, quantum entanglement.  And the two atoms somehow are communicating instantaneously across a distance of 20 miles.



STEVE:  And Leo, not at lightspeed.  Not at lightspeed.



LEO:  Much faster.



STEVE:  Instantaneously.



LEO:  Yeah, yeah.



STEVE:  Which tells you, definitely a simulation.  Okay.



LEO:  Or there's some dimension we don't know about in which those two atoms are right next to each other.



STEVE:  Or are the same thing.



LEO:  Or the same thing.



STEVE:  See, the whole idea of space could be just an illusion; right?



LEO:  Right.



STEVE:  There isn't actually any.  It's just...



LEO:  It's mindboggling that they're doing it.



STEVE:  It is.  And it does, it Hertz.



LEO:  It Hertz, yes.



STEVE:  It Hertz, yes.  Okay.  Four additional algorithms are under consideration still for inclusion in the standard, and NIST plans to announce the finalists from that round at a future date.  NIST is announcing its choices, they wrote, in two stages because of the need for a robust variety of defense tools.  As cryptographers have recognized from the beginning of NIST's effort, there are different systems and tasks that use encryption, and a useful standard would offer solutions designed for different situations.



And that, yes, how many times have we - we talk about the toolbox that we have today and how cool it is that you can just put these things, these little components together in all different ways.  So use varied approaches for encryption, and offer more than one algorithm for each use case in the event one proves vulnerable.  And that's what they've done here.  This is like we're - it feels like we're beginning to understand collectively as a planet how to do these sorts of things correctly.



So explaining this for the masses, NIST added:  "Encryption uses math to protect sensitive electronic information, including the secure websites we surf and the emails we send.  Widely used public-key encryption systems, which rely on math problems that even the fastest conventional computers find intractable, ensure these websites and messages are inaccessible to unwelcome third parties.  However, a sufficiently capable quantum computer, which would be based on different technology than the conventional computers we have today, could solve these math problems quickly to defeat today's encryption systems.  To counter this threat, the four quantum-resistant algorithms rely on math problems that both conventional and quantum computers should have difficulty solving, thereby defending privacy both now and down the road."



LEO:  Okay.  I've got a tough question for you.  Do we now trust NIST?  Because remember they intentionally recommended a weak algorithm at the behest of the National Security Agencies.



STEVE:  Yeah.  And I would say those were days gone by.



LEO:  Yeah.



STEVE:  There's no cryptographer who doesn't know that this random bit generator that RSA Corporation was sort of defaulting to had some sketchy background.  There was no reason for the NSA not to describe where the magic numbers came from that that digital random bit generator used.  And that would have made everyone feel good.  If somebody had said this is how we chose these numbers, then everyone would have gone, okay, that makes sense.  Instead, it was "Thou shalt use these numbers."  And it was like, uh, that's not the way we do things here.  And the point is it wasn't the way - it sort of was the way we did things then because nobody was that focused on those things.  Now we really are.



So I don't think that could ever happen again.  I mean, and so this is a - this was done very much like the way Rijndael was chosen, where a number of like really good candidates were examined, and sample implementation code was created.  And things like how fast can we make this work on a 64-bit x64 architecture, and can we design algorithms which will not be subject to side channel attacks.  I mean, just think about everything we've learned in the last 20 years.  All of that is now rolled into this.  And lots of debate.  That's why it took six years, you know, to decide this.



So in this case these first four of the eight total algorithms, the first four are designed for two main tasks, for which as we know encryption, or crypto, is typically used:  general encryption, which is used to protect information exchanged across a public network, and digital signatures used for identity authentication.  All four of the algorithms were created by academic experts collaborating openly from multiple countries and institutions.



To provide for general encryption, NIST - and it's not NIST as much as it's the collective.  And that's just it.  NIST is just sort of saying, yeah, we're going to do the press release.  But it was this - it wasn't NIST that chose it I guess is the point.  It was everybody coming to a final agreement that, okay, to do encryption we're going to use the CRYSTALS-Kyber algorithm, which is what was chosen for encryption.  And it was chosen because it uses comparatively small encryption keys which two parties will be able to exchange easily, as well as very good speed of operation.  On the digital signatures side, NIST and the collective selected three algorithms.



LEO:  I like his first one.



STEVE:  I do, too, Leo.  It's the Dilithium CRYSTALS algorithm.



LEO:  Yes.



STEVE:  And then we also have FALCON, and we have SPHINCS.  It's actually SPHINCS+ because there was some tweaking that was done later, so it's S-P-H-I-N-C-S-+,  which we're supposed to read as SPHINCS+.  Reviewers noted the high efficiency of the first two, and NIST recommends CRYSTALS-Dilithium to be used as the primary algorithm, with FALCON for applications that need smaller signatures than Dilithium is able to provide.  The third, and this is, again, why the thinking was so good on this, SPHINCS+ is somewhat larger and slower than the other two, but it's valuable to have as a backup for the reason that it is based on entirely different math than all of the other three NIST selections.  The other ones are based on lattice math, and SPHINCS isn't.  So again, we've learned that where crypto is concerned, there's nothing wrong with using a belt and some suspenders.



As I said, the first three of the four selected algorithms are based on a family of math problems known as "structured lattices," which is why the word "CRYSTAL" appears as part of the names of the first two; while SPHINCS+ uses hash functions.  Now, the next four algorithms to be chosen, which are still under consideration, are designed for general encryption and do not use structured lattices or hash functions in their approaches.  So again, we're going to do, like we're looking at a variety of different solutions like in advance.  And once we deploy these, all of them will be selectable in the various algorithms so that, if something is found to be wrong, it'll be like, whoops.  And it'll be easy to just switch over.  Or remember how the early versions of TrueCrypt allowed you to use like multiple different algorithms like at once, under the theory that, well, if one of them was broken, then the other ones would still be good.  So anyway, we sort of have a little bit of that, too.



So NIST wrote:  "While the standard is in development, NIST encourages security experts to explore the new algorithms" - oh, all the source is public, by the way, and posted online.



LEO:  And that's why we shouldn't worry about NIST being involved in this, obviously.



STEVE:  Yeah.  I mean, they really weren't.  They were just, again, based on, Leo, that text that I excluded from the announcement, you would know that whoever it was who wrote that nonsense...



LEO:  Senator Foghorn Leghorn believes in CRYSTAL lattices as the finest way to protect yourself.



STEVE:  Yeah.  That's exactly it, yeah.  Boy.  So they said:  "While the standard is in development, NIST encourages security experts to explore the new algorithms to consider how their applications will use them, but not to bake them into their systems quite yet, as the algorithms could change slightly before the standard is finalized."



LEO:  Can we use these if they're not baked?  I mean, can we use them now?



STEVE:  Yeah.  Yeah, yeah, yeah.  Oh, I mean, they've been pounded, had the crap pounded out of them already.



LEO:  They've pretty well baked, yeah.



STEVE:  I think it's likely that they're pretty much.  But they're just, you know, again, they're hedging their bets.  They don't want to be, like have fingers pointed at them saying, hey, you said these were final, and we burned it into our firmware and sent it up into outer space.  So, like, no, okay, don't do that quite yet.



LEO:  Don't do that.  So but what tools...



STEVE:  Elon is welcome to use them right now.



LEO:  Use them all you want, Elon.  PGP or, I mean, what kind of tools - TLS, I guess, yes?



STEVE:  Yeah, yeah.  Basically our crypto uses signatures all over the place and uses encryption all over the place.



LEO:  Yeah.  I would use PGP for that, which of course is an ancient and kind of crapulous bundle of algorithms, none of which are these.



STEVE:  Yeah.  We could hope that PGP does not incorporate these so that once quantum computing comes along, sorry, PGP, your time has come.



LEO:  SSH, though, would probably implement it, I would imagine.



STEVE:  Oh, oh, well, yeah, you mean, yeah, TLS definitely would.  And it'll be used for hashing and, I mean, like a next-generation set of functions, the idea being that assuming that big quantum machines actually do happen, and again, it's like these are fast enough that there's no reason not to switch over to them.  And that's the point; right?  At some point because remember the other danger that we've talked about is that things that are encrypted today cannot be decrypted today, but they could be decrypted tomorrow.  So this is why the NSA has that massive facility in Nevada which is why Vegas's lights are dimmer now than they used to be, is that the NSA's just storing everything.  They're like, well, we can't decrypt it yet, but we think that once quantum comes along we'll be able to retroactively decrypt all this crap that we've been storing.  So let's just keep it because storage doesn't cost anything.



So the point is we want to switch over to post-quantum crypto as soon as we know that we can trust it, assuming that it's not going to be a lot slower, and these algorithms are not slower than the ones we have, they're just bigger and different.  We want to switch over so that we start giving the NSA stuff that, oh, sorry about that, but this is the Dilithium CRYSTAL quantum crypto.



LEO:  I love it.



STEVE:  And you're SOL.  So, yeah.  So...



LEO:  And by the way, you can't crack these Dilithium CRYSTALS.



STEVE:  So I obviously have no problem with the idea that adopting advanced post-quantum cryptography under the name Dilithium CRYSTALS, it's like what we ended up with.  But I have to say, Leo, that in scanning through all of the candidate entries from which these four winners were selected, I did breathe a sigh of relief when I saw that the quantum algorithm named Frodo had not won.  



LEO:  Yeah.



STEVE:  I would have a hard time choosing Frodo as my post-quantum solution.



LEO:  I refuse to use an algorithm with hairy toes, I'm sorry, it's just not going to happen.



STEVE:  Thank you.  On that note, I'm going to take a sip of water.  And then we're going to talk about Yubico.



LEO:  Adrian in the chatroom says, "You mean Dilithium's a real thing?  I thought it was just some fictional Star Trek thing."



STEVE:  It's real now, baby.



LEO:  It is now.



STEVE:  It's real now.



LEO:  Congratulations.  You have entered the Dilithium Zone.  Our show today brought to you by - let me find out who it's brought to you by.  That's what I need to do.  Oh.  I know what I did wrong.  I should never use Bing for a browser.



STEVE:  Oh.  Lorrie keeps saying to me, how can I get rid of this thing?  It keeps coming back.  I hate it.  I hate it.  I hate it.



LEO:  I hate Bing.



STEVE:  To help Ukraine hold off Russia's cyberattacks, Yubico donated 30,000 FIDO YubiKeys.



LEO:  Bravo.



STEVE:  I know.  I thought that was so neat.  I just happened to see it in passing.  So far, more than half of those 30,000, around 16,000 YubiKeys have been deployed to Ukrainian government executives, workers, and employees of private companies in Ukraine's critical sectors.  The initiative is being coordinated by a company named Hideez, H-I-D-E-E-Z, which I guess ID as in identity, Hideez, they're a Ukrainian security firm specializing in identity services and FIDO consultancy, so they know their way around FIDO.  Earlier this spring Hideez secured a donation of 30,000 YubiKeys from Yubico, and way to go, Stina.



Since then Hideez's staff has been working with Ukrainian government agencies like the Ministry of Digital Transformation, the National Security and Defense Council, and the State Service of Special Communications and Information Protection, that's the SSSCIP, of Ukraine to ensure the devices can be - and it's one thing to have YubiKeys; right?  But you've got to know what to do with them.  So they're ensuring the devices can be imported into the country, that government infrastructure is prepared for the YubiKeys rollout, and that the recipients receive the necessary training to know how to use them.



So the idea is that once government and critical sector workers have a security key as an extra layer of protection, their accounts would finally be safe from what amounts to an onslaught of nonstop spear-phishing attacks which have been constantly hitting their inboxes every day.



Yuriy Ackermann, VP of War Efforts at Hideez, told the publication Risky Business, he said:  "We got YubiKey-certified, so they are allowed to be deployed into Ukraine instances."  He said:  "We have quite a few ministries that have moved a lot of their stuff to G Suite and Azure.  With them it's quite easy.  We just get them a key.  We made instructions in Ukrainian, video instructions and so on.  So it's really fast.  We had a department that pretty much moved to using FIDO, like 500 people in less than a week because they just needed to understand their policies, read our documentation, and that's it.  They just give the keys and roll them, and voila."



So meanwhile, efforts are underway to roll out the keys to individuals in other departments - you know, they still have, what, 17,000 of them available, or 14,000 rather - including those without the proper server-side infrastructure.  In these cases, Ackermann says Hideez has been providing the government with the company's solutions at minimal costs.



Anyway, as I said, I just happened upon this nice bit of news and wanted to acknowledge what Yubico had done to help Ukraine in their war effort.



LEO:  Is FIDO related to Passkeys?  FIDO2?



STEVE:  Yes.  FIDO was the original, and that was the one which didn't get off the ground because it absolutely is tied to a hardware token.  Whereas FIDO2 you're allowed to use devices that have some sort of biometrics in order to do the unlocking.



LEO:  Same concept, really.



STEVE:  Yes, yes.  Same concept.  So Apple's new - oh, I should mention though also that FIDO2 uses WebAuthn also as its protocol to the web server, whereas FIDO is not a WebAuthn user.  



LEO:  Oh, okay.



STEVE:  So you have to have specific support for FIDO on the server side.  Which is why Hideez is having to bring in some of its own technology where that's not available. 



LEO:  Got it, yeah.



STEVE:  So Apple's new extreme "Lockdown Mode."  And "extreme" is their word, which I thought was kind of fun.  In a blog post last Wednesday, Apple took the wraps off of Lockdown Mode, which will be rolled out later this year, first seen in macOS Ventura, iOS 16, and iPadOS 16.  This is an optional mode which will, again, in their words, severely restrict some features.  I mean, they've gone to a great degree here.  I guess on one hand it's selling the idea that it is so restrictive.  But they're also, like, making it clear that, yeah, we're not sure that this is for everybody.  So if you were to turn this on, be prepared for a bunch of stuff not to work.



The aim is to protect specifically highly targeted individuals such as human rights workers and researchers, by reducing their devices' available attack surface.  And they provided in their announcement a screenshot where this is the screen where you would go to turn this on.  And it says:  "Lockdown Mode is an extreme, optional protection that should only be used if you believe you may be personally targeted by a highly sophisticated cyberattack.  Most people are never targeted by attacks of this nature."



Then they said, second paragraph:  "When iPhone is in Lockdown Mode, it will not function as it typically does.  Apps, websites, and features will be strictly limited for security, and some experiences will be completely unavailable."  And then they've got a button to learn more, or this big scary one at the bottom, "Turn On Lockdown Mode."



So the way Apple put this in their announcement, and they used a term I hadn't seen before I thought was interesting.  They said:  "Apple expands industry-leading commitment to protect users from highly targeted mercenary spyware."



LEO:  Mmm.



STEVE:  And they said:  "Apple is previewing a groundbreaking security capability that offers specialized additional protection to users who may be at risk of highly targeted cyberattacks from private companies developing state-sponsored mercenary spyware."



LEO:  Wow.



STEVE:  Yeah.  They said:  "Apple today detailed two initiatives to help protect users who may be personally targeted by some of the most sophisticated digital threats, such as those from private companies developing state-sponsored mercenary spyware."  Okay, we get the message, Apple.  "Lockdown Mode  the first major capability of its kind, coming this fall with iOS 16, iPadOS 16, and macOS Ventura  is an extreme, optional protection for the very small number of users who face grave, targeted threats to their digital security.  Apple also shared details about the $10 million cybersecurity grant it announced last November to support civil society organizations that conduct mercenary spyware threat research and advocacy."  In other words, researchers who were like going to dig into what this is all about.



Apple's head of Security Engineering and Architecture was quoted:  "Apple makes the most secure" - yeah, blah blah blah - "mobile devices on the market."



LEO:  You can always tell when you're reading from an Apple press release.  Yeah, blah blah blah.



STEVE:  Okay.  "Lockdown Mode," he said, "is a groundbreaking capability that reflects our unwavering commitment to protecting users from even the rarest, most sophisticated attacks."  (Which we're unable to block.)  Okay, he didn't really say that.  "While the vast majority of users will never be the victims of such highly targeted cyberattacks, we will work tirelessly to protect the small number of users who are.  That includes continuing to design defenses specifically for these users, as well as supporting researchers and organizations around the world doing critically important work in exposing mercenary companies that create these digital attacks.



"Lockdown Mode," he said, "offers an extreme, optional level of security for the very few users" - and they really don't want you to turn this on, but they want you to feel very special if you do - "who because of who they are or what they do may be personally targeted by some of the most sophisticated digital threats" - because after all, otherwise it wouldn't get through the regular iOS security - "such as those from" - oh, and we're naming names - "the NSO Group and other private companies developing state-sponsored mercenary spyware.  Turning on Lockdown Mode in iOS 16, iPadOS 16, and macOS Ventura," you might as well just turn off your device.  No, it doesn't say that - "further hardens device defenses and strictly limits certain functionalities, sharply reducing" - and I actually believe in this a lot - "the attack surface that potentially could be exploited by highly targeted mercenary spyware."



Okay.  So at launch, Lockdown Mode includes the following protections.  We have five.  Messages:  Most message attachment types other than images are blocked.  Some features, like link previews, are disabled.  Because yes, those could be abused.



Web browsing:  Certain complex web technologies - bravo, Apple - like Just-in-Time JavaScript compilation.  Remember, we saw Microsoft experimenting with disabling that in Edge because it just seems to be where all the problems are.  Just-in-Time JavaScript compilations are disabled unless the user excludes a trusted site from Lockdown Mode.



Third, Apple services:  Incoming invitations and service requests, including FaceTime calls, are blocked if the user has not previously sent the initiator a call or a request.  Wired connections with a computer or accessory are blocked when iPhone is locked.  And maybe you guys talked about this over on MacBreak.  I'll ask you about that in a second, Leo.



LEO:  Sure.



STEVE:  And finally, configuration profiles cannot be installed, and the device cannot enroll into mobile device management while Lockdown Mode is turned on.  So to my eye, those all sound like very useful and sane restrictions.



LEO:  You bet.



STEVE: They would not hugely impact even most users, I think, while they would very clearly and significantly restrict the device's attack surface.  So I say "Bravo, Apple, nice going."  I'm sure that they've closely looked at the history of the way their devices have been compromised and then took steps to address future threats in a way that will keep their devices useful and useable, while being far less easily compromised.  So again, bravo.



So Leo, that fourth thing, wired connections with a computer or accessory are blocked when iPhone is locked.



LEO:  This is how I interpreted it, and the panel seemed to agree.  That's so that you can charge but not have a data connection with a USB port.  Right?  So it's to prevent you from plugging your iPhone into some strange port.  It actually allows you to do so.  It's like your USB condom, I think.  So what happens normally with an iPhone when you plug in a USB cable to a device, it says do you trust this device, you say yes, and now you can exchange data.



STEVE:  Right.



LEO:  Which is obviously a bit risky.



STEVE:  That's very, very possible.



LEO:  I think it's good.



STEVE:  A built-in condom for your...



LEO:  Yeah, a built-in condom.  That's how Apple should sell it, I think, yeah?  A rubber for your phone.



STEVE:  Most of you will not need a built-in condom.  But...



LEO:  I had a question about the Just-in-Time stuff.  Didn't we talk about that at one point, that Google's research showed that Just-in-Time JavaScript was problematic?



STEVE:  Yeah, it was Microsoft that were doing this because now they have Bing.  And as we know, Bing is now based on the Chromium engine.  And so it was their analysis that showed I think it was 80%, eight zero percent, of the problems that they were seeing in JavaScript resulted from tiny flaws in this  squeezing the last, every last bit of performance out of JavaScript.  And what they were saying was, you know, maybe five years ago, 10 years ago, computers were still slow.  I mean, remember back then we didn't want to use encryption because it slowed down things.  It was too slow.  Now it's like, bring it on.  We're going to go quantum, baby.  We're going to do Dilithium encryption.  So anyway, Microsoft said, hey, just turn this off, and you're going to be automatically protected from 80% of the problems that we're having.  So Apple is saying the same thing.



LEO:  And it's not like turning off JavaScript.  It's just turning off the JIT compiler or JIT...



STEVE:  Right.



LEO:  So you frankly maybe would run a little slower, probably not on a modern machine, and it eliminates a lot of those security flaws.  So I think that's - I agree with you, this didn't seem too onerous.  Google has their superior security where you have to have two Titan keys and all of that.  I tried that for a while, and you lose so much functionality from Google that it wasn't worth doing it.  But I could see turning this on for a normal person.



STEVE:  I'm going to turn it on and see how it feels.



LEO:  Yeah, I will, too.



STEVE:  Because, like, why not?  And, you know, I'll bet that would be - you know they're going to have some telemetry.  I'll be it'll be some interesting metrics that they're going to get back about how many people go, yeah, you know, I don't need all that crap that's...



LEO:  I forgot I turned it on, yeah.



STEVE:  Yeah, exactly.



LEO:  It's all the things that people do that are dangerous, like links in messages.  And the message rendering engines, I also use my Security Now! knowledge on MacBreak Weekly about that because we talked about this is where you see a lot of flaws, on Windows as well as on Apple products with this interpreter that has to somehow render this content in messages.  And it's so often a security issue.



STEVE:  Yup.  Okay.  So as you said, it's already the case that Microsoft is not going to do what they said they were going to do last week, which...



LEO:  It really confused the hell out of people.



STEVE:  Caused such a hubbub.  Although Paul Ducklin, who writes for Sophos, I loved what he said about this.  And I'll share it just because it's a great blast from the past.  So that everyone understands, this macro abuse that we've suffered for so long, I remember when they announced it at the beginning of the year, Leo.  You and I, back in February, were looking at the notice bars that used to be, and that would be, where it was so easy for you to click on "Allow Macros."  And so, like, you know, you'd get some piece of email, and it would say, oh, this is not going to display properly unless you allow macros.  Click here.  It's like, who would not click that?  Of course you would because it's like telling you to click it.



And so Microsoft said, oh, no, okay, we realize this has been causing lots of problems.  We're going to turn this off.  So what shocked everybody was when they said, uh, we're not going to tell you why, and they actually refused to tell people why exactly they changed their mind, but they announced last week they were going to do that. 



Anyway, so in Sophos Paul said:  "Remember 1999?  Well," he said, "the Melissa virus called, and it's finding life tough in 2022."  He said:  "It's demanding a return to the freewheeling days of the last millennium, when Office macro viruses didn't face the trials and tribulations they do today."  He said:  "In the 1990s you could insert VBA (Visual Basic for Applications) macro code into documents at will, email them to people, or ask them to download them from a website somewhere.  And then you could just totally take over their computer.



"In fact, it was even better or worse than that.  If you created a macro subroutine with a name that mirrored one of the common menu items, such as FileSave or FilePrint, then your code would magically and invisibly be invoked whenever the user activated that option.  Worse still, if you gave your macro a name like AutoOpen, then it would run every time the document was opened."



LEO:  Yuck.



STEVE:  I know.  How did we survive, Leo?  And he says:  "Even if the user only wanted to look at it.  And if you installed your macros into a central repository known as the global template, your macros would automatically apply all the time.  Worst of all, perhaps, an infected document would implant macros into the global template, thus infecting the computer; and the same macros, when they detected they were running from the global template but the document you just opened was uninfected, could copy themselves back out again to that document."  He said:  "That led to regular 'perfect storms' of fast-spreading and long-running macro virus outbreaks.  Simply put, once you'd opened one infected document on your computer, every document you opened or created thereafter would, or at least could, get infected as well, until you had nothing but infected Office files everywhere."



LEO:  Everywhere.  Nice.



STEVE:  "As you can imagine," he said, "at that point in the game, any file you sent or shared with a colleague, customer, prospector, investor, supplier, friend, enemy, journalist, random member of the public, would contain a fully functional copy of the virus, ready to do its best to infect them when they opened it, assuming they weren't infected already.  And if that wasn't enough on its own, Office macro malware could deliberately distribute itself, instead of waiting for you to send a copy to someone, by reading your email address book and sending itself to some, many, or all of the names it found there.



"The first macro malware, which spread by means of infected Word files, appeared in late 1995 and was dubbed Concept" - remember that? - "because at that time it was little more than a proof-of-concept."  And, you know, the Concept virus was a thing.  That's what it was.  "But it quickly became obvious that malicious macros were going to be more than just a passing headache.  Microsoft was slow to come to the cybersecurity party, carefully avoiding terms such as virus, worm, Trojan Horse, and malware, resolutely referring to the Concept virus as nothing more than a 'prank macro.'



"Over the years, however, Microsoft gradually implemented a series of functional changes in Office by incrementally, for example, variously, first, making it easier and quicker to detect whether a file was a pure document, thus swiftly differentiating pure document files and template files with macro code inside.  In the early days of macro viruses, back when computers were much slower than today, significant and time-consuming malware-like scanning was needed on every document file just to figure out if it needed scanning for malware."



He says:  "Microsoft also made it harder for template macros to copy themselves out into uninfected files.  Unfortunately, although this helped to kill off self-spreading macro viruses, it didn't prevent macro malware in general. Criminals could still create their own booby-trapped files upfront and send them individually to each potential victim, just as they do today, without relying on self-replication to spread further."



He also noted that Microsoft "popped up a 'dangerous content' warning so that macros couldn't easily run by mistake.  As useful as this feature is, he wrote, because macros don't run until you choose to allow them, crooks have learned how to defeat it.  They typically add content to the document that helpfully explains which button to press, often providing a handy graphical arrow pointing at it."



LEO:  Click Allow Here.



STEVE:  Click here, yes, with a little eh eh eh eh, and giving a believable reason that disguises the security risk involved.  And finally, he said:  "Adding Group Policy settings for stricter macro controls on company networks.  For example, administrators can block macros altogether in Office files that came from outside the network, so that users cannot click to allow macros to run in files received via email or downloaded from the web, even if they want to."



So he says:  "At last, in February 2022, Microsoft announced to sighs of collective relief from the cybersecurity community that it was planning to turn on the 'inhibit macros in documents that arrived from the Internet' by default, for everyone, all the time.  The security option that once required Group Policy intervention was finally being adopted as a default setting.  In other words, as a business you were still free to use the power of VBA to automate your internal handling of Office documents; but you would not, unless you went out of your way to permit it, be exposed to potentially unknown, untrusted and unwanted macros that weren't from an approved, internal source."



And of course yay.  As this podcast celebrated at the time, Microsoft described that change then by saying:  "VBA macros obtained from the Internet will now be blocked by default.  For macros in files obtained from the Internet, users will no longer be able to enable content with a click of a button.  A message bar will appear for users notifying them with a button to learn more.  The default is more secure and is expected to keep more users safe including home users and information workers in managed organizations."



So everybody was excited about that.  Sophos was enthusiastic, too, although a little bit less so than I was at the time.  Back then they said:  "We're delighted to see this change coming, but it's nevertheless only a small security step for Office users because VBA will still be fully supported, and you will still be able to save documents from email or your browser and then open them locally.  The changes won't reach older versions of Office for months, or perhaps years, given that change dates for Office 2021 and earlier haven't even been announced yet," they wrote.  "Mobile and Mac users won't be getting this change, and not all Office components are included.  Apparently only Access, Excel, PowerPoint, Visio, and Word will be getting this new setting."  On the other hand, that's by far the majority of Office things.



So anyway, Leo, you have the - so the news I was reporting yesterday was that they decided, well, actually what Microsoft said last week was:  "Following user feedback, we have rolled back this change temporarily" - more temporarily than I thought - "while we make some additional changes to enhance usability."  They said:  "This is a temporary change, and we are fully committed to making the default change for all users.  Regardless of the default setting, customers can block Internet macros through the Group Policy settings described in the article 'Block macros from running in Office files from the Internet.'  We will provide additional details on timeline in upcoming weeks."  And oh, boy, look at that...



LEO:  So you know what happened, which was - so until recently it would be a pop-up, it'd say there's a macro in here, and then a button that said, yeah, go ahead, run it.  And that was just too easy.  So what they were going to do was take that button, move it into the properties of the document, so you have to know  to get the info on the document, go into the properties, check a box, run the macro.  And I think pretty clearly what happened is a lot of businesses said, but no, but it's too hard.  And we have to train people how to do that.  So initially Microsoft said, okay, we're not going to do that.  Now, of course, everybody else has said, no, no, it's too easy.  And so they just kind of backed off on that.  This is, according to the Microsoft blog post, this is the new way to do it, and you can see that new.  



STEVE:  Do you have to click your heels three times?



LEO:  Well, so basically, if there's a macro, a VBA macro in there, this is the decision tree.  And if it's from a trusted location, if it's digitally signed and trusted publisher blah blah blah, it used to be that you could use Group Policy or Cloud Policy to block or unblock.  But now, if none of that's true, you get this final flow-through where in fact Office default macros blocked so show trust bar security risk with learn more.  This is what we were talking about. 



STEVE:  Okay.



LEO:  And then there will be a process.  So it looks like they're going to kind of bring that back.  But to make businesses happy there are a lot of situations...



STEVE:  Yeah, allow them to be signed.  I mean, that's going to...



LEO:  Yeah, signed, because if you open - it was a previously trusted document.  And you'll still have Group Policy that can default to block or unblock.  So I think this actually is Microsoft, they have to compromise all the time because business users, right?



STEVE:  Yeah.  It is so sad, though, that it is so difficult to turn up the security.



LEO:  Well, and now you know why.  Now you know why.  Every business says, well, yeah, but I don't want to retrain employees.  We need those macros.  We use them in our weekly spreadsheet flow.  And we don't want to have to have to do all that.



STEVE:  Wow.



LEO:  So I think that this is the process, and it's a good process ultimately, where all the stakeholders get to weigh in.



STEVE:  And then you just push it in the direction you want to incrementally.



LEO:  Yes.  Just like Google does; right?  And so you can see Microsoft's heart is in the right place.  They want to do this.  And they just kind of do it in a way that it doesn't upset people as much as it did, I guess.



STEVE:  Yeah.



LEO:  It should be, you know, for all Microsoft Home users it should be off.  But they're not going to do that, either.



STEVE:  Yeah, you're right.  That would absolutely.



LEO:  Yeah, yeah.  Anyway, so yeah, they rolled back the rollback.



STEVE:  Wow.



LEO:  That's what TechCrunch's headline is, is "Microsoft Reverses Its Reversal."



STEVE:  Wow.  Okay.  So Motherboard published an interesting story under the headline "This Is the Code the FBI Used to Wiretap the World."



LEO:  Oh, yeah, wasn't that interesting, yes.



STEVE:  Yeah.  And they followed that opening up with the subheading:  "Motherboard is publishing parts of the code for the ANOM encrypted messaging app, which was secretly managed by the FBI in order to monitor organized crime at global scale."



LEO:  And you nailed it, by the way.



STEVE:  I know.



LEO:  You figured it out.  You figured out how they did it.  That's pretty smart.



STEVE:  Yeah, well, and I actually talk about that here in a second.  But you're right, they did it the way I keep saying this is the way you would do it.  So what I thought was interesting is that the approach that Motherboard says the FBI took to pull this off was precisely the solution I have often hypothesized as being the obvious way in which an end-to-end multi-party messaging system would be compromised.  So here's how Motherboard's story begins.



They said:  "The FBI operation in which the agency intercepted messages from thousands of encrypted phones around the world was powered by" - what Motherboard described as, actually they had people describing as cobbled-together code.  Okay.  I disagree with the characterization.  They used open source code for an XMPP encrypted messaging system.  So it's like okay.  I guess you could describe open source as cobbled together.  It all kind of is.  But okay.  Anyway, they said:  "Motherboard has obtained that code and is now publishing sections of it that show how the FBI was able to create its honeypot.  The code shows that the messages were secretly duplicated and sent to a 'ghost' contact that was hidden from the users' contact lists.  This ghost user, in a way, was the FBI and its law enforcement partners, reading over the shoulder of organized criminals as they talked to each other."



Now, our listeners will recall that this has been my greatest criticism of any supposedly private and secure system where a user's keys are being in any way managed for them.  The reason that Threema's approach has always appealed to me is that the user is 100% responsible for their own key management.  And as we've often observed, if you're not managing your own keys, someone or something is managing them for you because the one thing any secure and private instant messaging system needs is keys.  The point being, key management must be occurring somewhere.  So if it's not something you're doing for yourself, then you don't have any direct control over what's going on.



Okay, now, that's not to say that I think that people should be doing their own key management.  When today's podcast is finished, I'll shoot an iMessage to my wife, Lorrie, and let her know that I'm heading home.  My point is, top-level state secrets are not being exchanged in my iMessages.  The fact is, when you get right down to it, no consumer smartphone can really be trusted absolutely.  But again, most people don't need that level of secrecy.



Anyway, Motherboard continues.  They wrote:  "Last year the FBI and its international partners announced Operation Trojan Shield" - I love the name - "in which the FBI secretly ran an encrypted phone company called ANOM for years and used it to hoover up tens of millions of messages from ANOM users.  ANOM was marketed to criminals and ended up in the hands of over 300 criminal syndicates" - you've got to love this - "worldwide.  The landmark operation has led to more than 1,000 arrests, including alleged top-tier drug traffickers and massive seizures of weapons, cash, narcotics, and luxury cars."



So, wow.  The FBI mounted a good old-fashioned high-tech sting operation.  Good going.  But Motherboard doesn't sound very impressed with the FBI's coders.  They wrote:  "Motherboard has obtained the underlying code of the ANOM app and is now publishing sections of it due to the public interest in understanding how law enforcement agencies are tackling the so-called "Going Dark" problem, where criminals use encryption to keep their communications out of the hands of the authorities."



Now, okay.  I'm unconvinced that there's any true public interest here, but okay.  Mostly Motherboard seems to want to embarrass the FBI over what they think is the low quality of the code.  They wrote:  "The code provides greater insight into the hurried nature of its development, the freely available online tools that ANOM's developers copied for their own purposes" - also known as open source - "and how the relevant section of code copied the messages as part of one of the largest law enforcement operations ever."



They said:  "The app uses XMPP to communicate, a long-established protocol for sending instant messages."  You know, Jabber uses XMPP.  "On top of that, ANOM wrapped messages in a layer of encryption.  XMPP works by having each contact use a handle," they wrote, "that in some way looks like an email address.  For ANOM, these included an XMPP account for the customer support channel that ANOM users could contact.  Another of these was 'bot.'"  Now, I do think it was a little inartful for them to name the secret account "bot," but okay.



Unlike the support channel, "bot" hid itself from ANOM users' contact lists and operated in the background, according to the code and to photos of active ANOM devices obtained by Motherboard.  In practice the app scrolled through the user's list of contacts, and when it came across the bot account, the app filtered that out and removed it from view.  So in that sense a little bit like having a rootkit.  That finding is corroborated by law enforcement files Motherboard obtained which say that bot was a hidden or "ghost" contact that made copies of ANOM users' messages.



Authorities have previously floated the idea of using a ghost contact to penetrate encrypted communications.  In a November 2018 piece published on Lawfare, Ian Levy and Crispin Robinson, two senior officials from UK intelligence agency GCHQ, wrote that "It's relatively easy for a service provider to silently add a law enforcement participant to a group..."



LEO:  Mmm.



STEVE:  Uh-huh.  I know, Leo, "...to a group chat or call."  And "You end up with everything still being encrypted end-to-end, but there's an extra 'end' on this particular communication."  Uh-huh.



LEO:  Wow.



STEVE:  Yeah.  The code also shows that in the section that handles sending messages, the app attached location information to any message that is sent to bot.  On top of that, the AndroidManifest.xml file in the app, which shows which permissions an app accesses, includes the permission for "ACCESS_FINE_LOCATION," as in fine-grained location.  This confirms what Motherboard previously reported after reviewing thousands of pages of police files in an ANOM-related investigation.  Many of the intercepted ANOM messages in those documents included the precise GPS location of the devices at the time the message was sent.  So, yeah, I mean, this is a golden honeypot operation.



Motherboard concluded their story by noting that Operation Trojan Shield had been widely successful, and that on top of the wave of thousands of arrests, authorities were also able to intervene using the intercepted and cloned messages to stop multiple planned murders.  Using a well-established open protocol and open-source software allowed the application to be assembled without excessive cost, and it got the job done.  And I just say, you know, well going.  I thought that was a very nice piece of work.  Wow.



Okay.  We have some closing-the-loop bits and our final discussion.  Let's take our last break, Leo. 



LEO:  Okay.



STEVE:  I'm a little parched.  And then we'll do that.



LEO:  Dealio.  Yeah, I mean, I guess now that it's known how to do this, law enforcement can do it all sorts of places.



STEVE:  Yeah, we've got some great listeners who tweet.  This is Mark Thoms.  He said:  "How should I vet client-side third-party EXEs I use as part of my web application?  How could I verify it's not malicious?"  He says:  "Example:  NeoDynamic.com JSPrintManager, a utility to allow silent printing from a website."



And, you know, my go-to is VirusTotal.  And we've talked about it before.  It is something like 70-plus different virus engines look at something.  And I use it all the time.  If I'm downloading something, like for example when I'm working on SpinRite, and I'm needing to do debugging on an ancient motherboard that has some strange LAN adapter, I need to get the motherboard on the 'Net.  I've got to find a device driver for the LAN adapter.  I go on the Internet.  And I'll, like, find it.  And, like, it looks like it's the right thing, but it's like, uh, kind of a sketchy site.



So I'll take it and drop the files on VirusTotal and have them give it a scan.  Oftentimes VirusTotal has seen the exact file that I'm dropping already because what it does is first thing it does is it makes a hash of the file to create a signature, and then it just looks at the signature.  And so it'll say, yup, already seen this one.  But you can ask it to rescan it just for your own peace of mind, if you want to.  Anyway, VirusTotal.com.  And it's a free service.  They're getting the benefit of lots of input from people who are wondering what this is.



And I know anybody who's listened to the podcast has heard me talk about it often because that's what security researchers use.  And they'll, in fact, if they find something, they're able to go back and look at the first time that somebody else submitted it to VirusTotal in order to get some sense for how long a piece of malware has been roaming around the Internet.  So anyway, a great solution for just checking out stuff that you're not sure about.  And free.  The price is right.



Isaiah tweeting from @boosted37, he said:  "@SGgrc You often recommend a separate IoT network from the main one.  However, devices like Chromecasts require your phone or tablet to be on the same network as the streaming stick to manage a show.  How would you recommend separating those from your main network?"



That's a good question.  I've had the problem myself.  I do have stuff, as I said, on a separate private guest network.  I'll just switch my WiFi from one to the other.  I don't normally - now, the Chromecast would be more difficult because you're wanting to be using it probably all the time.  And I guess I would say that it's a less sketchy device than some $5 plug that you got in a flea market or a garage sale somewhere, or freshly off of Amazon, for that matter.



So the solution I've used is that my various devices, my iPhone, my iPads, they've got - they know the passwords for both networks, and I'm able to switch back and forth between them with relative ease.  So in the case of a Chromecast you may want that to - you may trust it enough for it to run on your main WiFi, but not your less trustworthy devices, which you probably also don't need to access on a daily basis.  You know, you set the schedule for an outlet or a light switch, and then you let it go until daylight savings time changes. 



Tom Terrific, he said:  "You mentioned that you used an Asus router that you really like.  Which model is it?  It's time for me to buy a new router, and I always take your advice.  Thanks. Tom."



Okay.  So I haven't researched routers recently.  It happens that the one I'm using I still like, and that's an Asus RT-AC68U, which is now at v3, so it looks like it wasn't something that lived for just a short time.  I looked it up.  Amazon tells me that I purchased it from them in October of 2017.  So I know exactly when.  That's when I was setting up my life with the woman who became my wife, back in October of 2017, almost five years ago.  So it's an AC1900 WiFi gaming router, again, Model RT-AC68U.  Looks like it's highly rated on Amazon.  So anyway, that's what I'm using.  I don't know that it's the best.  There are cheaper routers.  I think it's like $119 at the moment for a Prime subscriber.  In fact, I think today is Prime Day.  Maybe it's even cheaper if you're a Prime person.  I don't know.  Anyway, that's that one.



And Michael Horowitz, who knows his way around routers, he was for years the columnist, I don't know if he still is, at Computer World.  He did his Defensive Computing column.  He tweeted, he said:  "About your Asus router."  He said:  "Are you sure the networks are isolated?"  Now, remember I mentioned that this thing I discovered had either three or four, I don't remember which, three or four guest SSIDs.  He said:  "You might find all the guest SSIDs share the same subnet."  He said:  "I no longer have an Asus router so can't verify."



And actually if he's thinking that they might share the same subnet, then I guess I could test that.  I was thinking that I'd have to actually try to send data across guest subnets to see if they're individually isolated from each other.  But I should mention also when I went over to Michael's Defensive Computing site, that pointed me to his Defensive Computing Checklist.  And Leo, this is the kind of thing I'll bet you would like.  It's defensivecomputingchecklist.com.  But I also made it this week's shortcut of the week so you can get to it more easily, or at least by typing fewer characters:  grc.sc/879.  Look how tiny the scroll thumb is.



LEO:  It's like yours.  I think I know why you like this.  This is pure HTML, baby.



STEVE:  It is just the facts, baby.  Just the facts.



LEO:  Ain't no JavaScript here.  No sir.



STEVE:  Just the facts.  But if you scroll you'll notice that the thumb is not moving down on the right because this page is...



LEO:  This long page.



STEVE:  ...so long.



LEO:  Long page.



STEVE:  What it is, is a page of advice.  And it is really good advice.  It's like just all kinds of stuff.  So I just think our listeners ought to know about it for their own purposes and also if they want to recommend it to people.  There's just lots of information there on this massive page:  grc.sc/879 or defensivecomputingchecklist.com.



LEO:  It ain't a checklist.  It's just a list.



STEVE:  It is, you're right.



LEO:  A long list.



STEVE:  It is a long list.  Just like Michael's collected ideas and thoughts about security over time.



LEO:  It should be a book.  This would actually - and I'll probably mention it on the radio show because there is so much good stuff in here.



STEVE:  It is really, really, really good stuff.  Also, when I went there, at the top of that page, but also his main page, there's a note that he'll be giving a presentation on Defensive Computing at the HOPE conference in New York City about 11 days from today.  It's actually called "A New HOPE," I'm sure probably...



LEO:  HOPE is Hackers On Planet Earth, just in case you didn't know that, yeah.



STEVE:  Exactly, Hackers On Planet Earth.  And wasn't "A New Hope"...



LEO:  Star Wars.



STEVE:  Yeah, yeah.



LEO:  The original Star Wars, yeah.



STEVE:  The original Star Wars movie.  



LEO:  Episode 3, "A New Hope."



STEVE:  "A New Hope."  And I'm sure that's what they're...



LEO:  Well, I think they had suspended for a while, so that's probably why.  It's "The New Hope."



STEVE:  I think they did because they also talk about, I went there in order to see what was up.  And they talk about this will be their first in-person gathering, no doubt since the COVID lockdown mess.  So anyway, it's a three-day conference.  It's not free, $200 for in-person all three, or streaming live for $99.



LEO:  This used to be a great hacker conference.



STEVE:  Yeah, it is a...



LEO:  Trying to remember if 2600 sponsored it or not.  I feel like it was related to 2600 at the time.



STEVE:  And of course that's the famous frequency that was used in order to disconnect the normal communications during a long-distance call and drop you down to the billing layer.



LEO:  Right.



STEVE:  Where you could do a so-called "phone phreaking" back in the day.  Anyway, finally, Simon Dearn.



LEO:  Episode 4, sorry, not 3, 4.



STEVE:  Was it 4? 



LEO:  Yeah, it's 4 because remember he started in the middle.  He did 4, 5, and 6.



STEVE:  And that's just always bugged the crap out of me.  It was like, what?  What happened to the first three?



LEO:  I think he was being clever because, I mean, you don't do that and say, oh, yeah, I'll be definitely making nine of these.  I think he was just being clever.  But then it turned out...



STEVE:  Oh, my god.  And then we had to put up with the first three, which were...



LEO:  Yes, the prequels were not good, yeah.



STEVE:  No.  So anyway, Simon Dearn tweeted:  "There are obviously good reasons for a company to use a VPN for allowing staff to connect remotely.  But what are your thoughts on domestic use?  Do you use a VPN yourself?"



And the answer, Simon, is I don't.  But that's only because during my daily current life I'm never needing to get online using OPNs.  "OPNs" is a handy abbreviation everyone should keep in mind.  It stands for "Other People's Networks."  And Leo, I grinned at the beginning of the show because you were talking about being on shipboard and how you would definitely be using a VPN.  If my life involved travel, so that I was wanting to be online in airports, in coffee houses, and hotels, there's no question that a VPN would be the only way I would feel comfortable getting online when I was not at home.



This is much less of a problem today, where everything is encrypted over TLS with server-authenticating certificates.  Back at the start of this podcast, everything was being done over HTTP with a brief switch to TLS only when logging in, for login forms and credentials were being exchanged.  Remember when we used to explain that it was important to verify that the URL of a form's submit button was HTTPS and not just HTTP?  Fortunately, we survived that.



LEO:  And Firesheep and all sorts of other horrible things.



STEVE:  Oh, my god, yes.  But it's still true that without universal certificate pinning, which I really don't see ever happening, or DNSSEC being used to publish certificates, which we're still a long way from, there's a vulnerability when a malicious man-in-the-middle could have control over our traffic.  It's uncommon, I'm sure, but it's probably still possible for state-level actors to mint their own certificates that our browsers and operating systems will trust without question.  It's true that the bad guys could be operating at the other end of a VPN endpoint.  Although Leo, your comment about ExpressVPN changing IPs all the time, that's also going to tend to make the VPN endpoint more diffuse.



But something about hotel WiFi networking in particular gives me the creeps.  For a long time it was unencrypted, and there's just no way to use an unencrypted hotel WiFi safely.  And I suppose if I was downloading lots of torrent content I might want to hide that fact from my Internet provider.  But I've found torrents to be more trouble than they're worth.  So I don't care whether Cox knows what I do on the Internet.  And if anyone was watching my Internet use mostly it would just put them to sleep.  So not a problem for me.  But again, I don't use a VPN during my normal daily life because I'm not traveling.  If I were a traveler, then yeah.  VPN, no question.



LEO:  Also I'm thinking you're not watching a lot of Manga and anime stuff on Netflix Japan, that kind of thing.



STEVE:  No, I'm not.  I'm not needing to appear to be other than in Southern California.  Good point.



Okay.  So the Rolling Pwn.  And that begs the question, do you own a Honda?  And are you sure it's still parked out in front?  Enterprising security researchers at Star-V Lab - maybe it's Star 5 Lab - one using the handle Kevin2600, and of course we know where he got 2600 - and the other named Wesley Li, have revealed, as Larry David might say, "a pretty, pretty serious" vulnerability in Honda's automobile keyless entry systems.  It rather trivially allows attackers to remotely unlock and start potentially all Honda vehicles currently existing on the market.  It's really, like, sobering.



Okay.  So it works in a way that's similar to, but even easier to pull off, than that recently discovered Bluetooth replay attack which we talked about not that long ago affecting some Tesla cars.  Both use readily available off-the-shelf SDR (software defined radio) equipment, which allowed the researchers to eavesdrop and to capture codes, then broadcast them back to the car in order to gain access.  



Recall that we talked about this related attack, the one I mentioned, that affected Teslas.  And it was very clever.  The key fob produced a high-quality cryptographically unpredictable pseudorandom sequence of codes which are derived from a monotonically increasing counter which is then encrypted by an unknown shared secret key.  So the vehicle knows the key, so it's able to decrypt the received code to determine whether it's the next code in sequence or at least forward of the key fob counter's last known position.  But the primary security feature is that the vehicle will NEVER accept any code that it has seen previously or that represents a count behind the most recently seen count that the key fob is known to have.  In other words, it will only allow its knowledge of the key fob's counter to be moved forward, never backward.  Okay.  So this would seem to robustly prevent any possibility of a replay attack.  But not so in the case we talked about before.



What the clever hackers did before was to receive the fob's code while simultaneously arranging to jam its receipt by the car.  The user would think, after pressing the button and nothing happened because the car's reception was jammed, they would think, well, okay, whatever, I guess I was too far away, or who knows.  So they would press the key fob a second time, causing it to generate the next successive code in sequence.  Now the hacker would capture the second code while replaying the first code which the car had never received, and the door would unlock, or the car would start, or whatever.  And that's the cool part that's so clever.  The attacker would have obtained and retained that second code which had never yet been seen by the car, and would still be available as the next unlocking code in that otherwise unpredictable sequence.



Okay.  So that was the clever, very active, got to get involved, very much man in the middle.  You're an active man in the middle, able to jam, on the fly in real-time jam the receipt of the code by the car.



Today Honda is the target, and their problem is significantly worse since the attack on Hondas is almost completely passive and much easier to conduct.  The Honda-attacking researchers have been able to remotely unlock and start the engines of Hondas dating from as far back as 2012, so a decade ago, up to and including this year.  So nothing's been fixed in the last 10 years.  The good news is, according to "The Drive," which independently tested and verified the vulnerability on a last year's 2021 Honda Accord, the key fob attack does work, allows the car door to be unlocked, and the engine to be started, but the attacker is unable to drive off with the vehicle, though it, as I said, can start the engine.  Presumably the car contains some dynamic continuous key fob presence technology that's able to sense the presence of the key inside the vehicle, which at least prevents it from being drivable.



Okay.  So the more active attack against Teslas worked the way I just described.  What Kevin and Wesley discovered was, as I said, significantly worse because it is so much easier.  They found that the counter in Honda's cars would be resynchronized when the car receives lock and unlock commands in consecutive sequence  even when that sequence is from long ago.  This means that Honda's counter can be reset to an earlier state.  So by being induced into moving backwards, this will cause the car to accept codes from previous sessions that should have been forever invalidated after they were used.



In their write-up, Kevin and Wesley said:  "By sending the commands in a consecutive sequence to the Honda vehicles, it will be resynchronizing the counter.  Once the counter is resynced, commands from previous cycles of the counter will work again.  Therefore, those commands can be used later to unlock the car at will."  They wrote:  "We've successfully tested the 10 most popular models of Honda vehicles from the year 2012 up to the year 2022 from the attacker's perspective.  Therefore, we strongly believe the vulnerability affects all Honda vehicles currently existing on the market.  Tested and known-vulnerable vehicles include the Honda Civic 2012, the Honda X-RV 2018, the Honda C-RV 2020, the Honda Accord 2020, the Odyssey 2020, the Inspire 2021, the Fit 2022, the Civic 2022, the VE-1 2022, and the Honda Breeze 2022."  So, bad.



Then we have the always-entertaining FAQ, from which I will excerpt a couple questions.  They asked:  "Why it is called the Rolling-Pwn, not a Honda-Pwn?"  And they answered:  "Because this bug may exist in other brands of vehicles, too."  "Am I affected by the bug?  As long as a vulnerable Honda vehicle is in use, it can be abused."  "Is there an assigned CVE for Rolling-Pwn?"  Yup.  "2021-46145 is the official reference to this bug."  "Can I detect if someone has exploited this against me?"  And they wrote:  "Probably not.  The exploitation does not leave any traces in traditional log files.  But considering the ease of exploitation and attacks leaving no trace, this threat should be taken seriously."



"Is this a Honda-vehicle-only bug?  No, although the main targets for the research is Honda automobiles.  We have leads to show the impact of this vulnerability also applies to other car manufacturers."  They said:  "We will release more details in the future."  "Is the risk real?  We've successfully tested the latest models of Honda vehicles, and we strongly believe the vulnerability affects all Honda vehicles currently existing on the market."  "What makes this bug unique, or what's the difference between CVE-2022-27254 and CVE-2019-20626?"  In other words, vehicle hacking has a rich history.



They wrote:  "During the research, we noted the other researchers have found similar vulnerabilities in Honda vehicles, based on the description of 'The remote keyless system on Honda HR-V 2017 vehicles sends the same RF signal to each door-open request, which might allow a replay attack.'"  Uh-huh.  "What they found is a simpler fixed code vulnerability where an attacker can simply record the transmission in advance and replay it later to cause the door to lock or unlock.  However, most modern vehicles including Honda automobiles implemented a proprietary rolling code mechanism to prevent fixed code replay attacks.  The bug we discovered," they wrote, "is in regard to the design flaw in the rolling codes mechanism implemented by Honda Motors, which needs to be taken very seriously."



Question:  "Is there more technical information about Rolling-Pwn?"  They said:  "You can follow the author on Twitter @kevin2600.  However, we will not be releasing any tools required to go out and steal the affected vehicles.  At a later stage we will release technical information in order to encourage more researchers to get involved in car security research."  Which, boy, really does seem to be lacking.



"How to patch the modern automobile for Rolling-Pwn bug like this?  The common solution requires us to bring the vehicle back to a local dealership as a recall.  But the recommended mitigation strategy is to upgrade the vulnerable BCM firmware through over-the-air updates if possible.  However, some older vehicles may not support over-the-air."  And I should mention Honda offers no patching for this and indicates they have no plans to.



"What does Honda think about this Rolling-Pwn Bug?"  They said:  "We have searched through the Honda official website, but we can find no contact info to report the vulnerability.  It appears that Honda Motors does not have a department to deal with security-related issues for their products.  And a person who works at Honda has told us 'The best way to report the Honda vulnerability is to contact customer service.'"



LEO:  Yeah, sure.  That'll work.



STEVE:  That'll work.  Yeah.  Is your seatbelt tight?  Are the brake lights working?



LEO:  Unbelievable.



STEVE:  "Therefore," they said, "we filed a report to Honda customer service, and we have not had any reply."  No kidding.  



LEO:  Oh, my god.  That's horrible.



STEVE:  Earlier in March of this year, following similar remote keyless entry attacks on Honda Civics, BleepingComputer reached out to Honda and learned that Honda had no plans to update any of their older model cars.  BleepingComputer wrote:  "Honda told us multiple automakers" - in other words, it's not just us, it's not just us - "use legacy technology for implementing remote lock-unlock functionality, and as such may be vulnerable to 'determined and very technologically sophisticated thieves."  And of course that's until Amazon starts selling the Honda Unlocker for 19.95 from China. 



They said:  "At this time, it appears that the devices only appear to work with close proximity or while physically attached to the target vehicle" - not so in this case - "requiring local reception of radio signals from the vehicle owner's key fob when the vehicle is opened and started nearby" - okay, that's true - "a Honda spokesperson told BleepingComputer.  In their statement, Honda explicitly mentions it has not verified the information reported by the researchers" - although others have - "and cannot confirm if Honda vehicles are actually vulnerable to this type of attack."  And one must imagine that Honda doesn't want to know, since knowing might make them culpable.



And Honda did add that, should the vehicles be vulnerable, "Honda has no plan to update older vehicles at this time.  And it's important to note, while Honda regularly improves security features as new models are introduced, determined and technologically sophisticated thieves are also working to overcome those features."  In other words, we give up.



So all this begs the question, why does this appear to be such a difficult problem to solve?  Both the Tesla-style forward-only counter advance and Honda's bi-directional resettable counter solutions are transparent to their users.  In other words, the keys just work.  Right?  But Tesla's forward-only system is clearly superior from a security perspective.  On a Honda, the ability to passively record a series of unlocking codes, which can then be replayed at any time later, seems like a significant oversight that any engineer who was designing this system would have understood.  Of course they would.



One thought is that there are likely some intellectual property issues here.  There's no question that the first implementers of such rolling codes would have sought patents.  That thought led to me ask the Google, where I immediately found U.S. patent 6225889 titled:  "Method of producing rolling code and keyless entry apparatus using the same."



The abstract of the patent starts off reading:  "A rolling code producing method is provided which may be employed in a keyless entry system for automotive vehicles designed to compare a rolling code derived in a receiver installed in the vehicle and a rolling code derived in a portable transmitter to allow authorized access to the vehicle if both rolling codes match.  The rolling code producing method includes producing different rolling codes" - thus rolling - "in sequence, using an initial code variable according to a given algorithm and changing the initial code variable in response to insertion of an initial code variable memory card carried by a vehicle operator into the receiver."



Okay, now, the good news is this patent was issued in 1995, and it expired in 2016.  Around the same time, also in 1995, garage door openers were suffering from the same lack of security.  So Brad Farris and James Fitzgibbon "invented," and I have that in quotes, a similar system for their garage door opener employer The Chamberlain Group, and obtained U.S. patent US44688695.



There's been a lot of litigation over these patents through the years, and there's a long trail of bodies.  But that's what the patent system does; right?  It mostly amounts to a significant source of revenue for intellectual property attorneys.  But all of these various patents appear to have finally expired back in 2016.  So it's unclear why Honda would still be using their broken system today.  They apparently were back in 2012, maybe to avoid any litigation or having to license somebody else's patent.  It was Nippon that got the auto patent back in 1995.  But it appears that for at least the past six years there's been no reason not to move to a much stronger forward-only counter scheme such as what Tesla and presumably others have implemented.



Overall, this truly is, if not a difficult, at least a little more expensive problem to solve.  It is difficult to robustly solve it in a one-way-only system.  The ultimate way to solve the problem is for there to be a full handshake.  The user presses the button on the key fob, which sends a fixed-code ping out into the world, identifying itself to any car within range.  The car that key is registered to receives the "Hello it's me" ping and replies with a nonce challenge.  The key fob receives the nonce challenge and either reversibly encrypts it under its shared secret key, or hashes it with an HMAC keyed with a shared secret.  Either way, it returns the result of that cryptographic operation to the car, which verifies that the key fob must know the secret they share, so the car performs the requested action.



That system, while ultimately secure and Internet-proven, is significantly more expensive, since now both the key fob and the car must support bi-directional communications.  The key fob must also be able to receive, and the car must also be able to transmit.  Given the cost and the complexity of this full solution, and the comparatively small additional security margin provided above the forward-only counter advancement used by Tesla and presumably other forward-thinking automakers, I would say that the small added security is probably not worth the cost.



But given that forward-only counter technology has been freely available in the public domain, unencumbered by any patent licensing requirements since at least 2016, Honda's continued use of resettable counter protection since then can only be ascribed as them just not caring.  Given the popularity of Hondas, and who knows what other car makers may also have been similarly lazy, the relative ease of collecting key fob codes and the ability to later replay them in an entirely passive attack likely opens Honda to some future consumer litigation, I would think.  So we'll see what more we hear of this in the future.  Fun stuff.



LEO:  What - so does that mean if you have a Honda you should keep your fob in a bag, one of the mesh bags?



STEVE:  No, no.  That won't solve the problem.  



LEO:  Oy.



STEVE:  It really means that...



LEO:  You're screwed.  You need a padlock on your Honda's door.



STEVE:  It does mean you're screwed.  It means, I mean, essentially it means that, I mean, this completely defeats the replay.  The fact is this is inexpensive hardware to do this now.  It's just, you know, a little SDR that you can buy on Amazon, a Software Defined Radio.  Borrow a friend's Honda fob, see how it works, capture the codes, and then you can capture the codes from anybody else.



LEO:  Wow.



STEVE:  And now we know that basically this renders the forward-only protection completely vulnerable.



LEO:  For a long time Honda Accords were the most stolen cars in America.  I think they're going to reclaim their crown.



STEVE:  Honda anythings, apparently.



LEO:  Oh, wow.



STEVE:  So it may well be that you can't drive off with the car, but you certainly don't want to leave anything important.  You don't to trust your car door locks, essentially, is what it means.



LEO:  Right, right, right.  The number one car in America stolen, Honda Civic.  Number two, Honda Accord.



STEVE:  Wow.



LEO:  And I don't know if that's related to this.  Probably not.  Just to their desirability in the stolen car market.  But still.



STEVE:  And their, yeah, their generic nature.



LEO:  Their generic-ness, yeah, because the Camry's third, the Nissan Altima's fourth.  Man, if I'm going to steal a car, I'm going to steal a Hummer.  I'm going to steal a Jaguar, a Porsche.



STEVE:  Going to have some fun.



LEO:  Get something good, yeah.



STEVE:  And arrested and taken away.



LEO:  Yeah.  Actually, maybe that's old because now I'm looking - no, that says May 13th.  Now I'm seeing number one is a Ford pickup.  Also popular.  You know, I have on my Ford, I hope it's a good technique, but I have on my Ford, and many Fords do, have a keypad on the thing.  And unfortunately it's only four digits.  No, I guess it's more than that.



STEVE:  The ones I've seen, they use all 10 digits, but they double them up on five keys.



LEO:  That's the problem.  There's only five keys. 



STEVE:  So it's like, what the hell?



LEO:  And I think this is related to what Honda is doing because it's basically cheapness; right?  These are the cheapest possible components.



STEVE:  Yeah.



LEO:  Five keys is half the price of 10.



STEVE:  It would be nice to know, I mean, because everybody now has key fob unlock; right?  I mean, like, you know.  I mean, even I...



LEO:  And car start.



STEVE:  Leo, even I have key - I do have to still - I have to stick it in a slot and twist it, yeah.  I've got to do that.



LEO:  Oh, that's very old-fashioned, yeah.



STEVE:  But at least I've got the fob on that.  And, wow, interesting.



LEO:  What a world.  All right, Steve.  Great show, as always.



STEVE:  We're going to miss you next week.



LEO:  I won't be here.  Jason Howell will take over for the day.  I will be back the following Tuesday.  I'm going on that TWiT cruise with 120 devoted fans who I am sure to a person will say "Tell Steve hi."  No matter where I go, that's what I get.  "Tell Steve hi."  So hi.



Steve Gibson is at GRC.com.  You can tell him hi yourself.  Just go to GRC.com/feedback.  While you're there, pick up a copy of SpinRite, the world's best hard drive, I'm sorry, mass storage maintenance and recovery utility, soon to be 6.1, version 6 the current version.  You'll get a free upgrade to 6.1 if you buy 6 today.  You also get to participate in the development of 6.1, which is imminent.



While you're there, you can also get a copy of the show.  Steve has two unique versions of Security Now!, a 16Kb version for the bandwidth-impaired, and really nicely done, human-written transcripts so you can read along as you listen, but you could also use it to search and find a part of the show that you're looking for.  GRC.com, the place to go.  He's on the Twitter, @SGgrc.  You can DM him there, as well, @SGgrc.



We've got copies of the show at our website, TWiT.tv/sn.  We also have a YouTube channel devoted to Security Now!, makes it easy to share.  You can send people clips.  And of course you can subscribe on your favorite podcast client and get it the minute it's available of a Tuesday afternoon.



We do the show every Tuesday, right after MacBreak Weekly.  That's 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Live audio and video streams are at live.twit.tv.  Chatroom is at irc.twit.tv  That's open to all.  And after the fact Steve's got forums at GRC.com.  We've got forums at TWiT.community.  Those are free and open to all, TWiT.community.  There's also TWiT.social, which is a Mastodon instance.  



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#880

DATE:		July 19, 2022

TITLE:		RetBleed

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-880.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we start with a quick update on last week's Rolling Pwn problem.  Then we look at the state of IPv4 space depletion and the rising price of an IPv4 address.  We have an interesting report on the Internet's failed promise, Facebook's response to URL-tracker trimming, Apple's record-breaking Lockdown Mode bounty, Clearview AI's new headwinds, a new feature being offered by ransomware gangs, the return of Roskomnadzor, last Tuesday's patches, and some feedback from our listeners.  Then we look at the details of the latest way of exfiltrating secrets from operating system kernels thanks to insecurities in Intel and AMD micro-architecture implementations.  Yes, some additional bleeding.



SHOW TEASE:  Coming up on Security Now!, it's me, Jason Howell, sitting in for Leo Laporte, who is on a cruise at the moment.  So you won't see Leo this week.  You will, though, see the man of the hour that is Steve Gibson, talking this week about Facebook encrypting its link URLs, incentives for cracking iOS Lockdown Mode, actually some pretty big incentives, so why don't you see what you can do.  You make some money in the process.  Clearview AI and how it's meeting total resistance around the world for the most part.  And the bleeding continues as Steve dives deep into RetBleed.  All that and more coming up next, Steve Gibson explaining it all on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 880, recorded Tuesday, July 19th, 2022:  RetBleed.



It's time for Security Now! with Steve Gibson, the man of the hour.  I'm Jason Howell filling in for Leo Laporte, who is on a cruise ship right now.  Who knows what Leo's up to?  But I'm here with Steve to hang out once again and talk security.  How are you doing, Steve?



STEVE GIBSON:  Let's hope he's not getting COVID.  That would be a good thing.	



JASON:  Yeah.  That's kind of top of the list of what we hope isn't happening.  But let's hope he is having a good time.



STEVE:  Having a good time.



JASON:  Meeting lots of awesome people.  Eating good food.



STEVE:  But not too much.



JASON:  We'll focus on the positive.



STEVE:  So we're at Episode 880 for here the middle of July.  And Jason, I don't know what it is, but whenever you're on the podcast, we're talking about bleeding.  And, you know, three weeks ago when Leo was off on the East Coast somewhere we had Hertzbleed.  And now you're back and we've got RetBleed, which is...



JASON:  Surprise.



STEVE:  ...subject and topic of the podcast.  So another...



JASON:  I bring the bleed.  I don't know what to tell you.



STEVE:  Yeah, you bring the bleed.  Another interesting side-channel attack on Intel and AMD processors.  And this one has an interesting back story because Intel was sort of telling people that the way you're fixing this isn't really good enough.  But then they decided, well, okay, but it does help performance, and we care about that.  So we're just not going to say anything.  Anyway, we're going to get to all that.  But first we're going to briefly look back at last week's Rolling Pwn problem.  Then we're going to look at the state of IPv4 IP space depletion and the rising price of an IPv4 address.  We have an interesting report on the Internet's failed promise, which I thought was really sort of sad, but it's good some people are acknowledging that, well, it didn't really work out the way we hoped.



We also have Facebook's unsurprising response to URL tracker trimming, which was a subject last week.  It didn't take this very long to drop.  And they clearly had it in the works because it would have been complex for them to do, but they did.  We've got Apple's record-breaking Lockdown Mode bounty.  Clearview AI's new headwinds that they're facing.  A new feature being offered by ransomware groups, three of them so far, but it's going to catch on.  We've got the return of Roskomnadzor.  Also last Tuesday's patches and some feedback from our listeners.  Then, as I said, we're going to take a look at the details of the latest way of exfiltrating secrets from operating system kernels, thanks to what amount to insecurities in Intel and AMD's microarchitecture implementations.  So yes, Jason, you're here, so we're going to talk about bleeding.



JASON:  Another fine podcast with bleeding somehow making its way in.  I don't know.  I didn't do this on purpose, Steve.  It's just the way the cookie crumbles sometimes.  And we thank Steve in advance for the Picture of the Week, which is coming up right now.  What you got?



STEVE:  So, okay, this is another one of those, it's sort of a variation on the theme of the path out in the middle of a large field that has a gate, like, across the path.  And in the case of the path, because I love it, because there's also then, like, well-worn side paths, like just going around this gate.  It's like, what?  What?  You know, what?



Anyway, so this one, though, is not that.  This is somebody who locked up, and I use that term advisedly, yeah, they locked up their very expensive and nice-looking e-bike to a post.  But, you know, it's a cylinder.  And there's nothing on the top of the cylinder to prevent the person from just lifting the whole thing up in the air, off the cylinder.  Like, I mean, so it's got the expensive bolt cutter-proof everything cord, cable cord thing, wrapped around the cylinder.  Not tightly, either.



JASON:  No, very loosely, actually, it looks like.



STEVE:  Yeah, they're very casual about this.  So, okay, good luck.  This has the advantage, unlike packets in the ether, that a thief would have to be very visible while they were lifting this bike up off of the cylinder that it's not really locked to.  But anyway, the caption I put on this was "Hmm..." because, you know?



JASON:  Yeah, this doesn't take too long to crack the code.  John actually just whispered in my ear and points out that the weight of the battery, at least that's a little bit of possibly a preventative measure for lifting it up.  You'd have to be a weightlifter to maybe lift that up compared to other people.  Perhaps.



STEVE:  Yes, it did occur to me that were this like a motorcycle, which no human could lift - well, maybe Schwarzenegger.  But, you know...



JASON:  This would probably lift over pretty easily, though.



STEVE:  Yeah, yeah.  And also for what it's worth the helmet is not secured.  It's just hung with its strap on one of the handlebars.  So, I mean, maybe the person - you could say maybe the person isn't far away.  In which case, why lock it at all?



JASON:  Yeah, yeah.



STEVE:  I don't understand the story here.  Maybe they don't like their helmet?



JASON:  Yeah, in general I think this person just has a cavalier attitude around security.  It's kind of like, yeah, I mean, you know, it's almost like - it's like putting the sign in your front door that says we are protected by this security system when you actually aren't.  It's like, you know, that's going to prevent some people from doing something.  Like maybe some thief is going to look at this and be like, well, there is a lock there, and that's more trouble than another thing, so I'm just going to move on to another thing.  But there's always someone out there that thinks it's worth the trouble.  I looked at this, and I was like, you know, I've almost done this a few times, but I never actually followed through.  It was like, well, I've got to put it to something.  Should I?  No, it really doesn't do anything.



STEVE:  You do wonder what this person's password is, though, don't you.



JASON:  Yeah, probably pretty easy.



STEVE:  And they only have one, and they use it everywhere, and it's probably not that difficult to figure out.



JASON:  It's "ebike."  Ebike.  They use ebike everywhere. 



STEVE:  That's right.  Or "ilostmybike" is their password.  That's right.



JASON:  Yeah, right, someonestolemybike, someonetookmybike.



STEVE:  Okay.  So following up to last week's Honda-centric story, where the Honda engineers made the mistake of allowing their system, which resynchronizes their autos to their keyless remotes, by allowing them to move back to a previous state, which should never happen.  Resyncing would have been fine if the resync was only allowed to move forward to a later state, which is actually all that should ever be necessary anyway.  There's no safe way to allow an earlier state to be restored.



Anyway, last week when we covered this, the spokesperson for Honda told The Record, who was doing some follow-up reporting - and I love this, too.  It hadn't really occurred to me.  They said:  "Hackers would need sophisticated tools and technical know-how to mimic remote keyless commands and gain access to certain vehicles of ours."  Okay, well, it didn't hit me until just now as I was writing this that that statement makes no sense.  If hackers did not have sophisticated tools and technical know-how to mimic remote keyless commands in the first place, then no rolling codes of any sort would need to be used at all.  It's specifically because hackers do have sophisticated tools and technical know-how to mimic remote keyless commands that it's necessary in the first place to design a system with rolling codes, which Honda has failed to securely do, for the purpose of defeating hackers who had sophisticated tools and technical know-how to mimic remote keyless commands.



But in any event, that's not why we're back here this week.  In addition, there was a dialogue which was spurred by last week's revelations.  Honda said:  "Honda regularly improves security features as new models are introduced that would thwart this and similar approaches."  And then the spokesman added that all, and they said "completely redesigned," and I'm not sure what that means, completely redesigned 2022 and 2023 model year vehicles have an "improved system" that addresses the issue.  Then they said:  "Currently this includes 2022 Civic, 2022 MDX, and 2023 HR-V," saying, "Our newer system transmits codes that immediately expire, which would prevent this type of attack from being successful."



Okay, now, I think there seems to be some miscommunication somewhere because what's confusing is that the original hacking team used their system to crack 10 Hondas, with four of them being year model 2022, and one of those four being the Honda Civic, which this spokesperson claims has fixed the problem by using advanced technology.  Like it's been fixed.  But, you know, also note that all rolling codes immediately expire.  That's the whole point of having them roll.  They're inherently meant to be single-use codes that somebody can't capture and immediately repeat.



So the good news behind all of this is that hacking cars is fun, and doing so is an easy means to generate headlines, which really is the only payoff that most researchers seek or receive; right?  I mean, they just - I often wonder, why did they spend so much energy doing this?  But it's apparently for a little bit of your moment in the sun, and then onto the next hack.  Since the hardware required to do this car hacking is now available inexpensively off the shelf, just put SDR or maybe "software defined radio" into Amazon's search and you'll get some.



So we can be pretty sure that automakers' past laziness with regard to their autos' true security will no longer go unnoticed and will be making future headlines whenever and wherever it is found to be lacking.  And that as a consequence of seeing that happen a few more times maybe, they'll actually, like, figure out their communication, if nothing else, because Honda is still way messed up in that regard.



Okay.  We've had a lot of fun through the years watching the saga of diminishing IPv4 address space.  According to SIDN, which is the Netherlands' official domain registrar, IPv4 space address price, that is, the price of individual IPv4 addresses, has doubled in the past year.  Back in 2015, so seven years ago, IPv4 space was selling around $5 per IP.  This time last year that $5 had grown to between 25 and $35.  That's last year.  Today, a year later obviously, we're at $50 to $60 per IPv4 space.



Contrast this, of course, to IPv6 where there's essentially no practical limit, no practical limit to address availability.  IPv6 addresses are not only free, but they are so freely available that ISPs hand out large chunks of IPv6 space to each of their residential subscribers.  In fact, routing tables will not route individual IPv6 addresses.  They won't.  They can't.  By definition there are too many of them.  So they are only allocatable in big blocks.  And so that's the way they're being allocated.



And I guess my point is the idea that here we have IPv6, and it is here.  It's been here for quite a while.  It's been defined for decades.  Nobody wants it.  They're willing to pay $60 for an IPv4 address rather than use free ones because, well, you know, those are weird.  So it's probably difficult to find a better example of an entrenched unwillingness to change, to adapt, than for IPv4 space to be selling at this kind of premium when you can have all the 6's, the v6's that you want for free.



And yes, everyone, I know, ShieldsUP! doesn't do IPv6 yet.  It's not that I'm unwilling to, it's I would love to.  But I was distracted by SQRL for seven years, and now I'm back to SpinRite where I should be.  Everyone agrees that's more important than having ShieldsUP! be IPv6 compatible.  Once SpinRite has caught up to all the hardware platforms it needs to run on, that'll be, absolutely, that and the DNS Benchmark, everybody wants that to be IPv6, too.  So yes, I've got to get my own house in order.  I understand that.  Everything's working except SpinRite.  So that's the top priority.



Okay.  I love this next piece.  I titled it - or did they title it?  Somebody titled it.  Oh, yeah, they did:  "Confronting Reality in Cyberspace:  Foreign Policy for a Fragmented Internet."  That's the official title of this huge 116-page report where apparently they were being paid by the page.  A pull-quote from the article headlines the Executive Summary.  It says:  "The utopian vision of an open, reliable, and secure global network has not been achieved and is unlikely ever to be realized.  Today the Internet is less free, more fragmented, and less secure."



Okay, now, I'm not going to drag us obviously through 116-page report.  And although the report is obviously U.S.-centric, having been assembled by a U.S. think tank - who is it?  Oh, yeah, the Council on Foreign Relations put this together.  I think that everyone will find this interesting.  I mean, if not a little sad and sobering.  So here's just the Executive Summary from the report, which sums it up.



They wrote:  "The global Internet - a vast matrix of telecommunications, fiber optics, and satellite networks - is in large part a creation of the United States.  The technologies that underpin the Internet grew out of federal research projects, and U.S. companies innovated, commercialized, and globalized the technology.  The Internet's basic structure - a reliance on the private sector and technical community, relatively light regulatory oversight, and the protection of speech and the promotion of free flow of information - reflected American values.  Moreover, U.S. strategic, economic, political, and foreign policy interests were served by the global open Internet.  Washington long believed that its vision of the Internet would ultimately prevail, and that other countries would be forced to adjust to or miss out on the benefits of a global and open Internet.



"The United States now confronts a starkly different reality.  The utopian vision" - and here is the pull quote I said - "of an open, reliable, secure global network has not been achieved and is unlikely ever to be realized.  Today, the Internet is less free, more fragmented, and less secure.  Countries around the world now exert a greater degree of control over the Internet, localizing data, blocking and moderating content, and launching political influence campaigns.  Nation-states conduct massive cyber campaigns, and the number of disruptive attacks is growing.  Adversaries are making it more difficult for the United States to operate in cyberspace.  Parts of the Internet are dark marketplaces for vandalism, crime, theft, and extortion.



"Malicious actors have exploited social media platforms, spread disinformation and misinformation, incited disparate forms of political participation that can sway elections, engendered fierce violence, and promoted toxic forms of civic division.  At the same time, the modern Internet remains a backbone for civilian critical infrastructure around the world.  It is the main artery of global digital trade.  It has broken barriers for sharing information, supports grassroots organization and marginalized communities, and can still act as a means of dissent under repressive government regimes.



"As the Internet of Things (IoT) expands in coming years" - god help us - "the next iteration of the Internet will connect tens of billions of devices, digitally binding every aspect of day-to-day life, from heart monitors and refrigerators to traffic lights and agricultural methane emissions.  The United States, however, cannot capture the gains of future innovation by continuing to pursue failed policies based on an unrealistic and dated vision of the Internet.  The United States needs a new strategy that responds to what is now a fragmented and dangerous Internet.  The task force believes it is time for a new foreign policy for cyberspace."



The major findings of the task force, which are then basically documented and substantiated by the remaining 115 pages, are - and we have a number of bullet points:  "The era of the global Internet is over.  U.S. policies promoting an open, global Internet have failed, and Washington will be unable to stop or reverse the trend toward fragmentation.  Data is a source of geopolitical power and competition and is seen as central to economic and national security.  The United States has taken itself out of the game on digital trade, and the continued failure to adopt comprehensive privacy and data protection rules at home undercuts Washington's ability to lead abroad.  Increased digitization increases vulnerability, given that nearly every aspect of business and statecraft is exposed to disruption, theft, or manipulation.



"Most cyberattacks that violate sovereignty remain below the threshold for the use of force or armed attack.  These breaches are generally used for espionage, political advantage, and international statecraft, with the most damaging attacks undermining trust and confidence in social, political, and economic institutions.  Cybercrime is a national security risk; and ransomware attacks on hospitals, schools, businesses, and local governments should be seen as such.



"The United States can no longer treat cyber and information operations as two separate domains.  Artificial intelligence and other new technologies will increase strategic instability.  The United States has failed to impose sufficient costs on attackers.  Norms are more useful in binding friends together than in constraining adversaries.  And indictments and sanctions have been ineffective in stopping state-backed hackers."



So they conclude:  "The task force proposes three pillars to a foreign policy that should guide Washington's adaptation to today's more complex, variegated, and dangerous cyber realm.  First" - I'm going to turn down my email notifications.  "First, Washington should confront reality and consolidate a coalition of allies and friends around a vision of the Internet that preserves  to the greatest degree possible  a trusted, protected international communication platform.



"Second, the United States should balance more targeted diplomatic and economic pressure on adversaries, as well as more disruptive cyber operations, with clear statements about self-imposed restraint on specific types of targets agreed to among U.S. allies.  And third, the United States needs to put its own proverbial house in order.  That requirement calls for Washington to link more cohesively its policy for digital competition with the broader enterprise of national security strategy."



So this is obviously what this podcast has been talking about for the last 17 years.  We've been watching this happen.  And I would argue that the Internet happened, and it wasn't very pervasive; right?  I mean, it wasn't mission-critical.  It was an interesting global communications platform.  But it was as it inevitably became what it has become and that we've all watched over the last couple decades, its nature changed.  It became important.  It became something you couldn't do without.  Communication, more and more communications moved to it.  And it got to a point where control over it became something that everybody wanted.



It was supposed to be free and open and utopian, and everybody gets to talk to everybody, and countries that try to restrict it are going to crumble because you can't restrict it.  Well, turns out you can pull the plug.  And, whoops, no more Internet.  We've had stories in the last few weeks we've been talking about where oppressive regimes are actually shutting the Internet down during national testing days because too many kids cheat and, like, use the Internet to do that.  So, yeah, let's turn it off.  So, okay.



The Executive Summary finished, listing 16 major recommendations.  And just a couple of them stood out to me as being worthy of note.  I've got five of them.  They said for their recommendations:  "Agree to and adopt a shared policy on digital privacy that is interoperable with Europe's General Data Protection Regulation," the infamous GDPR.



Now, that's interesting because we haven't done that in the states.  And from our perspective, the GDPR, you know, it's kind of a mixed blessing; right?  It's the reason we're having to say yes, dammit, I mean darn it, I accept these cookies, or do with cookies whatever you will.  Or whatever.  I mean, it's sort of created a mess.



Okay.  Second major recommendation:  "Declare norms against destructive attacks on election and financial systems."  Okay, well, good luck with that.  Third:  "Negotiate with adversaries to establish limits on cyber operations directed at nuclear command-and-control communications systems."  And obviously they would be bidirectional agreements, so we won't attack your nuclear reactors if you don't attack ours, and hold each other to that.  Fourth, "Hold states accountable for malicious activity emanating from their territories."



And that's interesting because we've seen them say, well, okay, so the IPs were in our country, but we didn't do it.  It must have been bad guys bouncing packets off of systems that they compromised.  And so the point is, okay, you're still going to be responsible.  If malicious traffic and activity comes from your country, then you need to be responsible for it.  You're certainly responsible for restricting communications within your country, so you should be able to restrict malicious traffic coming from it equally.



And finally, clean up U.S. cyberspace by offering incentives for Internet service providers and cloud providers to reduce malicious activity within their infrastructure.  And I thought that was interesting.  We've talked about how DDOS attacks traditionally spoofed their IP addresses.  That's happening less now that those sorts of attacks are less effective and are more easily blocked.  But it always was the case that ISPs were allowing traffic to exit their control having IP addresses that did not exist within their borders.  So it had to be spoofed.  And it would have been trivial to have ISPs block that.  But we're all one big happy Internet.  So no such regulations were ever imposed.



So anyway, I just thought this was a really interesting report.  I'm not going to go into it any further.  But as I was scanning through it and reading some of the many other interesting details, I kept thinking that our listeners would really find some of the report's details interesting.  So it is this week's Shortcut of the Week, so grc.sc/880, for anyone who's interested.  It's a big PDF.  But, boy, I think it's really interesting that this group has assembled a report that sort of formally states what the rest of us have all seen.  And that is that, well, it was a nice idea, but didn't quite work out the way we hoped.  So we need to, like, acknowledge that reality and figure out what we're going to do about it.  Because if we keep doing nothing and just sitting around hoping, that's not going to turn out well, either.  So grc.sc/880.



JASON:  Didn't turn out quite the way we hoped.  I feel like that could be on a T-shirt for the 2020s, you know, kind of the decade that we're in; right?  It was a great idea, but didn't quite turn out the way we hoped.



STEVE:  Yes.  While we're in a massive heat wave right now that is melting everything down.  It's like, well, how did that 21st Century go?



JASON:  Everything seemed like a really great idea at the time.



STEVE:  I hope the kids like the heat, yeah.



JASON:  Yeah.  All right, Steve.  What's going on with Facebook?  I feel like Facebook was like it.  It was like everything Facebook.  And then things got a little quiet and everything.  Are they doing something right, or are they doing something wrong right now?



STEVE:  Well, so, just last week we talked about how Firefox v102 had added a feature to strip some of the tracking information from URLs that it was going to be querying before handing them over to a web server, the idea being that it would be enforcing the privacy of its users in that way.  So this is something that users had to enable.  But when it was, when it had been enabled, a small set of URLs, domains and then specific tags in the URL, which Firefox had been trained to recognize and felt comfortable with altering on the fly, would be altered.  And we noted last week that Firefox was apparently being conservative about what they were stripping from the URLs since the Brave browser was reported to be significantly more aggressive.



Now, while discussing this last week I commented that, although I loved the idea of removing tracking identifiers from URLs, the whole thing felt flaky and uncertain to me since modifying a link's URL is inherently trouble prone, which is no doubt why Firefox was being apparently conservative in the URLs they were modifying compared to Brave; and because it would be so easy for Facebook, for example, to change the token name of the value in the URL link.  Then all browsers would need to update their URL exception handlers, and we'd be back into a cat-and-mouse game.



Well, all of that handwringing, with regard to Facebook at least, has been rendered moot because Facebook's links have suddenly transformed into opaque blobs.  And really this should not be a surprise to anyone.  It should have been obvious that Facebook would not be happy having anyone mucking around with their URL links.  The composition of any URL is by definition entirely up to the creator of the URL.  Way back in 1994, RFC-1738, whose lead author was the famous Tim Berners-Lee at CERN, made clear that a URL is inherently an opaque token that only needs to have any meaning to the server that receives it.  Once upon a time, URLs tended to directly reflect the hierarchy of the receiving server's file system, or at least some piece of that file hierarchy.  And that file system was often organized by a human in some reasonable structure.  So the whole thing meant something.



But as pages became more and more dynamic, being assembled on the fly by server-side PHP, ASP, or JSP scripting code after querying a big backend database, the primary reason URLs have remained at all understandable to humans is that they have been, and they'll continue to be for some time, but they've been a source of signals for Internet indexing search engines.  We'd like Google to learn something about a page's link from its textual content, so that's often been preserved.  But we've increasingly seen URLs being cluttered with things like GUIDs, those globally unique IDs, which only have any meaning whatsoever meaning to a server-side process.



Amazon's URLs, for example, have a short code near the front, which is surrounded by long hyphenated descriptive strings which describe the product.  All of that superfluous text is only there for search engines to pick up on.  Amazon has no need for it and completely ignores it.  Since those massive multiline Amazon URLs are annoying to share, one of my favorite tricks is to strip everything out of an Amazon URL other than an anchoring, it starts with /dp/ followed by the 10-character product ID.  That's all you need.  And that results in a very short Amazon URL that always works.



In any event, all of Facebook's content is obviously all being assembled on the fly, driven by code and a massive backend database.  So the construction of their URLs has always, or at least for a long time, been arbitrary and in no way reflects anything other than whatever their code wants it to reflect.  So Facebook apparently decided for whatever reason, and it should come as no surprise to anyone, that it was probably tired of having third-party browsers and add-on extensions that are supposed to be enhancing privacy messing around with its links, specifically stripping out tracking information that they wanted to stay there.  So now no one who doesn't know how to unscramble or decrypt a Facebook link can see anything about what's going on.  They have truly become long opaque tokens.



Now, since older pre-encrypted links, that is, from more than, like, from before this weekend, which is when this suddenly began to happen, since those links are still going to be around, probably forever, I'm sure that all incoming links are now being checked to see whether they are old-style in-the-clear format or this new opaque-blob format.  If they're old, they're accepted as is.  If they're obfuscated by this new encryption, they'll first be decrypted, then handled.



So it's clear, as it always should have been, that any anti-tracking privacy enforcement we're going to obtain will need to be created by policy and, you know, laws and mandates and so forth, not by technology.  Because ultimately this is something that Facebook has total control over, and they've just exercised another little bit of that control.



We discussed previously Apple's official launch, well, actually announcement because it's going to be happening in iOS 16,  their announcement of this very interesting new "Lockdown Mode" feature.  And that was announced during this year's World Wide Developer Conference.  I think this idea makes so much sense because it's the, you know, our phones have this insane level of "it'll do anything you could ever want it to" breadth of features.  And many are often unneeded, many are unwanted, and as a consequence, most of them go unused.  I mean, there's stuff in my iPhone I'm embarrassed to say I have no idea what it does.  There are things that annoy me, like the three dots that are now at the top.  I keep trying to pull down from the top, and now I'm getting the multitasking stuff, where I didn't before.  That's annoying, and you used to be able to turn it off.  Now you can't. 



Anyway, these things are like crammed with features that most people don't need, don't want, and don't use, yet being there hugely increases any device's attack surface.  There are just more things that could have parsing errors in them that could go wrong.  So simply turning off all of that unwanted and unneeded excess for individuals, especially for individuals for whom security trumps the ability to receive cat videos from strangers, seems like an obvious win.  And when Leo and I were talking about this last week, it seemed like something we would both be inclined to turn off.  At least, you know, take it out for a spin and see how it affects our lives.  It doesn't seem like it's going to be that restrictive, even though all the press talking about it is saying, "Oh, my god, it's super lockdown restrictive.  You can't do anything anymore."  I don't do anything anyway.  So it's not going to affect me very much.



So anyway, my feeling is it will probably go a long way toward limiting the victimization by commercial malware such as Pegasus, which is explicitly Apple's target here.  And because Apple thinks so, too, they've decided to, as they say, put their money where their mouth is by offering the industry's largest bounty ever  $2 million  to anyone able to reproducibly crack into an iOS 16 device when it's in Lockdown Mode.  I say bravo.  I think that's cool.  And I'll bet that a bounty of that size will likely give those who used to just enjoyed jailbreaks for the fun of it some new incentive because $2 million.  Wow.



We've talked a lot in the past through the years about Clearview AI.  They're the company, just to remind anybody maybe who hasn't been listening for years, that decided what they would do is send bots out onto the Internet, much as Google sends spiders.  They would send their own bots out to collect images from publically available social media, you know, crawling Facebook.  Crawling Twitter.  Crawling everything.  And building a huge database of people's faces, which they would then, using other means, tie back to their location and their availability, and build a big database.  That's Clearview AI.  And it's been a big hit with law enforcement and governments and any entities that have some need to identify people from photos. 



Okay.  So Clearview AI has been in the news just recently.  Essentially they've been fined by Greece's privacy authority, the Hellenic Data Protection Authority (HDPA), for violating parts of Europe's infamous GDPR.  The fine which has been levied against Clearview AI by the HDPA is a hefty 20 million euros.  And what's a little bit galling, even to me, is that it's not due to any use or abuse of Clearview AI's admittedly controversial facial recognition database technology.  It's just because Clearview AI exists, and Greece doesn't like the idea.  And the GDPR gives them the right to fine Clearview AI over their conduct, even though there's no implication of its use.  



A 22-page decision demands that Clearview AI stop processing biometric data on individuals in Greece, and said the company must delete all the data, that is, all the pictures of Grecians, it has already collected.  The decision stems from a complaint filed by a number of privacy organizations which questions Clearview AI's practice of scraping selfies and photos from public social media accounts as a means of assembling its facial recognition database, which is rapidly growing toward, well, actually I think it's at 10 billion, and they're trying to go to 100 billion.



Okay, now, as we know, since we've been tracking this interesting edge case since they emerged a number of years ago, Clearview AI sells - it's in the business of selling its facial recognition tools to law enforcement agencies around the world and has said they want to get to 100 billion images.  It's also the case that Clearview has been at work in Ukraine, helping to identify both deceased Ukrainian citizens for the government, and Russian soldiers so that families can be notified back in Russia in case they want to come and pick up their dead Russian.



The problem that Clearview AI faces surrounds consent.  More and more privacy regulations are requiring consent, but Clearview's autonomous image-scraping technology is inherently consent-free.  What I thought was interesting is that while Greece's Hellenic Data Protection Authority has levied this hefty fine, Clearview AI has never, never had any contact with either Greece's citizens or its law enforcement agencies.  They simply share the same planet.



Clearview AI said it does not have a place of business in Greece or the EU, and it does not have any customers in Greece or the EU.  The company also claimed its product has never been used in Greece and "does not undertake any activities that would otherwise mean it is subject to the GDPR."  One of the several privacy groups which filed the initial complaint explained that the fine and the ruling made clear that the GDPR is applicable because Clearview AI uses its software to monitor the behavior of people in Greece, even though the company is based in the U.S. and does not offer its services in Greece or in the EU.  The privacy organization said:  "Collecting images for a biometric search engine is illegal, period."



So one thing that made me just shake my head is that Clearview has made it clear that they're happy to steer clear of regions that don't want their services.  Yet the Greek authority also ordered Clearview to appoint a representative in the EU, even though they don't want to do business in the EU and haven't and aren't, to enable EU citizens to exercise their rights more easily, meaning I guess they would like someone local to sue, and so regulators have a contact person in the EU.  Yeah, I don't blame Clearview for not doing that.



So I don't mean to sound overly sympathetic toward Clearview AI. But this does sort of seem to be, I don't know, overreaching.  All of the images it's collecting are public.  Anyone can view them.  Just like the web pages that Google crawls across and indexes which allows us to later locate the information we seek. So it's clear that the difference is that pictures of people's faces are considered to be biometric data, even though faces are kind of public.  It's considered biometric data by these regulators and regulations, and are not regarded any differently than fingerprints or DNA.  If someone followed us around, dusting everything we touched to lift our fingerprints, that would likely annoy us.  The fact that Clearview AI's image collection is unseen doesn't render it any less noxious in the eyes of privacy regulators.



One country after another is lowering the boom on Clearview AI.  We previously talked about the U.K.'s 7.5 million euro fine last May, similar rulings have recently been made by France and Italy, and Austria is said to be preparing a similar ruling.  So it's looking like maybe this U.S.-based company will actually only be able to operate in a country where its privacy laws do not exclude it from doing so because we know that Illinois and their PIPA, the state of Illinois, that's a problem because of PIPA, which is where some of the earlier suits have been filed.  Now we've got lots of EU countries doing so under the GDPR.  So it's looking like the territory that Clearview AI's going to be able to cover is shrinking, and it's actually looking like this is a fight it's going to lose.



And speaking of searchable databases, several ransomware and extortion groups have been creating searchable databases of information they have stolen during their attacks.  As we know, it's not news that ransomware groups have been extorting organizations with the threat of leaking the data that they have stolen.  They steal it, they exfiltrate it, then they encrypt it so the company that owns it can't have it.  And also, adding insult to injury, they've got a copy of it, and they're threatening to release it publicly unless the ransom is paid.  Well, now they've gone one step further and created, started indexing the data and making it searchable.  Over the last month or so two ransomware groups, actually three - AlphV, Karakurt, and LockBit - have all debuted features on their leak sites which allow visitors to search through the troves of data by company name and/or other signifiers.



A senior staff researcher at Tenable has confirmed that all three groups have incorporated some kind of searchable database functionality into their leak sites.  And if we've seen anything, it's that an idea that's useful will be quickly picked up and mimicked by other ransomware groups.  So we can soon expect this to be a new feature of all the dark web exfiltrated extorted data leak sites.



Emsisoft's threat analyst Brett Callow said that the tactic was designed to further increase the pressure on organizations by weaponizing their customers and business partners.  Callow said:  "The gangs likely believe that making the data available in this way will result in more companies paying due to a perceived increase in the potential for reputational harm.  And they may be right."  He added that in the past, companies have been able to dodge accountability for the leaks by claiming that there is "no evidence user data has been misused," which is a line seen in hundreds of breach notification letters over the past few years.  Callow notes that such "soothing statements like that aren't really possible when people know their personal information was exfiltrated, compiled into an individual downloadable pack, and made available online."  Who knows, maybe Google will start indexing it, too.



Moscow has imposed a $358 million fine, $358 million, more than chump change, on Google over Google's continued failure - which I guess at this point you'd have to consider, you'd have to call it a refusal since here we are in July, and the attack on Ukraine was in February, as I recall - Google's failure to filter out information from its search results that Russia's Internet watchdog "Roskomnadzor" has demanded be removed.  I should note that the amount of the fine is much more fun when expressed in Russia's much less valuable rubles.  That would be a total of 21 billion very small rubles.



Anyway, Roskomnadzor announced that Google and its subsidiary YouTube have failed to remove the following materials after multiple requests:  information about the course of the "special military operation" in Ukraine, which discredits the Armed Forces of the Russian Federation; content promoting extremism and terrorism; content promoting harmful acts for the life and health of minors; and information that promotes participation in unauthorized mass actions.



So as we see, a free and open Internet isn't always the best thing for everyone.  I suppose this is what the Council on Foreign Relations meant when they said that the dream had not come true and the sooner we in the West, and the U.S. specifically, wake up and smell the packets, the better.



I guess Roskomnadzor realizes that Google is too useful to block outright, or they would have.  They've tried over and over to enforce sanctions based on various parts of Russia's Code of Administrative Offenses.  Last month, Roskomnadzor fined Google $1.2 million.  That's a measly 68 million rubles.  But as the fines remain unpaid, the multiple violations qualify it to be based upon a different practice, which is a piece of the action.  In this case, up to 10% of Google's annual Russian revenue.  Russian users of Google Search and YouTube will now also encounter a warning about Google's violation of the law, and the will not be allowed to place advertisements or use them as information sources.



So Russia is attempting to squeeze Google by the wallet.  And, for what it's worth, it's working.  Google's paid services are disappearing and being withdrawn.  After Russia's invasion of Ukraine and the so-called "anti-fake news laws" which were enacted in Russia, which amounted to don't say anything we don't like, Russia's Google subsidiary, Google LLC, filed for bankruptcy, claiming it had no ability to continue business after a series of massive fines and, ultimately, asset confiscation.  So loyal Russians will presumably think, well, that's just those corrupt Westerners getting what they deserve.  On the other hand, they will no longer have access to Google services.  And I suppose that was inevitable.  I guess I would give Google a tip of the hat for not bowing to Russian pressure and doing their part to keep the Internet open.



And speaking of getting what we deserve, last Tuesday Windows users received patches to hopefully fix a total of 84 individual flaws across Microsoft's sprawling software base.  One of those was a true zero-day privilege elevation bug which was being actively exploited in the wild.  The demographics of the patches  break down:  Of those 84, 52 were elevation of privilege vulnerabilities; 12 allowed remote code execution; 11 supported information disclosure; five were denial of service vulnerabilities, meaning that something crashed; and four were generic security feature bypass, whatever that means.  There were no reports of any big meltdowns following last Tuesday's updates, so nothing big and obvious was messed up this month.  A handful of bugs are no more.  Well, except for any new ones that may have been introduced.  Maybe we'll get to those eventually, as well as all the others that still remain in Windows and Microsoft's other products.



Okay.  So I got a DM'd tweet yesterday which sort of surprised me.  It was a fun SpinRite testimonial, and of the sort we haven't heard for a long time.  It came from a guy named Paul Jolley.  He said:  "Last week one of our power stations reported they needed to restore a GEM80 PLC."  PLC is a programmable logic controller; right?  He said:  "They had two separate backups on 3.5" floppy disks, but neither would read.  Configuration control," he wrote, "knowing what code is running on programmable devices performing process control in an OT environment is very important in our industry, so they were in a pickle.  They tried a number of ways to read the floppies using various freeware and were unsuccessful.  So I offered to try SpinRite as a last resort."



He says:  "I took delivery of the floppies this morning and set version v5.0 to work on Level 2.  It managed to recover about 90% of the file required from the first floppy.  Then from the second floppy, which had a totally corrupt file system," he said, "I was able to 'cat' the entire device to Linux to a file and subsequently extract the same file contents.  Combining the recovered data from both floppies provided full coverage, and thanks to you I was a hero."



So I think what he meant was that he recovered everything but 90% of one file, but that file he was able to recover from the other floppy.  So given the two and SpinRite, he was able to succeed.  What I found interesting was Paul's reference to SpinRite 5.  As I've previously mentioned, for some confounding, apparently mystical reason, v5 is superior to v6 for the recovery of diskettes.  I've stared at SpinRite 6's code for probably a total of days at this point, trying to explain the difference.  There is no difference.  So I have no idea why.  But in testing both, v5 has consistently produced superior results.



The other thing that was interesting was that, by being a DM, I had our previous DM thread.  It turns out back on February 13th of 2021, so about a year and a half ago, Paul had DM'd to ask.  He said:  "Happy to contact GRC support, but thought I could quickly ask you first.  I listened to a recent podcast where you said SpinRite 6 owners could download SpinRite 5 if they want by simply changing the download URL.  I was interested because at our site we still use floppy disks."  And of course he's talking about for these PLCs controllers.



He said:  "So I looked up my purchase email and followed the link to the download page where it asks for my transaction code, then generates links that don't have a version in the URL.  I must be missing something or didn't understand what you meant on the podcast."  Anyway, he later tweeted:  "Was just about to contact Greg this morning when I found the answer was on the FAQ page at the bottom."  And sure enough, down at the bottom we explain how to change the download URL to allow yourself to get a copy of SpinRite 5 because for reasons that will never be known at this point, I think it's safe to say, 5 is better than 6 at recovering data from floppies.



Okay.  A couple closing-the-loop bits.  Michael Swanson, he said:  "Hi, Steve.  I just listened to SN-879," so that was last week's podcast.  He says:  "And regarding the use of a VPN when traveling, or even at a coffee shop," he says, "I prefer to use a travel router like the TP-Link N300.  I connect the travel router to whatever Internet service is available.  And whatever devices I bring with me - laptop, tablet, phone, Roku, et cetera - connect to the WiFi network of the travel router.  All my devices are then behind a full NAT firewall.  Added security," he said, "the travel router is also using Google DNS to prevent DNS hijack, and it is also possible to set the router to be a VPN client to many VPN services, and thus tunnel through to any VPN exit point including my home network, if desired.  And the WiFi network on my travel router has the same SSID as my home network so all my devices connect automatically thinking they're at home."



So Michael, thanks for sharing that.  I thought that was very clever.  I liked the idea of using the same SSID and obviously the same password so that when you're on the road your devices don't know that you're not home, and they connect easily.  And obviously he also understands that to get the same security of a VPN, you still would need to use a VPN tunnel, although it definitely is nice to be behind a NAT firewall.  If something in the hotel was trying to get into your devices by port scanning them, having a NAT-based firewall would also solve the problem.



However, there was an even cooler idea, I think.  IcyvRan is his handle.  He said:  "Hi, Steve.  One solution, if one does not trust a WiFi hotspot, is setting up a Raspberry Pi at home" - or whatever you want to actually - "with Tailscale, and configuring it as an exit node."  And he provided a link, which I have in the show notes.  The link is to an FAQ, an explainer page about Tailscale's exit nodes.  First of all, remember that Tailscale, we've talked about before, is a so-called overlay network, very much like Hamachi was back in the days when 5-dot was unallocated Internet IPv4 space.  That didn't last forever.



Anyway, about exit nodes, Tailscale says:  "Exit nodes capture all your network traffic, which is typically not what you want.  The exit node feature lets you reroute all your non-Tailscale Internet traffic through a specific device on your network.  The device routing your traffic is called an 'exit node.'  By default, Tailscale acts as an overlay network.  It only routes traffic between devices running Tailscale, but does not touch your public Internet traffic, such as when you visit Google or Twitter.  This is ideal, they wrote, for most people, who need secure communication between sensitive devices like company servers, home computers and so forth, but don't need extra layers of encryption or latency for their public Internet connection.



"However, there may be times when you do want Tailscale to route all your public Internet traffic - in a cafe with an untrusted WiFi, or when traveling overseas and needing access to an online service, such as banking, only available in your home country.  By setting a device on your network as an exit node, you can use it to route all your public Internet traffic as needed, like with a consumer VPN."  So I thought that was a very cool tip.  So thank you, IcyvRan, for that.  And just a heads-up for all of our listeners.  Tailscale can do that.



Taskel tweeted:  "FYI on the Quantum-Resistant algorithms CRYSTALS-Kyber and CRYSTALS-Dilithium."  Remember that we talked about that recently, that the NIST had chosen four of the eight next-generation cryptographic algorithms that would be used to provide quantum resistant crypto.  I loved Dilithium crystals because of course they power the warp drive on Star Trek.  I didn't know what Kyber crystals were.  Well, he writes:  "Kyber crystals are what is used in lightsabers in Star Wars."  So there was something for Star Trek and for Star Wars fans there.  Thank you, Taskel.  Didn't know.



And someone tweeting as Lethal Dosage tweeted:  "I logged into Twitter for the first time in four years to poke fun at you.  You are losing geek points.  The first Star Wars movie was not Episode IV - A New Hope, it was just 'Star Wars.'"  He says:  "Episode IV - A New Hope was added later."  And then he said:  "Watch the original intro, only two minutes long."  And he provided a clip, a YouTube link, to, sure enough, a two-minute capture of "Star Wars 1977 original opening crawl" is the title of it.  It's had 1.4 million views, and it's there.



And I have to say it absolutely looked authentic.  But it's old and grainy, and in this day and age it could easily have been edited.  So I did a bit of digging around the Internet, and I got the whole story.  In the beginning, there was just "Star Wars."  But then fans of what turned out to be the most popular science fiction movie of all time were thrown a hyperspace curveball.  The film known as just "Star Wars" turns out wasn't the beginning of the story.  It was the middle.  Four years after the original film hit theaters, it was re-released, this time being called "Star Wars:  Episode IV - A New Hope."



So here's what happened.  In March of '78, right, Star Wars the original movie was released in '77.  Next year, March of '78, the science fiction author Leigh Brackett died, and George Lucas took over writing movie number two, which was titled "The Empire Strikes Back," a task which he shared with Lawrence Kasdan.



Next, Lucas decided that there's a bigger back story to all of Star Wars, which means that the Empire, you know, "Empire Strikes Back," is not Part II, but instead Part V.  So in 1980 "The Empire Strikes Back" identified itself as "The Empire Strikes Back:  Episode V," which totally blew everyone's mind at the time, resulting in no end of confusion.  Then the next year, in '81, the original Star Wars movie was re-released as Episode IV to make everything line up properly.



And what's confusing about all this is that I definitely saw the original Star Wars in 1977.  I mean, I was alive, so of course I saw it.  I was 22 years old.  So I recall still that afternoon, 45 years ago, sitting in the theater and seeing this movie with some friends that I worked with.  And I distinctly also recall anxiety and consternation being created by Star Wars episode numbering.  But I guess the anxiety must have been created when "The Empire Strikes Back" identified itself for the first time as Episode V, and it was like, what?  Rather than when the original Star Wars identified itself as Episode IV, which it didn't in the beginning.  So now we all understand that.  And Jason...



JASON:  That's interesting to hear that, yeah.



STEVE:  You obviously are not 45 years old.



JASON:  Close.



STEVE:  You have always had Star Wars in your life.



JASON:  I have always had Star Wars in my life.  And I don't think that I ever really - you know, okay.  So, well, I'm almost 47.



STEVE:  You are?



JASON:  So I'm close to the age that you were then now.



STEVE:  This is the best moustache you can muster.



JASON:  Yeah, this is all I can do, yeah.  Believe me, nothing even grows right here.  Anyway, that's beside the point.



STEVE:  Hey, I wish that were the case.  That would be nice.



JASON:  It kind of shaves itself, to be honest.  But yeah, I've always had Star Wars in my life.  I never really felt very weird about the numbering because Star Wars came out, yes, when I was too little to watch it.  I think I was two at the time, when it was actually in the theater.



STEVE:  Two, yes.



JASON:  And so the whole numbering thing was already in place by the time I was ever at all aware.  But I do remember "Empire Strikes Back" and loving it.  Back then that was my favorite of those three, anyways.  But what this makes me think is like, I've only ever watched "Phantom Menace," which is the first one, right, one time, and I thought it was awful.  This was right around the time it was in the theaters.  I thought it was horrible.  And so as a result I never gave Part II and III any chance.  I never actually watched them.  I've still to this day not watched them, and I don't know why.  Maybe I need to do that.  But I can't help but, like, hear what you just said, [crosstalk] watch them.



STEVE:  I do know why - [crosstalk] watch them.



JASON:  Okay.  See, there you go.  So I hear what you just read on the podcast, and I'm like, all right, it all makes sense why those earlier ones were not very good because it was kind of an after-the-fact thing, like oh, wait a minute, there's more we can do here, but I'm not sure what that more is yet.



STEVE:  Yeah.  And more wasn't necessarily better.



JASON:  Yeah, it wasn't necessary.



STEVE:  We immediately descended into little teddy bears running around.  And, you know, it's just like, what has happened?



JASON:  Yeah, little strange, the arc.



STEVE:  So I have one last thing to share.  Dave Pope, he said:  "FYI, my 2013" - okay, 2013 - "Ford key fob has bidirectional comms.  It has a light that shows me red or green if the remote start was successful or not."  He says:  "No idea if it does the handshake you mention in the episode, though."  Because I talked last week about the only way to really secure remote keyless entry.  Tesla does a better job than Honda because it'd be hard to do a worse job than Honda.  They do a better job by only ever moving the synchronizing counter forward so that codes actually do expire the first time that they're used, and there's no way to trick the system into using the same code again.  Although, as we saw, an active attack can use jamming in order to get the key fob to emit additional codes that aren't seen by the car, which an attacker can grab to use in the future.  But the way to absolutely solve the problem is bidirectional handshake, which is what we have on the Internet for all of our secure comms, and that's robustly secure.



Anyway, I just thought it was very cool that, what, nine years ago Ford has a key fob that lights to let you know whether the car has affirmatively confirmed that the car - that the engine started or not.  That's very cool.  So anyway, David, thanks for sharing that.  And Jason, let's tell us about our last sponsor, and then we're going to go to back into bleeding because you're here on the podcast.



JASON:  Oh, boy, yeah.  I hope next time I join it has nothing to do with bleeding.  Let's just put that on the table right there.  Let's talk a little bit about RetBleed.  What exactly is it, and why are these things appearing every time I host the show?



STEVE:  Yes.  So, okay.  Ret is the universal name, I think it's like universal across all processors.  I don't know if I've ever - I've programmed many chips in assembly language, and Ret has always been the name of the CPU instruction for causing a subroutine to return.  So Ret is short for Return.  It's placed at the end of a subroutine to cause the subroutine to stop at that point.  So it's a little bit like - it's like a special jump instruction.  It stops its execution, and it returns, it causes the processor to return to the instruction following the one that invoked the subroutine.  So in essence the instruction tells the CPU to return to the point where the subroutine was called, like just after the point it was called.  So execution resumes in a linear stream from that point.



In stack-based processors subroutines are often provided with some parameters which they will use for whatever work they need to do.  So the caller puts these parameters onto the stack and then the subroutine looks on the stack in order to access them.  They can be values or pointers or whatever.  And subroutines may place some of their own local temporary data onto the stack, as well.  And how many times on this podcast have we used the term "stack buffer overflow"?  Uh-huh.  Meaning that there was a buffer that some code had put on the stack, and it overflowed the stack.  That's always been a big problem.  And when the processor's return instruction is executed, all of this stack-based data is discarded.  Nobody bothers to, like, flush it to zeroes or overwrite it because that takes time.  Instead, the stack pointers just moved back above it as if it never existed, and we go on from there.  So it's a very elegant means for managing various sorts of temporary data.



RetBleed is the brainchild of two researchers from ETH Zurich, who have been behind a number of previous very clever attacks.  Their paper on RetBleed, which is what they named this, will be delivered in a few weeks from now, I think it's August.  I don't remember now.  I had the date in my head.  It's gone.  Anyway, a couple weeks from now during a Technical Session of the USENIX Security '22 conference.  They, being good guys, they responsibly disclosed their discovery to Intel and AMD back in February of this year, presumably with a six-month non-disclosure period.  They agreed to be silent.  That embargo was lifted last Tuesday, the 12th of July, which also happens to be Patch Tuesday, when it turns out some fixes for RetBleed were pushed out to the world.



Okay.  So I'm going to start by just reading their paper's abstract.  I'm not going to get into the weeds because the weeds are very deep and thick here.  But the abstract gives us an overall feel for what this is.  And then I will break it down some.



So they wrote:  "Modern operating systems rely on software defenses against hardware attacks.  These defenses are, however, as good as the assumptions they make on the underlying hardware.  In this paper, we invalidate some of the key assumptions behind retpoline" - I'll explain that in a minute - "a widely deployed mitigation against Spectre Branch Target Injection (BTI) that converts vulnerable indirect branches to protected returns.  We present RetBleed, a new Spectre-BTI" - again, Branch Target Injection - "attack that leaks arbitrary kernel memory on fully patched Intel and AMD systems.



"Two insights make RetBleed possible.  First, we show that return instructions behave like indirect branches under certain microarchitecture-dependent conditions, which we reverse engineer.  Our dynamic analysis framework discovers many exploitable return instructions inside the Linux kernel, reachable through unprivileged system calls.  Second, we show how an unprivileged attacker can arbitrarily control the predicted target of such return instructions by branching into kernel memory.  RetBleed leaks privileged memory at the rate of 219 bytes per second with 98% accuracy on Intel Coffee Lake, and 3900 bytes per second with greater than 99% accuracy on AMD Zen 2 chips."



So, okay.  There are a few things to observe here.  One is that this is another instance of the lesson that attacks never get worse, they only ever get better.  When we started off with the Spectre and Meltdown speculative execution attacks, they were purely theoretical.  This was at the end of 2017, early 2018.  It's all we were talking about.  Purely theoretical.  But they didn't remain that way for long.  Before long researchers were discovering how to use these once-theoretical attacks to probe the contents of memory that they had absolutely no valid access to.  That access limitation was enforced by hardware, and it didn't matter.



Essentially, they deliberately created a road that would not be taken by the CPU, but which the CPU would speculatively prepare to take anyway.  And in doing so, it would preload the contents of some memory down that road into its cache.  Then they would probe the cache to see what the CPU had cached in preparation for that never taken road.  In this manner, they would get the CPU to access memory for them which they could not legally access themselves.  Access violations were never triggered because speculation never triggers access violations.  This all amounted to some extremely clever manipulations of the insanely complex microarchitectures that have been incrementally added to, generation after generation, to modern processors, all in the name of squeezing out every last cycle of performance.



What's annoying to researchers, who are just wanting to, like, make the world more secure, is that the microarchitecture is undocumented.  It is never documented.  Intel just says, oh, don't worry about it, it's perfect.  Except it's not.  And so the first thing these guys all have to do is painstakingly reverse engineer the underlying architecture in order to figure out how it works.  That they can only do by observing performance in all kinds of crazy tests.  They reverse engineer how this all works underneath the chip's instructions.  Then they go about bypassing the protections in some instances that this system provides.  An amazing amount of work.  And, you know, what, they get a paper out of it.  They ought to be rich.



Anyway, the problem that's the subject of this paper, and of much sudden scurrying around - for example, as I'll explain in a minute, Linus just delayed the next Linux kernel release by one week as a result of this - this has the name "branch target injection."  It's also known as Spectre Variant 2.  There are essentially two available mitigations for this sort of speculation side-channel leakage.  There is retpoline, which is a contraction of return trampoline, thus retpoline; and IBRS, which stands for Indirect Branch Restricted Speculation.



Just over three years ago - oh, I should mention, IBRS, Indirect Branch Restricted Speculation, is Intel's official solution and has always been.  Retpoline is what Google cleverly came up with quickly as a fix for Chrome because Chrome would have been a big target for this.  It turns out that you could actually do this in a browser.  And so Google had to fix the Chromium engine, hardening it against this Spectre Variant 2.  Thus they invented retpoline, which people liked a lot better than Intel's IBRS solution.



Just over three years ago, the SUSE Linux blog posted an article titled "Removal of IBRS Mitigation for Spectre Variant 2."  And what was written is interesting in light of today's events.  SUSE wrote:  "As the Meltdown and Spectre attacks were published at the beginning of January 2018, several mitigations were planned and implemented for Spectre Variant 2.  Spectre Variant 2 describes an issue where the CPU's branch prediction can be poisoned, so the CPU speculatively executes code it usually would never try to.  For instance, user space attacker-controlled code could make the kernel code speculatively execute Spectre code gadgets that disclose secret kernel information, via flush-and-reload" - those are cache timing - "disclosure methods."



They said:  "Two major mitigations were proposed."  That is, for Spectre Variant 2.  "A CPU feature called Indirect Branch Restricted Speculation that would not use branch predictions from lower privilege levels to higher ones."  Meaning when jumping into the OS from userland.  They said:  "Or software workarounds called 'retpolines' and 'RSB stuffing.'"  They said:  "These can fully replace the IBRS mitigation."  Except not now.  But that's what they said at the time.  They said:  "On Intel Skylake there is the theoretical possibility that these software mitigations are not sufficient, but so far research has not shown any holes."  Well, of course that was true three years ago.  But as we know now, it is no longer true.



They said:  "SUSE backported the IBRS patches to our kernels" - meaning backported I'm sure from Linux - "to our kernels for the initial release of mitigations and enabled them, as the retpoline mitigations were not yet ready.  SUSE pushed the retpoline mitigation some months later after support in the compiler and kernel became available, but left in the IBRS mitigation."  Which they now wish they had left in.  "As of today" - again, this was three years ago - "the retpoline and RSB stuffing software workarounds provide the same level of mitigations that IBRS provides.  While IBRS support continued in the SUSE kernel, it was not accepted by the Linux upstream kernel community, and it was also shown to cause performance degradation."  And how.



Finally, they said:  "As retpoline and RSB stuffing completely mitigate the Spectre Variant 2 issue for the Linux Kernel, SUSE decided, with guidance from Intel, to remove the IBRS patches from our kernel releases.  While on Intel Skylake there exists a theoretical possibility that the software mitigations are not complete, so far no research has shown exploitable scenarios.  Should research show any exploitable scenarios there, SUSE will reenable the IBRS mitigation on these chipsets."



So now that research has shown exploitable scenarios, I'm sure that's what they've been doing the last week.  This means that the clever "no hardware required" retpoline hack that Google had originally invented to protect their Chromium browser from these attacks worked for about three years until enough time, focus, and reverse engineering had been applied by some very dedicated researchers to hack past the imperfect mitigation that retpoline was and turn a theoretical vulnerability into a very real threat.



Meanwhile, the day before yesterday, on Sunday, Linus posted into the Linux Kernel 5.19-rc7 thread, writing:  "It's a Sunday afternoon.  I wonder what that might mean."  He said:  "Another week, another rc.  We obviously had that whole RetBleed thing, and it does show up in both the diffstat and the shortlog, and rc7 is definitely bigger than usual.  And also as usual, when we've had one of those embargoed hardware issues pending" - meaning all of this RetBleed stuff - "the patches did not get the open development, and then as a result missed all of the usual sanity checking by all of the automation build and test infrastructure we have.  So no surprise.  There has been various small fix-up patches afterwards, too, for some corner cases.



"That said, last week there were two other development trees that independently also asked for an extension, so 5.19 will be one of those releases that have an additional rc8 next weekend before the final release.  We had some last-minute btrfs reverts, and also there's a pending issue with an Intel GPU firmware."  So anyway, this did affect the Linux kernel delaying its release by a week so that they could get the IBRS stuff back in and going.



Now, among all of this, more than anything else, I loved Intel's description of this problem.  It's CVE-2022-29901. And it starts out with the phrase, "non-transparent sharing."  Now, okay.  You've got to love that.  Somewhere in their technical press release department, someone called out:  "Hey, anyone.  I need a term for 'leakage' that doesn't sound like a bad thing."  And someone replied:  "How about non-transparent sharing?"  The writer said "perfect," returned to his keyboard, and wrote: "Non-transparent sharing of branch predictor targets between contexts in some Intel processors may allow an authorized user" - meaning someone logged in - "to potentially enable information disclosure via local access."



Okay.  Again, non-transparent sharing of branch predictor targets between contexts.  Okay, anyway, everyone gets the message.  They could not possibly have soft-pedaled this thing any more than they did.  What it means is at least several hundred bytes per second can be exfiltrated from your Linux kernel if you don't fix this.  Whoops.



Okay.  The good news is, not all processors will be affected.  The ETH Zurich researchers said they tested the RetBleed attack in practice on AMD Zen 1, Zen 1+, and Zen 2, as well as the Intel Core generations 6, 7, and 8.  This essentially means Intel CPUs from between three and six years ago, and AMD processors from between one and 11 years ago will likely be affected.



Fortunately, the industry is getting better about addressing these sorts of problems, and patches for RetBleed were incorporated into this month's Patch Tuesday in both OS and cloud infrastructure updates from all major providers.  So that.  This leaves us, though, with the performance hit that comes whenever we disable some performance-enhancing bit that had inherently exploitable features.  We've talked about this from the first glimmer of the first of these many microarchitectural side-channel vulnerabilities.  Since all of these fancy features were invented to speed up the execution of real-world code, taking them out or shutting them down, like when we need the most, means some performance loss.



The ETH researchers noted that installing these patches will have an impact on the CPU's performance metrics on affected processors between 14% and 39%.  And another issue they found in AMD processors only, that they named Phantom JMPs, might even come with a 209% performance overhead.  Yikes.



The ETH researchers concluded their paper by writing:  "We showed how return instructions can be hijacked to achieve arbitrary speculative code execution under certain microarchitecture-dependent conditions.  We learned these conditions by reverse engineering the previously unknown details of indirect branch prediction on Intel and AMD microarchitectures and its interaction with the Return Stack Buffer.  We found many vulnerable returns under these conditions, using a new dynamic analysis framework which we built on top of standard Linux kernel testing and debugging facilities.



"Furthermore, we showed that an unprivileged process can control the destination of these kernel returns by poisoning the Branch Target Buffer using invalid architectural page faults.  Based on these insights, our end-to-end exploit, RetBleed, can leak arbitrary kernel data as an unprivileged process running on a system with the latest Linux kernel" - that is, until last Tuesday.  Actually, that's a good question.  Until this coming next release, probably - "with all deployed mitigations enabled.  Our efforts," they said, "led to deployed mitigations against RetBleed" - oh.  "Led to deployed mitigations against RetBleed in the Linux kernel."  So presumably that has been resolved.  So, yay.



And Jason, I look forward to you coming back for the next bleeding attack that the industry suffers.



JASON:  I don't need this attached to me, this whole idea of when I return to the show something bleeds.



STEVE:  If we could get the name of Jason, I want JasonBleed.  I think that'd be very cool.



JASON:  No, I don't.  I don't.  I appreciate that we all want different things.  I do not want that.  But we'll see.  Who knows?  Maybe we can break the cycle, and there will be no bleeding on the next time that I return.  Let's hope.



STEVE:  Let's hope.  Let's hope.



JASON:  Thank you, Steve, for sharing all of your wisdom on this and everything else throughout the show.  You can find everything that Steve does at GRC.com.  That's where you can go to find, well, everything you need to know about SpinRite, of course, that best mass storage recovery and maintenance tool.  You can get your copy right there.



You can find audio and video of this show at GRC.com, also transcripts at GRC.com, which is I think the only place you can get transcripts of this show. I don't believe that we offer those on our site, but you can find them there.



If you want to go to our site, there is the show page on the web, TWiT.tv/sn for Security Now!.  You can find audio, video.  You can jump out to YouTube.  Everything you need to know about the show is listed there, as well, including our recording times.  We record live every Tuesday at 4:30 p.m. Eastern, 1:30 p.m. Pacific, that's 20:30 UTC.  So if you want to watch live, you can do that, TWiT.tv/live.  And you can follow along on all the bleeding each and every time that I'm joining Steve on Security Now!.



Steve, thank you so much for doing the show once again with me.  I appreciate you welcoming me back.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#881

DATE:		July 26, 2022

TITLE:		The MV720

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-881.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we start off by updating our follow-up to this month's Patch Tuesday.  Things were more interesting than they originally seemed.  Then we keep up with the evolving state of Microsoft Office's VBA macro foreign document execution.  We also have a fabulous bit of news about some default security policy changes for Windows 11 announced by Microsoft.  Then, with August rapidly approaching, we have a few calendar notes to mention; I have a welcome and long-awaited bit of SpinRite news to share; we have a bit of miscellany and some brief bits of listener feedback to cover.  Then we take a deep dive into the poor-by-design security of a very popular and frightening widely used aftermarket GPS tracking device.  You don't want one of these anywhere near you or your enterprise. Yet 1.5 million are.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with an update on the update on last week's Microsoft Patch Tuesday.  Maybe not as good as we thought, including the weirdest printer solution and problem you've ever heard of.  We'll then talk about the on and off again VBA macro solution from Microsoft.  This time Microsoft says no, no, this is definite.  And finally, he calls it the Demon Box, a GPS tracker that is so woefully insecure that if you have it on your car, and you may not even know, you must remove it immediately.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 881, recorded Tuesday, July 26th, 2022:  The MV720.



It's time for Security Now!, the show where we protect you, your loved ones, your privacy online with this guy right here, Steve Gibson.  And by the way, Steve, at least half a dozen people came up to me and said - because I had mentioned when I was going on the cruise two weeks ago, oh, everybody's going to say hi to Steve.  They came up and said, "Tell Steve we said hi."  So everybody...



STEVE GIBSON:  Mission accomplished.	



LEO:  Mission accomplished.  I said we begged Steve to go on it.  Maybe next time Steve will go.  If you...



STEVE:  Well, and now we know why we didn't.



LEO:  You were smart.  You were smart.  Because, yes, Lisa and I did contract COVID, and a couple of other of the TWiT group did that I know of.  But most of them emerged unscathed, had a great time.  Thank you all for joining us.  We really appreciate it.  And I promise I am going to strong-arm Mr. Gibson.  Maybe when COVID's over we could do it; okay?  Or you pick the trip you want.  You want to go to Ireland?  Whatever you want to do.



STEVE:  Sounds good.



LEO:  All right.  What's going on in the world, Steve?



STEVE:  Well, this is Security Now! 881, our final episode for July.  We're going to start off by updating, actually updating our follow-up to this month's Patch Tuesday, which we did last week.  Thank you, Jason, for filling in for Leo.



LEO:  Yes, thank you, Jason.



STEVE:  Things turned out to be more interesting than they seemed at the time.  Then we keep up with the evolving state of Microsoft Office's VBA macro foreign document...  



LEO:  Wait a minute, there's more?



STEVE:  Oh, my god.  And we've got drama now also.



LEO:  Oh, boy.



STEVE:  So we're going to add that.  We also have a fabulous bit of news about some default security policy changes for Windows 11, announced by Microsoft.  And we also see that those are not easy for them to make.  Then, with August rapidly approaching, we've got a few calendar notes to mention.  I have a welcome and long-awaited bit of SpinRite news to share.  We have a bit of miscellany and some brief bits of listener feedback to cover.  Then we're going to take a deep dive into - and this is why the podcast's title is "The MV720."  It's a poor-by-design security of a very popular and frighteningly widely used aftermarket GPS tracking device.  Yes, $26 from Amazon.



LEO:  Ooh, wow.



STEVE:  You don't want one of these little puppies.



LEO:  Oh, okay.



STEVE:  Anywhere near you or your enterprise, yet 1.5 million of this particular model are out there.  And you're not going to believe, like just the lack of care.  And it's another perfect example of the weird place we're in now where such high-end technology is available so inexpensively that there's almost no budget for security, or no apparent concern whatsoever.  And people are buying these things because, hey, 26 bucks, I want one, click.  And then, you know, it phones home to China and everybody else.  So anyway.  Oh, and Leo, we've got - this is one of the best Pictures of the Week in a long time.



LEO:  Cool.  I haven't peeked.



STEVE:  So I think another great podcast.  



LEO:  I haven't peeked at it.



STEVE:  You should be on mic when you first see it.



LEO:  Oh, good, all right.



STEVE:  Because it's, you know, so the audience can capture your reaction.



LEO:  I will.  I have not looked.



STEVE:  All right.



LEO:  I've been very good.  I look forward to this, so I like to save it for myself.



STEVE:  It's one of those that takes some visual parsing.  But when it hits you, it hits you hard.



LEO:  I love those.  I love those because you go [gasp].  I'm going to pull it up right now, and we will look at it together, all of us.  All right?  It says "Love this.  Computer Security Career Paths, Path 1, Path 2."  Path 1 is a long one, 20 years:  forensic analyst, forensic lab director, chief security official.  Or go the hacker, criminal, convict two years, and you can be a high-paid security consultant in 14 months with good behavior.  I love it.  It's unfortunately kind of true; isn't it.



STEVE:  That's part of what makes this thing work so well.  So it starts off with a computer security expert, and you have two choices of path you want to take.



LEO:  A fork in the road, yes.



STEVE:  You can become the forensic analyst and then get promoted to be a forensic lab director, eventually get to be the chief security official, and then become finally, after 20 years, the highly paid security consultant.  That's the traditional path.  Or you can, knowing your way around computers...



LEO:  It's the shortcut, yeah.



STEVE:  You start, you do some hacking.



LEO:  You get caught.



STEVE:  You get caught, yeah, you get caught, and you're a criminal.  So you get convicted.  And it shows two years later, or 14 months with good behavior, you're out, and now you're a highly paid security consultant taking the shortcut route.  Now, it has been noted that you will end up with a felony on your record, which may...



LEO:  Yeah.



STEVE:  On the other hand, you might just, you know, depending upon who's hiring you, they might just figure, hey, that's just, you know, that's the way we saved 18 years in hiring this kid.  That's the price of entry.



LEO:  I love it.



STEVE:  Okay.  So last week, which was the week following July's Patch Tuesday, I congratulated Microsoft for their having patched some 84 known flaws without simultaneously crippling Windows.  But since then it has come to light that I may have been somewhat premature in my praise.  Published under "Issue details for July 2022" is the topic - this is Microsoft's publication - "Printing to USB-connected printers might fail," with the status of "Confirmed" in their little table.  So the affected, or I should say afflicted, platforms include both client versions of Windows 10 - this is Windows 10 only - 20H2, 21H1, and 21H2, and the server version 20H2.



Microsoft explains:  "Microsoft has received reports of issues affecting some printing devices following installation of Windows updates released June 28th and later.  Symptoms observed may include:  Windows might show duplicate copies of printers installed on a device, commonly with a similar name and the suffix 'Copy1.'  Applications that refer to the printer by a specific name cannot print.  Normal printer usage might be interrupted for either scenario, resulting in a failure of printing operations."



Okay.  So as we know, Windows printing, like Windows LAN Manager networking, has pretty much always been a mess.  As we know, last year's Windows Printer Spooling security debacle dogged Microsoft for more than half a year.  Some of Windows' architecture has not aged well through the decades.  And it's understandably difficult to ever make that decision to scrap something that mostly works, in favor of a major redesign which, while fixing the problem, the underlying problems, is certain also to break a large number of things that are currently working, especially when there's so much hidden dependency upon the existing system just being left the way it is.



So it really is the case that at this point Microsoft can barely change anything without breaking everything.  I think that probably when a future history is written of this era of Microsoft and Windows, it will show that they painted themselves into a corner from which there was no escape.  And while it's easy for us to say, oh, look, they broke it again, because of the way how creaky it's all become, it's like, yeah, but when were they supposed to stop and, like, break it badly in order to fix it?  Actually, it's what I did with SpinRite; right?  I said, okay, no more.  Anyway, we'll talk about that later.  But it could not be any more clear that Windows is at this point not actually getting any better; right?  I mean, like nobody wants any of what they're doing because it's not better.  It's now clearly getting worse.  Anyway, but that doesn't work with or isn't aligned with, I guess you we should say, with Microsoft's need to appear to always be moving forward, even though no one wants them to.



Anyway, if by some chance your printing to USB stopped working earlier this month, and you have not yet decided to tackle that problem, hoping maybe that August's patches would bring it back to life again, the trouble appears to surround the spontaneous creation of a duplicate printer instance where it, the duplicate, somehow obtains the proper configuration while upsetting the configuration of the original instance.  They have like this section of workarounds.



Microsoft says:  "Open the Settings app, navigate to Bluetooth and Devices, and select Printers and Scanners."  They said, "If there appears to be a duplicate installation of an existing printer, such as with suffix 'Copy1,' confirm if printing works for this printer."  They said:  "This printer should operate as expected.  If there's a need to use the original printer installation and not the duplicate" - the one which now works - "right-click the duplicate printer, select Printer Properties, and select the Ports tab.  Observe the port in use.  Now open Printer Properties on the original printer.  Select the Ports tab.  From the list displayed, select the port option in use by the duplicate printer."  Can you believe this, Leo?



LEO:  Observe the port in use.



STEVE:  Yeah.  So, okay.  So apparently, reading between the lines of this workaround, it sounds as though whatever it was Microsoft was attempting to so intended to create a new instance of a USB printer, copy the original instance's settings into the new instance, then presumably remove the original instance, and give the new instance the name of the original.



LEO:  Why?



STEVE:  I know.  Because Windows.  So it sounds like for some users...



LEO:  Such a kludge.  



STEVE:  It's a disaster.



LEO:  Oh, my god.



STEVE:  And this is what Microsoft is saying.  If you really to print - like, oh.  You want to print in Windows?  Well, here's how.



LEO:  Oh, my god.



STEVE:  So it sounds like the process got part of the way along and then died.  And it did not back itself out and revert to the original configuration, which was working before all of this began.  Right?  Like everything was fine.  USB printers were going.  And then you got a Copy1 which works.  The original one stopped working.  And they said if your application addresses printers by name, like oh, okay.  Well, the name it has broke.  Like we broke it.  So...



LEO:  So copy the name to a new printer.



STEVE:  Oh.



LEO:  Oh, my god.  This is why I do not answer printer questions on the radio show.  Right there in a nutshell.



STEVE:  Right.  In fact, Lorrie, I think it was early last week, she said, "Nothing's printing."



LEO:  Oh.



STEVE:  And I said, oh.  Because, I mean, and I had - I was right in the middle of like this really cool work on SpinRite.  And I just - or maybe it was after dinner, and I was ready to go back to it. 



LEO:  Oh, it's the worst, yeah.



STEVE:  I had my evening planned.  Well, there went my evening.  



LEO:  There goes your evening.  You're a good husband.  You're a good husband.



STEVE:  And she said, "Honey," like when it was all working again, she said, "How can anybody else do what you just did?"  And I said, well, you open the printer dialog.



LEO:  Observe it working.



STEVE:  And you observe the working printer.



LEO:  Oh, that's, in a nutshell, that's exactly what Lisa says.  It's what everybody I know says.  Probably what everybody who knows the people who listen to this show say, which is how does anybody use this crap?  Unless you've got somebody, an expert, a local expert.



STEVE:  There are people who go buy a new one, Leo.



LEO:  Yeah.  Most people.



STEVE:  They just go, okay.



LEO:  Most people.



STEVE:  Yeah.  It's like, well, you know, things began falling off, and finally I just decided to go to Best Buy.  You know?  Just, you know, they're not that expensive anymore.  I got one that's smaller and lighter and faster.



LEO:  Of course, and it's exacerbated by terrible printer drivers, I mean, multiple versions of Windows that don't...



STEVE:  Oh, Leo, yes.  In fact, I was going to talk about that.  I went to HP's site just to remind myself how many hundreds of megabytes.



LEO:  Yes, yes.



STEVE:  It's a quarter gig of printer driver.



LEO:  Yes, and you'd better get the right one because you - oh, god, it's a mess.



STEVE:  Yeah.  And the time before this last time when Lorrie needed me to fix her printer, I realized...



LEO:  You mean you've done this more than once?



STEVE:  Oh, yeah.  We got about six months out of the last round.  So that's pretty good.  So I realized you just need like ungodly patience.  It's like something is spinning, and just leave it alone.



LEO:  Don't touch it.



STEVE:  I'm so tempted to think, oh, it hung, or it's not going anywhere.  It's been 15 minutes.  No.  It needs hours.  And I don't know why.  But I just, I like, finally I just said, I just gritted my teeth, and I go, I'm just going to wait.  I'm going to just wait.  And then like after a couple hours, something changed.  It was like, oh.  Look.  And then when I just let it really take - and this is - we're running on a state-of-the-art NUC with like NVMe memory.  This stuff is fast.  Hers is the fastest machine in the house because of course I work on stuff you have to wind.



LEO:  No.  Lisa has the fastest machine in the house, too.  That's, again, we're good husbands.  We're good husbands, yes.  That's all I can say.



STEVE:  Oh, lord.  Anyway, this is, yeah, this is Windows.  And yeah, it is.



LEO:  It's life.  It's life as a tech guy.  This morning before I came to work I spent an hour and a half trying to get the Sonos stuff to talk to each other.  And literally the last thing I did before I came in to work was order a Vizio sound bar and say screw it.  No more Sonos.  All right.



STEVE:  Yeah, yeah.  And I've told the story about my wonderful realtor friend Judy, who went from what she called her "modem," which was actually a TI Silent 700.  She was a realtor, and so she would dial the phone and then take that olive green handset and stick it into the acoustic coupler cups on top of the thermal printer, and then she would like...



LEO:  Get the listings, yeah.



STEVE:  Yeah, get her scroll-y, that stinky thermal paper...



LEO:  It curls up, yeah.



STEVE:  ...roll that curls up, yeah.  And she would take that off and to a client.  Anyway, so we moved her, I was there when we moved her to Windows.  And, oh, it was just - she called, she didn't realize that the Internet was not Google, so she thought it was the same.  And she used to say to me, like when I would right-click, I'd say, oh, no, Judy, you've got to right-click.  What?  I said, you see how your mouse has two little ears up there, a left and a right ear?  You push the right one in order to get that little menu.  



LEO:  So, what, those are called ears?



STEVE:  Well, I had to help her along.  Because if I said button, she'd say, nope, there's no button.



LEO:  There's no button.  No, no.



STEVE:  Oh, and that used to happen, too, oh, with her husband Jan.  I'd say, Jan - because I'd give up talking to Judy.  I said, "Judy, put Jan on the phone."  And I'd say, "Okay, now, do you see that button?"  And he'd go, "What?"  And finally I realized we were both looking at the same thing, but he saw a rectangle with a shadow, and I was calling it a button.



LEO:  Yeah, that's right, a button.



STEVE:  And he said - I said, "No, okay, Jan, it's that rectangle where - see how it's shadowed to kind of look like it's 3D."



LEO:  Kinda looks like a button.



STEVE:  Oh.  "And what should I do with that?"



LEO:  We have all been there.



STEVE:  Oh, boy.



LEO:  We have - everyone who's listening is immediately identifying with this, when you start to describe stuff.  It's just hysterical.



STEVE:  Yeah.



LEO:  There's an old video, John Mayer, the folk singer, talking to his dad outside a concert.  And he's on the street talking to his dad, and he's going, "No, Dad, the start, the start button.  It's in the lower left."  And when you see that, you go, yup.  It's universal.



STEVE:  Well, and I remember saying to someone, I don't remember now who, but when I was trying to describe something by voice over the phone, I finally said, okay.  Here's the problem.  We do not share a vocabulary.  Like I cannot describe to you what has to happen here.



LEO:  Oh, it's not just a vocabulary.  It's a worldview.



STEVE:  Because I have to use terms...



LEO:  No, no, it's an entire different vision of the world and how it works.  It's we're in different planets.  I think there might even be a gene.  There's something.  It's just - and I'm not making a value judgment.  It's not good or bad.



STEVE:  No, no.



LEO:  But it's different, maybe it's different dimensions of, you know.



STEVE:  Well, and Judy finally asked me, she said, "How..."



LEO:  How is a very big question.



STEVE:  "...will people ever get this?"  And I said, "No, Judy.  You will die.  I mean, it's like the dinosaurs.  They're gone.  You will die.  Your inability to navigate with a mouse will go with you."



LEO:  And everybody will know.



STEVE:  The world will be full of people who do.  They've always had mice.



LEO:  Who knew, though?  Who knew?  The mouse is now going away.  It's all touch, baby.



STEVE:  Yeah.



LEO:  That's probably why.  Nobody could figure them out.



STEVE:  I've actually made the mistake, after using a machine with touch for a while, you start touching the screen of things that don't.



LEO:  Touch the screen.  Nothing happens.  Yeah.



STEVE:  And it's like you get a weird little ripple that kind of moves away from where you touch because you disturb the liquid crystal underneath.  But it's like, okay.



LEO:  We have a - Evanescence Photo has provided us with new album art.  Steve, I think you'll enjoy it.  Disaster Now!, ladies and gentlemen.  That's the name of the show.  Disaster Now!.



STEVE:  Oh, that's a...



LEO:  New album art.  All right.  I'll stop interrupting.  Go ahead.



STEVE:  Okay.  So the continuing saga of Windows VBA macros.  As yet another example of one of Microsoft's very poor early design decisions not aging well, and their refusal for many years not to simply do the right thing, we have the continuing saga and drama of Windows VBA macros.  Last Wednesday night...



LEO:  Okay.  Now wait a minute.  Now, when I left, this was all resolved.



STEVE:  No.



LEO:  No.  Okay.



STEVE:  No.



LEO:  All right.



STEVE:  Last Wednesday night they confirmed, Microsoft confirmed that it was resuming the rollout of their plan which they first announced earlier this year, back in February, which is when we threw the party.  But then that announcement back then was greeted with great relief by everyone who understood what it would mean for the security of Microsoft's much-abused Office documents.  After years of head-in-the-sand policy, Microsoft would finally be blocking the execution of remotely received VBA macros by default across most Office apps.



Predictably, this would break some things.  Which of course explains Microsoft's reticence to do the right thing sooner.  We've never really talked about the pushback against this change, but I came across some interesting bits which address that.  Even though Microsoft declined to provide information about why the effort had been paused, several experts said customers complained about this new, well, this change.  It's not really a new feature.  It's a change.



Michael Tal, the technical director for Votiro, which is a company specializing in malicious content filtering in the cloud, he told The Record that he works closely with partners in the banking and financial sectors and explained that macros play "an integral part of our clients' business workflows."  He said that the initial block caused a "massive hindrance on business productivity."  Basically, the recession that we're heading into was caused by Microsoft's decision - no.  Okay.  What happened was that something changed.  Just as Microsoft warned everyone it was going to back in February, when they said to get prepared.  They told everybody in February we're going to do this, so everybody should, like, do what you need to because macros are going to stop running by default the way they have been.  Well, guess what?  It turns out Microsoft wasn't kidding.  



Michael Tal explained, he said:  "Macros are a powerful tool in the financial sector, as they are used to create robust financial modeling, calculate loan interest, automate repetitive, labor-intensive tasks.  They're recorded sets of actions which can be run to save time and labor.  It's also used to simplify budget forecasting and makes a difference in a day-to-day workload of any entity who's using it as it speeds up the process to generate a task after finalizing the creation of the macro and setting the variables."  Yeah.  Right.  And you're going to have to now press a button to make that happen rather than not.



Anyway, he added that while he understood Microsoft's desire to combat malware like Emotet, Trickbot, Qbot, and Dridex, they should have come up with a more creative approach - so he's complaining, right, about increased security - a more creative approach to deal with legitimate business use cases for macros and allow for continuity without compromising security.



Okay.  So it's not as if macros have been stripped out of Office tools and are gone.



LEO:  No.  No, no, no.  Just a property now.



STEVE:  Yes.  They simply no longer run without provocation.  You just need to click a button to explicitly permit their use.  And, you know, as we've seen, you showed two weeks ago, Leo, that flowchart.  I have it in the show notes here again.  We've seen that crazy flowchart that enterprise-wide group policy settings can be made to cause it to, like, no behavior change.  Decision box 4a in the flowchart is "Cloud policy to block?"  You set it to "no."  Then decision box 4b gets control, which is titled "ADMX or Group policy to block?"  That could be used to enable macros at that point so behavior doesn't change.



LEO:  It's just really a question whether you want to fail secure or fail insecure.  Right?



STEVE:  Right.



LEO:  What the default is.



STEVE:  Exactly.  And so basically everybody's having a conniption because they want the insecurity of allowing unsolicited, unsigned, documents of unknown provenance to be received by email or clicked on on a web page and just have everything work.  At the same time, they want Microsoft to somehow magically not allow malware to do the same thing.  Well, folks, sorry about that.  You know?  I mean, all the enterprises would have to do if they wanted that was to sign the document.  Signing them is another way of immediately...



LEO:  There's all sorts of things, yeah.



STEVE:  Yes.  This is a decision tree where most of the outputs of the tree are macros enabled.  There's only one red box that has them blocked by policy.  And Microsoft added one at the end.  If you get through step after step after step after step after step, there are five steps, and you still haven't reached a decision, it used to just be okay, fine, let's go.  Now it's okay, wait a minute.  Let's make sure this is what you want.  Are you absolutely sure?  And that was too much to ask.



LEO:  So often security takes a back seat to the stupidest user.  Right?  And unfortunately...



STEVE:  Yes, the lowest common denominator.



LEO:  ...that's the guy in the corner office.  And because he signs the checks, he's the guy who gets to say to the IT guys, "I like this behavior, Simmons."



STEVE:  You know?  So how can any moron think that it's a good idea to allow macros to run unbidden in a document received through email?  



LEO:  Well, you have to press the Allow button.  Come on, it's not completely unbidden.  You have to say its name three times.



STEVE:  Oh, Leo.  Again, it's just, I mean, so I get it that Microsoft doesn't want to offend or upset or ruffle any features.  These people should just be ignored.  I mean, Microsoft warned the industry, the world, in February, this was coming.  It finally happened, and so oh, my god, it's the end of life as we've known it because our macros don't run themselves.



LEO:  So what's the current status?  Because it's been back and forth and back and forth.



STEVE:  The current status is, quote, this is what they said:  "We're resuming the rollout of this change in Current Channel."



LEO:  Good.



STEVE:  "Based on our review of customer feedback, we've made updates to both our end-user and our IT admin documentation to make clearer what options you have for different scenarios."



LEO:  Good.  So get off your butts, set the Group Policy if you want it to be different.  But we're going to default secure.



STEVE:  Yes, exactly.  If you insist on allowing all the malware, you know, Follina used Office macros in May to infect a whole bunch of people.  And if you want to be admissive of all of this malware, by all means.  Just set your document policies.  Set Group Policy.  But the rest of us have decided, okay, it's an inconvenience.  We're going to have to actually act a little more securely.  Fine.



LEO:  Good.  I actually think signing the macros is a great idea.



STEVE:  Yes.  That's a win.  Just somebody is building these documents and has all this financial, tricky financial stuff in it.  Just give him a certificate.



LEO:  Yeah.



STEVE:  Have him sign it.  And it'll just - it moves like grease through a goose.  Not a problem.  Speaking of which, let's take a break.



LEO:  I have some grease for your goose.  I've been waiting for you to ask for it.



STEVE:  And in another bit of happy news, I mean, I don't want to draw any great conclusions here and suggest that maybe Microsoft is actually finally getting a clue, but Windows 11 now blocks RDP brute force attacks by default.



LEO:  Wow.



STEVE:  Which is astonishing.  Last Thursday Microsoft's VP for Enterprise and OS Security, Dave Weston, he tweeted from @windowsinsider, he said:  "Windows 11 builds now have a DEFAULT" - and he, I mean, he understands.  It's all caps in his tweet - "DEFAULT account lockout policy to mitigate RDP and other brute force password vectors.  This technique," he says, "is very commonly used in Human Operated Ransomware and other attacks."  He says:  "This control will make brute forcing much harder, which is awesome."  Now, everyone listening to this podcast is acutely aware of the importance of default settings.  There may as well not even be any settings since the default is almost universally what is left to apply.



"The other thing everyone listening to this podcast knows is that the inherent insecurity created by Microsoft's remote desktop protocol being placed out onto the public Internet without any sort of brute force credential-stuffing protection in place for years has been insane.  It's been responsible for untold numbers of remote network intrusion, pain, and loss to Microsoft's users.  In fact, the FBI said RDP is responsible for roughly 70 to 80% of all network breaches."



LEO:  What?  Oh, my god.



STEVE:  70 to 80% come through Remote Desktop Protocol.



LEO:  That's incredible.



STEVE:  Because there's no...



LEO:  There's no protection.



STEVE:  They're unmonitored.



LEO:  Yeah.



STEVE:  Right?  Yeah.  So, David's news was incredibly welcome. But my jubilation was somewhat tempered when I saw the Local Group Policy Editor settings that he was announcing and celebrating.  It is true.  Windows 11 now has its failed login attempts account lockout triggering after 10 invalid logon attempts.  But the lockout duration is only 10 minutes.



LEO:  Oh, come on.



STEVE:  I know.  And the failed attempts counter also resets every 10 minutes.



LEO:  I guess that could be enough for the random brute force guy.  But not for targeted attacks.



STEVE:  Right.  Could someone explain to me how any legitimate user of remote desktop protocol - whose RDP client has probably memorized and stored their logon authentication for them anyway, which makes it automatic for the user, they don't even have to do anything - how any legitimate user is going to be inconvenienced by a lockout which doesn't engage until they have somehow failed to properly authenticate themselves 10 times?



Since, as we've seen, Microsoft clearly goes to great lengths to never inconvenience any user in the name of security, they must believe, as I do, that no one but an attacker would ever trip the "10 strikes before you're out" rule.  So why, then, give an attacker a clean slate 10 minutes after those 10 failed attempts?  So, okay, I get it.  I guess baby steps; right?  So let's just hope and pray that the error message returned by the RDP endpoint is the same for a failed logon attempt as for a block by policy.



Just, please, Microsoft, just say your authentication failed.  Don't tell them that a policy has been switched on, which is why authentication has been blocked.  Because of course that would allow them to probe the endpoint to figure out what the timeout is and what the number of failed attempts counter is and so forth.  Just tell them nothing.  Let the bad guy believe that every single one of their attempts is still hitting a wall.  And so that they're not able then to throttle themselves and slow down.



Anyway, at least now we have the concept in place of default mitigation against brute forcing unmonitored authentication endpoints.  That's some progress.  And really that's the problem, right, is that almost nobody knows that their security is being pounded on by a credential stuffing attack 24/7.  You know, everyone just sort of goes, oh, that's just, you know, Internet noise; right?  Well, in fact I coined the term "Internet background radiation," just these packets flying around, just like doing stuff.  It's like, yeah, yeah, we just ignore that.  It's like, well, except that it's pounding at you, trying to get into your network.  And as the FBI said, 70 to 80% of ways in is that.  So worth taking note.



As I mentioned at the top of the podcast, this is our final podcast for July as we're going to be heading into August.  August traditionally contains a few interesting security events.  We have the two traditional major hacking conferences, Black Hat and DEF CON.  This year, running from the 6th through the 11th, is the 25th Black Hat USA.  It'll be holding, as a consequence of the changes that COVID has wrought, a hybrid conference allowing the cybersecurity community to choose how they wish to participate.  The conference's first four days will be virtual trainings conducted in real-time online, with all instructors accessible throughout each class.  So virtual online real-time and interactive.  Then the final two-day main conference will occur both virtually online and live in-person in Las Vegas at the Mandalay Bay Hotel and Casino.



Then, as always immediately following Black Hat, is DEF CON, August 11th through the 14th.  And I had to smile and just sort of shake my head as I was checking in on DEF CON's description of itself and its planned proceedings.  Since I think everyone will get a kick out of what they wrote, I'll share what they had to say.  They said:  "Started in 1992 by the Dark Tangent, DEF CON is the world's longest running and largest underground hacking conference.  Hackers, corporate IT professionals, and three-letter government agencies all converge on Las Vegas every summer to absorb cutting-edge hacking research from the most brilliant minds in the world and test their skills in contests of hacking might.



"DEF CON comes right after Black Hat, a conference and trade show for cybersecurity professionals.  While Black Hat feels more like a traditional Vegas trade show, DEF CON is anything but.  How is it different from other conventions?  Well, first, DEF CON is run by volunteers, and it has no corporate sponsorship.  Secondly, there is no online registration, so even the organizers really don't know who is attending.  When you arrive, everything is paid for with cash.  They don't take credit cards.  Most of these people attending really don't want a record of them being there.  Everyone from your average everyday hacker to criminals and agents from government agencies like the FBI, CIA, and NSA will attend.  When you enter, you pay $280 cash, and they hand you a generic badge.  No ID is required for admittance."  So those two conferences begin...



LEO:  But then you may be subject to Spot the Fed.  Right?  That's part of the game; isn't it?



STEVE:  Exactly.



LEO:  So grow that crew cut out.



STEVE:  And we are also celebrating Security Now's 17th birthday, Leo.



LEO:  Today?



STEVE:  No.



LEO:  Oh. 



STEVE:  On August 19th...



LEO:  Oh, good.  I have time to bake a cake.  Good, okay.



STEVE:  We have some time.  You have time to fully recover.  August 19th, the anniversary of the first episode of this podcast.  This coming August 19th we'll be finishing out our 17th year, and we begin into year 18.



LEO:  Wow.



STEVE:  So Leo.



LEO:  You're an adult.  



STEVE:  It is almost at adulthood.



LEO:  You can drink.



STEVE:  You've almost reached maturity.



LEO:  No, you can't drink.  But you can join the army and shoot a gun.



STEVE:  I do like my cabernet.  



LEO:  I think you have to wait till 21.



STEVE:  I think so.



LEO:  In most states, yes.



STEVE:  Yes.  I remember Mom, after her divorce, was dating someone.  And when she was going out she'd set us up with a frozen pizza which she'd quickly bake.



LEO:  Of course, yes.



STEVE:  And glasses of red wine that were 2:1 watered down.  I got my start early.



LEO:  That'll keep them - that'll get them to sleep.



STEVE:  And the point is I remember the guy that she was seeing coming over and looking at the kitchen counter, and here were two eight and six year olds...



LEO:  With wine glasses.



STEVE:  ...with red wine.  Yeah.  We're liking this dinner a lot.  Have a nice time.



LEO:  Have a good time, Mom.



STEVE:  That's right.



LEO:  My mom didn't do that, but she did give us TV dinners.  And I still have a taste for the weird cardboard-flavored corn.



STEVE:  I do, yes.  It was a weird - they were sectioned off.



LEO:  Yeah, they were sectioned off.  But the flavors combined somehow.



STEVE:  Yes.



LEO:  Everything tasted the same, yeah, yeah.



STEVE:  The peas sort of had a, what was that...



LEO:  Apple pie mixed in with them.  And there was the fried chicken.



STEVE:  Salisbury steak.



LEO:  Salisbury steak, oh, the gravy would spill over, yeah.



STEVE:  Yeah, the gravy and the Salisbury steak, that's right.



LEO:  Oh, god.



STEVE:  And we used the term "steak" very lightly in this context.



LEO:  They don't still sell TV - does Swanson still sell TV dinners?



STEVE:  Why would they have stopped?



LEO:  Good guess.  I don't know.  I don't know.



STEVE:  Microsoft still has printing problems.



LEO:  Yeah.



STEVE:  So I think Swanson's probably still selling TV dinners.



Last Friday afternoon I posted to the grc.spinrite.dev newsgroup under the subject "It's Alive!"  As we know, I essentially had to take SpinRite completely offline and down to perform the degree of surgery that was needed, not only to completely strip out all of SpinRite's traditional dependence upon the BIOS, but to also, as I've explained during this journey, to completely re-architect SpinRite around a data-recovery-centric, device-independent mass storage device abstraction so that not only can SATA and IDE drives connected with AHCI and PCI bus mastering adapters now be communicated with at their lowest possible hardware level, but also so that the next step in SpinRite's evolution, which will add similar direct access for USB and NVMe devices, and whatever else might show up in the future, will be able to support plug-in drivers essentially, without needing, any, again, any similar reworking.  I've done all of that work upfront.



I'm mentioning this because I can finally report that SpinRite is beginning to come back to life.  Humpty Dumpty is getting its pieces reassembled. 



LEO:  Woohoo.



STEVE:  It is starting to run again.



LEO:  Yeah.



STEVE:  It is by no means ready yet.  I don't want to give anyone that mis-impression.  I still have lots of work left to do because the surgery that SpinRite needed broke virtually every assumption that it was originally built upon, assumptions which were made back in 1987 when we had 4.77 MHz Intel 8088 processors with a maximum of 640 Kbytes of RAM, and a 20MB hard drive was a luxury.  Actually, Leo, in the show notes I snipped out the first three lines of the sr.asm file.  I wasn't sure whether it was '86 or '87.  Anyway, the header of the file says file:sr.asm by Steven M. Gibson, created 03/30/87.



LEO:  That's history right there.  That's great.



STEVE:  That's the creation of the first SpinRite dot ASM file.



LEO:  Wow.  Did you have a macro to do the +---+?  Or did you type that by hand?



STEVE:  No, no, no.



LEO:  I figured you'd have a macro.



STEVE:  That was Brief.  I don't - oh, back then?



LEO:  Oh, of course, yeah.



STEVE:  Actually that was before I hired - Steve Rank was my first non-security or my first techie employee.  I hired Sue.  And Sue's been with me for I think, like, 36 years or something.  Steve was employee number two.  And one of the first things I had him do just to sort of get his feet wet was to write this massive macro package for Brief.  And, I mean, it did everything I could ever ask it to do.  And so I was able to like snap text boxes with a beautiful line drawing characters around.  And so this is actually an ASCII-ized version of that.  Because I later wrote a conversion of the beautiful line drawing characters back into ASCII when I - actually it was recently, it was a couple years ago because I needed to be working, I had to give up Brief because it was 16-bit code, and I would have had to run in a VM, and it was just not worth the hassle.  So now I'm doing everything...



LEO:  You used Brief for almost as long as I knew you, though.  I mean, I remember you were the Brief guy.



STEVE:  Yes.



LEO:  By the way, I looked more through that listing, that assembly language listing from 1987.  And buried deep within it I found this, which I thought was kind of surprising.  There is a Salisbury steak.



STEVE:  Oh, it is, Salisbury steak.



LEO:  Now, I don't know if you got the free safety-colored rain poncho.



STEVE:  It looks like a brownie in the middle, there, Leo.



LEO:  It's gingerbread or some - and then remember those mashed potatoes that were stiff, and they were crusty, and oh, yeah.



STEVE:  We were, Leo, we were hungry.



LEO:  We were.  We'd eat anything.  That glass of wine didn't hurt, though, you know, with the appetite.



STEVE:  Yeah, that helped to wash it down, that's for sure.



LEO:  It's so great that you - so you posted that source code, or no?  Is that private, that source code?  The original assembly language?



STEVE:  I will ultimately release it to the world.



LEO:  Oh, nice.



STEVE:  Because the world ought to have it.  And I will do that at a point when I am no longer maintaining SpinRite.



LEO:  Somebody can fork it, then, which would be cool.



STEVE:  Yeah, exactly.  At some point I'm just going to say, okay, I'm...



LEO:  It's yours.



STEVE:  Hopefully it'll be at a point around where we said goodbye to Jerry Pournelle, somewhere at that level.



LEO:  Yes.  I can't wait to have Steve Gibson come on the show and talk about the good old days.



STEVE:  How those things used to spin.  They actually spun.



LEO:  They spun.



STEVE:  Why would you spin something like that?



LEO:  Thank you, Steven "Tiberius" Gibson, for joining us for Episode 20,000.  Anyway.



STEVE:  So anyway, everything since then has changed.  None of those assumptions, they no longer hold, but they had been allowed to remain in place, even through SpinRite 6, though like with all things Microsoft they were becoming quite old and creaky.  So as everyone knows, 6.1 is not a patch to SpinRite 6.  Even though it's a minor version bump and therefore a free upgrade for everyone who owns 6, I am making this multiyear investment in SpinRite's future today because I've seen the future.  And to my utter amazement, SpinRite is still in it.



LEO:  Right on.



STEVE:  You know?  So mostly I'm looking forward to writing the code beyond 6.1.  But we have to have 6.1 first to create the platform for that.  So the point is it's alive.  I'm sort of astonished.  I mean, like I actually have it running.  And now I'm going through and like finding weird little bits because among other things, it used to be 16-bit code, which means that I'm having to write within segments which are 64K.  And SpinRite used to fit all in one segment.  That is, all of its data and all of its code was in 64K.  But with maybe SpinRite 5 I think is where it would no longer fit.  I needed to add things, and there was nowhere for it to go.  So I created a second segment.  But it's a pain to cross segment boundaries, for lots of reasons.



Anyway, so one of the things that I've been doing is I've been moving chunks of SpinRite over into this secondary segment, just so make some elbow room for the future.  So anyway, it's coming back to life.  I'm finding, the reason I mention that, is that there were like weird little side effects of code running in a different segment that thinks its data is in the same segment as it is because that's always been true.  But it's no longer true.  And so there are side effects of that.  So I have a ways to go.  But we're getting there.  And I'm asking questions in the newsgroup like should the log files still use the line drawing characters, or should when we log, should I translate everything into ASCII?  And everyone agrees it should all be moved over to ASCII because no one's viewing the log files in DOS anymore.  They're viewing them in their own operating systems.  So maybe UTF-8.  We'll see.  But anyway, progress on that front.



LEO:  Woohoo.



STEVE:  Also pfSense and Tailscale.  Everyone knows that pfSense is my preferred Internet firewall router solution.  It's open source, has a fully capable free community-supported release. It's rock solid, runs on most hardware, inducing little fanless consumer routers like my little favorite, that Netgate SG-1100.  And it has a very comfortable web-based UI for configuring it.  And it'll do anything you could want.



As I mentioned, I use it to glue my various locations together over permanent links to essentially run a single, what we would now call an "overlay network."  But I did it like the old-fashioned way before we had easy-to-use overlay networks.  Which brings me to Tailscale.  Among pfSense's many features is a modular package management system which makes managing the router what I would call "manpage-free" pleasure.  You don't need that.  You know, you just point and click, and things go.



LEO:  Everything should have a package manager.  That's just the best, yeah.



STEVE:  Yes, yes.  It's just it makes life possible.  So the news I wanted to share is that pfSense will soon, with its forthcoming v22.05, which is now the dev release, when it goes into the main channel it will be receiving built-in drop-menu selectable support for the Tailscale VPN mesh overlay network.



LEO:  Nice.



STEVE:  That's the way to do this.  You missed somebody last week, Leo, commenting that - we were talking, remember, the week before we were talking about using a VPN in a caf or somewhere.  Someone mentioned just bring a little box that has a Raspberry Pi on it running Tailscale.  And essentially you can simply plug into that and be on your home network, wherever you are, thanks to an overlay network, which works very much the way Hamachi did back in the day.  Hamachi was the first overlay network to become really popular.



LEO:  So that's kind of cool.  So like Hamachi you'd be appearing to yourself to be on your home network.



STEVE:  Yes.



LEO:  Would all the other home network features be available, as well?



STEVE:  Yes.



LEO:  Oh, that's nice.



STEVE:  It'd be like you were plugged into your router at home.



LEO:  That's very cool.  I like that.



STEVE:  And Tailscale is built on WireGuard, so it uses that state-of-the-art minimal next-generation VPN, which is the improvement over OpenVPN, which very much like OpenSSL has just gotten too long in the tooth.  It's got so much crap in it now that's no longer needed.  So WireGuard is a clean rewrite.  Tailscale runs on top of it.  It's all free.  It does automatic key rotation, NAT traversal, single sign-on with two-factor authentication.  These guys have it nailed.  And shortly you'll be able to do a point-and-click in order to install it under pfSense.  So I wanted to give everyone a heads-up about that.



And lastly, two little bits of Closing the Loop.  Someone whose Twitter handle or his Twitter name is "Dangerously Close to Hijinks" - apparently you can use long names in Twitter.  He said:  "Thanks for all you do.  Would you share the software solution you use for grc.sc?  Do you recommend it?"  And then before I could reply, he sent me another follow-up.  He said:  "Thanks to you and Elaine, I found the reference for URL shortener YOURLS in Episode 858." 



So yes, yourls, Y-O-U-R-L-S, dot org, is short for "your own URL link shortener," yourls.org.  And I love it.  It's what I use.  It is a tiny little PHP library so you can bring it up on any system that has an SQL server.  He likes MYSQL 5 or later.  PHP 7.4 or above.  You need mod_rewrite enabled if you want to mess around with the API.  It runs on pretty much any, you know, certainly Apache, Nginx, Cherokee.  I run it on IIS.  It does need HTTPS support.  Anyway, it's a very - all it is is what it does.  There's an admin panel.  You're able to drop in a long URL, tell it what you want the short one to be.  It saves it to the SQL database.  And then when someone comes in with that short URL, it just issues a 302 redirect to the long one.  And for a while I was using bit.ly, but I was using episode numbers as a convenience to people.  And of course, naturally, somebody began...



LEO:  Somebody stole it.



STEVE:  ...grabbing the episode numbers in advance of my use of them.



LEO:  There's always one - this is why we can't have nice things. 



STEVE:  This is why we can't have nice things. 



LEO:  So is it going to be - you can do your custom domain, too; right?  So it'll be grc.com/?



STEVE:  Actually, since I had sc for shortcut, it's grc.sc. 



LEO:  Oh, that's nice, slash whatever.  And you can...



STEVE:  Slash whatever.



LEO:  That's great.  And no one else can do it because - yeah.  Good.



STEVE:  Because it's just mine, yeah.



LEO:  It's yours, yeah.  I used bit.ly the other day to create our shared photo library for the cruise.  And I was thinking, why am I not running my own URL shortener?  So now you've pointed me in the right direction.  



STEVE:  Yes.  And twit.sc, if it's not already gone.



LEO:  Yeah, yeah.



STEVE:  In fact, [crosstalk] I just said that.



LEO:  We have TWiT dot - no, no.  We own TWiT.to, so...



STEVE:  Perfect.



LEO:  Yeah, that's a good one, yeah.



STEVE:  Nice, nice.  Yeah, because it wants to be short, too.  And I just love this little bit of humor from @biswbmatt.  He said:  "In regards to Episode 880," he said, "IPv6 is the technology of the future.  And it always will be."



LEO:  That is a great line.



STEVE:  Isn't that great?  I love that.



LEO:  Wow.  That's good.



STEVE:  Something came up, we were talking about - oh, I know what it was.  It was, Leo, the price of an IPv4 IP address is, I've forgotten now what it was.  It doubled in a year.  I think that's what it was.  It's now 25 to $30 per IP because, again, no one wants IPv6.  They'll pay whatever the going price is for IPv4, even if it's seeing crazy, it was like exponential price increase over the past few years.  So again, IPv6 is the technology of the future, now and forever.



LEO:  I'm ready for the topic of the moment.



STEVE:  I probably should have titled this podcast "The Demon Cube."  



LEO:  Oy.



STEVE:  So the MV720 is a tiny cube measuring about an inch by an inch by an inch.  I've got a picture of it in the show notes at the top of this topic.  And if someone were to tuck it under your car's hood without your knowledge, plugging it into your car's wiring harness, you would be hard pressed to know that anything was out of place.  In fact, if your car's authorized service people were working under the hood, they, too, would likely pass it off as just some "supposed to be there" relay.



LEO:  It's just a little plastic box that sits in between the connector and the plug.



STEVE:  Yup.  And this innocuousness would be by design since the manufacturer of this sneaky little cellular radio-equipped GPS satellite monitoring and vehicle control overriding demon boasts at the top of its web page, below the title "Easy to Hide," it says that "MV720 looks like a relay, but is actually a locator."  So the question is, who put it there, and why?  Since this thing only costs - now, in my notes I wrote 20 bucks, but later I looked on Amazon, and I found it for $26.  So it could be anyone who put it there who has reason, legal or otherwise, to want to monitor and track a vehicle's location and speed while having the option - get this - to remotely shut down the motor's flow of oil and gas, causing the vehicle to gradually slow down to a point where the engine can be shut off and disabled.



LEO:  Ow.



STEVE:  All remotely.



LEO:  What?



STEVE:  And now, wait till you hear how insecure this is.  I mean, it's one thing for your light switches and plugs to, like, have crappy security.  This thing probably takes the cake.



LEO:  So it's really, it's not a tracker, it's a cutoff valve.



STEVE:  Yes.  That, too.



LEO:  It's a kill switch and a tracker.



STEVE:  Yes.  Yes.  Yes.



LEO:  And it's only $26.



STEVE:  And, yes, a single click on Amazon.



LEO:  Holy cow.



STEVE:  So again, the question is, who put it there and why?  These things exist.  And while I'd be happy...



LEO:  Well, you know, I mean, there's a legitimate use like sometimes now when you buy a car, if you don't make your payments, they've got a kill switch in the car so they don't have to send the dog after you, bounty hunter to get you.



STEVE:  Instead of the repo man.



LEO:  Yeah.



STEVE:  Right, right.



LEO:  You just kill the car.  And then they have the GPS to know where it is.  So I imagine that's the market they thought they were going after, anyway.



STEVE:  Right.  Well, and unfortunately they're getting more than they bargained for.  These things exist.  While I'd be happy to be talking about them if only for the sake of noting their existence, as you and I just have been, Leo, they wouldn't normally rise to the level of being a headline topic for this podcast.  So our long-term listeners can probably see where this is headed.



What do we know about this thing's manufacturer?  Okay?  MiCODUS (M-I-C-O-D-U-S) is a Shenzhen, China-based manufacturer and supplier of automotive electronics and accessories.  The company's main products are asset, personal, and vehicle GPS trackers.  MiCODUS devices are available for purchase via Amazon, AliExpress, eBay, Alibaba, and other major online retailers.  And as I said, I found one on Amazon for $26.  If you just put in "MV720" into Amazon, up it comes.  There's some other reseller is the one that Amazon lists, but it's the same device.  You can just tell by looking at it, and all the features are identical.



So in addition to GPS devices, the company provides a cloud-based platform via web, iOS, and Android apps for remote management, fleet and asset tracking, as you were suggesting, in fleet mode and vehicle-specific applications.  MiCODUS states it provides "a secure, open and scalable platform that plays an essential role in the optimization of resource utilization by enabling visibility and simplifying management."  And of course everyone assumes.  They said it's secure.  Oh.  Okay.  Then we don't have anything to worry about.



The security vulnerability research firm BitSight took a look at this little device's security.  BitSight chose the MiCODUS MV720 because it's the company's least expensive model with fuel cut-off capability.  As we'll see in a minute, it's a cellular-enabled GPS tracker which uses a SIM card to transmit status and location updates to supporting servers and to receive SMS commands from its user.  And, unfortunately, also from pretty much anyone else.  As I'm sure no one is going to be surprised learning by what they found.



They found six vulnerabilities of a severity up to, two of them, CVSS of 9.8.  If there were only a couple of these little one-inch cubes wandering around the planet somewhere, hopefully mainly in China, that would not represent a clear and present danger.  But 1.5 million of just these particular models, these little demons, are currently present in vehicles located throughout 169 countries.  Later down in the show notes I have a heat map showing where they are.  They're present in the vehicles used by several Fortune 50 firms in the U.S., also by European governments, and by state government agencies in the U.S.  There's a South American military agency that is employing them, as well as a nuclear plant operator.  Given the tracking power...



LEO:  Oh, well, what could possibly go wrong there?



STEVE:  Uh-huh, and the ability to remotely cut off a vehicle's fuel supply, multiple security vulnerabilities become worrisome.  Okay, now...



LEO:  There's a lot of them in China.  That's really where they mostly are; right?



STEVE:  Yes.  They are largely, fortunately, in China.  So, good.  Unfortunately, not all of them.



LEO:  Yeah.



STEVE:  We know that mistakes happen, right, and that anyone can make a mistake.  That's my mantra on the podcast.  It's like, it's okay.  It's how you deal with the mistakes, how you own up to them and fix them that matters.  And this is where things go from worrisome to worse.



On September 9th of last year, 2021, BitSight initiated contact via the only email available on the MiCODUS website, sales@micodus.com.  MiCODUS replied, asking for additional information to pass on to the MiCODUS sales department.  BitSight requested a security or engineering contact.  MiCODUS never responded to that request.  So they waited until October 1st, again contacted MiCODUS using the only email address they had, and again requesting to speak with a security or engineering contact.  This request was refused.  Then MiCODUS contacted BitSight.



LEO:  I'm sorry.  It's Russia.



STEVE:  It's Russia.



LEO:  Oh, oh, thank you for the correction.  You're right.



STEVE:  Yeah.  You're right.



LEO:  Wojo said that's Russia, baby.  It's actually green in China.



STEVE:  Yeah.



LEO:  The Russians love them.  Is great.



STEVE:  We love them, Demon Cubes.



LEO:  We put this everywhere.  We put them everywhere.  Is great.  KGB love Demon Cube.



STEVE:  So after nine days, BitSight did get email from MiCODUS.  This was on October 10th, and I love this, claiming to be "working on the issues," despite the fact that BitSight had not yet shared any technical information with MiCODUS about what the problems were.  So don't worry.



LEO:  They don't know.  They don't know.  Nobody does.



STEVE:  We got it.  We're working on it.



LEO:  Yeah, it's a mystery.



STEVE:  We're going to, like, fix the security that you apparently are trying to tell us about, even though we haven't yet - we haven't let you do so yet.



LEO:  I knew that.  I knew that.  What makes you think I didn't know that?  I knew that.



STEVE:  So a month goes by.  That was October 10th.  Now on October 23rd BitSight made another attempt to contact the vendor.  MiCODUS did not respond.



So now, that was November, and toward the end of November.  They let December go by, and most of January.  Mid-January, on the 14th, BitSight shared its research - and you'll see why in a minute because, I mean, this is worrisome - shared its research and findings with CISA to further its efforts, thinking, okay, maybe the U.S. government security agency can get a response from these clowns.  BitSight requested CISA engage with the vendor and share the information.  No luck.



On May 1st of this year, CISA again attempted to contact the vendor to share information.  CISA established a connection with the vendor and shared the original research and findings.  However, CISA has not heard from the vendor since it shared the research.  So that was on May Day.  Nothing.  May goes by.  June goes by.  First half of July goes by.  Nothing.  On July 19th, after reasonably exhausting all options to reach MiCODUS, and given the lack of engagement from the vendor, BitSight and CISA collectively determine that these vulnerabilities warrant public disclosure.  So CISA and BitSight decided to publish the research.  That's what happened on the 19th, exposing all 1.5 million of these little demon boxes, the Demon Cubes, to immediate compromise because none of these problems have been fixed.



So what do we and now the entire rest of the world know about these 1.5 million insecure Chinese vehicle tracking devices operating throughout 169 different countries, mostly in Russia?  Gee, I wonder if somebody could take advantage of that?  Hello, Ukraine.  Anyway.



LEO:  Oh.  Maybe there's a bright side to all this.  Oh, interesting.



STEVE:  "BitSight discovered six severe vulnerabilities in the MiCODUS MV720 GPS tracker" - this is them writing - "a popular automotive tracking device designed for vehicle fleet management and theft protection for consumers and organizations.  The MV720 is a hardwired GPS tracker allowing for external physical control of the device," device meaning vehicle.  "In addition to GPS tracking, the MV720 offers anti-theft, fuel cut-off, remote control, and geofencing capabilities."  You drive too far, your  car stops.  "The exploitation of these vulnerabilities" - I wonder if you have to, like, then push it back within, you know, inside the geofence.



LEO:  Just a few feet.



STEVE:  And you can then use reverse in order to get back inside the working area.  "The exploitation of these vulnerabilities," they wrote, "could have disastrous and even life-threatening implications.  For example, an attacker could exploit some of the vulnerabilities to cut fuel to an entire fleet of commercial or emergency vehicles."



LEO: Oh my god.  Oh my god.



STEVE:  "Or the attacker could leverage GPS information to monitor and abruptly stop vehicles on dangerous highways.  Attackers could choose to surreptitiously track individuals or demand ransom payments to return disabled vehicles to working condition."



LEO:  Yup.



STEVE:  "There are many possible scenarios which could result in loss of life, property damage, privacy intrusions, and threaten national security."  And wait till you hear, Leo, where there's a non-password protected command that allows, believe it or not, allows you to tell the Demon Cube that its command-and-control server's IP has changed to something else.  So now it won't ever try to actually even...



LEO:  Oh.



STEVE:  I know.



LEO:  That's devious.



STEVE:  You could permanently tell it that now you're in control of it.



LEO:  Hijack it.



STEVE:  Yes.



LEO:  Oh, my gosh.  Holy moly.



STEVE:  So they wrote:  "BitSight's research was conducted with the sole purpose" - just to put everyone's mind at rest, and unfortunately now it's all public, right, because they had no choice - "the sole purpose of assessing the security of the MV720 GPS tracker and to determine whether an attacker could access a user's GPS position."  Oh, boy, can they.  "Although the results surpassed the proposed initial goal, this report does not represent a full security audit" - I mean, like they just thought, okay, we've done enough - "of the MiCODUS ecosystem.  However," they wrote, "we believe other models may be vulnerable due to security flaws in the MiCODUS architecture.  MiCODUS states there are 1.5 million of their GPS tracking devices in use today by individual consumers and organizations.



"Organizations and individuals using MV720 devices in their vehicles are at risk.  Leveraging our proprietary data sets, BitSight discovered MiCODUS devices used in 169 countries by organizations including government agencies, military, law enforcement, as well as businesses spanning a variety of sectors and industries including aerospace, energy, engineering, manufacturing, shipping, and more.  Given the impact and severity of the vulnerabilities found, it is highly recommended that users immediately stop using or disable any MiCODUS MV720 GPS trackers until a fix is made available."  And once again, children, $26 on Amazon.  You can have one of your own.



LEO:  So Eric Duckman in our chatroom has analyzed the wiring diagram.  He says it is a relay, so you have to put it in between your power supply and your oil pump or your gas pump.  You have to put it somewhere where it would have that cutoff capability; right.



STEVE:  Right, right.



LEO:  So that's what to look for, kids, if you just wanted to check.  You might want to.



STEVE:  See if you have a Demon Cube in your car.  Okay.  So they said:  "Through packet and traffic analysis observed between the website, the Android application, GPS trackers, and servers, BitSight determined that the MiCODUS architecture is organized as follows:  All services appear to be hosted by a single server, www.micodus.net, which is at IP address 47.254.77.28.  It provides a website via HTTPS, therefore port 443."  So that.  That's the website portal.



"Also an unencrypted API server to support the mobile apps via HTTP port 80" - that's at app.micodus.net, same IP - "and a GPS tracker custom protocol server running on port 7700, that's d.micodus.net, same IP.  Although the website that's used to access MiCODUS GPS trackers via a browser uses HTTPS, the mobile app," as I mentioned, "uses unencrypted and unauthenticated plain HTTP.  GPS trackers communicate with the backend server via a custom protocol on TCP port 7700.  The protocol does not appear to be encrypted.  Users can directly control and access the GPS tracker via standard SMS text messages."  So you can just text these little Demon Cubes directly.



The full command list for model MV720 is - and I have a link in the show notes.  And Leo, I couldn't embed it in the show notes because it needs a full screen in order to see it.  But it's a little bracing what you can do with the commands.  Without any logon or authentication, you can send the tracker the simple SMS command "where," and it will reply with a Google Maps link centered on the vehicle's present location.  So, you know, you just send it an SMS "where," and you get back a Google Maps link that allows you to view where the vehicle is located.  Or also with no password, sending a "555" turns off the vehicle's fuel, and sending a "666" resumes the fuel.  Actually, it's too bad those weren't reversed.



LEO:  Oh.



STEVE:  It'd be better if 666 were to be the shutoff. 



LEO:  Wow.



STEVE:  BitSight managed a couple of classic attacks which are enabled by the vulnerabilities they discovered.  There's the Man-in-the-Middle Attack.  An attacker performs a man-in-the-middle attack intercepting and changing requests between the mobile application and supporting servers, taking advantage of the unencrypted HTTPS communications between them.  This would give the threat actor complete control of the GPS tracker; access to location information, routes, geofences, and tracking in real-time; as well as the ability to cut off fuel, disarm the vehicle alarms, and more.  And note that by being a man in the middle, you can change what the GPS tracker reports.  So after obtaining that status, you wait for the vehicle to go somewhere, you report its location somewhere else, and it can no longer be found by GPS because the man in the middle is in real-time changing the GPS tracker's reports back to the mothership.



Second attack:  Authentication Bypass.  And I'll detail this in a second.  But a flawed authentication mechanism in the mobile application could allow an attacker to access any device via a hardcoded key.  Using the key, and by that they mean a password, an attacker could send messages to the GPS tracker as if they were coming via the SMS channel, so you don't even need to use SMS, which should only accept commands from the GPS owner's mobile number.  Again, this would give an attacker complete control of the device; access to location information, routes, geofences, and tracking in real-time; and the ability to cut off fuel, disarm alarms, and more.



And finally, the Persistent Invisible Monitoring Attack.  It is possible to remotely reprogram the GPS tracker - oh, here they're talking about what I already mentioned - to use a custom IP address as its API server.  This would give an attacker the ability to monitor and control all communications to and from the GPS tracker.  The attacker could completely control the GPS tracker, with all the implications listed above, including the reporting of incorrect locations to the GPS server.



The ability to remotely reprogram these devices to use a persistent custom Internet IP as its API server strikes me as one of those "wouldn't that be cool" or "we can do this so we should" sorts of things that engineers toss in just because they can, without there ever being any possible need or justification for the feature, and at a significant and serious cost in security.  So it's just so dumb for that feature to be in there, and so prone to exploit.



Okay.  So what are these six vulnerabilities that BitSight found and that CISA agreed were worthy of CVS designation and in two cases a CVSS of 9.8?  Okay.  The first 9.8 one.  Get this.  The API server uses a single - I can hardly even say this, Leo - a single master global password for all devices.



LEO:  Is it 123456?



STEVE:  No.  It is 7DU2DJFDR8321.



LEO:  Oh, well, no one's going to guess that.



STEVE:  They're not going to guess it, but now everyone knows it.  And it's global.  That's all you need.



LEO:  Oh.  And you can't disable it or anything.



STEVE:  No.  Cannot be disabled.  They can't ever change it because there's no provision in there for changing it.



LEO:  They don't do firmware updates.  What, are you crazy?



STEVE:  Unh-unh.  No.



LEO:  That would be insecure.



STEVE:  On the Demon Cube?  No.  We just let the demon go off on its own.  This is the password used by the user's mobile apps to query and perform actions and execute all remote commands.  This allows an attacker to log into the API web server.  And notice, everyone now knows the IP.  Everyone now knows the password.  And in a minute, everyone's going to know the protocol.  And then all 1.5 million of these things are vulnerable because their device IDs are also knowable.  This is just unbelievably bad, Leo.



Okay.  This allows an attacker to log into the API web server, impersonate any user, and directly send SMS commands to their GPS tracker as if they were coming from the GPS owner's mobile number.  Using the master API password, a remote, unauthenticated attacker can gain complete control of any GPS tracker; access location information, routes, geofences, tracking locations in real-time; cut off fuel to vehicles; and/or disarm alarms and other features.  So it's impossible, I would argue, to label this as a mistake or oversight.



The developers, the people who coded this, must be well aware that this is not per account API authentication.  All authentication is shared globally.  And what's worse, it's not even necessary to reverse engineer one of their API endpoint apps or rig up some fancy sort of DNS spoofing and TLS intercepting man-in-the-middle proxy with a fraudulent cert or certificate authority.  Since the app's API endpoint is HTTP unencrypted to port 80, any passive web sniffer can be used to capture the application's interaction with the MiCODUS API endpoint server.



LEO:  Wow.



STEVE:  So it's not surprising that CISA gave this a CVSS of 9.8.  And as I said, the master password, which is stored in all apps and which works universally, when using the protocol you can sniff with a sniffer on the API, I mean, okay.  It occurs to me - this hadn't occurred to me before, Leo.  For $26 every one of our listeners can buy one of these.  You can then download the app.  You can sniff your Internet connection while this thing phones home, look at the protocol, and play.



LEO:  Oh, man.  Sure.



STEVE:  This is a perfect toolkit for anyone who wants to play around with what is hacking like.  Because there's never been a more readymade opportunity to do this.  It's just amazing.



LEO:  Yeah.



STEVE:  The password, once again, which all apps use, which works universally for logging into the API endpoint at the IP that I gave earlier, and port 8 over HTTP, meaning standard, you know, in the text query and reply, is 7DU2DJFDR8321.



LEO:  Would you stop saying that out loud?  Holy moly.



STEVE:  Vulnerability number two.  The second major problem, also a CVSS of 9.8, is broken authentication.  These guys write:  "The API server provides a way to directly send SMS commands to the GPS tracking device as if those commands were coming from the administrator's mobile device.  123456 is the default GPS tracker password, which should be changed.  However, some commands work even without a password."  Notably 555 to kill the vehicle's fuel.  You don't need a password for that.  Go figure.



"The web interface and mobile app also require a password when directly contacting the tracker via SMS.  However, it shares the same default password issues as the GPS tracker.  Even if the user changes the password, the device is not secure.  Some SMS-like command messages sent directly from the API server do not need the device password to function, leaving the device exposed to hackers.



"One potential attack can be perpetrated by abusing the adminip command" - this is the feature that I noted earlier that should never have been included in the API in the first place - "which defines the API endpoint of the GPS tracker."  Meaning where the GPS tracker, the little tracking demon, connects to, the IP of the server, you can change it.  "This enables an attacker to achieve a persistent man-in-the-middle position, controlling all traffic between the GPS tracker and the original server, and gaining total control over the GPS tracker."



In their full disclosure, they provide a working proof of concept.  I didn't put it in the show notes again.  There's a link in the show notes to their full disclosure.  You had brought it up earlier in the video, Leo, where they show screenshots of all of this being done, redirecting the API through their own server to the original server so that they are able to install their own man in the middle.



Vulnerability 3.  Okay.  This one came - this was an 8.1, only because probably they just got tired of issuing 9.8s.  We always have the popular default password of 123456.  They said:  "As noted above, all devices ship preconfigured with the default password 123456 [yup] as does the mobile interface."  I'll just jump to a different place in the show notes because I happen to have remembered.  They did a test of 1,000, they randomly chose 1,000 devices.  And again, how do you do that?  The device IDs are not hard to guess.



LEO:  Can you go to Shodan?  No, because it's port 80 traffic.



STEVE:  Right.  Yeah, they actually, because the API is unencrypted, and all you need is the password which we now know, you can then guess the device ID.  They guessed 1,000 device IDs, found them.  945 of the thousand had not changed the password.



LEO:  Of course not.  Of course not.



STEVE:  From 123456.



LEO:  Of course not.  Of course not.  By the way, there's a simple command to change the password.  But no, of course not.



STEVE:  Uh-huh.  So the device ID follow the pattern 720, which of course is the model number, right, MV720 is the title of the podcast.  72011, followed by six digits.  And they said:  "With the value represented by the X's occurring in sequential order.  So you start at zero."



LEO:  This is so pathetic.



STEVE:  It is unbelievable.



LEO:  Everything they could do wrong, they did.



STEVE:  Yes.  That sums it up.  That should be the title of the podcast.  Everything they could do wrong, they did.  So, you know, a big problem is that the need for security, the things that security is securing, are more often than not a complete mystery to a device's new user.  If someone explained to them what the consequences would be of not changing the password, they would almost certainly happily change it.  But 945 out of a thousand users didn't bother.  So anyway, this is just a - I think one of the problems, Leo, is that we have devices that sell for $26.  And so they're just not expensive enough to take them seriously.



LEO:  Right.



STEVE:  Yet when you plug them in, and they drive off, they're now enumeratable.  Every hacker in the world can now know where the vehicle is that is carrying this Demon Cube.  And if you happen to wire it into the fuel supply, they can stop it wherever they want to.  It's just unbelievable.



LEO:  A lot of government agencies are using these.



STEVE:  Yup.



LEO:  That's scary.  Jiminy.  Okay.



STEVE:  The takeaway lesson for us here is that, to take this as a valuable case study, we need to recognize that when we're using anything such as this, which connects to a remote Internet service of unknown repute, we truly are placing a huge amount of trust into entities whose trust has not been earned and may not be deserved.  It's bad enough to have one's light switches and plugs connected back to potentially hostile foreign soil.  But giving remote, and in this case clearly irresponsible, entities real-time knowledge of vehicle location and movement and even over the vehicle's real-time fuel flow seems reckless at best.



LEO:  Yeah.



STEVE:  And yet it's inexpensive, so it's done.



LEO:  Unbelievable.  Does this qualify as an IoT device?



STEVE:  I would say it is.



LEO:  Yeah.



STEVE:  It's an IoT.



LEO:  It's in your car and not your house, but it's - yeah.  It's out there, and it's on the Internet.  Does it use standard cell networks?  How does it...



STEVE:  Yeah.  It won't do the latest.



LEO:  5G or LTE.



STEVE:  5G.  I think it's actually limited to 2G, as I recall, because I read the specs on it.



LEO:  Well, the good news is there are not a lot of 2G networks left.  That's probably why it's so big in Russia and Mexico.



STEVE:  Yeah.  So it's like 900 and 1800, and I remember seeing the various old-school cellular frequencies.



LEO:  So the new version, I think they're going to update it, the new password will be 1234567.



STEVE:  Ah.



LEO:  Just a little tip.



STEVE:  Whoa, Leo.  Nobody will get in.



LEO:  We're going to address that security flaw right there.



STEVE:  It'll just go silent.



LEO:  Wow.  Yeah, so this must be an older device if it's 2G.  Unless it's for fleets that might have their own 2G cell communications or something like that.  The U.S., 2G's been turned off pretty much everywhere.



STEVE:  That's interesting.  So there is a red X on one of their web pages in their specs, which says not for use in United States.  So it may just be that it just doesn't work there, thank goodness.



LEO:  The last 2G network in the U.S. goes down at the end of the year, but most of them have already been turned off.



STEVE:  But, boy, Ukraine.  Hello, Ukraine.



LEO:  Yes, yeah.  You see where it is, developing nations.  Well, you know who uses it?  Ukraine also uses it.  But you could really go after Russia with this.



STEVE:  Oh.  It's a hot tamale on that map.



LEO:  Wow, wow.  Unbelievable.  What a story.  If you want all the details, there are lots of links in the show notes.  And if you're looking to hack one of these, everything you need to know is right there.



STEVE:  I would.  It's not illegal if you own it; right?  It's your own device?  Hack it.  I mean, it's a perfect introduction to hacking.



LEO:  Yeah.  Yeah.  And you could - it's a certain kind of relay, though.  You need the right pinout.  So I'm thinking maybe you could use it for an irrigation sensor or something.  Ping your Rain Bird or something.  I don't know.  It just seems like there could be some uses.



STEVE:  Or stick it on your Segway, and you'll know where it went.



LEO:  Stick it on your Segway, yeah.  Well, that's the good news.  None of my electric vehicles have either a gas pump or an oil pump.  So I think...



STEVE:  That's good.



LEO:  I don't know what they'd attach it to.  I think we're all right.



STEVE:  They attach it to the battery and disconnect that.  



LEO:  Gone.  Gone.  It's an NC relay, so you have to have an NC device.



STEVE:  That just means normally closed. 



LEO:  Oh, oh, thank you.  Chat room, of course, always on top of this.  There is good news.  If you go to AliExpress, there is now an updated 4G-compatible device.



STEVE:  Ah.



LEO:  It's a little more expensive.  But it's the same fine company, MiCODUS.  



STEVE:  Ah, nice.



LEO:  It's the 4G ML500G, and this one's waterproof.



STEVE:  So you can, yes, you can know where your submersible sub went.



LEO:  Yes.  Yeah.  So there you go.  And I bet you it's just as insecure.  But it'd be worth...



STEVE:  Leo, it's all based on the same infrastructure.  It has to be.



LEO:  Right, has to be.



STEVE:  Just as insecure.



LEO:  Yeah.  Because it's the server that's insecure, not the device.



STEVE:  Yup.  It is the protocol and the server which is not encrypted.



LEO:  This one has a device temperature and voice monitor, as well.



STEVE:  Oh, that's great.



LEO:  Think of what you could do.



STEVE:  Oh, my god, it's a spybot.



LEO:  It supports the latest 4G LTE CAT 1 network on multiple bands for operation globally.



STEVE:  There's a map with little happy smiley faces all over the globe.



LEO:  Everywhere.  All over the globe.  And of course still support for 2G, in case you want to - oh, look.  This one can be magnetically attached under the chassis.



STEVE:  Oh.



LEO:  So this isn't a cutoff, I think.  This is just a cheap tracker.



STEVE:  Oh, god.  My stomach hurts from laughing.



LEO:  Yeah.  Oh.



STEVE:  Oh, lord.



LEO:  Remotely voice monitor.  Look, here's a good use.  They've got an eight-year-old girl with her backpack, voice monitoring her.  And who's this creep listening in?  Geez, Louise.



STEVE:  Oh, my god.



LEO:  This is not good.  This is not good.



STEVE:  Oh.  Oh.



LEO:  Okay.  Thank you, Steve, as always.  This is the show that gives you the screaming meemies at night.  But if you don't listen, just imagine how scary the things you don't know about are.  Right?  Every Tuesday we gather together, 1:30 Pacific, 4:30 Eastern, 20:30 UTC to learn the latest from this guy right here, Steven M. Gibson, I now know.  Steven M. Gibson.  If you want to get the show from him, you can.  It's at GRC.com.  That's his website, the Gibson Research Corporation.  You can leave feedback for Steve there, too, GRC.com/feedback.



Steve has 16Kb versions.  He's got 64Kb full bandwidth audio.  He's also got transcriptions.  GRC.com.  While you're there, check out check out SpinRite, the world's best hard drive or mass storage, really, any kind of mass storage, even SSDs, maintenance and recovery utility.  Currently 6.0, soon to be 6.1.  We are inching close.  And if you get 6.0 now, you'll get the free upgrade, so this is a good time to get your copy of SpinRite.  Everybody, if you've got mass storage, you need SpinRite.  Lots of other stuff there, free stuff that Steve does pro bono.



We have copies of the show at our website, ad supported, at TWiT.tv/sn.  YouTube has a channel dedicated to Security Now!. You could join the club, get the ads out.  There's a couple of ways to get the ads out.  You can either go to TWiT.tv/clubtwit, join the club for 7 bucks a month, then get all the shows ad free, plus the Discord, plus the special TWiT+ feed.  Or just get Security Now! by itself for $2.99 a month, if you're a cheapskate.  Spend the 7 bucks.  Get it all, plus the Discord, which is a great place to hang.  But again, we always have free versions available at the website.  If you want to chat with us while you're watching the show, irc.twit.tv, as well as the Discord channel for club members.



And best way to get it, honestly, subscribe.  Subscribe to the audio or the video feed.  Each format has its own feed.  And you'll get it automatically on a Tuesday evening, right after we chop it up and mince it and put it into fine delicious squares.  I guess that's what the editors do.  I don't really - I don't understand how that stuff works.  Get your favorite podcast client and subscribe to Security Now!.



You watching any good sci-fi, Steve?



STEVE:  Actually, we just started "The Dropout."



LEO:  Love that show.



STEVE:  Yeah.



LEO:  And if you want more dystopian startup stuff, "WeCrashed" on Apple TV is all about WeWork and is fantastic, as well.  Those two shows together.



STEVE:  Ooh, good, good, good.



LEO:  If you like "The Dropout," you'll love "WeCrashed."



STEVE:  Fun.  And we also finished the final season of...



LEO:  "The Expanse"?



STEVE:  "Ozark."



LEO:  "Ozark."  Yeah, it's over.



STEVE:  I know.  That was great.



LEO:  When I go home tonight, last episode of "Better Call Saul."  If you have not watched - did you watch "Breaking Bad"?



STEVE:  Oh, of course.  And we watched all of "Better Call Saul."  Was there more?



LEO:  Oh, you haven't seen it all.  He had a heart attack, literally, Bob Odenkirk, and they stopped.  And there are four final episodes which have been coming out.



STEVE:  I thought it was as good if not better.



LEO:  I agree.



STEVE:  I mean, it was well - it was at the top of its game.



LEO:  Dude?



STEVE:  Cool.



LEO:  It had a cliffhanger.  Didn't you notice there was a big cliffhanger?  I don't want to say anything.  Go back and check your listings.



STEVE:  Okay.  Cool.



LEO:  Oh, it's so good.  And everybody's probably already seen the final episode because it came out.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#882

DATE:		August 2, 2022

TITLE:		Rowhammer's Nine Lives

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-882.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're going to note an urgent vulnerability created by an add-on to Atlassian's Confluence corporate workgroup server.  Next week's USENIX Security Conference will be presenting TLS-Anvil for testing TLS libraries.  Google has decided to again delay their removal of third-party cookies from Chrome, and attackers were already switching away from using Office Macros before Microsoft actually did it.  We have a bunch of listener feedback, some thoughts about computer science theory and bit lengths, and some interesting miscellany.  Then we're going to look at the return of Rowhammer thanks to some new brilliant and clever research.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We will talk about TLS-Anvil, a really interesting testing framework for TLS libraries, works really well.  Google is delaying - who could have predicted it? - once again the removal of third-party cookies from Chrome.  And proof positive that turning off macros in Microsoft Office really works.  Plus a look at Rowhammer.  It's back again, the attack that just never quits.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 882, recorded Tuesday, August 2nd, 2022:  Rowhammer's Nine Lives.



It's time for Security Now!, the show where we cover your security, privacy online with this guy right here.



STEVE GIBSON:  All that.



LEO:  And all that jazz.  Mr. Steve Gibson.  Hello, Steve.



STEVE:  Yo, Leo.  Great to be with you.  We're at Episode 882.  The next one of note will be in six weeks.  It'll be 888, so just because that's a fun number.  But that'll, you know, here we're in August already.



LEO:  Already in August.  Man.



STEVE:  Okay.  So this is not the actual name of the exploit.  The name of the exploit didn't really move me much.  It was called "Half-Double."  But what it is...



LEO:  Half double is a single.



STEVE:  Yeah, exactly.  But it is brilliant.  And so I just thought it would be too fun not to share it with our listeners.  So today's title is "Rowhammer's Nine Lives."



LEO:  Oh, man.



STEVE:  Because, yes, it isn't gone yet.  And these guys have figured out a way to still flip bits in DRAM and exploit it.  In fact, I don't mention this until the very end.  But on a brand new, completely up-to-date Chromebook, an unprivileged user can obtain root, kernel access in 45 minutes.



LEO:  Oh.  On a Chromebook.



STEVE:  On a Chromebook.  So of course this has all of the cloud people terrified because they're all sharing hardware assuming that you've got protection against a random virtual server in the cloud obtaining root on its host.  And this makes it possible.  Anyway, we're going to have fun talking about that.  But we're going to first note an urgent vulnerability created by an add-on to Atlassian's Confluence corporate workgroup server.  Next week's USENIX security conference will be presenting not only the subject of the show, this new Rowhammer attack, but also something very cool called TLS-Anvil, which is used for testing TLS libraries.  And 13 of them were tested.  We'll talk about what it found.  And it's like, it's not nothing.



Google has decided again, although I'm on their side on this, well, at least I'm on their side on the initiative, they've decided to again delay their removal of third-party cookies from Chrome, and for presumably good reasons.  We'll talk about that.  And it turns out that attackers were already switching away from using Office Macros before Microsoft actually did it.  We'll talk about the back story there.  We also have a bunch of listener feedback, some thoughts about computer science theory and bit links that I thought our listeners would also find interesting.  Some interesting miscellany, and then we're going to look at the return of Rowhammer which we never really seem to get rid of, thanks to some really just awesome brilliant and clever research.  And of course I would argue that this is one of our better Pictures of the Week.  We've had...



LEO:  People are howling over this thing.



STEVE:  ...some that are really fun.  I gave it the title.  It didn't come with a title, but I like the title.  And anyway, it's a great one.  So I think another overall great podcast for our listeners.



LEO:  Going to be a lot of fun.  We'll get to the Picture of the Week.  All right.  Picture of the Week time, Steve.



STEVE:  Okay.  So this is two captures from Star Trek, the original series.  And I titled this "Keptin, we cannot go to warp."  So the first one is from Kirk's vantage point looking out across Zulu on the left and Chekov on the right and to the view screen at the front of the bridge, where we see the little what I call the rollercoaster dots, and a blue screen, the little spinner saying "Working on updates:  30%."  And so we see Chekov sort of like turning his head back to look at Kirk, presumably saying, "Keptin, we cannot go to warp."



And the second frame is a very unhappy-looking James Tiberius Kirk, kind of like growling because he wants to go to warp, and no.  His ship will not let him.  And I will note, we'll be talking about this later in Miscellany, but Uhura back there, ever present on the bridge through all of the original series and all six movies, as well.  So anyway, yes, James is not happy.



LEO:  [Growling]



STEVE:  That he's having to wait for his starship to update.  And lord help us if Windows is embedded into future starships.  Please don't let that happen.  Everybody has always wanted me to write a Windows operating system, Leo.



LEO:  Oh, no.



STEVE:  I would.



LEO:  Come on.



STEVE:  Well, I mean, I'm not going to.  I've said no many times.  But just like imagine if Steve wrote Windows.  Well, I'm not going to do that.  But okay.  If anyone tried to stick it into a starship, okay, I would write that operating system.  I'm just saying, if that's what it takes not to have updates ever, then fine, I'll step up.



Okay.  So the Australian software company Atlassian offers something called Confluence, which is a web-based corporate Wiki, written quite a while ago, 18 years ago, back in 2004, in Java.



LEO:  We use it, actually.  And have for years, yeah, yeah.



STEVE:  Oh, cool.  It's one of Atlassian's more popular products.  It's packaged as a standalone turnkey that includes a built-in Tomcat web server.  And the database is HSQLDB, H-S-Q-L-D-B, which is also a Java-based database, so it makes sense that that's what it would choose, although other databases can  be used with Confluence.  It's modern collaborative software.  It's used to help teams collaborate and share knowledge.  In a corporate setting it's used, for example, to capture project requirements, assign tasks to specific users, and manage one or more calendars.  It's typically used on-premise all over the place, both in corporate enterprise and government environments.



LEO:  And now I'm waiting for the other shoe to drop.  Is there a...



STEVE:  That was the good news.



LEO:  Yeah.



STEVE:  We're talking about it today because it appears to have recently come to the attention of ever-prolific attackers who, based on the troubles it's been having recently, it's come to their attention because it's been, you know, vulnerabilities have been surfacing.  And unfortunately, as with any product that was first written 18 years ago, before today's hyper-attack-aware climate existed, some of its coding practices from back then appear to be being turned into vulnerabilities.



Now, although Confluence has recently suffered, as I said, from a series of troubles, it popped up again just last week due to a very serious problem with the installation of, believe it or not, default hard-coded credentials.  As we know, any time you hear that a problem has been created by the discovery of default hard-coded credentials, meaning a username and password with privileges no one should have, by default being part of an installation, it's difficult to chalk that up to anything other than some previous very bad decision.



In this instance, the trouble arises from the installation of an app called Questions for Confluence.  So it's not the core Confluence server itself, it's an add-on app.  It's often installed on Confluence Server and Data Center on-premise instances because it allows employees to ask questions and receive answers from a company's various internal departments.  The installation of this Questions for Confluence app creates a user named - the user's name is "disabled system user" with the username "disabled system user" and the email of "dontdeletethisuser@email.com."  So, you know, a fake email placeholder, but "disabled system user" as the name of the account and the name of the user.  And unfortunately, contrary to the name given to the account, it's not at all disabled, although it is indeed a system user.



Atlassian released a patch that disables this built-in hardcoded account 13 days ago, on July 20.  However, Confluence server admins did not have much time to install Atlassian's fix because, through no fault of Atlassian's, the system account username and credentials for this account were posted the next day to Twitter.



LEO:  Of course.



STEVE:  Uh-huh, by someone using the handle "annoyed researcher."



LEO:  Oh.



STEVE:  Okay.  I don't know the back story for that, but it's uncool, right, to do that under any circumstance.  I don't care how annoyed somebody is, that's just not the way it's done.  In fact, I would argue you cannot call yourself a researcher, no matter how annoyed you may be, if you're posting credentials for a just-released flaw that no one will have had time to fix.  



LEO:  Yeah.  You have to do responsible disclosure.  That's just - everybody has to do that, yeah.



STEVE:  So as things go these days, it didn't take the bad guys long to put "annoyed researcher's" information to nefarious use.  The cybersecurity firms, both of them, GreyNoise and Rapid7, both reported seeing immediate and continuing exploitation of this, I guess we'll call it a vulnerability, although it's hard to chalk up hard-coded credentials.  I guess it is a vulnerability.  It's not like they broadcasted it.  But still, shouldn't be done.  It's the kind of thing you do by policy rather than by mistake.  I guess that's the problem I'm having.  Anyway, within days of the release of the patch it was being used in attacks.  CISA has stepped up and urged Confluence server owners to check to see if the vulnerable app has been installed on their servers; and, if so, to install Atlassian's patches.



And Atlassian noted that disabling the app, due to the nature of the problem, doesn't fix the issue because it's the app's installation that created the problem by creating this instantiation of this non-disabled disabled user.  So Confluence server owners must either install the security fixes or manually disable that hard-coded "disabled system user" account which was created by this Questions for Confluence app.



So anyway, because this thing, I mean, I've been seeing Confluence problems pass by, and they just haven't risen to the level of bringing them to our listeners' attention.  But this one really seemed like it was important enough that you just want to make sure that, if you're using this thing, that you've responded to Atlassian's security fix and/or killed or disabled that user.



And in fact Kevin Beaumont, the true security researcher, who tweets as @GossiTheDog, he said:  "I would recommend Confluence be placed behind a VPN or reverse proxy," and he says, "for example, Azure App Proxy," he says, "requiring authentication."  He said:  "It's simply too historically vulnerable to leave online.  You're a sitting duck," was his tweet.



And, you know, it's difficult to argue with Kevin's logic.  Everyone knows that I think this should be standard practice now for all publicly exposed services.  The rule would be don't publicly expose anything that doesn't utterly and absolutely, by definition, need to be publicly exposed.  So what does?  Web servers, obviously, need to be publicly exposed by definition.  So does email, and probably DNS, although I even managed to not have my DNS publicly exposed.  I use Level 3's big iron DNS servers as slaves to my master.  So the DNS, the only DNS that my system allows is zone transfers from my two designated name servers for GRC.com, which are Level 3 monster DNS servers.  And who knows?  They're probably virtualized and distributed and not even actual single machines anymore.



But anyway, so I guess the point is it is possible to absolutely protect all of an enterprise's stuff by putting them behind, and this is what Kevin is talking about, an additional layer of authentication.  Put them behind a VPN.  Put them behind some additional boundary that requires authentication, and only then are you able to get to it.  Or use an overlay network, and stick them on an overlay network so that the people have to authenticate to that in order to get there.  But just don't have them as a public IP and port that Shodan is going to index so that the bad guys can jump on whatever it is the instant a problem is found.  I mean, that's the model today.



So the only sane response to today's reality is, no matter what it is, no matter how secure you think it is, give it additional protection.  It has to be the way we roll.  And I've been following my own advice from like before we started talking about this.



During next week's 31st USENIX Security Conference being held Wednesday through Friday the 10th through the 12th in Boston, Massachusetts, a team of security researchers will be unveiling what they call "TLS-Anvil."  And I guess the point of the name is to use this TLS Anvil to forge fully secure and compliant TLS connection libraries because that's what it's designed to test.



In the description of their forthcoming talk, which as I said they'll be giving at the end of next week, they explain.  They said:  "Although the newest versions of TLS are considered secure, flawed implementations may undermine the promised security properties.  Such implementation flaws result from the TLS specifications' complexity, with exponentially many possible parameter combinations.  Combinatorial Testing," which they abbreviate CT through their work - "is a technique to tame this complexity, but it is hard to apply to TLS due to semantic dependencies between the parameters, and thus leaves the developers with a major challenge referred to as the 'test oracle problem,' which is determining if the observed behavior of software is correct for a given test input.



"In this work," they write, "we present TLS-Anvil, a test suite based on Combinatorial Testing that can efficiently and systematically test parameter value combinations and overcome the oracle problem by dynamically extracting an implementation-specific input parameter model that we constrained based on TLS-specific parameter value interactions.  Our approach thus carefully restricts the available input space, which in turn allows us to reliably solve the oracle problem for any combination of values generated by the Combinatorial Testing algorithm."



And finally they say:  "We evaluated TLS-Anvil against 13 well-known TLS implementations, including OpenSSL, BoringSSL, and NSS," Netscape's Security Suite, the original one.  "Our evaluation revealed two new exploits in MatrixSSL, five issues directly influencing the cryptographic operations of a session" - which they refer to as handshakes, at the handshake level - "as well as 15 interoperability issues, 116 problems related to incorrect alert handling, and 100 other issues across all tested libraries."  So needless to say, this was worth doing.  And it is just so cool that we have a world where academics are able to tackle something this complex, all of this mumbo jumbo about combinatorial testing and input parameter modeling and/or testing oracles.  The point is, we could reduce all that to "Testing TLS is hard.  We did it anyway."



Their paper is really interesting.  I've got the link to it in the show notes for anybody who is interested.  The 13 libraries that they tested were:  BearSSL, BoringSSL, Botan, GnuTLS, LibreSSL, MatrixSSL, Mbed TLS, NSS, OpenSSL, Rustls, s2n, tlslite-ng, and wolfSSL.  As it happens, through the years we've talked about most if not all of these.  They're all familiar to me.  I'm sure they are to our listeners, those who have been with us the longest, at least, and recall, for example, that "s2n" is Amazon's from-scratch open source implementation of TLS.  They wrote it because of AWS.  They needed a TLS for their own cloud services.



But they observed that OpenSSL, as we've often observed, had grown in size to half a million lines of code, 70,000 of which were tied to TLS processing.  But they could be replaced, and they did replace them with about 6,000 lines of code, which Amazon observed is far easier to audit and to actually have some sense of knowing what the code is actually doing.  How can you do that with half a million when they're all tangled up with each other? 



So anyway, this chart which you had on the screen earlier, Leo, thank you, in their paper gives us a cross reference to those 13 TLS proposed libraries with the number of different type of exploits found, problems with the handshake crypto, problems affecting client and server interoperability, the generation of alerts, which is to say in the protocol when either end does something illegal, the other end is supposed to send an alert and disconnect.  It's like, you're bad, and I'm hanging up on you.  And then other problems.



Their paper was 18 pages.  I'm not going to go into it in great detail.  But they definitely found problems of varying degrees in every one of the 13 TLS libraries.  And looking over the chart, it appears that the best libraries were BoringSSL, Botan, NSS, OpenSSL, and Amazon's s2n.  Oh, as well as tlslite.  And these guys now really understand the state of play with the industry's TLS support.  They wound up having, like, creating the tool, running the libraries against it, and then having to sit back and ask themselves, okay.  It's not exactly what we expected.  We didn't know what we were going to get.  How do we decide to think about this?



And they ended up defining two degrees of success for this test.  They wrote:  "A strictly succeeded test means that a library behaved exactly as expected.  If multiple test cases are performed during the execution of a test template, the system under test must have behaved correctly across every one of them."  But then they also created a softer, gentler definition.  They said:  "A conceptually succeeded test means that an implementation did not precisely fulfill the RFC requirements, or did not do so in all test cases, but effectively behaved correctly."  And I'll explain what happened in a minute because when they had this thing, they then, to be responsible, they needed to talk to the implementers.  And it's actually the case that the implementers convinced them that it was necessary not to be strict.  So they said, okay, let's just be conceptual, then.  That's good; right?



So they said, for example:  "This usually applies to tests where a fatal alert was expected, but the library either only closed the connection, but did not send an alert first; or the alert description did not match the RFC's specification."  So we should understand that these guys implemented TLS-Anvil strictly from the RFC's.  That was their starting point was what does the spec say the library must do in each and every case?  And the act of testing that, I mean, we don't know what internal tests TLS developers may have.  But if we presume that they've been using their internal tests, and how could you not, then this TLS-Anvil's results suggest that in at least some cases those internal tests were wanting.



So they then give some examples of what they found.  They said:  "We count all tests as 'passed' that either succeeded strictly or conceptually" - and now we know what that means - "and include the percentage of passed tests.  We further list the ratio of conceptually to strictly succeeded tests as an additional metric to compare how close an implementation is to the RFC for our tests.  Rustls, for example, passed many tests, but most of them only succeeded conceptually, as Rustls often did not send any alerts."  And again, it's like, you're supposed to send an alert to tell the other end why it just misbehaved.  But Rustls just said, you know, eff off, and disconnected.  Which it's been argued...



LEO:  It's better than nothing.



STEVE:  It gets the message across.  And "Botan, in contrast, often fulfilled the expectations," they wrote, "of our tests, and at the same time was very accurate with alert descriptions."



LEO:  Good, yeah.



STEVE:  Yeah.  They said:  "We generally found that most libraries pass a high ratio of the test templates, with NSS, BoringSSL, tlslite-ng, and OpenSSL passing around 97%" - which I think is really good - "of their applied server tests.  Among the client tests" - okay, so of course "server tests" meaning that you're using the library to accept connections, "client tests" meaning you're using the library to generate new TLS connections to a remote server.  So they said:  "Among the client tests, BearSSL, BoringSSL, and Botan have the highest ratio of passed tests with 97.3%, 96.8%, and 96.2%, respectively."



They said:  "We expand upon the results of libraries with significantly worse ratios, listing how many test templates of an RFC passed and how many were executed for each library.  We grouped the results of test templates based on similar error cases and identified a total of 239 issues.  We further categorized these findings based on their impact, and determined that three immediately led to exploits in wolfSSL and MatrixSSL."  In other words, exploitable problems in those libraries.  They're gone now, so long as you've updated recently, because these guys were doing all of this work before they went public and gave everybody plenty of time to fix.



They said:  "Additionally, we found five issues affecting the cryptography of a handshake.  As an example, the clients of MatrixSSL, Amazon's s2n, and wolfSSL are willing to negotiate parameters they did not offer."  Which is interesting and not good.  And they said:  "While none of the parameters negotiated are sufficiently weakening the security to pose an immediate threat now, parameter negotiation is a basic security property of every cryptographic protocol to prevent current and potential future attacks."  So again, another nice thing that these guys checked which the authors of MatrixSSL, s2n, meaning Amazon, and wolfSSL, missed.  So now they know, and presumably they fixed that.



They said:  "We identified 15 issues affecting the interoperability to an extent where a peer that operates within the boundaries of the RFC may not be able to complete a handshake."  They said:  "Note that this may also include intentional deviations by the developers if they break the implementation's correctness in regards to the specification."



They said:  "100 issues account for various likely uncritical cases where a library deviated from the RFC beyond alert codes and where interoperability should not be affected.  Examples of these findings are a bug in OpenSSL, which allowed multiple TLS 1.3 HelloRetryRequest messages, which can keep the client in a handshake loop."  Okay, again, like technically a violation of the RFC, but nothing to worry about.  But still, what's interesting about these is, if that isn't what the developers intended, then getting them to look at why that happened could reveal some other side effect of this.  So always good to at least be sure you know why you deviated from the intended behavior.



They said:  "Finally, we grouped 116 cases where a library did not send an alert or sent a different alert than requested by the RFC.  These are minor deviations from the standard.  However, in the past, information gained from the type of alert sent by an implementation has been used to mount side-channel attacks."  They say:  "To avoid such deviations, great care must be taken when designing the alert handling of an implementation.  We hence chose to include these findings in our report to the developers."  Meaning that they then, after creating this, produced a comprehensive report to each of the 13 developers of these packages.



And what they had to say about responsible disclosure and feedback I thought was very interesting.  They said:  "We responsibly disclosed all of our findings to the respective developers.  During the disclosure process, multiple developers stated that they intentionally violate RFC requirements in specific cases.  As an example, in TLS 1.2, peers are not allowed to resume a session that has been terminated by a fatal alert.  However, when multiple sessions take place in parallel, this requirement is difficult to implement."  Okay.  So what I loved about that was that the ivory tower developers of the RFC said, if a peer terminates with a fatal alert, then you're not allowed to resume a session.  But the guys who implemented this, or attempted to, said, okay, what do we do about multiple parallel sessions?  You didn't say.  You didn't tell us what to do.  So these guys went one way that the TLS-Anvil guys took issue with.



They said:  "Multiple developers also stated that they intentionally sent different alerts or no alerts at all."  Intentionally.  "One reason was to minimize the risk of creating an alert-oracle for attacks."  And these guys themselves mentioned that in the past the alerts that were sent had been used, had been leveraged in side channels.  However, now they say:  "We do, however, stress that the specified alert handling of current TLS RFCs does not result in a known exploitable oracle, but is considered to be secure."  So they're saying it's not a good excuse.



They said:  "Our original test suite contained 18 additional test templates, which we removed from the test suite after discussions with different library developers.  Their reasoning convinced us that in these cases our interpretation of the RFC was too strict and that their library behavior was indeed valid."  Very cool.



So they said:  "Our presented evaluation does not contain these additional test templates anymore.  There were some cases where the developers argued that it is unreasonable to follow the specification.  For example, in some tests, a server that supports both TLS 1.3 and TLS 1.2 would need different alert handling for the same effective test.  The situation becomes even more tricky when the server has not decided which protocol version to speak yet."  Right?  So again, two different protocol versions require different alerts to respond differently with alerts, depending upon the problem.  Yet at this stage in the handshake, the version that's being negotiated hasn't been determined.  And it could be either of them.  So again, I love the fact that these guys were so insanely rigorous that these, I mean, they're edge cases, clearly.  But, I mean, they're like an edge case's edge case.  Yet they came out as a consequence of this.



So, they said:  "For example, a server that receives a malformed or illegal ClientHello message would first need to evaluate the supported version of the client to decide upon the correct alert handling rules.  Correctly handling these nuances," they said, "can be complex, and it is arguable whether the strict RFC conformance across all supported versions is worth the added complexity."



And they finished:  "The security bugs we reported and most of our other reports have been acknowledged by the developers and will be considered for future releases."  None were run around screaming immediately patch or they would have been done already.  "Since most failed test templates only failed for single or very few libraries, we conclude that the developers in general share our understanding of the RFCs."  So, just so cool.  And I guess another way to read this would be ivory tower RFCs meet real world implementations.  There are some places where it just doesn't make sufficient sense to go to all the trouble that absolute slavish adherence to the RFCs would require.



Anyway, all this beautiful work is on GitHub.  The site is tls-anvil.com, which offers Dockers to allow the use of these tests by anyone.  And I'm just - I salute this.  This is precisely the sort of work that we need, and as much of it as we can get.  So bravo to these guys.  Very, very cool work.  And it now exists.  So if there's a 1.4, then the test suites can be augmented to incorporate that.  This ends up being a benchmark reference against which TLS suites can be tested.  And I just - that's so cool.



Google, well, last Wednesday...



LEO:  They keep putting this off, don't they.  They just...



STEVE:  They do.



LEO:  They keep putting it off.



STEVE:  And I guess I'm sympathetic because I recognize, we all should recognize, how difficult change is.  If there's one of the overriding themes of the podcast's lessons, it would be "Change is hard, and nobody wants it."  Just say IPv6, and you can rest your case.



Anyway, their VP of the Privacy Sandbox initiative, Anthony Chavez, said last Wednesday:  "The most consistent feedback we've received is the need for more time."  Right, it's not Google, no no no no, they're ready to pull the plug.



LEO:  Oh, yeah.



STEVE:  That's right.  "More time to evaluate and test the new Privacy Sandbox technologies before deprecating third-party cookies in Chrome.  This feedback," he wrote, "aligns with our commitment to the CMA" - we'll get to that in a second - "to ensure that the Privacy Sandbox provides effective, privacy-preserving technologies, and the industry has sufficient time to adopt these new solutions."



Now, okay.  I would argue that the industry will not adopt anything until you pull the plug on the old solutions, and you've said, we told you we're going to pull the plug.  We gave you plenty of time.  We're sorry that cookies are gone, and you can't track anybody anymore.  Here's the API that we created to let you do that.  So start doing it.  Again, it's like, okay.  The lesson we keep seeing is you have to make people change.



Okay.  So anyway, let's back up a bit.  Google once again delayed its plan to terminate Chrome support for third-party cookies is what this amounts to.  They first unveiled this privacy sandbox initiative back in 2019, when my moustache was a little less gray, with the announcement that its implementation would begin this year in 2022.  Meaning that this would be the only way to do, what do we call it, interest-based advertising or something.  I'm sure there's a term.  Anyway, last year, after being scrutinized by the U.K.'s Competition and Market Authority, that's that CMA that Anthony referred to, and the U.S. DOJ, our Department of Justice, Google announced their intention to delay their third-party cookie phase-out until the middle of next year, so 2023.



And now, last Wednesday, what Anthony is announcing is another delay saying that now it won't be - I saw August of 2024 somewhere.  But, you know, basically they won't end, they're promising not to end support for third-party cookies before the second half of 2024.  And again, my feeling is, just do it.  Just, you know, have the system in place.  Have it well tested.  Have it working.  And then end the way it used to be, and make people switch.  And, you know, in two years - are we still going to be here in two years?  I think we are, Leo, yeah.  So we'll get to see what happens in the second half of 2024.  We'll be right here. 



Under its current timeline, Google will expand the availability of its Privacy Sandbox trial to what they said was "millions of users globally," meaning that Chrome will begin to get this technology.  And they're saying by early next month, so early September.  Millions of users using Chrome will have what we've talked about before.  The Topics API will be live.  Then they plan to gradually roll out the test to more and more users throughout the rest of 2022 and 2023.  They hope to officially launch the Privacy Sandbox APIs by the third quarter of 2023.  Meaning, okay, about a year from now.  Anthony said:  "This deliberate approach to transitioning from third-party cookies ensures that the world can continue to thrive" - thrive.



LEO:  The world?  You mean the advertisers.



STEVE:  Yeah, uh-huh, "without relying on cross-site tracking identifiers or covert techniques like fingerprinting."  Okay.  So on the podcast we've talked about and we've been, if you'll pardon the choice of words, we've been tracking Google's proposed tracking technologies closely.  The first attempt at replacing third-party cookies was their awkwardly named Federated Learning of Cohorts, you know, FLoC, which, Leo, you grabbed the fact that that was about birds, which...



LEO:  All of the codenames in that generation were bird-focused.



STEVE:  Right, right.  Then when that failed to take off, largely because no one understood how it worked, and it was way too opaque, Google announced their new approach called Topics.  Which is largely the same, but is far more understandable and inherently transparent in the way it works.  And as the name suggests, and we did a podcast on this, so we've completely covered this, Topics more transparently, you know, it works more transparently and makes it obvious how and what interests it is sharing with websites that users visit.  It's got about 300 different topics.  And when a user visits a website that requests the information, Chrome will offer from a pool three different topics in a random sequence that it knows that the user is interested in.  So it's sufficiently cloudy, fuzzy, that you can't track a user by seeing what topics their browser is offering from one site to the next.



The site's publisher, then, that is, the site's server that receives these three randomly selected from a larger pool that the user has shown their interest in by virtue of where they've gone, to the sites they've visited in the past, that server can then choose to share that information with their advertisers, which they certainly would. 



Okay.  And being a technology hound, I understand and love these initiatives from Google.  They make sense to me.  But the big question in my mind will be whether any of this can actually replace all other side-channel tracking.  As we've observed, if you turn off cookies, bad guys will just - or gray guys will just use some other means of tracking, like fingerprints, all of the stuff that the Panopticlick site shows is leaking from our browsers.  I think that the only way we're ever going to be free of tracking altogether is through legislation, which makes it flatly and permanently illegal.  Of course we'll first need a privacy protecting replacement, which Google's Topics does offer.  But once that's in place, we're going to need governmental regulation, probably led by the EU, since they seem to have a penchant for that, to completely outlaw any and all other forms of anonymous Internet user identification, aside from the use of the sanctioned API which will hopefully become an industry-wide standard, and I think has the chance to.



So again,  it's going to - everybody, every step of the way, this is going to require people being forced to change.  So far it's already been slow and painful.  I don't see any reason for that to be any different.  Google's got this thing in place.  They've got to stop moving the goalposts, or no one's ever going to believe that they're going to be serious.  So hopefully, you know, latter half of 2024 we'll finally see Chrome remove third-party cookies completely, altogether, from the Chromium core.  And by that time topics will have been around long enough so that, if somebody says, wait a minute, how are we supposed to track people?  It's like, well, you're supposed to use Topics, and it's been around for a couple years.  So it's your fault if you're not.



LEO:  It's going to be a race because that's about Episode 990, thereabouts.  It'll be a race between whether...



STEVE:  Wait, we have our - oh, yeah, I was just going to say we have our birthday on August 19th, but that doesn't correspond with Episode 999 because...



LEO:  Fortunately, it does not.



STEVE:  Right.



LEO:  Thank goodness.  But yeah, it's going to be a race between whether Google clobbers third-party cookies or you clobber Security Now!, which will happen first.  No one knows.



STEVE:  Aww.



LEO:  Yeah, you can back out, Steve.  It's not a permanent, you know.  I'm glad to hear you saying that.  No one wants you to stop.



STEVE:  Okay.  Well...



LEO:  All right.



STEVE:  We'll see how it goes.  May have to hang in, well, no, I'm not making any promises.



LEO:  No, don't say anything, just we'll get there.



STEVE:  Don't say anything.



LEO:  We'll see what happens when the time comes.



STEVE:  Okay.  So Proofpoint released the results of their email-based malware attack analysis.  In their report, they observed that their analysis of campaign threats, meaning malware campaigns, which include threats manually analyzed and contextualized by Proofpoint's threat researchers, they found that the use of macro-enabled attachments by threat actors had decreased approximately 66%, meaning it's dropped to one-third of what it was before, between October 2021 and last month, June 2022.  And remember that Microsoft previously announced it would begin to block XL4 and VBA macros by default for Office users in October of 2021 for XL4, which coincides exactly with the beginning of the observed drops in their use.



LEO:  It works.  It works.



STEVE:  Yes, yes.  And also in February of this year for VBA.  And as we know, there was some confusion last month by Microsoft saying, oh, we're not sure.  Okay, now we are again.  But now it's going to just be a documentation change.  The disabling has ultimately remained in place, thank goodness.  So not surprisingly, threat actors across the landscape have responded by shifting away from macro-based threats, even before the boom had been lowered, because they knew that the way they've been doing it was going to end.  Based on Proofpoint's campaign data since October '21, threat actors have pivoted from using macro-enabled documents attached directly to messages to deliver malware, and have increasingly been using container files such as ISOs and RAR attachments and Windows Shortcut .lnk, LNK files.



So what's going on is that Microsoft blocks VBA macros based on the so-called, and I love this, the Mark of the Web, which tags files based upon their source.  The Mark of the Web identifies whether a file originated from the Internet Zone within their computer.  And the Mark of the Web, I just love saying that,  tags the file that's downloaded.  But it cannot tag files contained within other files; right?  So the Mark of the Web can be bypassed by using a container file format.  An IT security company Outflank - that's a great name - has detailed multiple options for red teamers to bypass - the acronym of course is MOTW, Mark of the Web, mechanisms.  And of course these techniques can be used by threat actors, as well.



Threat actors can use container file formats such as, as I mentioned, ISO, RAR, ZIP, and IMG, image files, to sneak macro-enabled documents into a user's machine, bypassing that flagging.  The container gets flagged, but of course its contents do not.  So the file system will not identify the document as coming from the web, and some of these new defenses will be bypassed.  And, you know, it occurs to me - Microsoft, hello Microsoft, are you listening?  Why not mark the contents of containers?  You know, you're marking the file.  Mark the contents.  Maybe they can't because the Mark of the Web is an NTFS property.  And so you probably can't reach in.  But if you watched the container being decontainerized, then you can get smarter, Microsoft.  So maybe you need to get smarter.



Anyway, the cat and the mouse race continues.  Having Microsoft finally hampering, thank goodness, the auto running of these macros by default was a huge leap step forward.  I'm still somewhat amazed by the amount of pushback that Microsoft received.  People want to have the power without being willing to take responsibility for what having it means.  So Microsoft finally, after decades, did the right thing by simply taking that power away while providing them with ample safe workarounds.  And again, I would argue Google is going to have to do the same thing.  They're going to end up pulling the plug on third-party cookies, and people are going to scream.  And the good news is they will have had years to adapt their technology.  So pull the plug anyway, Google.



Okay.  Lewis, who hangs out in the spinrite.dev newsgroup, he wrote sometime between last week and this week.  He said:  "I was surprised to hear this week that SpinRite 6.1 is still using 16-bit addressing, limiting it to 64K."  And then he said, "I forgot the word you used," and he means segments, 64K segments.  He says:  "Is this actually helpful in 2022, and wouldn't it be simpler for the code to use 32-bit addresses rather than swap between 64K blocks?"  And he finished:  "Haven't we been at 32 bits for almost 40 years?"



Okay.  So Lewis's question raises an interesting point about practical computing and computer science that I wanted to take a moment to address.  So let's for a minute delve into some theoretical aspects of computer science which not everyone may have stopped to consider.  Okay.  First of all, let's understand that whatever they are being used for, the number of binary bits collected together determines the number of possible states that the collection of bits can have.  We might be using a collection of bits to represent a value, in which case the number of bits determines the number of possible discrete values that can be represented by that collection.  Or the collection of bits might be a cryptographic key, in which case the number of bits in the collection determines how many wrong keys that one right key is hiding among.  Or the collection of bits might be a pointer to one object in a linear array of other objects.  In that case, the number of bits in the collection determines how many discrete objects that collection of bits is able to refer to.



So far, it would appear that the more bits the merrier.  After all, what's wrong with being able to represent more values than you might need to?  Or what's wrong with being able to refer to more objects than you could have?  In all cases, logic would suggest that there will be some minimum number of bits required to do the job, but that having unused bits would not be a problem.



There are at least two problems with having unneeded and unused bits.  One is power consumption.  All other things being equal, and they pretty much are, 32 bits consumes four times the power of 8 bits.  And those 32 bits consume equal amounts of power whether they are zeroes or ones because they still need to be checked and moved around.  And speaking of moving them around, whatever the bit-size is, that is, the size of the collection, all of those bits need to be moved around.  They probably need to be loaded and stored from and to main memory.



But as we know, memory bandwidth has become one of the limiting and crippling factors in modern computing.  Since main memory has been unable to keep up with our data-hungry processors, all manner of multilevel caching has been deployed in an attempt to decouple the slow main memory from our super-fast, super-hungry CPUs.  So if a CPU is using 32 bits to refer to a small collection of objects that could be referred to by only 8 bits, then 24 bits out of the 32 bits of precious memory bandwidth are being completely wasted.  The CPU is only obtaining 25% of the system's possible memory bandwidth if, as in this example, 32 bits are being used where 8 bits would have been sufficient.



The other consequence of unused bits is the size of the program's code and data.  If the code is using 32 bits where 8 would have been sufficient, the resulting code will be much larger, by as much as four times.  That means it takes as much as four times more space, takes four times longer to load, and runs as much as one quarter the speed since there's just so much more code bulk for the CPU to slog through.  So this all tells us that the theoretical optimal number of bits to use would be just as many bits as are required, but no more.  In fact, there were, back in the early days of digital computer design, variable bit-length computers.  Early serial memories such as paper tape and magnetic tape, drum and delay line, did not inherently bring along any natural boundaries.  So they worked well with the idea of variable bit-length computing.



And remember that back in the '40s and '50s this was all being invented.  No one knew back then what answer would be best, what the answer would be.  So who knew?  Perhaps variable-length computing would turn out to be the best.  But when core memory was invented, it turned out to be far more convenient to "stack" the "planes" of core memory grids and to read out a fixed number of bits in parallel, one from each plane of memory.  So it became natural for computers to load and store data in the same bit lengths as the core memory could produce and consume per cycle.  And that approach of grouping data bits into bytes and into words having fixed lengths has stuck with us ever since.



So what does all this have to do with SpinRite?  SpinRite was born when the IBM PC was using the 8088 x86 chip.  It had 16-bit registers and 20 bits of addressing.  The 16-bit registers could represent 64K different values.  And 20 bits of addressing could refer to one megabyte of RAM.  But back then, one megabyte was so much memory that it wasn't even standard. It was quite common for the first PCs to have 256K or 512K of RAM, because programs back then were small, efficient, and didn't really do very much.



The PC's BIOS, MS-DOS, and all of the DOS clones were, and still are today, 16-bit code, and they would only run 16-bit programs.  Yet an incredible amount of work was accomplished by WordStar, WordPerfect, VisiCalc, Lotus 123, dBase III, FoxPro, and many other 16-bit programs, including SpinRite.  Today, SpinRite is a 16-bit program because it's still running on the 16-bit DOS operating system.  And even today, I'm not unhappy about that because most of what SpinRite is doing fits nicely into 16 bits.  And again, as we've just seen, more bits is not always better.



But even back then, many of those other programs were bursting at their seams within the constraints imposed by 16-bit code.  SpinRite wasn't one of them.  But the PC's memory was first expanded, and then it was extended using elaborate page-swapping schemes to allow data hungry tools like Lotus 123 to operate upon much larger data sets.  And toward the end of all that, DOS extenders were created to host true 32-bit programs on poor old 16-bit DOS.  Doing this was an act of sheer desperation.  It was the definition of kludge because DOS could still only run under the CPU's real mode, and protected mode was required to go beyond 16 bits to true 32 bits.  So the CPU was being dynamically switched back and forth on the fly between real mode and protected mode.  It was a mess.



But SpinRite is not pure 16-bit code.  It's true that it's running in real mode alongside the 16-bit BIOS and 16-bit DOS.  But SpinRite 6.1, not 6.0 but 6.1, takes advantage of what must have been an inadvertent bug in the way the very first 80286 chips operated, and which bug has appeared in every chip since because Intel must have been worried about breaking backward compatibility if they were to fix this bug.  The bug arises from the fact that the limitations inherent in the 8088's and 8086's real mode, which is the only mode they had, can be simulated by protected mode.



So the 80286 chips and all of the chips which have followed don't actually have real mode.  They have what amounts to a "clamped down" protected mode with full access to the system's I/O hardware.  So there are no restrictions there.  They're basically simulating real mode, and there was always a subtle bug in that simulation.  Since I've talked about this before in greater detail, I won't go into it again.  But suffice to say that SpinRite uses this bug to obtain 32-bit access to any machine's first 4GB of RAM from within 16-bit real mode code.  It's kind of the best of both worlds.



SpinRite itself remains small, with most of it remaining pure 16-bit code.  But when and as necessary, it's able to directly address any memory that's accessible with 32-bits, which is 4GB.  Doing this was necessary in order to obtain the absolute maximum possible performance from AHCI-connected drives.  And boy, will it be necessary for NVMe under SpinRite 7 because it's able to read very large 32K-sector, 16MB blocks with a single command, all at once.  And, boy, we've seen what that means.



But in real mode the program counter, which points to instructions, is still 16 bits long.  So code must be executed within 64KB blocks.  And one of those is what SpinRite has outgrown, thus now needing a second code segment block to contain its growth.  Once SpinRite 6.1 is finished and published, I plan to immediately begin the work of recoding 16-bit SpinRite into true, pure, 32-bit code because it is moving to a 32-bit operating system in order to be able to boot over UEFI.



LEO:  Interesting.  Is it still FreeDOS?



STEVE:  No, because no DOS will ever be 32-bit.  So it's not FreeDOS.  I found a little embedded 32-bit protected-mode real-time operating system, which is like a perfect host for this.  And it will be able to dual boot BIOS or UEFI.  And that will just - that basically takes all the limits off of what SpinRite will be able to do.  For example, in the future, running on all of a user's drives or a system's drives at once rather than one at a time.



LEO:  Oh, nice, yeah.  That's a great thing.



STEVE:  So, yeah, it'll be - it's going to be very cool.  So anyway, I just sort of wanted to, you know, just sort of touch on the reality that, when I hear people especially talking about 64 bits, I just think, oh, really?  Come on.  I mean, again, there really are real-world problems associated with having to shuffle all those bits back and forth, if you don't need them.  So okay.  Some feedback from our wonderful listeners.  Oh, and Leo, you forwarded this email to me this morning.



LEO:  Yes.  And you saw it, too, apparently, yes.



STEVE:  Yeah, I did.  Jonathan Leitschuh - I hope I'm pronouncing your name correctly, Jonathan, L-E-I-T-S-C-H-U-H.  He tweeted two tweets yesterday and, as I said, followed up with a piece of email to the TWiT gang.  Anyway, I just wanted to share it.  He's with the Open Source Summit.  And he said:  "@SGgrc and @leolaporte, I've been listening to @SecurityNow for years.  This year I'm speaking at @BlackHatEvents and @Defcon, and @BSidesLV!"  He said:  "I learned so much from your show.  I wouldn't be where I am today without it.  Thank you so much for the amazing education you've provided."



And then in the next tweet he continued:  "It's from your stories of great hackers like Tavis Ormandy and Dan Kaminsky and many others that I learned the norms of vulnerability disclosure.  I learned the importance of building software that is secure by default."



LEO:  Oh, that's nice.



STEVE:  So Jonathan, thank you for the tweets and congratulations on being at Black Hat and DEF CON as a speaker.



LEO:  No kidding.  Speaking of Dan Kaminsky, I forgot to tell you this.  But Dan's mom Trudy was on the cruise with us. 



STEVE:  Ah.



LEO:  And you remember, you told quite a story about Dan's mom.  She was quite famous defending her son.  She told more stories like that on the cruise, including when he was 11 he'd been writing for computer magazines.  And the computer publisher, not knowing he was 11, said, "I've got a big project for you."  And his mom, who was very protective - Trudy was amazing.  It was really nice to meet her on this cruise.  She was very protective.  She said, "You know he's 11?"  And Dan was really mad at her, saying, "But I can do it."  But yeah, she was great.  And she had not heard your tribute to Dan.  Of course Dan, as we know, passed away last year at the age of 42.  It was very tragic.  So her friend who brought her on the cruise is going to play that tribute to Dan for her.



STEVE:  Oh, good.



LEO:  But she was, I think, really glad she came on the cruise.  She'd been pretty depressed.  And she came on, and she got so much adulation and got so much out of telling her stories about Dan to the group that I just thought I'd pass that along to you.  It was kind of neat, yeah.



STEVE:  Very cool.



LEO:  Thanks for joining us.



STEVE:  And an example of what you get when you don't expect it on one of these cruises.



LEO:  Besides COVID, yeah.



STEVE:  Yeah.  Oh.  Yeah.  I said what you didn't expect.



LEO:  Well, yeah, I kind of expected the COVID; you're right.  Did not expect Trudy.  That's one of the fun things about meeting our audience.  As you know, you've learned this, our audience is very diverse and sometimes very interesting.  One of the guys on there was Army Human Intelligence.  I said, "What do you do for the Army?"  He said, "HUMINT."  I said, "Oh, what's that?"  He says, "Well, I do interrogations.  I was at Abu Ghraib."  And I went, oh.  And we had some very interesting conversations.  Really fascinating stuff.



STEVE:  Very cool.



LEO:  So just some super smart, talented, interesting people.  So as you know, it's always fun to go on these meetups and trips and so forth because...



STEVE:  Oh, and Lorrie and I had a ball when we were out traveling around meeting everybody.



LEO:  Oh, people love you, yeah, of course.



STEVE:  So Alex Neihaus - Neihaus?



LEO:  Neihaus, yeah.  We know Alex well, yeah.



STEVE:  We know him well, a great friend of the show.  The very first sponsor that this podcast ever had.



LEO:  Exactly right.  Thank you, Alex.



STEVE:  He said:  "I especially enjoyed your and Leo's mirth at the astonishing flaws in the MV720 in Episode 881."  He says:  "I worked in China in the mid '90s.  This doesn't surprise me.  It's a cultural thing, a fundamental business drive to maximize profit by cheapening the product."  To which I say, well, god help us.



LEO:  That's what you get.



STEVE:  Yeah.  Mark0j, he said:  "Hi, Steve.  Long-time listener here.  Love your show.  Just wanted to drop an information that there is an open source version of Tailscale that is called Headscale."  That's kind of clever, head versus tail.  "Seems good alternative to the paid version.  Available here."  And a link to it at GitHub.  So Headscale is it, and I just wanted to pass that on because there is a community-supported Tailscale, but they're pushing people to subscribe, and I would also head toward the free one.



Someone tweeting as Henning said:  "Dear Steve.  Just listened to GRC" - oh, he said GRC-881.  He means SN-881.



LEO:  Close enough, yeah.



STEVE:  "And your criticism of the way Microsoft implemented the protection against brute force password guesses," he said, "(allow 10 failed logins, then ban any logins for 10 minutes, then allow 10 failed logins again and so on).  You asked why Microsoft did not block the logins indefinitely."  And frankly, once he said it, it's like, duh, of course.  He says:  "And this is my answer because I designed a login system for our university's web applications that works the same way.  If we would block accounts, then it would be easy for anyone of our aspiring computer science students to produce a small script that locks out all of our users..."



LEO:  Of course.



STEVE:  Uh-huh.



LEO:  Oh.



STEVE:  In one night.



LEO:  Just by attempting failed logins.



STEVE:  That's right.



LEO:  You could lock everybody out.



STEVE:  Completely prone to abuse.



LEO:  See, you and I - but this is a problem all security has.  We didn't think evil enough.



STEVE:  We did not think evil; you're right.



LEO:  Yeah, yeah, yeah.  Wow.



STEVE:  Anyway, thank you very much.  And to the other - a number of our listeners said, uh, Steve, you know what the problem with that is.  It's like, uh, yeah.  I do now.  I mean, yeah, thank you.  Now, many of our xkcd-following listeners sent me the following graphic.  I've got it in the show notes.  And it's cute.  And so this is just classic xkcd.  The little picture is titled "Energy tip:  Increase the security of your home power supply by installing an air gap."



LEO:  Yeah, now I get it.



STEVE:  And so it shows the cord coming in from the right to a flood lamp which is aimed at and maybe like about a foot away from a solar panel that then has a cord coming from it to a little box, presumably an inverter, that turns the DC back into AC, and off it goes.  And of course I loved the concept.  It's funny.  But the engineer in me immediately is appalled by the inefficiencies.  You have inefficiency converting the electricity to light.  You have inefficiency of the light escaping in all sorts of directions.  I mean, we can see the light bulb shining here from the side so that's bad.  Then we have the inefficiency that's inherent in solar-to-DC conversion, and the inefficiency of a DC-to-AC inverter.  So yes, I know I'm being far too pedantic.  But I would not have my home air gapped for that reason.



LEO:  I don't think he's serious.  I think it's...



STEVE:  No, no, no, no.  Just it's a wonderful cartoon.  So thank you again, xkcd.  You've provided us with many moments.  Richard, somebody who's got his name as Richard COVID with five syringes and the word "Flu" in his Twitter name, he liked our mention last week and another bit of mirth from us, Leo, about this crazy Microsoft printer weird cloning copy creation.  And I didn't include it in here, but he mentioned, he had a long tweet that went with this picture showing Brother MFC-J985DW Printer, and then another identical one (Copy 1).  And notice it's the second one that has been set to default.



LEO:  Yes, he solved it.



STEVE:  Yes.  He noted that exactly this happened.  For reasons he was never able to figure out, now he knows, the real one stopped working.  The copy worked.  And he said, okay, fine.  He fixed it the way most people do:  I'll just use that one.  It's like, okay.



LEO:  Oh, that's kludge-y, yeah.



STEVE:  Someone tweeting as Hiveware said:  "@SGgrc A little pushback on your Windows OS being creaky and brittle."  He says:  "Take a look at 11's Settings controls.  No longer are they a thin veneer over ancient code.  They are a beautiful evolution of the venerable PropertySheet control, top to bottom.  Credit to MS where credit is due."



Okay.  So after reading this tweet and wanting to include it in the podcast for the sake of balance - I would like to be balanced - I thought I might have missed something.  So I fired up a VirtualBox VM where I keep an instance of Windows 11 because, you know, it's not actually good for anything, and I went over to the Settings controls, after waiting for it to update itself somewhat endlessly.  Much like Kirk on the bridge, I was unable to go to warp drive, or warp speed.



So anyway, it looks, first of all, exactly like Windows 10.  Perhaps I missed what Hiveware was referring to.  But I poked around.



LEO:  Beautiful.  What are you talking about?  It's beautiful.



STEVE:  It's beautiful.  Just like Windows 10.



LEO:  Just like Windows 10.



STEVE:  So I poked around, and actually it was your discussion with Paul from last week, Leo, that led me to the control panel for Sound.  It looks exactly like what we had, perhaps as far back as Windows 3.1.



LEO:  It is in fact the old control panel.  You nailed it.



STEVE:  Yes, it is.  It is the old property sheet control.



LEO:  Yeah, yeah.



STEVE:  Or go to Internet Options dialog.  It's the same as we've had forever.  Or the Device Manager, the same as we've had.  And I could go on and on.  So he says:  "No longer are they a thin veneer over ancient code."  Well, I think that's precisely what those ancient dialogs are.



LEO:  Look at these.  This is hysterical.



STEVE:  I know.  Nothing has changed.



LEO:  This looks like XP.



STEVE:  They are ancient code.  And I think that's great because those are the parts of Windows that still work.



LEO:  Right.



STEVE:  Most reliably.



LEO:  In fact, when you want to change user settings, you don't want the thin veneer crap.



STEVE:  No.



LEO:  That Microsoft has given us.  You want the old school.



STEVE:  You don't want something that looks like a web page.



LEO:  You don't want this, no.  No, you don't.  You don't.  You want the old school.  In fact, many people have learned what the command is to open those old user property settings.



STEVE:  Yup.  So anyway, I didn't mean to diss on Hiveware, but I just, you know, I thought, okay, wait.  Am I missing something here?  Maybe 11 did change everything, and I just haven't dug in deep enough. But no.  It's still Windows 3.1 with, I mean, it doesn't have the same borders and the same colors.



LEO:  No, and it's got rounded corners now, come on.



STEVE:  And for what it's worth, again, I'm happy that they're leaving the stuff alone that works because for that reason, it works.



Okay.  I noted Uhura on the bridge behind Kirk at the top of our show notes.  And I just wanted to make a mention of her passing this last weekend.  Nichelle Nichols at age 89 passed.  And of course she was Uhura through all of the original series three short years, and all six feature-length films.  And the bridge would not have been the same without her.



LEO:  Unh-unh.



STEVE:  We have previous lost Spock, Leonard "Bones" McCoy, Scotty, and Nurse Chapel.  Kirk and Chekov and Sulu are still with us, and we'll keep our fingers crossed that they keep going for a long time.  So what an incredible legacy.  There was a book, I guess I read two of them, it was called "The Enigma Cube," Enigma something.  There were two of them.  Anyway, there was a funny little bit there where - I'm trying to remember how - it was about time travel.  And somebody, a really very gifted physicist was confessing to some friends like in the far future that he really liked this really cheesy science fiction series that he felt was really beneath him, and it only had three seasons, and it was called "Star Trek."  And he was sure that nothing was ever going to come of it, but he liked it anyway.  And of course the joke was that, yeah, it's still spawning new series even today, in 2023.



And also, Leo, I wanted to mention, I think we talked about it on the air last week, two series, "The Dropout" on Hulu and "WeCrashed" on Apple TV.  I really enjoyed them.  And for those who don't know the story of Theranos, a very young female Stanford University undergrad, who basically wanted to achieve Steve Job-style success and apparently who believed that all she needed was to want it badly enough, decided that being able to perform hundreds of blood screening tests given a single pin-prick drop of blood from a user's finger would change the world.  And of course in that she was correct.  That would certainly would have changed the world.  But she was wrong in her belief that all she needed to do was wanting it badly enough.  And she had, believe it or not, no education or training in the science that would be needed to realize her dream.  None at all.  There was never any reason to believe that it was possible, and there was a lot of reason to think that it wasn't.  But those pesky details didn't give her any pause.



Now, being a bit of a biohacker myself and being somewhat enthralled by this presentation, I was interested in a couple things.  I wanted to know how accurate the series was, and I was interested enough in the blood testing technology that this Elizabeth Holmes and her Theranos were attempting to commercialize that I followed up by watching as much of the various YouTube clips from the time and subsequently as I could, that is the real stuff, because everything's on YouTube now.  And I even did some research in the U.S. Patent and Trademark Office to get some sense, as I said, for how accurate its portrayal of these events and characters was.



LEO:  Well, it was based on a podcast, so you know it's accurate.  "The Dropout" was based on a podcast, as was "WeCrashed," by the way.  They both came from podcasts, oddly enough.



STEVE:  Right.  So for anyone else who may be interested or curious, given the available public record, everything portrayed in "The Dropout" was astonishingly accurate.



LEO:  It was well researched, yeah.



STEVE:  Right, oh my god, right down to the lighting used to shine off the corneas of Elizabeth Holmes' eyes...



LEO:  Yes, I noticed that, yes.



STEVE:  ...in a series of memorable close-ups.  I even found a multi-page document which dissected the "Edison" mobile lab machine that they were never able to get to work.  What was clear was that, even if it had worked, it would never have been able to achieve what they were promising.  In the series that we saw in "The Dropout," the original scientists who were working on the original concept ended up leaving the company through various reasons.  And their work was discarded, and basically a robot glue dispenser was used to replace this thing.  And it was downhill from there.  But for anyone who's interested, if you haven't seen it, I really enjoyed it.  I recommend it.  And "WeCrashed" I thought was equally good.



You know, both are - and you and I, Leo, were talking about this before the show.  I think you and I particularly, and other people in the industry, would be drawn to this because we've seen this over and over and over; right?  I mean, it is the Steve Jobs personality type that is able to bring Apple back to life from the dead.  I mean, remember how close it was to being gone.  And to create these things.  I mean, sketchy as Elon has become, when you see those rockets come back to Earth and land, it's like, oh, god.



LEO:  They work.  They work, yes.



STEVE:  It's astonishing.  And it takes people who have this kind of personality.  I thought that "WeCrashed" was mostly a demonstration of that personality.



LEO:  Oh, I agree, yeah.



STEVE:  To me it was less - yeah.  And well cast.  Boy.



LEO:  Oh, is he good.



STEVE:  That guy, he was a...



LEO:  Jared Leto is so good, yeah.



STEVE:  Oh, just fabulous.



LEO:  Very believable, yeah.



STEVE:  So, yeah.



LEO:  Yeah, I think they both are indictments of that Silicon Valley culture where you can raise a lot of money based essentially on hype.



STEVE:  Yes.  And charisma.  And the venture capitalists certainly know that for every one that succeeds big, 20 die horribly.



LEO:  Right.



STEVE:  And burn up a lot of cash.  And BMW leases are ended and blah blah blah.  I mean, massive amounts of money gets burned through.



LEO:  Oh, yes.



STEVE:  What was not clear to me about "WeCrashed" is the timeline.  I think they didn't succeed in showing us that this was a 10-year span.



LEO:  Right.



STEVE:  Because, you know, it was crammed into eight episodes.



LEO:  Right.



STEVE:  So somehow I missed the sense of the time that went by, which I think would have, you know, it was sort of missing from that.  I don't know how you do it.  I mean, they kept flashing years up on the screen, the date.  But they didn't convey the time that was taken.  But anyway, I recommend both without hesitation, if you're interested in this idea of fundraising personalities.  It's not the way I operate.  When I was creating the light pen, I built one and got it to work.  And then I built, like, 12, and I sold them.  And I took the money from those and built a hundred, and sold them and took the money from those and, you know.  Bootstrapping, the old-fashioned way.



LEO:  Very old school, yeah.



STEVE:  Which is the way Jobs and Wozniak started in the beginning with that Apple I; right?



LEO:  Right, right.



STEVE:  I mean, they got the Byte Shop to agree to buy some.  And, who knows, they probably got a down payment from him.



LEO:  They did.



STEVE:  And then they used that in order to build something.



LEO:  There's a famous phone call Steve Jobs made to get the parts on credit, which no one would have given this hippie kid, but somehow he was a good talker, and he really - he in some ways is the prototype for both Adam Neumann and Elizabeth Holmes; right?



STEVE:  Yes.  Well, he was clearly Holmes' inspiration.  She talks about it.



LEO:  Oh, yeah.  Black turtlenecks and...



STEVE:  She went to the black turtleneck, yes.



LEO:  Yeah, yeah, yeah.  Good TV, too.  That's, I think, the other side of that is both entertaining shows, yeah.



STEVE:  Yeah, really good, and really well produced.  And I will close, before we talk about our show topic, with an observation that Winamp has released, is about to release, well, it's released a release candidate after four years in development.  And I don't recall what caused me to mention Winamp last, but when I did, I remember like adding something, "Winamp, really?"  And I was immediately scolded by a number of our listeners who said, "Hey, I'm still using it.  It's the best media player ever created by man."



LEO:  Yup.



STEVE:  And I can certainly respect that since I've created a few things that others regard as the best ever, and they've been around for quite a while.  So I just wanted to give our intrepid diehard Winamp fans the news that after four years of development, the first new release candidate of Winamp has been released.  Lawrence Abrams over at BleepingComputer was where I learned of this.  And basically they've spent all of this time moving the code from Visual Studio 2008, which by the way is what I'm still using because it works great, to Visual Studio 2019.  They now have it under 2019 so future movement will be easier.



He wrote:  "Winamp ceased development after version 5.666" - ooh, that's the wrong place to stop - "was released in 2013."  And he says:  "That was until October 2018, when Winamp 5.8 was leaked online, and the developers decided to publish it themselves on the Winamp.com website."  And I think it was that leakage online that must have been the reason I mentioned it previously because that seems about, like, 2018, about the right timeframe.



And he said:  "Since then, the developers have promised an updated version with cloud streaming support and more modern features."  Oh, it looks just wonderfully funky still.  Lawrence has a screenshot of it, and it's so retro that it just warmed my heart, Leo.  Anyway: "Finally, in November 2021," he said, "the Winamp.com website received a facelift with a new logo and a beta signup form to be notified when new versions were released.



"Last week, Winamp 5.9 RC1 Build 9999 was released, marking it as the first version released in four years and as the first release candidate of the revitalized media player.  While the Winamp release candidate does not contain too many changes, the main goal of this release was to upgrade the code base from Visual Studio 2008 to 2019.  Now that this has been completed, the team can add new features and capabilities to the media player."  So anyway, Winamp is not dead.  It hasn't been moving a lot lately, but there's still life left in it.



LEO:  Not dead yet.



STEVE:  It's just a flesh wound.



LEO:  Just a flesh wound.



STEVE:  That's right.



LEO:  No, I think it's really cool.  And it's a great product and has been for years.



STEVE:  The importance of language.  And one of those sort of conversations and you have in college, like along with the meaning of life.



LEO:  Right.



STEVE:  It is, okay, we think in the language that we speak.  That's the way we manage our own thoughts and the way we process reality.  So that means that the thoughts we have need to be expressible in the language we speak.  So to what degree does our choice of language constrain what we're able to think.



LEO:  Yes.



STEVE:  And that'll keep you up late at night.  It's like, wait a minute.  You mean I can't think anything I want?  No.  You can only think what you're able to express in your language.



LEO:  This is one of the things the great Noam Chomsky, who is very important for computer science, but he was a philosopher of language.  Gosh, he's 90 now, I'm seeing.  



STEVE:  Yeah.



LEO:  Wow.  But great philosopher of language.  And that was one of the things he was always intrigued by is how much of the language you use, how much of what you think is informed by your choice of language.  And I think it's a fascinating, fascinating subject, yeah.



STEVE:  Yeah.  Okay.  Yes.  Rowhammer is back, alive and kicking.  Not only is Winamp not dead, but neither is Rowhammer.



LEO:  Well, there you go.  Nothing ever dies.



STEVE:  And it really shouldn't surprise any of us by now.  The principle we've seen and learned over and over in context after context is that "mitigating" a security flaw is not the same as fixing it, and security flaws turn out to be stubborn things.  All we need to do is look at the continuing saga being brought to us by Spectre and Meltdown, where the discovery of fundamentally exploitable flaws arising from the advanced performance optimizations which have been deeply built into modern CPUs, which can be used to leak secrets across process isolation boundaries, and we see an example of a family of flaws that similarly refuse to die.



And as we know, another completely unrelated, yet still fundamental flaw was discovered years ago, and we talked about it at the time, in the operation of today's dynamic random access memory.  Security researchers discovered that modern memories - this is DRAM memories, main memories of our systems -  had become so hyper-dense with every possible margin engineered out, that adjacent rows of bits which were, of course, supposed to be completely independent of one another, were in fact often able to interfere with each other.  And being the super clever engineers that they are, these researchers figured out how to turn the resulting spontaneous "bit flips" that could be induced to occur into active and effective security exploits.  I mean, just the idea of leveraging a hardware anomaly like that into a security exploit, I mean, that's just - that's genius.



Of course the DRAM manufacturing industry responded, but not by reducing the storage density of RAM back down to where there would be sufficient noise immunity to prevent adjacent row interference.  Oh, no.  It responded with a  wait for it  "mitigation" of the problem by making the memory system still more complex by selectively refreshing the endangered rows of DRAM which surround the rows which might be causing interference, whether that interference was deliberate or accidental.  This was named "Target Row Refresh," or TRR.  And it's now built into DDR4 RAM.



So you know you're in trouble when DRAM is the subject of an apparently endless series of CVE-worthy exploits.  There was the original Rowhammer.  Then we had DRAMmer, which was the double Rowhammer.  Then RAMBleed.  Then SPOILER.  Then TRRespass with TRR, so two R's, TRRespass.  And then last year's Blacksmith.  It's been a never-ending parade.  And now, having not yet used up its nine lives, we have the less glamorously named Half-Double attack.  The name for this latest attack comes - get this - from a crochet stitch which is taller than a single but shorter than a double.  I know.



The point is, the actual fundamental problem of DRAM activity noise immunity being too low was never addressed.  In case after case, since the cost of treating the underlying disease was too high  it would have meant backing off on DRAM storage density  instead of treating the disease, one symptom after the next has been treated.  So today even the most recently engineered DRAM has remained diseased, with some fancy patchwork added in an attempt to shore up its fundamental problem.



I named today's podcast "Rowhammer's Nine Lives" because this problem, as I've said, refuses to die.  These amazing researchers are back, having almost not surprisingly worked around the most recent iteration of Rowhammer mitigations.  Their paper, which will be delivered during the same USENIX security conference next week where the TLS-Anvil will be shown, is titled "Half-Double:  Hammering From the Next Row Over."



The abstract of their paper explains.  They said:  "Rowhammer is a vulnerability in modern DRAM where repeated accesses to one row, the aggressor, give off electrical disturbance whose cumulative effect flips the bits of an adjacent row, the victim.  Consequently, Rowhammer defenses presuppose the adjacency of aggressor-victim pairs, including those in Low Power DDR4 and DDR4, most notably TRR."  That's the Target Row Refresh.



"In this paper we present Half-Double, an escalation of Rowhammer to rows beyond immediate neighbors.  Using Half-Double, we induce errors in a victim by combining many accesses to a distance-2 row with just a few to a distance-1 row.  Our experiments show that the cumulative effect of these leads to a sufficient electrical disturbance in the victim row, inducing bit flips.  We demonstrate the practical relevance of Half-Double in a proof-of-concept attack on a fully up-to-date system.  We use side channels, a new technique [they invented] called blind-hammering, a new spraying technique, and a new Spectre attack in our end-to-end Half-Double attack.  On recent Chromebooks with Error Correct Code (ECC) and TRR-protected Low Power DDR4 memory, the attack takes less than 45 minutes on average."  And as I mentioned, they get root.



So what these guys have figured out is sheer brilliance.  They came up with a way of turning the Target Row Refresh Rowhammer mitigation, which is now present in all of the latest DDR4 DRAMs, against itself.  They've figured out how to use TRR to induce the problem that it's designed to prevent.  Here's how they described what they accomplished.



They said:  "Rowhammer is a widespread DRAM issue caused by the unintended coupling between its constituent rows.  By repeatedly accessing one row, the aggressor, an attacker can corrupt data in adjacent rows, the victims, by accelerating their charge leakage.  As a powerful means of bypassing hardware and software memory protection, Rowhammer has been used as the basis for many different attacks.



"Previously, Rowhammer was understood to operate at a distance of one row.  An aggressor could only flip bits in its two immediate neighbors, one on each side.  This makes intuitive sense.  As a coupling phenomenon, the Rowhammer effect should be the strongest at closest proximity.  Indeed, this assumption underpins many countermeasures which have been proposed against Rowhammer, especially the ones that rely on detecting aggressors and refreshing the charge in their intended victims.  In fact, Target Row Refresh (TRR), a productionized countermeasure widely deployed as part of Low Power DDR4 or standard DDR4 chips, falls into this detect-and-refresh category.



"In this paper, we present Half-Double, a new escalation of Rowhammer where we show its effect to extend beyond just the immediate neighbors.  Using Half-Double, we're able to flip bits in the victim by combining many accesses to a far aggressor, at a distance of two, with just a few to a near aggressor, at a distance of one.  Both aggressor distances are necessary.  Accessing just the far aggressor does not flip bits in the row that's two away, whereas accessing just the near aggressor devolves into a classic attack that's easily, and is, mitigated.  Based on our experiments" - get a load of this - "the near aggressor appears to act as a bridge, transporting the Rowhammer effect of the far aggressor onto the victim.  Concerningly, TRR actually facilitates Half-Double through its mitigative refreshes, turning their recipient row into the near aggressor that co-conspires with the far one that necessitates the refresh in the first place."  It's just brilliant.  "In effect," they wrote, "the cure becomes the disease."



They said:  "While the discovery and evaluation of Half-Double is the main contribution of this work, we also demonstrate its practical relevance in a proof-of-concept exploit.  However" - and boy, did they have to work to do this - "current systems limit the attacker's control, introducing four challenges," all of which they had to overcome.  



"First, the adversary needs to allocate memory contiguous in a DRAM bank.  However, without access to physical addresses and huge pages, we have to introduce a novel approach combining buddy allocator information with a DRAM timing side channel to reliably detect contiguous memory.



"Second challenge, ECC-protected memory can make bit flips unobservable depending on the victim data which the attacker does not control.  The adversary cannot template the memory like in previous Rowhammer attacks, since hammering requires knowledge of the cell data.  As the state of the art does not solve this problem, we invented and introduced a novel technique called Blind-Hammering to induce bit flips despite the error correction (ECC) mechanism of LPDDR4.



"Third challenge to overcome:  Reduced address space sizes on recent ARM-based systems break the page table spraying mechanism from previous attacks.  Therefore, we had to develop a new spraying technique that is still unmitigated.



"And fourth challenge:  Without templating, we need an oracle telling whether Rowhammer induced an exploitable bit flip  without crashing the exploit.  For this, we invent and introduce a novel approach using a Spectre-based oracle for exploitable bit flips."



And here it is.  They said:  "We combine all of these techniques into an end-to-end successful proof of concept, the Half-Double Attack, which escalates an unprivileged attacker to arbitrary system memory read and write access.  In other words, full kernel privileges.  The Half-Double Attack runs within 45 minutes on a fully updated Chromebook with TRR-protected Low Power DDR4 memory."



And they finish:  "To summarize, we make the following contributions.  First, we discover a new Rowhammer effect, Half-Double, and evaluate a set of devices and modules for susceptibility.  We perform a thorough root-cause analysis to empirically prove that TRR is responsible for the Half-Double effect.  We analyze the stop-gap mitigations present in today's systems and show that, with a new exploit using Half-Double, we can bypass them and build an end-to-end attack.  And our end-to-end Half-Double Attack runs on up-to-date Chromebooks and combines the Half-Double effect with exploitable techniques, side channels, and a Spectre attack."



So the moral of our story is responding to fundamental exploitable design flaws with mere mitigations only delays the inevitable.  I expect that DDR5 DRAM will learn the lessons of the Half-Double attack to render it, too, less effective by further complicating our DRAM.  Maybe that will be the end of it, or maybe DRAM and Rowhammer will turn out to have still more life left in it.



LEO:  Do you maybe, would you say maybe that you should eradicate, not mitigate?  I don't know if that makes sense.  It rhymes, though.



STEVE:  Hey, Leo, great marketing term.  You could raise some money with that.  Eradicate, don't mitigate.



LEO:  Don't mitigate, eradicate.



STEVE:  That's right.



LEO:  I'm good at writing ad copy.  That's my one and only skill.  That's it.



STEVE:  That would do it.  That would do it.



LEO:  Steve?



STEVE:  So anyway, Rowhammer's still with us.  And, boy, so clever to be able to use the previous hardware mitigation in order to facilitate the next-generation attack.



LEO:  Gumby says, "Leo, leave the sloganeering to the pros."  Okay.  Steve Gibson, you did it again, as you do every Tuesday on Security Now!.  Thank you so much for being here.  This is of course the one and only show you have to listen to every week just to keep up with the fast-paced world of bad guys.  Steve is at GRC.com.  That's the Gibson Research Corporation.  There you'll find his bread and butter, his life's work, the best mass storage maintenance and recovery utility in the world, SpinRite.  6.0 currently.  Somebody in the Discord was saying when is 6.1 coming out?  And I told them, when it's done.  Not one minute sooner.  Or later.  So you'll find, right now if buy 6.0, you'll get 6.1 when it comes out as a free upgrade.  You can also participate in this development.  It is awfully close now, I have a feeling, I feel like.  But you're right not to say it.  Don't...



STEVE:  Can't.  Can't.



LEO:  No, no, no.  It's tempting, I know, but no.  If you're at GRC.com, you'll note there is also a place where you can get, besides all the other great stuff, this podcast.  Steve has 16Kb audio versions.  That's unique to him.  And he also has the transcripts, written by Elaine Farris.  Those are very, very useful if you like to read along while you listen, but also if you want to search for something.  You can search the transcripts and go right to that part of the show.  Also 64Kb audio, as well.  That's at GRC.com.  Leave feedback for Steve there at GRC.com/feedback, or on his Twitter.  He's @SGgrc on Twitter.  His DMs are open.  It's worth following him to keep up on the latest.



We also have a copy of the show at our website, TWiT.tv/sn, audio and video available on our site.  Or subscribe in your favorite podcast client.  You'll get it automatically the minute it's available.  There's even a YouTube channel.  If you hear something you say "I've got to send this to the boss or to the IT department," the easiest way to probably send a clip is to go to the YouTube channel, and then you can just snip out that little part and send a link to them.



If you are a member of Club TWiT, of course, you could add free versions of the shows.  You also get the fantastic Club TWiT Discord, where the party goes on seven days a week.  You also have the TWiT+ feed.  Coming up, some exciting events.  August 18th we've got Alex Lindsay's Ask Me Anything.  His AMA's at 9:00 a.m. Pacific.  On the 25th, the following week, Stacey Higginbotham, Ant Pruitt, and I will do the Stacey's Book Club, "Klara and the Sun."  If you haven't read it yet, you've got a couple weeks, three weeks.  Hurry up.  Read it now.  It's a great book.  So if you want to join the club, go to TWiT.tv/clubtwit.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#883

DATE:		August 9, 2022

TITLE:		The Maker's Schedule

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-883.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine the collapse of one of the four NIST-approved post-quantum crypto algorithms.  We look at what VirusTotal has to tell us about what the malware miscreants have been up to, and at the conditions under which Windows 11 was corrupting its users' encrypted data.  We also celebrate a terrific-looking new commercial service being offered by Microsoft, and we briefly tease next week's probable topic, which is cryptographer Daniel Bernstein's second lawsuit against the United States.  I want to share a bunch of interesting feedback in Q&A style from our terrific listeners, then I want to share my discovery of a coder, serial entrepreneur, and writer by sharing something he wrote which I suspect will resonate profoundly with every one of our listeners.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Yes, it's a Patch Tuesday, but we'll save deets on that till next week.  Just, you know, we just like to prepare you for what is about to come.  We are talking about a whole bunch of interesting things.  Steve's found a new favorite writer, Paul Graham.  It's not sci-fi; it's fact.  We'll talk about that.  We're also going to talk about the failure of one of NIST's proposed solutions for post-quantum crypto.  And deception at scale.  What's the biggest problem with viruses today?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 883, recorded Tuesday, August 9th, 2022:  The Maker's Schedule.



It's time for Security Now!, the show where we cover the latest in security, privacy, bad guys, good guys, white hats, black hats, gray hats, and Steve Gibson, who doesn't need a hat.



STEVE GIBSON:  And even really smart people.



LEO:  Even them.



STEVE:  Even them.  We've got two really smart people that are topics:  Dan Bernstein, who is one of my favorite cryptographer mathematicians, and the person who's responsible for the show's topic, a guy by the name of Paul Graham, who I'll be introducing people to who may not know of him.  He really never crossed my radar one way or the other, although he's long been on yours, Leo.



LEO:  Oh, yeah.  



STEVE:  So this is Security Now! Episode 883 for the 9th of August.  This is Patch Tuesday.



LEO:  Whoo, boy.



STEVE:  So next Tuesday we'll find out what happened.



LEO:  Now I know why my Windows machine was so slow starting up this morning.  Ahhh.



STEVE:  Yeah, and why I've been saying no, no, no, please don't reboot right now, I'm trying to do a podcast.



LEO:  Yeah.



STEVE:  So we're going to examine the collapse of one of the four NIST-approved post-quantum crypto algorithms. 



LEO:  Oh, good, I was hoping you'd do this, yes.



STEVE:  We just announced them a couple weeks ago, and remember Scotty's Dilithium crystals and so forth.  Anyway, we lost one.  But the lessons it teaches us are very important.  We're also going to look at what VirusTotal has to tell us about what the malware miscreants have been up to lately, based on all of their stats.  And at the conditions under which Windows 11 was corrupting its users' encrypted data.  Yes, got to love those upgrades.  We also celebrate a terrific-looking new commercial service being offered by Microsoft.  I'm not quite sure what it is from their description, but it looks great.  And we briefly tease what will probably be next week's topic, which is cryptographer Daniel Bernstein, as I mentioned, his second lawsuit that he's filed against the United States.



So before we get into the other stuff, the end of the show stuff, we'll talk about that.  I also want to share a bunch of interesting feedback, sort of more of in a sort of a Q&A style, from our terrific listeners.  And then we're going to wrap by - because I want to share my discovery of a coder, a serial successful entrepreneur, and a surprisingly good writer by sharing something that he wrote which I suspect will resonate profoundly with everyone of our listeners.  So today's title is part of the title from one of Paul Graham's essays.  This is "The Maker's Schedule."  And, oh, we've got a great Picture of the Week.  I will do my best to describe it.  I saw it, and I thought, oh, this is just too perfect.



LEO:  Oh, a big show coming up.  And I'm looking forward to, yeah, Paul Graham has been around for ages.  He founded Y Combinator, which is a big startup incubator in the Bay Area, and very well known.



STEVE:  More than 3,000 startups.  And, boy.



LEO:  Yeah, some of them huge.



STEVE:  And many that we all know.



LEO:  Yeah, absolutely.  But also a great thinker, I agree with you, brilliant writer and essayist and Lisp aficionado.



STEVE:  Yes, he is, near and dear to your heart.



LEO:  You bet.  All right.  Back to Steve and the Picture of the Week.



STEVE:  Okay.  So the way to describe this.  We don't often get, like, thick fog in Southern California.  But I spent the first third of my life in Northern California.



LEO:  Yeah, we get it, mm-hmm.



STEVE:  Oh, boy.  And especially like Sacramento, driving around the Sacramento area, for some reason like this super thick fog.  So remember how, if you've ever driven in fog, how all you can see are the lights of an oncoming car.  It's just two white glowing things in the darkness.  Or even in the day, depending upon what time of the day the fog is. But so the point is that the fog obscures everything that's not lit up.  Okay.



So, and I have to say, this Picture of the Week, it's worth getting the show notes just to see this because this was an actual photo taken.  In the distance is a digital billboard which has - I know, it's just so good - which has crashed or for some reason caused its backing Windows OS to display an error message in a dialog box.  So you don't see the normal - so the billboard itself is not displaying whatever ad it normally would.  Instead, it's this dialog box.  And because this is taken on a foggy night, all you see, like, hovering in the sky is this eerie glowing Windows error dialog.  And so the caption on this great photo says "A digital billboard in Odessa malfunctioned, in the fog, convincing unknown numbers of motorists not only were they living in the Matrix, but it was being run on Windows 98."  So anyway... 



LEO:  That we know can't possibly be true.



STEVE:  Yeah, I don't think that's the case.  But anyway, just a great picture.  So thank you, whomever it was, a listener of ours who sent that saying I think this would be a Picture of the Week.  Boy, were you right.  Absolutely.



Okay.  So we already know crypto is hard.  In fact, it's even harder than we know.  Okay.  How many times have we observed the fallacy of rolling one's own crypto?  And even when professional academics carefully design, test, and vet an algorithm, they sometimes get it wrong.  The biggest news of the past week in the crypto sphere is the fall of one of those four final and chosen post-quantum cryptographic algorithms. But to their credit, even while NIST was announcing their final status, recall that everyone was told not to commit them to code just yet.  Well, that turned out to be sage and prescient advice.



Some guys whose work we've covered in the past, the researchers with the Computer Security and Industrial Cryptography group at KU Leuven managed to break one of those four late-stage candidate algorithms which was ready to be deployed, like soon, for post-quantum encryption.  Fortunately, the Dilithium crystals are still intact.  Those algorithms are okay.  The algorithm they cracked, SIKE (S-I-K-E), which is the abbreviation for Supersingular Isogeny Key Encapsulation, it made it through all of the earlier stages of the U.S. Department of Commerce's National Institute of Standards and Technology, the NIST competition.



So that competition has been running since 2017, five years ago, and started out with 69, 69 candidates in its first round.  That weeded the number down two years later to 26 surviving candidates from the 69.  That is to say, all the other ones, something was, like, problems were found.  It could have been performance.  Most likely it was some security problems, like there were defects found in some of those.  Then the next year, which brings us to 2020, the third round, we had seven finalists with eight alternates.  And as we know, when we talked about this a couple weeks ago, this was the end of the fourth round where we had three finalists and one alternate being selected to become the standards.



So the good news is, even after the announcement of the final four, or perhaps because of the announcement of the final four, the KU Leuven researchers rolled up their sleeves and got to work.  They approached the problem, however, from a different angle than the cryptographers who designed it.  They came at it from a pure math angle, attacking the core of the algorithm's design instead of any potential code vulnerabilities, which is actually how a lot of these previous candidates got weeded out, as I mentioned.



SIKE's base encryption was based upon something known as Supersingular Isogeny Diffie-Hellman, or SIDH for short.  But the mathematicians were able to show that SIDH itself was in turn vulnerable to a pure math attack which had been developed back in 1997 known as the "glue-and-split" theorem, which is an attack on elliptic curves.  In response to this, like the discovery of this, SIKE's co-inventor, David Jao, a professor at the University of Waterloo said:  "The newly uncovered weakness is clearly a major blow to SIKE."  Which is, you know, you can imagine he's feeling a little protective of his algorithm.  So in other words, it killed it.  Or as McCoy would have said, "It's dead, Jim."  And he added that all this goes back to cryptographers' sometimes imperfect dominion of pure mathematics, in that the approach taken by the researchers, he said, was "really unexpected."  Uh-huh.  Well, in any event, unexpected or not, it wasn't going to be good enough.



So essentially this means that the researchers used math rather than mechanics to understand SIKE's encryption scheme and were then able to predict and then retrieve its encryption keys, which is specifically what it was designed to protect.  And for their efforts they received a bounty award of $50,000 from Microsoft after they published their paper titled "An Efficient Key Recovery Attack on SIDH."  And although not directly on SIKE, SIDH being the basis for SIKE, they are the same.



The people who've been watching and participating in this process are concerned that this appears to be as difficult as it is.  In a note which was written, an email actually, to Ars Technica which Ars Technica published, Jonathan Katz, an IEEE member and professor in the Department of Computer Science at the University of Maryland, wrote:  "It is perhaps a bit concerning that this is the second example in the past six months of a scheme that made it all the way to the third round of the NIST review process, and in this case the fourth round, before being completely broken using a classical algorithm."



He added that:  "Three of the four post-quantum crypto schemes rely on relatively new assumptions whose exact difficulty is not well understood, so what the latest attack indicates is that we perhaps still need to be cautious and conservative with the standardization process going forward."  In other words, we knew crypto was hard, but now it appears that it might be even harder than we thought it was.  And part of the process is we're stepping into it like into unfamiliar territory, pre- and post-quantum.  They're, like, it's a whole different thing, a whole different nature of computing that we're needing to be hardened against compared to classical computing, which is what we've been using until now.  Right?  So there's a lot of understanding and basis for grokking the way computers work today.  That's not the way they work tomorrow.



So we've already been doing this work, working to come up with the algorithms which we'll be able to rely on post-quantum, for five years.  And thank goodness that as an industry we started as early as we did, and that we're now focusing upon this as intently as we are.  Even though it's true, and you and I, Leo, have had some fun with this, that sufficiently powerful quantum computers still seem to be a safe distance away - have they factored the number 31 yet?  I don't know - it's increasingly looking...



LEO:  I think 31's a prime, though, so I think they may be a way off.  If they factor 31, then we've got a problem.



STEVE:  Okay.  How about 27?  Let's go with that.



LEO:  There you go.







STEVE:  Or 35 I think is what I meant.  Anyway, it's interestingly looking as though we're going to need more time than we thought to get ready.  And remember that everything being encrypted and stored today, still under pre-quantum crypto, is potentially vulnerable to disclosure once capable, sufficiently capable quantum computing emerges.  And at some point, depending upon what it is that's being stored, it might even be worth upgrading existing stored encrypted data at rest from its current pre-quantum encryption to post-quantum encryption.  In other words, read it, decrypt it under pre-quantum, re-encrypt it under post-quantum, and then store it back.



Now, of course the NSA, they're happily storing everything they can get their hands on, presumably, that is pre-quantum.  And I wouldn't be surprised if they're one of the first people to order one of these big monster post-quantum machines, as soon as they are available.



LEO:  So they have, so just to be clear, it's kind of confusing.  You mentioned Dilithium, Crystals Dilithium.  They have four, already standardized four candidate algorithms which are Crystals-Kyber, Crystals-Dilithium - Crystals-Kyber is the KEM or Key Establishment Mechanism.  And then there's Crystals-Dilithium, Falcon, and SPHINCS.  This was part of a second class of four that - SIKE was part of a second class of four that they were looking at.



STEVE:  Right, right.



LEO:  So it doesn't undermine what they've already done, although it does, as you say, make us wonder, you know, how good these things are.  And I think they've already - they already say they're going to standardize on Crystals-Falcon and SPHINCS; right?



STEVE:  Correct.  So the lattice-based algorithms, cryptographers and mathematicians have been working with lattice-based crypto already for some time.  So it's reasonable to hope that those are going to be stronger and more resistant to I guess what we would call "new approaches" to cracking them.  And of course we want every possible approach; right?  I mean, we want something that is absolutely resistant because the worst thing that could happen would be that the whole world standardizes on something that we all collectively believe is super solid, and then some complete crack is found.



LEO:  Whoops.  You mean like DES or something like that.



STEVE:  Kinda like that.



LEO:  Yeah.



STEVE:  Although generally we've, you know, in the case of DES it was the NSA who said, oh, you know, I think maybe you want to do that three times, rather than just once.  And thus triple DES was found to be sufficiently secure because just doing it three times, each time with a different third of the whole key resulted in enough security margin.  So the nice...



LEO:  So this does point out this is a good process.



STEVE:  Yes.



LEO:  That's what you hope is that somebody comes along and says, no, you can't use that one.  That's why you do this.



STEVE:  Oh, yes.  And so this was useful in two ways.  It both took one of the not-safe-enough contenders out of consideration, and it sort of shocked everyone.  It's like, wait a minute.  And this is the point that this guy was making was this thing made it through five years of scrutiny, down to the fourth round, where people were so tired they finally said, okay, fine.  We've been picking away at this thing for five years.  There are these four.  They all look fine.  Well, they were wrong about one of them.  Are we sure they're not wrong about the other three?



LEO:  Right, right.  Well, that's the point.  They weren't 100% sure of any of those four.  They aren't the four candidate algorithms that they've put forward for standardization.  These were the second choices anyway.  But I'm glad they got rid of one.



STEVE:  Got you.



LEO:  And I hope they continue to look at the four that they have decided to standardize. 



STEVE:  And it's fair to say that they got rid of the one that was less well understood of the four.



LEO:  Yeah.  They liked it because it had a small key.



STEVE:  Yeah.  And I like the other ones because they've got Dilithium in them.



LEO:  Can't go wrong with that.



STEVE:  That's great; you know?  Either warp engines or light sabers.  One way or the other.  Okay.  So VirusTotal.  Last week they published a report titled "Deception at Scale," where they laid out the terrain of the malware samples that are uploaded to them more or less constantly to be analyzed.  They sit in the perfect place to see what's going on.  They've got great scope.



I've explained in the past that signing my own executables, I've discovered the hard way, because people were saying, hey, Windows is saying this is not safe, you've got a virus, it's like, no, I don't.  Actually, it didn't say that.  It just said this is, you know, you don't have any reputation here.  So the point is that signing my executables was not sufficient proof of the integrity of my apps to bypass various of what are now hair-triggered malware cautions.



But VirusTotal reported among other things, get this, that fully 87% of the more than one million malicious samples which were signed at the time they were uploaded to VirusTotal since the start of last year, January 2021, contained a valid signature.  87% had a valid signature, those that were signed.  So what that tells us is that signing code no longer means much.  It's necessary, but not sufficient.  The bad guys are arranging to obtain code-signing credentials, just like any other legitimate code publisher would.  Just like I do.



So moving forward, the only thing that can be used, that is, can be relied upon, is the reputation of the hash of a given executable that is earned over time.  Any new hash will need to start over from scratch earning the reputation that that specific exact code that it's the hash of is trustworthy.



And there was another little interesting tidbit.  If you care to protect yourself somewhat by inspecting the Certificate Authority who issued the Authenticode certificate that was used to sign a program which you're considering running, it's worth noting that more than half, actually more than 58% of the most-often-abused code-signing certificates were all issued by just one company, a Certificate Authority known as Sectigo.  And if the name Sectigo isn't ringing any bells, it's probably because they renamed themselves after their repeated conduct spoiled and soiled their previous name, which was Comodo.  We've talked about Comodo quite a bit in the past, all the different mistakes they made like allowing people to create their own certificates through problems in their web interface and giving certificate minting authentication to people who didn't warrant it and so forth.



Anyway, I imagine that they're the favorite of malware authors mostly because their certs are less expensive than the competition.  And really it's not their fault that VirusTotal sees most malware signed by their certs, since anyone can purchase a code-signing certificate from any certificate authority, so going to go with the cheapest.  I don't, but I don't want to be signed by Comodo, now named Sectigo.  And the whole thing is roughly analogous to what Let's Encrypt did to TLS connections; right?  Once upon a time having a web server certificate meant something.  Not anymore.  Today, everyone needs to have one, and they mean nothing because they're just being minted by automation based on the domain of the server that they're sitting behind.  So okay.



Anyway, VirusTotal also revealed that the top three most-often-spoofed programs were Skype, Adobe Reader, and VLC Player.  Malware is masquerading as those three utilities - one of those three, Skype, Adobe Reader, and VLC as the top three - as basically, obviously, as a means to abuse the well-earned trust that they've earned, that those apps have earned with users everywhere.  And while those are the top three, the top 10 are rounded out by 7-Zip, TeamViewer, CCleaner, Edge, Steam, Zoom, and WhatsApp.  So, yeah, the top of the popular apps that people are needing now to grab wherever they are.



So VirusTotal said in their report last week:  "One of the simplest social engineering tricks we've seen involves making malware look like a legitimate program.  The icon of these programs is a critical feature used to convince victims that these programs are legitimate."  Just the icon.  Of course, no one is surprised that threat actors employ a variety of approaches to compromise endpoints by tricking unwitting users into downloading and running seemingly trusted executables.



The other way this is achieved is by taking advantage of genuine domains, at least the top-level or second-level domains, to get around IP-based firewall defenses.  Some of the most abused domains which VirusTotal has seen are discordapp.com, squarespace.com, amazonaws.com, mediafire.com, and qq.com.  In total, more than 2.5 million suspicious files were downloaded from 101 domains belonging to Alexa's top 1,000 websites.  In other words, 10% of the top 100 website domains have been used as sources for malware.  And the misuse of Discord has been well-documented, with that platform's content delivery network becoming a fertile ground for hosting malware alongside Telegram, while also offering a perfect communications hub for attackers.  



So ultimately, checking anything that's downloaded which might be suspicious against VirusTotal, I think, is the best thing anyone can do.  As I mentioned a while ago, back when I was needing to bring old DOS machines onto my network in order to debug SpinRite on them, I was sometimes needing to go to well-off-the-beaten-path driver repositories to locate old drivers for old network adapters.  Driver repositories are classic sources of malware.



So in every case, I ran anything that I downloaded past VirusTotal to make sure that it didn't raise any alarms.  And normally you get like one or two, some weird obscure, you know, VirusTotal I think scans across or against as many as 75 different virus, you know, antivirus engines.  And you'll typically get a couple reds, misfires, false positives from some scanners you've probably never heard of.  And so that's not a problem.  It's when you see like 20 or 30 of them lighting up red that it's like, okay, do not click this thing so that it's able to run.  And stepping back from all this a little bit, it's so annoying that so much energy is being spent holding back the forces of darkness.  Look at how much we put in now to doing that.  But on balance it's worth it because what can be done with computers today is truly amazing.



Leo, I think I'm going to take a sip of water, and you can tell us why we're here.



LEO:  I just - maybe we should underscore, don't download drivers from third parties.



STEVE:  No.



LEO:  Steve's a trained professional.  Do not try this at home.



STEVE:  Good advice.



LEO:  Yeah.  It just makes me really squeamish.



STEVE:  I know.  And believe me, it made me just, like, I...



LEO:  Is it because they were gone?  They were no longer available?  Is that why?



STEVE:  Oh, yeah.  You try to find a driver for a 1986 network interface card, I mean, it's just for DOS.



LEO:  I guess you have no choice, yeah, yeah.



STEVE:  For DOS.  For DOS.



LEO:  Yeah, what are you going to do?



STEVE:  And it's a high level of pucker factor.  And I wouldn't run them on Windows.  I ran them, I moved them immediately to DOS.  There's not much you could do on DOS.  Basically, if a virus comes alive on DOS, it's like, oh, crap.



LEO:  Yeah, I mean, yeah.  Seriously.  That's a virus that's been hanging out for a long time.  It's pretty - that's a geriatric virus.



STEVE:  It's like, what's 32 bits?



LEO:  I don't know what this Windows thing you're talking about is.  There's a story about a mother and daughter in Nebraska who just got arrested and charged with felonies.  And how did the authorities get the information?  They subpoenaed their Facebook DMs.



STEVE:  Wow.



LEO:  You know, we're really now in a situation where the government is happy to use these technologies against you.  And I think we're [crosstalk] to defend ourselves.



STEVE:  And it's why the big question is whether encryption will be allowed to survive.



LEO:  I'm sure they won't.  I'm sure.  They hate this.  They hate this.  They don't want you to be private.



STEVE:  It's just a matter of time.  So two stories about Microsoft.  One, the first is a little rough.  But the second I give them some props.  The rough one is last week Microsoft warned in their knowledge base article KB5017259 that "Windows devices that have the newest supported processors might be susceptible to data damage."  Now, though their poorly worded title doesn't make it clear, it's not the Windows devices with the newest supported processors that might be damaged.  It's users' encrypted data that has been damaged by the simultaneous use of Windows 11 with the new vector encryption instructions that are present only in the latest processors.



Microsoft's posting explained that:  "Windows devices that support the newest Vector Advanced Encryption Standard" - Advanced Encryption Standard is AES, the Rijndael cipher, so this is VAES - "instruction set are susceptible to data damage.  The affected Windows devices are those that use either AES XEX-based tweaked-codebook mode with ciphertext stealing" - which is AES-XTS, and that meant something to me, I'll explain it in a second - "or AES with Galois/Counter Mode."  That's of course GCM, so AES-GCM.  Both of those immediately raised my eyebrows since AES-GCM has become the preferred authenticating encryption mode for bulk encryption, and AES-XTS is the way data at rest is stored encrypted in mass storage.



And sure enough, in their knowledge base article, Microsoft wrote:  "To prevent further data damage, we addressed this issue in May 24th, 2022 preview release and the June 14th, 2022 security release.  After applying those patches," they said, "you might notice slower performance for almost one month" - and I thought, what?  A month?  What? - "after you install them on Windows Server 2022 and Windows 11," they said, "(original release).  The scenarios that might have performance degradation include BitLocker, Transport Layer Security (TLS), and disk throughput, especially for enterprise customers."



In other words, the previously faulty encryption that was being used by Windows 11 for BitLocker and TLS communications was fast, but broken.  And it was damaging its users' data.  So they fixed that quickly in May and June by no longer using the VAES instructions at the cost of performance.  Which is why they said you may notice things going slower, but at least your data's not being scrambled.



They wrote:  "If this affects you, we strongly urge you to install May 24th, 2022 preview release or the June 14th, 2022 security release as soon as possible to prevent further damage."  Then they said:  "Performance will be restored after you install the June 23rd, 2022 preview release or the July 12th, 2022 security release."  In other words, they're saying that they found and fixed the broken VAES implementation and have restored Windows 11 to use VAES with the most recent updates.  So now you get speed; and, happily, it's not damaging your data.  So this was all resolved by last month's Patch Tuesday.  But there was a period, and no one really knows how long it was, where Windows 11, when used on the most modern processors which had this VAES set of instructions, the Vector AES, had broken Windows core crypto algorithms.



Microsoft wrote:  "We added new code paths to the Windows 11 original release and Windows Server 2022 versions of SymCrypt to take advantage of VAES, the vectorized AES instructions.  SymCrypt," they wrote, "is the core cryptographic library in Windows.  These instructions act on Advanced Vector Extensions (AVX) registers for hardware with the newest supported processors."



Okay, now, obviously whatever these bugs were, they were not destroying everyone's data.  Or, I mean, we would have known about that.  Or perhaps it was just that very few people were using Windows 11 original release on the very latest processors which also had these new VAES instructions.  But in any event, it's a bit frightening to have this somehow escape from and to be shipped by Microsoft.



BleepingComputer carried this story, and someone commented on their story on its page by quoting the phrase "data damage" and said:  "Data damage, the new marketing gloss-over for data loss and filesystem corruption."  He said:  "Don't be fooled.  It's yet another case where Microsoft's bungled agile development practices have screwed the pooch.  Their testing harnesses are entirely inadequate to support the massive legacy code bases they have to support in the time scales they need to release."  And I think that's an accurate criticism, as we know.  We've seen lots of evidence of this increasingly in the last couple of years.  I don't think that Paul, Mary Jo, or I could have summed things up better than that.



But on the brighter side, I also have some happy Microsoft news to share.  They've announced a new security offering which looks pretty good to me.  It promises to provide security teams with the means to spot Internet-exposed resources in the organization's environment that they may not be aware of.



On the front page announcing this new what Microsoft calls the "Microsoft Defender" - and Paul and Mary Jo the other day said that pretty much everything was now called "Microsoft Defender."  So it's Microsoft Defender this or Microsoft Defender that.  They're liking that jargon a lot.  Anyway, this is the "Microsoft Defender External Attack Surface Management."  And on the announcement page they note the highlights.



They said:  "Discover unmanaged resources:  Understand the full extent of your attack surface, including shadow IT and assets created through common, everyday business growth.  Multicloud visibility" - this is the first time I'd seen the term "multicloud."  It's like, okay, one's not enough, let's get multiclouds.  So we've got "Multicloud visibility:  Maintain a dynamic inventory of external resources across multiple cloud and hybrid environments."  And then, finally, "Identify exposed weaknesses:  Prioritize vulnerabilities and misconfigurations hidden in unmanaged resources, then bring the resources under management to remove those exposures."



So, okay.  From everything they've written, it's unclear to me, because of the terminology they've used, whether this is an external but, for example, a far more comprehensive scan, like GRC's "ShieldsUP!" service; or whether it's local network packet monitoring.  If a network monitor was placed truly upstream of everything else that the enterprise was exposing, that could do the job.  But it strikes me that even that could be prone to some mistakes.  The focus is on mistakenly unmanaged, forgotten, or unknown network assets which might be added to the environment after, you know, they said business growth, so mergers or acquisitions, created by shadow IT, somebody's uncle plugging something in somewhere, or missing from inventory due to incomplete cataloging, or just left out due to rapid business growth.  And I think all of this is a great idea.  



Microsoft's Corporate VP for Security said:  "The new Defender External Attack Surface Management gives security teams the ability to discover unknown and unmanaged resources that are visible and accessible from the Internet, essentially the same view an attacker has when selecting a target.  Defender External Attack Surface Management helps customers discover unmanaged resources that could be potential entry points for an attacker." 



And I still can't figure out exactly what it is from what they've written.  As I said, the language they're using is aggravatingly imprecise.  Elsewhere in describing it, they wrote:  "Microsoft Defender External Attack Surface Management scans the Internet and its connections every day."  Well, okay.  I don't know what "the Internet and its connections" means exactly.  Maybe Paul and Mary Jo will talk about this tomorrow.



Anyway, they continue.  Microsoft says:  "This builds a complete catalog of a customer's environment, discovering Internet-facing resources" - that sure sounds like an external scanner to me - "even the agentless and unmanaged assets.  Continuous monitoring, without the need for agents or credentials" - which again sounds like outside - "prioritizes new vulnerabilities.  With a complete view of the organization, customers can take recommended steps to mitigate risk by bringing these unknown resources, endpoints, and assets under secure management within the security information and event management" - and we have an acronym for that, SIEM - "and extended detection and response" - that's XDR - "tools."



So it sounds like sort of a Microsoft-offered Shodan scanner for their enterprise customers.  Anyway, whatever it is and whatever it costs, in an enterprise environment where there might be too many overlapping regions of IT authority, and without any absolutely central single omniscient management, this sure seems like something that would be worth pricing out and exploring for an enterprise.  It is just - it is so easy to make a mistake, and this might work to catch any such mistakes before the bad guys do.  So I wanted to bring it to everyone's attention.  I imagine that our enterprise-bound listeners may want to know about it.



Okay.  So I'm going to talk about Dan here in a minute.  First I want to do some Closing the Loop with our listeners.  Someone who asked me to keep his name anonymous, so I'll just call him K.A., sent me a DM.  He said:  "Steve, just wanted to share my experience with 0patch as you have mentioned it a few times on Security Now!.  I heartily recommend this program for anyone still having to support Windows 2008 servers or Windows 7 systems in their enterprise."  Which as we know have gone out of security patch cycle unless you buy it, and it gets more expensive every year, from Microsoft.  Anyway, he says, "...due to business reasons such as having to support legacy applications.  Anyone can sign up for the free account to get a copy of the 0patch agent."  And that's 0patch.com.  And he says, "...and see in an instant what Windows modules are at risk, and how often they are being called by the OS."  That's really cool.



He says:  "The cost to patch a Windows system with 0patch is a fraction of what you would have to pay Microsoft, and the patches are installed instantly, with no reboot required."  He said:  "I was able to protect my systems within seconds of purchasing my subscription as the agent immediately implemented the patches on my machines.  I hope this is helpful to our Security Now! community, as I am sure several of us would like to simply shut down non-supported Windows systems, but are unable to for business impact reasons.  Please keep my handle anonymous, as I have identified myself through this message as having to support legacy systems, and I would not like anyone to trace my association with my company which could put them at risk.



"Thank you for producing my favorite podcast and equipping our community with the information we need each week to digitally protect our business and loved ones' systems."  So thank you, K.A.  I'm happy to share this information with our listeners.  I think that the 0patch, the micropatch guys do a great job, and we're often talking about them.



Someone calling themselves Oaksong tweeted:  "Does the CIA care about felonies when hiring security consultants?  Do they just move them overseas?"  Which struck me as a really good question.  It's related, of course, to our more tongue-in-cheek Picture of the Week last week, the two paths to reaching professional security guru status.  I don't remember exactly what we called it.  One was the 20-year path of working your way up through the hierarchy.  The other was be a hacker, get convicted, go to prison, get out in 14 months on good behavior and get hired as a security professional.  So anyway, I got a kick out of that.  I can certainly see both sides of it.



Simon Kirkman said:  "Hi, Steve.  I'm trying to set up a guest WiFi network, and I found an issue which I can't see how you got around when you did it at home."  He said:  "I set up a new router for visitors to our village hall, which is connected to a modem router in an office belonging to a business who are willing to allow Internet access, but not LAN access due to CCTV, et cetera, being on their network.  My new router is set to a different subnet and subnet mask, and in theory is separate from the business network.  But in practice, an IP address gets passed upstream to the other router, which then allows access.  I can't see how to do this correctly, and it allows my guest users to access the business LAN.  How did you do it with your smart home network?"



Okay, well, first of all, I did it with my smart home network by using what we affectionately, really very affectionately call "dumb routers."  Mine was a smart router where the individual ports were separate network interfaces, separate NICs, not a hub or a switch, where all of those ports were all on the same LAN.  That allowed me to assign different LANs to different ports.  But there is the famous "three dumb routers" solution that I developed years ago on this podcast.  It's a way of creating two mutually-isolated networks using three simple and standard NAT routers and pretty much zero configuration.  The three routers are wired in a "Y" configuration.  We'll call it Router A connects to the Internet on its WAN interface and provides Internet service to Routers B and C by connecting each of their WAN interfaces to two of Router A's LAN interfaces.  Thus the "Y" connection.



A useful simplification for simple NAT routers is to think of them as a one-way valve where traffic can easily leave the network, passing out of the router toward the Internet, but unsolicited and unexpected traffic cannot enter the network.  In fact, we now utterly depend upon this feature of NAT routers to act as our Internet firewalls for us.  This same principle works for the "Y" configuration to prevent any traffic from one of the LANs from having any access whatsoever into the other LAN because each LAN is protected by its own dumb router which is acting as a one-way valve.



Now, that requires three routers, each of which can be dumb.  So, speaking to Simon, if it's feasible for you to place a third router upstream on the WAN side of both the business router and your village hall router, that would provide perfect isolation.  But there is a two-router variation which might also work.  Switch the roles of the two routers.  Place the village hall router on the Internet and connect the business router which requires the privacy to one of the LAN ports of the village hall router.  In that way, the traffic on the business's LAN is protected by its router's one-way NAT firewall, and the village hall router that doesn't need that protection doesn't need that.  It might still be able to see the village hall traffic, but not the other way around.  So in other words, three dumb routers are needed for two-way privacy; but two routers can be used when one-way privacy is sufficient.



So as it happens I sent this answer back to Simon, who later replied:  "Hi, Steve.  Thanks for that.  I have a third router around.  I'll look to set that up.  Had not thought about doing it that way.  Thanks very much."



LEO:  It's nice that you can do it all wireless.  That's cool.



STEVE:  Yes, yes.  Jose C. Gomez.  He said:  "Hi, Steve.  Here's a pretty complete demo and explanation of how Passkeys are going to work and interface between Microsoft, Google, and Apple, presented by some of their product managers and engineers."  And in his tweet he sent me a YouTube link.  I turned it into this week's shortcut.  He finished his tweet saying:  "Looks like there is no Passkey sharing at all, and it's more cross-device auth and recreate."



So for those who are interested in seeing this working, the 14-minute YouTube video posted by the FIDO Alliance on their YouTube channel is very good.  It has a blessedly brief introduction by a FIDO marketing person, followed by brisk walk-throughs by Google and Microsoft product managers.  So as I said, the video is this week's shortcut of the week, meaning that it's grc.sc/883.  That'll bounce you to the YouTube link. 



So the short version is that it's all exactly what we thought.  And they do succeed in making it all look wonderful.  The Google guy highlights the "magic" created by Apple's iCloud synchronization, such that a Passkey created on one Apple device will be known to all of your other Apple devices.  And they show how a Bluetooth-enabled phone and a Bluetooth-enabled desktop can use a QR code displayed on a web page to allow the phone to authenticate the user on that desktop; and how the user, now having been authenticated, may then choose to create another Passkey locally on that device, that is, that desktop device, so that future logins can be done natively without the phone.



As we know, it's not as good as we could have had.  And bridging isolated brands such as Apple, Google, and Microsoft will - this pretty much confirms it - require creating multiple functionally duplicate Passkeys for every website on every machine that lacks a means of synchronizing and sharing existing Passkeys.  But it's the system we're going to get.  And thanks mostly to the authentication automation which WebAuthn brings to finally create an alternative to the kludge of clunky form-fill-in authentication, it will be better than what we've had.



But implementing it on the server side, as we know, still requires some major work individually from each and every website.  That's where the form-fill password managers excelled is that the websites were spoiled.  They didn't have to change anything.  Everything was done by filling in the form.  But it's a mess.  So it's going to be very interesting to see how all this transpires over the next few years.  But our initial impression is confirmed in this 14-minute video.  But it also, I have to say, I mean, they make it look breezy and not burdensome.  It's just not as nice as it could have been.



LEO:  Yeah.



STEVE:  @ElectronicAthro said:  "Hi, Steve.  Love Security Now! and recently came across something interesting I thought I'd ask you about.  At least I thought it was interesting.  Maybe it's trivial, and I should know better."  He said:  "I recently took a United flight, and they allow their in-flight WiFi to be used for IP-based messaging apps for free.  However, they block the sending of images.  So how do they detect that one is sending an image if one is using a messaging app that has strong encryption like Signal?  I would have thought that since Signal does the encryption at the 'end,' any image sent by a Signal message would be indistinguishable from a text message.  But after attempting to send an image, United has clearly figured out how to detect an image in a message and block it.  How is this possible while maintaining Signal's encryption?"



Okay.  My guess would be that it's all about bandwidth usage and size.  Text is truly tiny, whereas any image is huge, massive by comparison.  So it would be easy to simply watch each user on the airplane for the rate of data that they are exchanging; and, if it exceeds some very low maximum, which is all texting is ever going to be, right, like writing some text and hitting SEND is a little tiny little blurch of bandwidth use, it would be very easy for them to set a very low maximum and, if they exceed that, cut them off.  And also note that a simple bandwidth cap, a very low bandwidth cap is also what United or any carrier would want.  It's sort of a nice compromise.  Their travelers can trickle out, in and out, text for free, so long as it's at such a low bandwidth as to be insignificant to them.  But if you want the cap lifted, fork over some cash.



Relief Twitcher tweeted:  "@SGgrc You're a lifesaver!  Today, a PC in a pharmacy that I support installed Microsoft Update KB5014666, the one that makes the duplicate USB printer and deselects the port for the original.  I'm not sure why my organization let the patch through, but this PC shares a vital label printer with the rest of the pharmacy, and suddenly no one could print labels.  As soon as I saw the duplicate printer with the '(Copy 1)' in its name, I knew exactly what do.  The reason I knew was because I had listened to Security Now! 881.  You saved me a lot of work and my pharmacists a lot of down time.  Thank you."



Oh, and he said:  "While I have your attention, I'll make another plea for you to read 'Sea of Tranquility' by Emily St. John Mandel."  He says:  "I found it to be compelling speculative fiction.  I think they call it that because it's light on the science-y details, opting instead to just concede that things like time travel and domed cities exist, and to focus instead on the story of the humans in that environment."  He says:  "I think you would like it."



So first, it's very cool when something from the podcast so nicely lines up with a real-world event.  And as for his book recommendation, since I'm currently without an engaging science fiction novel, I purchased the book for $11 for my Kindle.  And I'd have to say it has endless amazing over-the-top reviews there.  So it looks like an interesting possibility.  My nephew, who is similarly hooked on Ryk Brown's Frontiers Saga, and who with me as been waiting for the next one to drop, has not yet discovered Peter Hamilton.  So he's heading into "Fallen Dragon," which I'm sure he's going to go nuts over, and then I'm going to aim him at "Pandora's Star."  So I envy him for not yet having found Hamilton because, boy, does he have some great stuff ahead.



Martin Rojas.  He said:  "My sister had a severe allergic reaction" - he said she is fine - "but in talking to the paramedics we talked about medical bracelets or ID cards.  She has a complex medical history and medication, as do many of the people they pick up.  The paramedic mentioned that if there was a card with NFC or a QR code they could scan that information, and it would be a great help to them in an emergency."  He says:  "That part is easy, but it would also be public for anyone to scrape the info.  My question is whether there is some pattern that could serve both as secure, but also easy to access by emergency personnel.  I was thinking password printed in the card, but maybe there is something better."



From a theoretical privacy protection standpoint, I had two thoughts in response to Martin.  The first would be to have a publicly accessible QR code that anyone could use to access medical records, which would be carried by the person, but in such a way that it was not readily accessible to anyone.  For example, make it a comfortable silicone wristband that's never removed, which identifies itself as offering critical emergency information on its underside only.  Thus it provides a degree of physical privacy by physically limiting the circumstances under which someone could obtain access to the QR code.  It could also make very clear what the person's most important health requirements are, which would be spelled out in the region's most common language on the underside so that you wouldn't have to even scan the QR code.  You could immediately determine what was most important, and then the QR code could provide additional backup.



The second solution, which could be applied as an additional layer of privacy protection if required, would be for the QR code, which would presumably take anyone who scans it to an emergency information supply service, that could also require login authentication by an accredited and confirmed emergency services supplier.  While that would offer greater privacy protection, the worry would be that the information not be made available as quickly as it could be or maybe at all if the authentication failed, or if the provider of the service didn't have an account with the information provider.



So I think if it was me, I'd worry less about privacy and more about being certain that any special medical needs, allergies, et cetera, were readily known to someone who needed to obtain them.



LEO:  And let me put in a plug here for turning on Medical ID.  If you have an iPhone, and all first responders know this, go to the Health app, scroll down, set up Medical ID.  Mine is set up with my medications, my allergies, everything that first responders would need to know.  The most important thing a first responder needs, I'm told by first responders, is a number to call for your next of kin or, well, that sounds like you're dead.  Your nearest, you know, your contact, emergency contact, because they often will just use that.  But there is that information.  iPhones have it built in.  They know how to get to it, if you have to press and hold the side button and the volume up button at the same time and swipe the Medical ID slider.  So it's not something a casual privacy thief would get to.



Android unfortunately does not have this built in on all versions of Android, but there are third-party apps.  JointCommission.org has the information.  There's one called Medical ID on the Google Play Store.  Use those because first responders know, who doesn't have a smart phone?  They know immediately to go for the smart phone.  So I think Medic Alert bracelets and necklaces and other things like that have been superseded these days by your smart phone.  So if you have a smart phone, turn that on.  It's really important.



STEVE:  Very cool.  And I guess I would argue, even if you don't have any medical issues, but you do want an emergency responder to be able to get a hold of the person that you need to have notified.



LEO:  Yes.  Most important information of all.  Exactly.



STEVE:  Yeah.  Okay.  So finally, Dan Bernstein sues the NSA.  As I mentioned at the top, Dan is one of my favorite cryptographer mathematicians since he's the father of the most efficient 25519 family of elliptic curve crypto and a number of other core crypto primitives that I adopted for SQRL and which have subsequently been adopted for use by TLS and even optionally by WebAuthn that would make it available for Passkeys.  And coincidentally, Dan and I independently came up with the idea that was - it's called SynCookies, as a way to prevent resource depletion in TCP/IP stacks which are caused by SYN flooding attacks.  The idea was a way to encode the important details of the SYN packet in the replying SYN/ACK so that stateless connection setup became possible.



Anyway, Dan was born in 1971, so he was 24 years old when, as a student at UC Berkeley, he brought his first lawsuit against the United States.  Dan's 50 years old now.  He wanted to publish a paper at the time with its associated source code on his - Snuffle was the name of it, the Snuffle encryption system.  But that would have been illegal at the time, so he sued and won.  He sued the United States and won.  After four years and one regulatory change, the Ninth Circuit Court of Appeals ruled that software source code was speech protected by the First Amendment, and that the government's regulations preventing its publication were unconstitutional.  And we owe Daniel for that.



I'm bringing this up today, and I called that "Dan's first lawsuit" because as I mentioned already, last Friday he announced that he has now sued the NSA in a blog posting titled:  "NSA, NIST, and post-quantum cryptography:  Announcing my second lawsuit against the U.S. government."



LEO:  It worked once before.



STEVE:  His blog post is lengthy, and I want to read, digest, and research the entire thing.  So unless something more interesting pops up before next week, it will likely be next week's topic.  For anyone who doesn't want to wait for me, the link to Dan's blog post is in the show notes.  So I think that's what we'll be talking about next week.  And Leo, after our final sponsor break, we're going to talk about somebody I just discovered who you have known about for quite a while, and a really compelling piece of his writing.



LEO:  Oh, good.  Can't wait.  And I put into the chatroom and the show notes, well, no, yours are the show notes, but I put into the chatroom and the Discord information about turning on Medical ID.  Many Android phones, unless my Samsung Galaxy phone has that built-in, otherwise they're third-party apps.  And all iPhones to my knowledge support that.



STEVE:  Here's what happened.  When I settled down late yesterday morning to begin assembling today's podcast, I started by catching up with my week's past Twitter DMs.  The first and most recent DM I encountered was from a listener named Theron Keller who pointed to something that astounded me, and I thought it was so important that it became today's topic.  He tweeted:  "Hi, Steve.  I'm a few weeks behind on SN.  I just heard the episode where you mentioned coding all night long.  Then today I saw this and of course thought of you.  I'm sure other coders would agree."



So he pointed to a posting on Facebook where someone had apparently just discovered something someone else had written back in July of 2009.  After scanning the Facebook posting I followed the source reference link to the original content, and thus stumbled upon the work and writings of someone I had never been very much aware of.  The guy's name is Paul Graham.  And here's a very brief bio of Paul that could clearly be much longer.



He is a programmer, writer, and investor.  In 1995, he and Robert Morris - yes, that Robert Morris - started Viaweb, the first software-as-a-service company.  That's 1995.  And I believe it was a Lisp-based storefront-creating service.  So Viaweb was acquired by Yahoo in 1998, where it became Yahoo Store.  In 2001 Paul started publishing essays at PaulGraham.com, which now gets around 25 million page views per year.  In 2005 he and Jessica Livingston - now his wife - Robert Morris, and Trevor Blackwell started Y Combinator, the first of a new type of startup incubator.  Since 2005 Y Combinator has funded over 3,000 startups, including Airbnb, Dropbox, Stripe, and Reddit.  In 2019 he published a new Lisp dialect written in itself called Bel.  Paul is the author of "On Lisp" published by Prentice Hall in '93.



LEO:  Own it.



STEVE:  "ANSI Common Lisp," Prentice Hall '95.



LEO:  Own it.



STEVE:  And "Hackers & Painters," of all things, which was published by O'Reilly in 2004.



LEO:  I own all three, yeah.



STEVE:  Uh-huh.  He has his Bachelor's degree in Philosophy from Cornell; his Master's and a Ph.D. in Computer Science from Harvard.  He's also studied painting at the Rhode Island School of Design and at the Accademia di Belle Arti in Florence.  The well-known technology journalist Steven Levy has described Paul as a "hacker philosopher"; and, given what I've seen, I would tend to agree.  I was curious about his Ph.D. dissertation, so I tracked it down.



LEO:  Wow.  I have not read that.



STEVE:  It is quite something, Leo.  It's titled - I love the title - "The State of a Program and Its Uses."  It's wonderfully mystical.  I read the abstract.  As you might expect, it's some seriously nice pure computer science thinking.



So poking around a bit more, I was getting intrigued by this guy.  I looked at a couple of his Twitter postings.  A recent post of his from Saturday, three days ago, he tweeted:  "In office hours today" - and I should mention, as we'll see, that the use of this term "office hours" is important to him.  He says:  "I talked to a pair of founders who needed a new idea.  It turned out they already had a great idea, but had been ignoring it because they didn't know how to monetize it.  I told them to just build it.  This thing could have 100 million users."



And yesterday he tweeted - I love this.  "Effective organizations are unnatural.  The natural state of organizations is bureaucracy and turf wars, and once deprived of effective leadership they revert to their natural state with shocking speed."



LEO:  Oh, boy, is that true.  Holy cow.



STEVE:  Isn't that great?



LEO:  Yes.



STEVE:  Isn't that great?  Oh.  And looking a bit further back, on August 1st, he tweeted:  "The hardest people for founders to hire are so-called C-level executives because these people are the best fakers in the world."



LEO:  Aha, yes.



STEVE:  He said:  "Even the best founders make absolutely disastrous mistakes hiring these people.  It happens far more often than anyone realizes."



LEO:  Oh, yeah, it really does.



STEVE:  "Because neither party wants to talk about it.  So after nearly destroying one company, the exec cheerily goes off to their next opportunity."  And of course actually this put me in mind of someone I've talked about on the podcast before, a horrible person by the name of Ron Posner, who Peter Norton hired because Peter thought he needed like an executive.



LEO:  Big mistake.



STEVE:  To run stuff.  Oh, boy, yeah.  So anyway, if you're into following people on Twitter, Paul might be someone worth following.  I don't follow anyone on Twitter, but I'm really tempted to follow him.  He tweets as @paulg, and you'd be joining his 1.5 million current followers.  And I'm unsure why I find this guy so fascinating.  That doesn't happen that often.  He's got something.



So it seems pretty clear that in Paul Graham we have a serious computer science guy with a strong creative side and a very strong entrepreneurial business side.  And that might be what's hooking me.  He made money early in the run-up of the Internet and the dot-com revolution.  It also appears that he's one of those still rarer guys who didn't make it by chance, by being in the right place at the right time, but then never able to again recreate that first early success.  He's a serially successful entrepreneur.  And he's either spent a lot of time thinking, or he's very good at it.  And it turns out that Paul is also an outstanding writer, which brings us to today's topic.



As I said earlier, as I began reading what Paul wrote, its subject and content resonated so deeply with me  as I know it will with so many of this podcast's listeners  that I knew that sharing it here would be the best possible use of everyone's time this week.  It helped that there was not a huge amount of compelling security industry news this week.  But I had already made the decision to share this as this week's topic before I even knew that.  He gives explicit permission for his essays to be included, in full, in school newspapers and the like, asking that the URL to its original page be included, as I've already done several times in these notes.



So here's what Paul Graham wrote just over 13 years ago, in July of 2009, under the title "Maker's Schedule, Manager's Schedule."  He said:  "One reason programmers dislike meetings so much is that they're on a different type of schedule from other people.  Meetings cost them more.  There are two types of schedule, which I'll call the manager's schedule and the maker's schedule.  The manager's schedule is for bosses.  It's embodied in the traditional appointment book, with each day cut into one-hour intervals.  You can block off several hours for a single task if you need to, but by default you change what you're doing every hour.  When you use time that way, it's merely a practical problem to meet with someone.  Find an open slot in your schedule, book them, and you're done.



"Most powerful people are on the manager's schedule.  It's the schedule of command.  But there's another way of using time that's common among people who make things, like programmers and writers.  They generally prefer to use time in units of half a day, at least.  You can't write or program well in units of an hour.  That's barely enough time to get started.  When you're operating on the maker's schedule, meetings are a disaster.  A single meeting can blow a whole afternoon by breaking it into two pieces each too small to do anything hard in.  Plus you have to remember to go to the meeting.  That's no problem for someone on the manager's schedule.  There's always something coming on the next hour; the only question is what.  But when someone on the maker's schedule has a meeting, they have to think about it.



"For someone on the maker's schedule, having a meeting is like throwing an exception.  It doesn't merely cause you to switch from one task to another; it changes the mode in which you work.  I find," he writes, "one meeting can sometimes affect a whole day.  A meeting commonly blows at least half a day by breaking up a morning or an afternoon.  But in addition there's sometimes a cascading effect.  If I know the afternoon is going to be broken up, I'm slightly less likely to start something ambitious in the morning.  I know this may sound oversensitive, but if you're a maker," he says, "if you're a maker, think of your own case.  Don't your spirits rise at the thought of having an entire day free to work, with no appointments at all?"  Oh, my lord, yes.  "Well, that means your spirits are correspondingly depressed when you don't.  And ambitious projects are by definition close to the limits of your capacity.  A small decrease in morale is enough to kill them off.



"Each type of schedule works fine by itself.  Problems arise when they meet.  Since most powerful people operate on the manager's schedule, they're in a position to make everyone resonate at their frequency if they want to.  But the smarter ones restrain themselves, if they know that some of the people working for them need long chunks of time to work in it.



"Our case is an unusual one."  Speaking of Y Combinator.  He says:  "Nearly all investors, including all venture capitalists I know, operate on the manager's schedule.  But Y Combinator runs on the maker's schedule.  Rtm" - that's Robert Morris - "and Trevor and I do because we always have; and Jessica does too, mostly, because she's gotten into sync with us.



"I wouldn't be surprised if there start to be more companies like us.  I suspect founders may increasingly be able to resist, or at least postpone, turning into managers, just as a few decades ago they started to be able to resist switching from jeans to suits.  How do we manage to advise so many startups on the maker's schedule?  By using the classic device for simulating the manager's schedule within the maker's:  office hours.  Several times a week I set aside a chunk of time to meet founders we've funded.  These chunks of time are at the end of my working day, and I wrote a signup program that ensures all the appointments within a given set of office hours are clustered at the end.  Because they come at the end of my day, these meetings are never an interruption.  Unless their working day ends at the same time as mine, the meeting presumably interrupts theirs.  But since they made the appointment, it must be worth it to them.  During busy periods, office hours sometimes get long enough that they compress the day, but they never interrupt it.



"When we were working on our own startup, back in the '90s, I evolved another trick for partitioning the day.  I used to program from dinner until about 3:00 a.m. every day because at night no one could interrupt me.  Then I'd sleep till about 11:00 a.m. and come in and work until dinner on what I called "business stuff."  I never thought of it in these terms, but in effect I had two workdays each day, one on the manager's schedule, and one on the maker's.



"When you're operating on the manager's schedule you can do something you'd never want to do on the maker's.  You can have speculative meetings.  You can meet someone just to get to know one another.  If you have an empty slot in your schedule, why not?  Maybe it will turn out you can help one another in some way.  Business people in Silicon Valley, and the whole world for that matter, have speculative meetings all the time.  They're effectively free if you're on the manager's schedule.  They're so common that there's distinctive language for proposing them, saying that you want to 'grab coffee,' for example.



"Speculative meetings are terribly costly if you're on the maker's schedule, though.  Which puts us in something of a bind.  Everyone assumes that, like other investors, we run on the manager's schedule.  So they introduce us to someone they think we ought to meet, or send us an email proposing we grab coffee.  At this point we have two options.  Neither of them are good.  We can meet with them and lose half a day's work, or we can try to avoid meeting them and probably offend them.



"Till recently we weren't clear in our own minds about the source of the problem.  We just took it for granted that we had to either blow our schedules or offend people.  But now that I've realized what's going on, perhaps there's a third option, to write something explaining the two types of schedule.  Maybe eventually, if the conflict between the manager's schedule and the maker's schedule starts to be more widely understood, it will become less of a problem.  Those of us on the maker's schedule are willing to compromise.  We know we have to have some number of meetings.  All we ask from those on the manager's schedule is they understand the cost."



So I just thought that the crystallization of that is what hit me so clearly.  And I heard you getting it at the beginning of this, Leo.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  What I like so much about what Paul has created here is the clarity of the distinctions he's made.  I've often explained to my friends and family about what I call the - I call it "switching cost."  It is so much more efficient to stay on one thing until it's finished than to switch back and forth among multiple things.  In programming we call it "context switching," and it's expensive there, too.



And I definitely operate on a Maker's schedule, and I always have.  A day with no scheduled interruptions planned is joyous.  It's a block of unbroken time I savor in anticipation and actively appreciate.  And it completely explains why I was so miserable running a company with 23 employees, which was GRC's size at its peak.  I liked every single employee I had.  I enjoyed them as individuals.  But I was worried that they were going to need something from me when I was in the middle of doing something else.  And the point is I am always in the middle of doing something else.  You know?  And I want to be.  That's the way I want to spend my life.  Pretty much most of the time I just want to be left alone to work.  And Leo, I sort of feel I have a kindred spirit in you.



LEO:  Yeah.



STEVE:  If I'd had the wisdom back when GRC was a cauldron of chaos, I should have worked at home in seclusion four days a week and held office hours, as Paul does, on Fridays.  On those Fridays I would have pre-resigned myself ahead of time that I would not be getting any code written that day, and perhaps I would have both been more available and less miserable and grumpy.



So anyway, I can imagine many of the Maker's Schedule listeners we have here maybe sending a link to this little piece of writing to their managers, asking and pleading with them to read it.  If I were drowning in a corporate setting, I think I would do so.



LEO:  I am going to recommend another one which I blogged about a couple years ago.



STEVE:  Oh, good.



LEO:  It goes along with this one, Steve, that he wrote in 2016, called "Life Is Short."  Because it's the same premise extended even farther, you know.  Don't waste time on B.S. because life is short.



STEVE:  Yeah.  So Paul's articles page is at PaulGraham.com/articles.html.  It lists this and 211 other short essays.  I've only poked around at them a bit, but I find something about them to be quite compelling.



LEO:  Yeah.  Once you start reading, you won't stop.



STEVE:  Yes.  I did not want to stop reading them, but I had this podcast to finish.  And I think before I start in on this next book I'm going to spend some time there.  At the top of his page he says:  "If you're not sure which to read, try 'How to Think for Yourself,' or 'Do Things That Don't Scale,' or 'How to Lose Time and Money.'"  And of course we have Leo's recommendation, "Life Is Short."



LEO:  "Life Is Short."  He also wrote THE article on why you should learn Lisp.  So I can't help but recommend that, as well.



STEVE:  Oh, cool.  I will be getting to it.



LEO:  You will.



STEVE:  I leave our listeners in Paul Graham's very capable hands.



LEO:  Yeah.  He's amazing.  A great thinker and a great writer.  And those two, when you get the two together, fantastic.  Fantastic.



STEVE:  Yeah.



LEO:  All right.  I think that means you're going to go back to work, and I'm going to go back to reading, and the rest of you, well, you could keep listening to podcasts.  That's my suggestion.  Steve Gibson does this show every Tuesday, 11:00 a.m. Pacific, I'm sorry, 1:30 p.m. Pacific, that's 4:30 p.m. Eastern, 20:30 UTC.  If you want to watch us do it live, that's why I say the time, so you can watch it live at TWiT.tv/live.  There's audio and video there.  Chat with us live at irc.twit.tv or in the Club TWiT Discord.



On-demand versions of the show available at our site, TWiT.tv/sn.  We've got 64Kb audio and video available there.  And of course you can subscribe to it in your favorite podcast client, as well.  Steve has two unique versions of it at his website, GRC.com.  He's got a 16Kb version for the bandwidth-impaired.  He also has really good transcripts written by a human, Elaine Farris, so you can read those and follow along, or use them to search for parts.  All of that is at GRC.com.



While you're there, pick up SpinRite, Steve's bread and butter.  We've got to keep him in bread and butter.  All you have to do right now, get 6.0.  You will participate in the development of 6.1 and get it for free if you buy today, the minute it's available.  GRC.com for SpinRite, the world's best mass storage maintenance and recovery utility.  Lots of other free stuff there.  You can leave a message for Steve there, GRC.com/feedback.  But probably more useful to leave a DM for him.  His DMs are open on Twitter.  He's @SGgrc.  You can DM him there, @SGgrc.  I think that covers everything, Mr. G. 



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#884

DATE:		AUGUST 16, 2022	

TITLE:		TLS PRIVATE KEY LEAKAGE

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-884.MP3

LENGTH:	99 MINUTES



DESCRIPTION:  This week we look back at last week's Patch Tuesday to learn how much better Microsoft various products are as a result.  We look at Facebook's announced intention to creep further toward end-to-end encryption in Messenger, and at the puzzling result of a recent scan of the Internet for completely exposed VNC servers.  I want to take a few minutes to talk about the importance of planning ahead for a domain name's future, share my tip for a terrific website cloning tool, and a few more updates.  Then, after sharing some feedback from our ever-attentive listeners, we're going to address the question:  "Can a remote server's TLS private key be derived simply by monitoring a sufficient number of its connections?"  What?!  We all know that everything has been designed so that's not possible.  But edge cases turn out to be a surprising problem, and the details of this research are quite interesting.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We conclude our 17th year of podcasting with this episode.  We start Year 18 next week.  We'll look back at Patch Tuesday.  How many years have we been doing that?  Yes, more Microsoft stuff.  We look at Facebook's intention to increase end-to-end encryption in Messenger.  It's about time.  And then Steve answers a very interesting and puzzling question:  Can a remote server's private key be derived simply by monitoring a sufficient number of connections?  The answer might surprise you.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 884, recorded August 16th, 2022:  TLS Private Key Leakage.



It's time for Security Now!, the time to talk about all those things that protect you and your loved ones on Windows and Mac.  And there he is, living long and prospering, the great Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo Leo.  Great to be with you.  I frightened you a little bit before we began recording when I said that this was the final podcast.	



LEO:  Yes, don't say that.



STEVE:  And I needed to follow that up with "of our 17th year."



LEO:  Oh, whew.  What a relief.



STEVE:  We have now finished 17 years.



LEO:  That's amazing.  That is amazing.



STEVE:  And next week we're going to start to get serious.



LEO:  Yes.



STEVE:  Because we've figured out how to do this.



LEO:  Enough of this silliness.  Let's do a show, yes.



STEVE:  No more fooling around.



LEO:  Unh-unh.



STEVE:  So we've only done 883.  This will make 884 of these little things.



LEO:  That's amazing.



STEVE:  I know.



LEO:  Neither of us had any idea it would go on this long.



STEVE:  This was all your fault, too, of course.



LEO:  I take full blame.



STEVE:  Leaning against the set up in Toronto, you said, "What would you think about doing a weekly podcast?"



LEO:  Hey, Steve.  What about this?



STEVE:  And I remember thinking, oh, I hope he doesn't bring this up again.



LEO:  Really?  You really didn't want to do it.



STEVE:  I just didn't know how fantastic it was going to be.  It's the best thing that ever happened.



LEO:  Oh, okay.  So you're glad now in hindsight.



STEVE:  Oh, I am so glad.



LEO:  Oh, I'm relieved, okay.



STEVE:  Absolutely.  Absolutely.



LEO:  Because you never told me you didn't want to do it.  It's a little late now.



STEVE:  We're going into 18 years.  As many of my various exes have said, oh, he doesn't ever do anything he doesn't want to do.  So...



LEO:  Eh?  That's good.  Actually, I like that because then I know I'm not imposing on you because if you didn't want to be here, you wouldn't be.  So I appreciate that.



STEVE:  It is truly the best decision that I didn't make.  You made it, and I'm really glad.



LEO:  Well, and I agree, I mean, you're the bulwark of the whole podcast network, so thank you.  I appreciate it.



STEVE:  Okay.  So a really, really interesting thing.  I noticed on MacBreak you covered some of the...



LEO:  Oh, man.



STEVE:  ...security issues that I skipped over.



LEO:  Yeah.



STEVE:  Because they were like, okay, you know, like Zoom fixed their remote code execution vulnerability, and the whole Mac security layers bypass has been resolved.  And there was - so of course this is the first podcast after the big Vegas summer - it's considered hacker summer camp, that whole week in Las Vegas with Def Con and BSides and all the other stuff.



LEO:  Black Hat, yeah, yeah.



STEVE:  Black Hat of course.  So but the big news came out of the USENIX Security Symposium, I felt.  We've talked about several stories already.  There was another one which ended up replacing what I sort of thought we might talk about, which was Bernstein's suing the NSA over a Freedom of Information Act because he wants to know where they came up with some of the decisions that they've made.  And I said, you know, if something bigger than that happened,  we would talk about it.  Well, a group of four researchers at UC San Diego and in Boulder, Colorado, two from each University, did some watching of the Internet.  And what they found really needs to get our attention.



But first we're going to talk about last week's Patch Tuesday, which also needs to get our attention, to learn how much better Microsoft's various products are as a result.  Actually it didn't turn out to be so bad; but, boy, was there a lot that happened.  We're also going to look at Facebook's announced intention to creep further, carefully, apparently, with a lot of caution, toward end-to-end encryption by default in their built-in native Messenger app.  Also at the puzzling result of a recent scan of the Internet for completely exposed VNC servers, not something I think we've ever spoken of before.



I also want to take a couple of minutes to talk about the importance, and I'm sure you're going to have fun chiming in, Leo, the importance of planning ahead for a domain name's future.  Several things have happened in my off-the-podcast life that sort of brought this to my attention, and I thought I just should take a minute to talk about that.  Also I want to share my tip for a terrific website cloning tool and, additionally, a few more updates.  Then we've got some feedback from our ever-attentive listeners.



And we're going to wrap by addressing the question, believe it or not, can a remote server's TLS private key, which is the thing they guard more than anything else, I mean, how many hours of podcast time have we talked about the issues surrounding protecting the private key, warning people if it leaks out, how do we revoke it, I mean, just like nothing more important.  Can a remote server's TLS private key be derived simply by monitoring a sufficient number of its connections?  What?  We all know that everything has been designed around that not being possible.  But edge cases turn out to be a surprising problem, and the details of this research are quite interesting.



LEO:  Ah.



STEVE:  So today's topic is TLS Private Key Leakage, and we do have a fun Picture of the Week.  Somebody sent this who knows some of my gripes about graphing.  So I think another great podcast for our listeners.



LEO:  Yeah, we've griped about graphs with no origin and all, you know, every Apple event I have to gripe again about those.  So good.  It'll be fun.  I look forward to it.  And Mr. G. and the hard work he does to keep us save and secure.  And laughing with our Picture of the Week.



STEVE:  Yes, indeed.  So we've often complained.  My biggest gripe is when something is trying to show like how big something has gotten or small something has gotten, like the amount of change.  And it looks like, oh, like a lot.  And then you realize, wait a minute, the scale goes from 111 at the bottom to 112 at the top.  And it's like, what?  So if you put it on a zero-based scale, you couldn't tell the difference between the two bars.



LEO:  Exactly, yes.



STEVE:  Which is just, you know, it's cheating.  Anyway, this is just a beautiful piece.  And is this xkcd?



LEO:  It sure looks like Randall's work, yeah.



STEVE:  Yeah.  I thought that was his style.  And one of the things, I think one of the reasons his work appeals to techies is his attention to detail.  And this has that.  So this is a chart of energy, fuel energy density in megajoules per kilogram.  And it's got five bars because he's got five different sources of energy.  We've got sugar at 19 is the lowest bar.  Then coal at 24 is the second.  Fat at 39 megajoules per kilogram is in the middle as the third bar.  Gasoline is up at 46.  But then we hit uranium.  Now...



LEO:  Properly handled, I might add.



STEVE:  Yes.



LEO:  I mean, you could say the same thing about hydrogen, but it takes a little to get that powered.



STEVE:  It does.  So uranium has a graphed megajoules per kilogram.  Whereas the others are 19, 24, 39, and 46, uranium is 76 million.  



LEO:  Yes.



STEVE:  Now, okay.  Now, we have a problem if this is a non-logarithmic scale.  And so the punchline here, the title of this is Science Tip:  Log scales are for quitters who cannot find enough paper to make their point properly.



LEO:  That's hysterical.



STEVE:  And so what we have is we have a - it looks like, oh, it's probably several miles long strip of paper which has been scotch-taped to the top of the uranium bar, which of course immediately shot off the top of the chart because you've got a chart that shows a modest-sized bar for 46 megajoules per kilogram for gasoline.  Well, that chart cannot hold 76 million using a linear scale.  But no, this person said, no, I'm not switching to logarithms.  I'm going to stay linear.  So what we have is this insanely long piece of ribbon paper which it goes off into the distance, then it fanfolds itself about seven feet down to the floor, comes back off the bottom, and the end of it curls up so that we're able to see the top of the bar properly labeling the height of 76 million megajoules per kilogram for uranium.  So, yeah.  Anyway, another nice little science-y Picture of the Week.



And it put me in mind of the fact that one of the things that I have, and I mentioned on the podcast a couple times because I love the app, is it's monitoring the SMT - not SMTP.  What's the - I can't think of the acronym for things - oh, MIBs, M-I-B-S.  But there's a name for the counters in networking gear that's - it's escaping me right now.  But it's - so the router that I have running pfSense is maintaining total bytes in and total bytes out, and also rates for counters.  And so the app that I've got running in Windows is polling that and graphing it.



And the point is that I've got I think 300 megabits down and 30 or 40 megabits up.  Which is plenty for me.  But on a linear scale, it's inconvenient because from time to time I will pin that 300 megabits.  And if I want to have 300 megabits be the maximum point of the chart when not much is going on, it looks like a flat line.  The point is that I have it set to a logarithmic scale which is one of the options this cool graphing tool has.  And what that has the effect of is of course compressing the top so that I get relevant-looking graph all the time, regardless of what scale my bandwidth variation is running in.  So I'm sure everybody knows logarithms certainly do have their place, although xkcd had fun without it.  



LEO:  I think that that was his point, is use logarithms.  He was being sarcastic.



STEVE:  Yes.



LEO:  Yes.



STEVE:  Yes, yes.



LEO:  Use a log scale, please.



STEVE:  So after last Tuesday's monster Microsoft patch event, so far it appears that nothing new was badly broken.  Which given how much they did is a little bit of a miracle.  It was quite a month, in which 121 new patches were delivered.  In fact, so many things received updates that it's unclear whether there was anything that didn't.  There were identified CVEs in Windows and Windows various components:  Azure Batch Node Agent, Real Time Operating System, Site Recovery, and Sphere; Microsoft Dynamics; Edge browser; Exchange Server; Office and its many components; PPTP, SSTP, Remote Access Service PPTP; Hyper-V; System Center Operations Manager; IIS.  And of course the Print Spooler didn't escape being fixed some more.  And we've got Microsoft Windows Defender Credential Guard.



And on top of all of that there was an additional 17 CVEs which patched Edge, plus another three patches for Secure Boot.  So that brought the month's total CVE-patched count to 141.  And if you were thinking that that seems like a lot, yes, it's nearly triple the size of last year's August release  talk about year-over-year inflation  and it's the second largest release of 2022.  So this has been a big month.



Of those 121 new CVEs, 17 are critical, 102 were marked as important, one was only moderate, and there was one last one that was low in severity.  Two of the bugs are publicly known, with one of those, affectionately known as DogWalk, is or at least was, and actually it will be until everybody gets themselves patched, under active attack at the time of the release.



Not long ago we were talking about the trouble with Microsoft's custom protocol handlers and how they could be readily abused. This particularly critical zero-day remote code execution vulnerability is another one of those.  And as before, Microsoft was responsibly informed of this problem and declared that it was not a security problem.



LEO:  Oh, were they wrong.



STEVE:  Oh, my god.  And I suppose it wasn't until it was, since now it is, and it's being exploited in the wild to damage Microsoft's users for which, as we know, they take no responsibility because we all give that up when we start using any of their stuff.



LEO:  Yeah, it's your fault.  It's your fault.



STEVE:  That's right.  That's right.  So the trouble is known as "DogWalk" because it's another path traversal flaw, thus walking the directory hierarchy.  When a targeted victim opens a specially crafted .diagcab, so it's a file ending in .diagcab, archive,  which contains a diagnostics configuration file, it allows an attacker to save their malicious executable file into the - I have a hard time even saying this - into the user's Windows Startup folder.  Which, you know, is really not where you want attackers' malicious executable files to be stored on your computer.



LEO:  Well, at least it's easy to find.  I mean, they're not hiding it.



STEVE:  Yeah.  So of course the next time this hapless victim logs into the system after a restart, that file will dutifully be automatically run by Windows.  And this vulnerability affects all Windows versions starting from 7 on and from Server 2008 and on.



So, okay, get this.  The problem was originally, as I said, responsibly reported to Microsoft on December 22nd, 2019 by security researcher Imre Rad.  Even though a case was opened by Microsoft the next day acknowledging the receipt of his report, six months later Microsoft declined to fix the issue.  They told Rad that to make use of the attack, an attacker would need "to create what amounts to a virus" - okay - "convince a user to download the virus, and then run it."



LEO:  Yeah?



STEVE:  Yeah, gee, like maybe some malware.  So Microsoft said that "This wouldn't" - I'm quoting - "this wouldn't be considered a vulnerability."  Okay?



LEO:  Huh?



STEVE:  They said:  "No security boundaries are being bypassed," meaning we designed it to work this way.



LEO:  Well, then, okay.  It's supposed to be doing this.



STEVE:  It didn't bypass anything.  It's working the way it's supposed to.  And they said:  "The proof of concept doesn't escalate permissions in any way" - okay, I guess it didn't need to, but it would be bad if it did, but it doesn't.  Or, they said,  "do anything the user couldn't do already."  Because, you know, users are perfect, and they never click something that they shouldn't.  So what's the problem?  Right.



So apparently because the proof, by their logic, because the proof of concept was only a proof of concept and not an explicitly and massively destructive bug, and since in a sense it was by design, Microsoft said not a problem.  In fact, they wrote:  "There are a number of file types that can execute code in such a way but aren't technically 'executables.'"  What?  "And a number of these are considered unsafe for users to download/receive in email."



LEO:  Even there.



STEVE:  "Even .diagcab."  In other words, again, this is what we told you it was going to do.  And they said:  "Even .diagcab is blocked by default in Outlook on the web and other places."  Wow.  So okay.  So much for layered security and defense in depth.  They're saying, well, Outlook blocks it.  So what's the problem?  Okay.  Back then - I know.  Microsoft essentially said that, yes, these sorts of files are indeed unsafe for users to download and receive, and they should not do that.  But don't worry, our email client, Outlook  which as an aside has recently been crashing so badly for many users that it has been unusable, but that's a different story that we'll get to in a minute  they said, our email client, Outlook, blocks these by default.



Consequently, despite Microsoft's assurances 2.5 years ago, today we have CVE-2022-34713 identified as a critical zero-day remote code execution vulnerability.  And this is exactly what that is.  Which they said, no, this is what we wanted.  Okay.  Since that was finally fixed this month, last week with August's patches, it would appear that Microsoft's position on allowing attackers to execute their code, uninvited, on a victim's Windows machine has since changed, and not a moment too soon, though about 2.5 years too late.  And of course many people were hurt in the meantime.



This CVE leverages Windows Support Diagnostic Tool (MSDT).  And it's not the first time an MSDT bug has been exploited in the wild just this year.  We previously talked about this.  This bug allows remote execution when MSDT is called using the URL protocol from a calling application, typically from Microsoft Word.  So perhaps Microsoft missed that other way to invoke this very dangerous diagnostic feature.  Wasn't just Outlook.  Word could do it, too.



LEO:  Well, even worse, this is exactly the kind of thing those evil call center guys do.



STEVE:  Yes.



LEO:  They would say, oh, you know, we see you have problem on machine.  Please launch diagnostic tool.  Yeah.  That seems sensible; doesn't it.  Geez.



STEVE:  What could possibly go wrong?



LEO:  What could possibly go wrong?  They're trying to help me.



STEVE:  Yeah.  So exactly as you're saying, there's an element, Leo, of social engineering to this because a threat actor would need to convince a user to click a link or open a document.



LEO:  Yeah.  But how harmful could the Microsoft diagnostic tool be?



STEVE:  Well, yeah.  That means we have nothing to worry about because all users are smart, and they would never click on a link or open a document that they weren't supposed to, you know, even if their mother sent it to them.  Or because their mother sent it to them.  Anyway, okay.  Recall that we were also just talking about files downloaded from the Internet being tagged with, I love this, the MOTW, the Mark of the Web.  The idea being that this would cause Windows to refuse to proceed or to at least issue a pop-up warning.  Well, we would not want that to confuse people using Microsoft's Support Diagnostic Tool which fields these .diagcab files.  So it was designed not to check for the Mark of the Web so that its files can be opened without any warning.



0patch's founder told The Hacker News that:  "Outlook is not the only delivery vehicle.  Such files are cheerfully downloaded by all major browsers including Microsoft Edge by simply visiting a website.  And it only takes a single click, or mis-click, in the browser's downloads list to have it opened.  No warning is shown in the process, in contrast to downloading and opening any other known file capable of executing an attacker's code."  Wow.  That was fixed, finally.



The infamous SMB Server Messages Block protocol is back with another remote code execution vulnerability.  This one's a server side bug which allows a remote, unauthenticated attacker to execute code with elevated privileges on affected SMB servers.  That's not good.  But there is some good news.  Only Windows 11 is affected.



LEO:  Oh, great.  That's what I use.



STEVE:  Oh.  I had here in my notes only the five people using Windows 11.



LEO:  Six now.



STEVE:  That would be six will be at risk.  But seriously, although we don't have any details about this one, it does suggest that some new functionality added for Windows 11 also introduced a new vulnerability.  If code keeps changing, it is never going to be secure.  The concern about this one, since it affects Windows 11 systems that are publicly exposing their SMB service, is that it would potentially be wormable.  So disabling SMBv3 compression is a workaround, but why do that?  Just apply the update from this month in order to squash this bug.



And this again reinforces my admonition against ever publicly exposing any Windows service that does not need to be offering its services to anonymous public users, which as I've said before pretty much limits us to web and email.  Those need to be public and anonymously available.  Pretty much nothing else does.  SMB, as we know, has always been a catastrophe.  It's the reason I created ShieldsUP! when Windows was first put on the Internet, which was sharing everyone's "C" drive to the world.  And they just broke it again with Windows 11.



Okay.  Then we have three CVEs for Exchange Server because it has multiple Elevation of Privilege vulnerabilities.  And it's a bit unusual for elevation of privilege (EoP) bugs to receive a critical rating, but these qualify.  These three bugs could allow one authenticated attacker, an authenticated user who was in attack mode, to take over the mailboxes of all Exchange users on a server.  This attacker could then read and send emails or download attachments from any mailbox on the Exchange server.  Administrators must also enable Extended Protection to fully address these three vulnerabilities.  But, you know, another mess.



Microsoft has also been having constant problems with their Network File System (NFS) code, and we have another remote code execution vulnerability fixed this month, making it the fourth month in a row that NFS has received a patch to close a code execution vulnerability.  This one has a CVSS of 9.8, so it could be among, I would argue, the most severe of the month's patches.  Fortunately, there is not a huge population of people, Windows users, with NFS exposed.  To exploit this, a remote, unauthenticated attacker would need to make a specially crafted call to a vulnerable network file system server.  But this would grant the attacker elevated code execution privilege.  And oddly enough, despite its CVSS of 9.8, Microsoft has this one categorized as only one of those 102 "important" in severity problems.  But of course anyone using NFS should take this very seriously because your system can be scanned for and found on the 'Net.



And speaking of Outlook, Leo, you're going to love this one.  This is CVE-2022-35742, which addresses that annoying Microsoft Outlook technically Denial of Service is what you would call it, vulnerability which I mentioned earlier.  If an Outlook user were to receive a specially crafted email  and a great many did recently  their Outlook executable would immediately crash, and it could not be restarted.  Upon an attempt to restart it, it would immediately terminate again once it retrieved and attempted to process that invalid message.



LEO:  What a mess.



STEVE:  And every time you tried to start it, it would just crash.  And it's not even necessary for the targeted victim to open the message or to use the reading pane.  Just its presence in the inbox keeps Outlook from starting.  The only way to restore Outlook's operability is to access the email account using some other email client, perhaps webmail, to remove the offending emails from the mailbox before then finally restarting Outlook.  But don't worry.  As Microsoft explained, Outlook is your first line of defense against executing any of those malicious Diagnostic Tool extension files, so you should feel completely safe.



And I know, yes, I'm being tough on Microsoft.  But there's a large and growing consensus that they're generally losing control of this beast that they've created.  I love Windows.  So I sincerely hope it's just a phase they're going through as a result of what was in retrospect...



LEO:  Just a phase.



STEVE:  Just a phase.  Turned out to be a poor decision to outsource Windows quality control to their early adopter consumers.  Most of those people just want to screw around with Microsoft's latest crap.  They're not actually interested in controlling anyone's quality.  And we've noted how, in the past at least, Microsoft often ignores those outside pleas to repair problems that have been identified in this process.  They're just pesky.  You know, they don't work for Microsoft.  Buzz off.  So this new scheme is clearly not working.  And again, let's just hope it's a phase.



Last week, Paul and Mary Jo were observing that there are growing indications that consumers are not making Microsoft any money.  So Microsoft appears to be more or less abandoning the consumer for the enterprise.  Now, speaking as a consumer, that would be fantastic.  Please.



LEO:  Leave us alone.



STEVE:  Just ignore us, Microsoft.  Just leave us with Windows 10 forever, like you originally promised.  Just patch its flaws and don't change anything else.  It's fine for us now the way it is.  Don't break it, please.  And meanwhile add all the new features you want to the enterprise.  Really, just give that your entire focus, and all of the rest of us lowly consumers will just quietly be getting stuff done.  Sounds like a great plan.



Okay, Leo.  I have to have a sip of water after that.



LEO:  Okay, take a breath.  Relax.  It's going to be okay.  Of course, I came in, and my Windows said, "Something went wrong.  You'd better reboot."



STEVE:  Windows what?  What was that Windows?



LEO:  Oh, 11, 11.



STEVE:  Oh, yeah.



LEO:  Yeah, 11.  You know.  You know that Windows.  Well, you know, I have to use 11 for the same reason you have to use Windows.  It's what everybody's using.  All right, Steve.  On we go.



STEVE:  So Facebook is cautiously creeping toward default end-to-end encryption.



LEO:  Kind of because they have to, you know, at this point.



STEVE:  Yes, yes.  In a move that's sure to turn up the heat on the question of whether consumers have the fundamental right to have their interpersonal communications end-to-end encrypted, making them truly and irreversibly private, Facebook's Sara Su, there's actually a position called the "Director of Trust" for Facebook's Messenger app, posted last Thursday.  She said:  "This week we'll begin testing default end-to-end encrypted chats between some people.  If you're in the test group, some of your most frequent chats may be automatically end-to-end encrypted, which means you won't have to opt into the feature. You'll still have access to your message history, but any new messages or calls with that person will be end-to-end encrypted.  You can still report messages to us if you think they violate our policies, and we'll review them and take action as necessary."



As we know, this is a big deal.  The key word is "default"; right?  I call it the "tyranny of the default."  Even though WhatsApp, based upon the beautiful Signal protocol, offers nothing other than end-to-end encryption, Facebook's native Messenger app, which offers end-to-end encryption as an option, has never had that enabled by default.  And even now, they're moving toward that cautiously.



They're saying that the move is unrelated to the recent outrage and backlash from privacy advocates after Facebook complied with an order to reveal the content of "private" messages between a mother and her daughter.  The order came from a Nebraska police department as part of their investigation into an abortion-related case, of course in the wake of the U.S. Supreme Court's reversal of its previous decision, made under different Justices, in Roe v. Wade.



So, okay, fine, whatever.  Despite Facebook's denial, the timing of this move is at least suggestive, but it's a welcome move nonetheless.  And since most Facebook users just use Facebook's built-in native Messenger app, much as mom and daughter did in that instance, you know, if "encrypted by default" is eventually true, it will go a long way toward protecting the privacy of many more of Facebook's three billion users.



LEO:  And from Facebook's point of view, they would, regardless of the politics, they would like to be able to say, well, we don't have access to that information.  So, sorry.



STEVE:  Yes.  That's what Apple has said over and over and over.  And it upsets out law enforcement personnel.  But, you know, really the question comes down to I think how is the greater good served?  Is the greater good served by truly honoring privacy that you're presumably providing?  And yes, some small minority of people will abuse that.  There will be criminal use of the abuse of that privacy.  Or are you actually not going to give privacy to everyone and open everyone to the potential of abuse, open non-criminals to the potential of abuse by suppressive governments and so forth.  So anyway, as I've said often, I think this question of how we actually deal with this -  the technology will do anything we want.  The question is what do we want, and where is the greater good served?



The security firm "Cyble" (C-Y-B-L-E) recently scanned the Internet for instances of - and it's just hard to imagine this - completely open VNC servers lacking any password protection, any authentication of any kind.  You connect to them, and you're looking at a desktop with mouse and keyboard access.  They found more than 9,000 of those which they were able to freely log in to.  And as I'll get to in a moment, these were not all safe to leave open.



Okay.  So first of all, for those who don't know, VNC stands for Virtual Network Computing.  It's a very old, open, platform-independent and very popular remote console access system which implements a protocol called RFB, which is Remote Frame Buffer.  So one runs a VNC server on some machine, typically any random desktop operating system, and this server allows a remote networked user with a matching client to view the machine's console and use its keyboard and mouse remotely.  It's essentially Windows RDP, you know, remote desktop protocol, but it's non-proprietary.  It's not Microsoft's.



So this firm "Cyble" scanned and found more than 9,000 machines openly exposing their access without any form of authentication.  So this begs the question, why?  What could the possible reason be?  How is it possible?  Is it the result of negligence, or error, or maybe a misguided deliberate decision for convenience?  Most of the exposed instances are located in China and Sweden.  Those are the top two by far.  But the United States, Spain, and Brazil were also well-represented.  Cyble identified 1,555 passwordless instances in China, 1,506 in Sweden, with the U.S. coming in third at 835, followed by Spain at 555 and Brazil with 529.  And as if just having consoles exposed was not bad enough, they found that some of these exposed VNC instances were making accessible industrial control systems.



They wrote:  "During the course of the investigation, researchers were able to narrow down multiple Human Machine Interface (HMI) systems, Supervisory Control and Data Acquisition Systems (SCADA), Workstations, et cetera, connected via VNC and exposed publicly over the Internet."



In one of the explored cases, the exposed VNC access led to one of these Human Machine Interfaces, an HMI, for controlling pumps on a remote SCADA system in an unnamed manufacturing plant.  So anybody could, like, literally go there, bring up this control panel, and start turning things on and off.  And maybe it's just a spoof.  I mean, it occurs to me there could be a honeypot, right, to see if someone does that.



Being curious to see how often attackers are targeting VNC servers, Cyble used its tools to monitor for attacks over port 5900, which is VNC's default server port.  They found that there were over six million connection attempts in a month.  Most attempts, I mean, six million attempts in a month, I don't know how many IPs they were monitoring.  But wow.  Most attempts to access VNC servers, they were able to determine that, originated from the Netherlands, Russia, and the United States.  So nothing to be proud of here in the U.S.  Unless, you know, it could be security services benignly scanning.  But you don't do that six million times.



It's worth noting that this was just a scan for VNC servers requiring no authentication.  Cyble's report noted that if they had raised the bar just a bit by including VNC servers using weak and easily cracked passwords, like "password" or probably "monkey," the number of exposures would have sky-rocketed.  And to that end, sadly, many VNC products are old and not super-securable at best.  Many, for example, offer only passwords of up to eight characters.  That's no longer enough when you're able to conduct a brute force attack on something that looks juicy.  Like I said, much of this stuff is old, so they are inherently insecure.



And so once again, we face the rule of the Internet that really needs to take hold:  The only servers that should ever be made publicly accessible are those that have to be accessed anonymously by the public.  Everything else should be behind a VPN, SSH login, or an overlay network like Tailscale.  And we have to do it.



LEO:  I wonder if - my favorite VNC is one of the affected ones, Chicken of the VNC.  Have you heard of that one?



STEVE:  Chicken of the VNC.



LEO:  I kid you not.



STEVE:  That's great.



LEO:  It was a Windows VNC.  I don't use VNC anymore, but...



STEVE:  No.  I actually, when I was - a couple years ago when I was getting my world set up to do SpinRite debugging, I was looking for like the best way to do remote debugging.  And so I was looking to see if there was any, like, text-based VNC that I could use.



LEO:  Oh, that would be cool.



STEVE:  Because, yeah, and so I dusted off VNC and took a look at it and looked at various ways of using it.  And I ended up not finding anything that was better than just good old Windows networking.  But anyway...



LEO:  Well, and RDD has had its problems, too.  It's not like it's failproof, either.  



STEVE:  No.  And but in the case of VNC, one of the problems is its age.  But of course the fact that you even know of Chicken of the VNC demonstrates it's been around forever.



LEO:  Oh, yeah.  Oh, yeah.  I didn't make that up, kids.  In my day, that's what we used, Chicken of the VNC, yes.



STEVE:  And it was really good, and its mercury content was low.



LEO:  Very low, very low.  It was line caught, line caught.



STEVE:  Okay.  So for the third time in the last year, a neighbor who had heard that I knew my way around computers stopped Lorrie and me during our nightly walk to ask what she should do about her website.  The common denominator in all three instances, which is why I'm mentioning this today, is that these people were not webmasters, nor were they people who were content with having Facebook host their pages.  They were professionals who wanted to have their own website located at their own domain. So they got a referral from someone else, or perhaps noticed at the bottom of someone's website a mention that the site was created by "Websites R Us" or "Johnny's Websites" or whatever.  One way or the other they found a service, and they used that third-party service.  Typically it's a one-man shop; right?  A DBA firm to function as an intermediary between the different things they needed - a domain name registrar and a hosting provider to register the domain name of their choice, create a website, and populate it with their content.



These relationships are almost always problematic because a great deal of time can be spent getting everything set up.  Then of course the website's owner invariably wants to make changes, which are met with grumbling of various degrees by the webmaster who just wants his client to shut up and pay to maintain the site, as is, without actually doing any further maintenance.  Since the people who own the content and are having it put there are typically unable to manage the site themselves, it's a constant problem.



So every listener here is now thinking to themselves:  "Okay, Gibson, we all know this.  What's the point?"  The point is, what I have seen over and over is that these relationships do not age well over the long term.  Or are at risk of not.  And in these cases have not.  The reason is the domain name.  Without exception, none of these regular nice people, who have their own website under a domain name that they originally chose, having had it for years and having invested emotionally, intellectually, and spiritually, have any actual control over what they consider to be their domain name.  In every case their domain name was registered by someone else for them in the beginning.



LEO:  Yeah.  I talk about this on the radio show a lot.  Yup.



STEVE:  I'm not surprised, Leo.  It's got to be a problem that is beginning to happen more and more because domain names are becoming, you know, they're beginning to age.  So at a time when none of that accumulated value existed in "the name," this was all created.  And without an understanding of the way domain name property is managed, they never stopped to wonder what would happen in the future.  Whoever it was who set all this up for them had an account with a registrar, and it was under their that their domain name was, and probably still is, registered.



This all came to mind recently when I was explaining to the most recent neighbor who stopped me.  And what I explained was that where their domain names were concerned, the rule is not that possession is nine-tenths of the law.  With domain names, possession is 100% of the law.  It's the only law.



LEO:  The only thing if you had a copy, if it was a trademark, you might be able to wrest it back.  You could appeal, you know, if it's your trademark and somebody else has the domain name.  That's happened in the past.  But it's hard.



STEVE:  True, although at a great deal of expense and attorneys and court cases and everything.



LEO:  Right, right.  Well, no, I think ICANN has an appeal process.  It's not as bad as all that.



STEVE:  Oh, so you don't have to actually sue and get it.



LEO:  You go to ICANN; right.  You can say to ICANN, hey, you know, I'm McDonald's, and this guy is not.  Can I have my name back?  Yeah.



STEVE:  Right, right.  So, and this sort of puts me in mind, we've talked about the problem of somebody dying with a whole bunch of passwords which are not known to anyone else.  The time before this most recent time, the long-time webmaster of a different neighbor actually had died rather suddenly, being survived by his wife.  Eventually, the web and email services, which continued for some time, were terminated, and my neighbor went in search of a solution.  The man's wife was nice, but she knew nothing about technology, and my neighbor learned that all of his other clients were also furious and fuming that their web domains collectively had been allowed to expire, had then been snatched up, and were now hosting click-bait pages.



LEO:  Oh, god.



STEVE:  So I had to give that neighbor the bad news that she really had no practical recourse.  And in this most recent case, the webmaster is an 80-year-old geriatric good friend of the website owner.  Working through him, she explained, she's created an extensive website filled with links and artwork, plays and songs.  It is a content-rich labor of love, and she's been frantic that something might happen to it.  So I told her that I could easily relieve some of her worry by cloning her entire website into a directory, so that at least all of its content would be archived for safekeeping.  And so the next morning I did that, and she was hugely relieved.



But I explained the situation to her about the control of domain names; and that if anything should ever happen to this person, in the absence of some sort of plan, she stood to lose any and all use of that domain because of the way it was registered.  She had no claim to it whatsoever.  It actually is not hers, it's his.  So anyway, so I did a little bit of WHOIS poking around, and I discovered that he's using a service called Enom  (E-N-O-M) which is for domain name resellers.  Enom, it turns out, is a branch of Hover, my own chosen registrar with whom I am very happy.  So I told her that the best thing she could do would be to create her own account at Hover, then arrange with her friend to transfer the domain, with all of its existing settings intact, from his Hover account to hers.



She said it would be a bit dicey because he was a friend, and she didn't wish to offend him.  So I told her, okay, in the first place he will at least understand that it is an entirely valid concern for anyone to have.  So if transferring the domain to her now is not an option, could he at least explain what he has in place for dealing with his own potential inability, for whatever reason, to continue?



And so as a consequence of all this, I just wanted to take a moment to mention this to everyone who's listening because even though we all probably manage our own domain properties, it must be that we're all aware of others who are in a similar place, and it might be worth asking them about the plans their domain name providers have for their domain names upon any event that might cause their registrations to expire and be lost.



LEO:  And of course the strong advice is, if you're going to have a domain name, register it yourself.  Don't let the third party register it for you because you're not controlling it.  You don't control it.



STEVE:  Yes.  And I was impressed.  I looked at the registration, and he had renewed it two days after one month before expiration.  So that is to say, it was probably set up for auto-renew.  It was renewing annually.  So it looked like he was tending it well.  And of course the flipside is, if you are registering your own domain, then you do have that responsibility.  You need to absolutely make sure that it is, you know, that it's set to auto-renew.  That you're maintaining a current email address for them so they're able to notify you of any problems, that they've got a credit card that they're able to charge.  I mean, so the point is, if you're going to take on that responsibility, you know, there is some responsibility that comes with it.  But, boy, I'm glad to hear that you're seeing the same thing among your callers on the weekend.



LEO:  Oh, yes.  I see it all the time.  And she should approach, I mean, if this guy's a friend, this 80-year-old guy is a friend, just say can I transfer it over to me because it's mine?



STEVE:  Yup.  Right.



LEO:  You know, he did it probably as a favor to her because he knew how.  Because the reason people don't want to do this is they don't know how to set up the DNS to point to their website.  And I understand.  This is a little techie.  So I understand why they don't do it.  But, yeah, you need control.  You need to control it.  Especially if it's a business.



STEVE:  And you know, it made me think, Leo, that shouldn't there maybe be a provision added to domain name registrations where they actually have something like a next of kin.



LEO:  Right.



STEVE:  It's like part of the registration.  So that, for example, in this instance he could put her...



LEO:  Designate her.



STEVE:  Predesignate her as this domain's next of kin.



LEO:  Or say I'm registering this as a third party, and the domain really belongs to this.  There should be a way to do that.  One of our chatters, PCGuy8088, who does this for a living, he says, "Whenever I create a domain for an account, I create a separate account for the domain, and their information is registered in the person's name, using their credit card number."  That's a responsible webmaster.  That's the right way to do it.



STEVE:  Yeah.  And I have not yet run across anyone who does.  So anyway, I just - because this actually happened, I thought, you know, let me just take a minute to just remind our listeners that I'm sure they don't have some third party managing theirs.  If they do, you know, get it.  But boy, I'll bet they know people who do.  And these problems keep cropping up when something happens.  And mostly I think what people are surprised by, both of the people who approached me, my neighbors, were shocked that there really was no recourse.  I mean, no practical recourse for some non-trademarked random domain name.  They don't have any rights.  It is the rights of the accountholder.



So, okay.  And speaking of making a web copy, I fired up my favorite cloning tool.  It came up, and it said, hey, I've got an update for you.  I thought, oh, that's nice.  I went there, and I was reminded that it was free and donation-ware.  And I gave them 10 bucks because there's a long line of maintenance that's been done over time.  They are keeping it current.  They are giving it new features.  Anyway, this thing, I just wanted to give them a shout out.  It's Cyotek WebCopy.  Cyotek WebCopy.  And I'm using it because it is just so good.  You point it to the root of a website.  You point it to a directory that either exists or doesn't, and you say, make me a copy.  And this thing just, it does all of the link following, and it changes all the links so that you end up with a locally browsable copy of the site.



And these days, with drives being as large as they are, and there's been discussions of this in GRC's newsgroups about people just cloning sites because they don't want to ever lose some of the content of a site that they really care about.  And yeah, the web archive kind of has it, but it sometimes doesn't have all of the assets that you wish it did.  So anyway, Cyotek WebCopy is the thing that I've been using, and really happy with it.



There was a quote that I saw that I just had to share with everybody.  I ran across it because Ryan Sleevi, who's one of the security guys at Google, he retweeted a tweet from a Jens Axboe.  Jens Axboe was quoting @cra - so, boy, a three-letter Twitter handle, that's rare - whose self-description on Twitter says "Building a better world through open collaboration."  So when Jens quoted @cra, he added to his retweet "I don't think I've ever seen anything more true posted."  So here's the original quote of the open collaboration guy via Jens Axboe's retweet and Ryan Sleevi's retweet retweet.  They said:  "Running a successful open source project is just 'Good Will Hunting' in reverse, where you start out as a respected genius and end up being a janitor who gets into fights."



LEO:  I love it.



STEVE:  And think about it.  It is exactly right.  Running a successful open source project is just "Good Will Hunting" in reverse.  You start out as a respected genius, and you end up being a janitor who gets into fights.  Because that's what these things, you know, it is a thankless task.  And you end up, exactly as these guys said, just having to defend every decision you ever made, and arguing back and forth about features or not features and so forth.  And anyway, just a great quote.  So thank you, Ryan, for bringing it to my attention via Jens.  Oh, and Leo?



LEO:  Yes?



STEVE:  Progress from Ed Cano, the father of the Sandsara, which is that thing you and I both invested in years ago.



LEO:  Oh, wait a minute.  This was like the Zen garden thing?



STEVE:  Yes.  It's the Zen garden thing.  It's a beautiful wooden round table filled, sort of a, well, Zen garden is a great description, round, filled with very fine sand, and a ball bearing is rolling around in the sand leaving its wake behind it, all controlled by a very clever two-stepper motor mechanism that allows this ball to be moved in really cool geometric patterns.



Ed just sent:  "Hi everyone.  Recently I posted a comment stating that the Sandsara App was available in the App Store.  We had to make further improvements for the Android users, so it took us more time.  But now it is also available in the Google Play Store.  You can follow the links below to download the app."  And for anyone who's curious, and Leo you might be, I've got both of the links to the Sandsara apps in the show notes.



He said:  "The app will help you to personalize your Sandsara by selecting the patterns that you like the most, adding more designs, or changing the colors of the lights.  Even when we are happy with our current development, we are working on more features for the app, so please send us any feedback so we can improve our current version."  And here's the news:  "Regarding shipping and manufacturing, the container with 300 Sandsara has been cleared!"



LEO:  Oh, so it's stuck in shipping and customs, yeah.



STEVE:  Yes.  It turns out for some reason getting black sand through customs is a huge problem.



LEO:  Oh, wow.  Interesting.



STEVE:  He said:  "We're only a few days away from the first orders" - meaning you and me, Leo, and I know many of our listeners got onboard also because Ed and I had a conversation afterward.  He listens to Security Now!.  And so he was first of all surprised when I had discovered it and heard me talking about it, and then they got a bunch of backers as a consequence of the podcast.  Anyway, he said, "...the first orders to receive their units.  For those backers, you will receive your tracking numbers by the end of the week.  And it will take another week to receive your Sandsara by your door.  After clearance, the container is on its way to the U.S. warehouse, where our partner will split everything up in orders and start the individual fulfillment."



And he said:  "For the rest of the orders, we mentioned that all orders would be completed by late August, and we are on track to accomplish so."  He said:  "We expect to complete all processes in around one week.  Depending on the courier, your package might take two to three weeks once we start shipping.  Please be wary of any messages this week because you might receive your tracking number."  And blah blah blah.  So anyway, his communication has been great through these years.  And as I mentioned last time I talked about him, he is clearly a perfectionist.  It has been - obviously it took far longer than he expected.  All kinds of problems came up.  But he never quit. And I, based on everything I've seen, we are going to be really  happy with this thing when it finally arrives.  And it looks like it may.



LEO:  I don't know if I ordered it.



STEVE:  I think maybe you still can.



LEO:  I hope I didn't because I don't know where the hell I'm going to put it.  We have cats.



STEVE:  You have cats, yes.



LEO:  Cats might see this as an alternative bathroom.



STEVE:  Yeah.  Well, now, you are able to get a glass lid for it.  So it can be closed, although it could still be toppled over.



LEO:  Yeah. 



STEVE:  So, yeah, cats might be a problem.



LEO:  This might be for - maybe for work I'll have it, yeah.



STEVE:  Anyway, a couple bits - oh, that'd be good, yeah. 



LEO:  Yeah.



STEVE:  A couple bits, oh, that'd be good, yeah.  Couple bits of closing-the-loop feedback.  Alex T said:  "Hi Steve.  Thanks for another great show.  I think the way airplanes restrict images" - he's referring to the comment last week about how could the free WiFi allow text messaging but not images.  And he said:  "I think the way airplanes restrict images in messaging clients is even simpler than that."  That is to say, simpler than a bandwidth cap that's very low.  He said:  "I believe WhatsApp and others send image blobs to a separate endpoint for storage, and insert a reference to that blob into the message. A firewall block to the media storage endpoint would do the trick."  And of course given that that's true, Alex is completely correct.



Thomas Apalenek, I hope that's how I pronounce his name, he said - oh.  He said:  "Copy-As-Path in Windows 11."  And Leo, you're able to check this out.  He said:  "Hi Steve.  I use Shift-Right Click and then Copy-As-Path all the time since you showed us that a few years ago.  So thank you for that.  I don't know how much you've played with Windows 11."  Safe to say, as little as possible.  He said:  "Copy-As-Path has finally been moved to the standard right-click context menu."



LEO:  Oh, good.  Yeah, in fact I see it.  Let me see.  Let's see.



STEVE:  So no shift required anymore.



LEO:  Yeah, yeah.  Or hit CTRL+SHIFT+C without even a context menu, and it'll get it.  CTRL+SHIFT+C is the Copy-As-Path.  Which is great for pasting into terminal or whatever, code.



STEVE:  That is nice, yes, yes.



LEO:  Yeah.  Thank you.



STEVE:  Also Brad Cochran, he said:  "Hey Steve, long-time listener, first-time caller."  He said:  "I even dusted off my Twitter account to message you."



LEO:  Aww.



STEVE:  He said:  "Regarding your most recent Security Now! episode, it sounds like you haven't come across Attack Surface Management."  Apparently that's an acronym, ASM.  He said:  "I wanted to share a bit of info about how it works."  Now, he's referring to my talking last week about Microsoft's new, apparently it was a scanning service which they're making available to enterprise in order to check their attack surface.  He said:  "I wanted to share a bit of info about how it works.  Full disclosure, I work for Palo Alto Networks and sell a competing solution called Cortex Xpanse, but because of that I can explain at a high level how ASM works.



"You're right in your assumption that this is an external scan.  Essentially, ASM tools" - meaning Attack Surface Management tools - "scan the global public Internet on a daily basis, which is possible now due to advances in computing resources" - meaning lots of cloud and bandwidth - "for open ports and protocols.  You mentioned Shodan in the show, and that's similar.  One important note here, due to the Computer Fraud and Abuse Act (CFAA) and similar laws in other countries, these do not tend to be invasive port scans such as Nmap," he says, "which would violate the law if you ran on a network you don't control without approval."



And I'll mention that my own ShieldsUP! port scanner goes to some length to detect an open port without ever actually completing a connection.  Since ShieldsUP! emits packets under my control, as opposed to just using a full TCP/IP stack, I send out a SYN packet.  If a remote system replies to my probing SYN with its answering SYN/ACK, I never follow through with the final ACK to actually fully open the connection.  Instead, I send an RST packet to force that half-open connection to be aborted and immediately closed.



Anyway, he continues:  "Microsoft's solution comes via their acquisition of RiskIQ."  Which we talked about when that happened.  He said:  "Presumably they're augmenting the external attack surface data collected with that tool with their own intelligence, and even tying that with data from other tools in the Defender suite."  And he says, "We do something similar here."  He says:  "Other common players in the ASM space include Shodan, BitSight, SecurityScorecard, Randori, and CyCognito."  He says:  "And more popping up all the time.  This is definitely a growing cybersecurity market."  He says:  "I hope that helps."  And indeed, Brad, thanks very much for the follow-up.



And lastly, an interesting point was made by a Tom Malaher.  He said:  "Steve, in Security Now! 883," which was last week, "you said 'Any new hash will need to start over from scratch earning the reputation that that exact code file is trustworthy.'"  He said:  "How does this interact with creating unique downloads of SpinRite EXE for each paying customer?  Aren't those in conflict?"



And I replied to Tom.  I said:  "You are 100% correct, and it's something I've considered at some length.  As it has always been, SpinRite's EXEs will continue to be uniquely created for each of its users.  And I now have an HSM  a Hardware Security Module from DigiCert  installed on GRC's main server which will allow it to perform automated on-the-fly EV code signing."



LEO:  Oh, how cool is that.



STEVE:  Yeah.



LEO:  That's neat.



STEVE:  "Thus providing the highest integrity assurance possible.  So each custom-created SpinRite will be individually and validly signed, which is the best I can do.  But there will be no way for each of those signed hashes to ever earn a reputation for themselves..."



LEO:  Because they're supposed to be unique.



STEVE:  "...since each one will be unique.  Therefore, it may become common and expected for any reputation-based anti-malware service to be warning its SpinRite user that their freshly downloaded copy of SpinRite has never been seen before."  As indeed it wouldn't have been.



LEO:  Yeah.



STEVE:  But at least they will be EV code signed.



LEO:  That's good.



STEVE:  So the highest level of integrity possible.



LEO:  Yeah.  That's really good.  All right.  Let's talk about TLS Private Key Leakage.



STEVE:  Ooh.



LEO:  Something you never want to have happen.



STEVE:  You don't want your key leaking.



LEO:  You don't want any leakage on that one.  All right, Steve.  Let's talk about TLS.



STEVE:  Okay.  So as I said at the top of the show, four researchers, two each from the University of California at San Diego and the University of Colorado in Boulder, performed an amazing piece of work, described in their paper, which they titled:  "Open to a fault:  On the passive compromise of TLS keys via transient errors."  Their work was just presented during the 31st USENIX Symposium held in Boston last week. 



Okay.  So to get everyone's attention, I'm going to first share their paper's abstract.  They said:  "It is well-known in the cryptographic literature that the most common digital signature schemes used in practice can fail catastrophically in the presence of faults during computation.  We use passive and active network measurements to analyze organically occurring faults in billions of digital signatures generated by tens of millions of hosts.



"We find that a persistent rate of apparent hardware faults in unprotected implementations has resulted in compromised certificate RSA private keys for years.  The faulty signatures we observed allowed us to compute private RSA keys associated with a top 10 Alexa site, several browser-trusted wildcard certificates for organizations that used a popular VPN product, and a small sporadic population of other web sites and network devices.  These measurements illustrate the fragility of RSA PKCS #1 v1.5 signature padding" - which is the standards used - "and provide insight on the risks faced by unprotected implementations on hardware at Internet scale."



And I'll break all that down.  But what they found - okay.  Well, I'll break down what they did and what they found.  But one of the things I loved about the way they introduced the nature of the way digital systems can be fragile was to remind us of a true-life example that we may all recall.  They wrote:  "During 2009 to 2011, Toyota issued multiple vehicle recalls after hundreds of crashes had been reported relating to unintended acceleration."  Remember those?



"Initially, Toyota placed the blame on driver error, shifting floor mats, and sticky accelerator pedals.  In 2013 expert witness Michael Barr testified in the Bookout v. Toyota Motor Corporation case that a single bit flip sufficed to kill a throttle monitoring task, resulting in uncontrolled acceleration.  Toyota lost the case and began settling with crash victims out of court.  The exact cause of the memory corruption in Toyota vehicles was never established.  It could have been a buffer overflow, cosmic rays, or hardware faults.  No matter the underlying cause, the existing hardware protections were insufficient, and the software was brittle in the face of hardware errors."



And it's a little chilling how perfect this analogy is.  Now, we don't need to look further back than the week before last when this podcast was titled "Rowhammer's Nine Lives."  With DRAM, we have an unfortunate and persistent flaw where it's entirely possible for a bit of DRAM to spontaneously flip.  It was during the 25th USENIX Security Symposium in 2016 that the paper "Flip Feng Shui" was delivered, and in their summary they noted that Flip Feng Shui relies on hardware bugs to induce bit flips in memory.  They weaponized that unfortunate characteristic of weak memory operation.  And we know that this can happen without a memory system being under active attack.



The reason DRAM can be equipped, at extra expense, with parity checking and ECC is because DRAM memory is not perfect.  Parity checking cannot correct, but it will at least catch a single bit flip.  And ECC can correct such an occurrence.  And all Internet packets contain checksums to detect any simple errors introduced between the time a packet is created and the time it is received at the other end.  These measures and many others are in place because, in reality, computers can and do make mistakes.  While we would like to believe that every time we multiply the same two large prime numbers we're going to get the same result, in reality that's true almost all of the time, but it's that "almost" that can be weaponized.



So with that bit of background, here's how these guys describe their work, what they did, and what they found.  Referring back to their Toyota example they begin:  "Cryptographic software engineering is, fortunately, less often considered to be a matter of life or death."  You know, than crashing a Toyota.  They said:  "Nonetheless, faults can have a similarly catastrophic impact on cryptographic systems.  As prior work has shown, attacker-induced or naturally occurring bit flips can corrupt cryptographic computations, causing them to produce incorrect results, or even leak secret information or keys.



"In this paper, we show that these attacks can be applied entirely passively, allowing a network adversary to derive TLS RSA private keys simply by observing network traffic.  When errors occur during a server's RSA signature computation, the resulting failed handshake can give an attacker sufficient information to derive the server's long-term private key."



Now, okay.  To clarify before I continue, when a server makes a mistake during its computation of a TLS handshake's RSA signature, that handshake will fail.  But due to some known vulnerabilities in the specific cryptographic operations, actually something known as the Chinese Remainder Theorem, the way the signature is...



LEO:  Ooh, ooh.  I learned about that two years ago in the Advent of Code, the Chinese Remainder Theorem.



STEVE:  Oh, very cool.



LEO:  I never knew it even existed.  Geez.



STEVE:  Yup.  So the way the signature is wrong leaks a bit of information about the private side of that RSA computation.  So they continue:  "We demonstrate these attacks by collecting 5.8 billion TLS handshakes from two different university networks.  These handshakes included 3.3 billion connections using TLS 1.2 or earlier and 2.7 billion server signatures.  Over a few months, we found nearly 2,000 non-validating digital signatures from failed handshakes."



Okay, now, again, to be clear, that number should be zero.  But some fault in the servers were resulting in a very low but non-zero number of failed signature computations, nearly 2,000 out of 2.7 billion.  But that's all it took.



They said:  "Some of these failed handshakes allowed us to compute three RSA private keys associated with Baidu, a multinational technology company in top 10 Alexa.  These three keys were used to secure more than a million connections to hundreds of hosts in our dataset corresponding to dozens of Baidu's cloud-based services.  This passive attack is particularly concerning in the context of nation-state adversaries conducting mass surveillance.



"Unlike active attacks or remote compromise, which risk leaving evidence of tampering, passive fault analysis leaves no trace on either the client or the server.  A network adversary only needs to observe network traffic passively" - which again reminds us of the NSA that was tapping major exchange points, we didn't know why.  "A network adversary only needs to observe network traffic passively and perform simple cryptographic calculations, capabilities that modern nation states are known to possess and employ for the purpose of network surveillance.  This attack is exacerbated when TLS servers and clients negotiate non-forward secure ciphers, allowing the network attacker to passively decrypt encrypted TLS payloads using the server's private key, without leaving any trace of compromise."



Okay, again, interrupt.  We've talked about this before.  Recall that I observed that expired and no longer valid web server certificates should be securely destroyed and not allowed to escape, even though they were no longer valid.  The reason for that is when protocols without forward secrecy are used, the private key, if it's later disclosed, can be used to fully decrypt any conversations that may have been archived while that key was in use for possible future use.  So these guys are observing that if all of a server's traffic is stored, and if that server later makes some mistakes during its RSA signature computation, then all of the stored communications can be retroactively decrypted.



They continue:  "In addition to demonstrating passive fault attacks, we also carried out active scans of TLS hosts, and performed a retrospective analysis of historical TLS scan data between 2015 and 2022 that included tens of thousands of non-validating signatures."  In other words, this is actually a problem.  Tens of thousands of non-validating signatures.  "In total," they said, "we computed 127 private RSA keys from these active scans."  And so by "active scan" they mean that rather than passively collecting server handshakes being initiated by random clients, they became a TLS client and actively initiated a great many of TLS connections to specific servers.  And sure enough, by doing lots of their own TLS connections, they were able to essentially force the remote server to eventually make a mistake, and they were able to then capture its security certificate.



They said:  "We compare our results to active scans from a 2015 technical report by Florian Weimer of Red Hat.  He appears to have been the first to observe that active scans could be used to detect or trigger these types of RSA signature faults at scale."  So this was known in 2015.  It's still happening today.  "He found that several open-source TLS libraries did not implement countermeasures against signature faults, and performed active TLS scans over a period of months that resulted in a few hundred invalid signatures that successfully compromised private keys, mostly from devices from several vendors."



They said:  "Our passive analysis and recent active scans show that these problems are still present in current implementations.  We were able to compute the browser-trusted private keys for a handful of user-facing websites from sporadic faults, as well as observing dozens of certificate private keys compromised by devices.  These certificates span from untrusted device default certificates to CA-signed browser-trusted wildcard certificates for entire organizations.  Although all of the open source libraries we inspected have implemented countermeasures, it appears some proprietary TLS implementations are still vulnerable to this attack."



And I'm not going to go deep into the math because it's not really relevant to understanding the impact of this attack.  But there is some historical background that I think everyone will find interesting.  They said:  "The flaw we exploit is well known in the cryptographic side-channel literature, first described in a 1997 paper about an RSA key recovery attack.  Almost all RSA implementations use the Chinese Remainder Theorem optimization for modular exponentiation in RSA signing.  But errors that occur in one of the half-exponentiations in this algorithm can result in leaking information that can be used to derive the RSA private key.  A researcher named Arjen K. Lenstra had published a technical report the year before in 1996 titled just 'Memo on RSA signature generation in the presence of faults.'



"Lenstra improved the attack to require only one signature when the message is known.  This attack works against any deterministic RSA signature scheme using the Chinese Remainder Theorem optimization.  The countermeasure to these attacks is to validate the RSA signature before sending it."  And I should mention that I saw another reference in this that Peter Gutmann indicated that that incurred about a 10% overhead in performance.  So you've got to do it, but it does cost you a bit.



"Prior to Florian Weimer's work in 2015," they said, "almost no implementations validated RSA signatures before sending."  So it was just in the technical crypto literature until Florian noted this working for Red Hat in 2015.  Before then, nobody was doing RSA signature validation.  And even after that, even though, well, they said:  "Following the report, all of the software libraries Weimer contacted implemented countermeasures" - you bet - "and Cavium issued a patch for their cryptographic accelerators, which appeared to be at fault for several of the vulnerable devices he discovered."



They said:  "In this work, we find that spontaneous faults compromising RSA keys through PKCS #1 v1.5 signatures continue to be present at a low but persistent rate in both passive and active network measurements over time, despite the attention drawn to this vulnerability in 2015."  In other words, as should hardly come as a surprise to anyone, not everyone got the memo and fixed their code.  Or since then perhaps new code was created, and the wisdom of the past was lost.



They said:  "In the present era in 2022, we find that this flaw is not just present in the types of network devices that have already been observed to suffer from cryptographic implementation flaws in previous studies, but that it also affects user-facing websites and infrastructure that receive significant amounts of network traffic."



They finish:  "These vulnerabilities are due to a hazardous combination of cryptographic libraries vulnerable in the face of computational errors, and the brittle nature of the RSA PKCS #1 v1.5 signature padding scheme as used in TLS 1.0 through 1.2.  PKCS #1 v1.5 signature padding makes key compromise trivial in the presence of Chinese Remainder Theorem faults.



"Prior to TLS 1.3, handshakes take place in plaintext, providing all the information a passive network adversary needs to validate signatures on observed connections, or derive keys when errors occur.  But on a more positive note, TLS 1.3 provides multiple countermeasures against these issues, including moving the key exchange earlier in the handshake in order to ensure that certificates and signatures are sent encrypted, and using RSA-PSS signature padding, which prevents this type of key compromise if implemented correctly.



So a long ago discovered and known, yet still present and haunting the Internet, significant weakness in the current implementation of some TLS handshake stacks exists.  The eventual migration to TLS v1.3 will finally fix this, but TLS 1.2 support will not be disappearing for a long time.  So an active adversary could insist upon negotiating under TLS 1.2 while its support continues.  Which as I said won't be going away soon because there's going to be lots of clients that don't yet do 1.3.  So a bad guy negotiates under 1.2 and hopes to get the remote server to stumble over its RSA signature computation.  If that were to happen, its private key could be disclosed.  And at that point, well, its security is completely compromised, just by listening or in some cases actively attacking.  Wow.



LEO:  It's so fantastic that I can finally use my Chinese Remainder Theorem.



STEVE:  Yes, for more than reheating your kung pao chicken.



LEO:  Yeah, getting Santa Claus to the elves in time.  That's great.  That's great.  All right.  I don't think I understand.  But I will listen again, and maybe I'll understand it on the second time through.  Steve is a genius.  That is clear.  And by listening to him we all benefit.  I hope you enjoy the show.  Hope you come by again next Tuesday when we do Security Now!.  Let me give you some of the information you might need to stay in touch.



Of course Steve has a copy of the show at his website, GRC.com, the Gibson Research Corporation.  He actually has two unique versions of the show, a 16Kb version in addition to the normal 64Kb version.  It sounds a little scratchy, but it is a lot smaller.  So if you have limited bandwidth, that's a great place for you.  I think Steve started doing this years ago because we had listeners in Australia who had extreme bandwidth limits, and they wanted to listen to the show, and this was the only way they could do it.  I'd hate to get rid of it.  Somebody probably is using it.  Do you see any downloads on it?



STEVE:  Well, as far as I know Elaine is still using it.



LEO:  Oh, that's who you made it for.



STEVE:  She had a satellite connection, and the files were uncomfortable large for her.



LEO:  Perfect.  So for anybody with bandwidth caps.  And Elaine does take the 16Kb version, listens carefully to the scratch, sounds like Thomas Edison's first dictaphone recording.  But anyway, she then makes a very good transcript of every show so you can search the transcripts or read along as you listen.  Both of those are at GRC.com, along with a 64Kb version.



When you're there, do check out SpinRite.  As you hear, it's in active development right now, current version of SpinRite, the world's best mass storage maintenance and recovery utility, is 6.0.  But 6.1's on the way.  You buy 6.0 now, you'll get 6.1 free when it's available.  You can also participate in the development.  That's GRC.com.  And then when you're there, gosh, there's so much other wonderful stuff worth checking out.  Steve has a wide - what did they say about Isaac Newton?  A mind ranging freely.  He is all over the place would be another way to describe it.



STEVE:  More contemporary way to phrase it, yes.



LEO:  Yeah.  There was a great quote about Newton, and I can't remember it exactly.  It was originally the Apple logo.  That's the only reason I know it.



STEVE:  Oh, cool.



LEO:  Anyway, yeah.  I think it was - "a mind forever voyaging."  Thank you.  I think that's a good name for you, actually, "A mind forever voyaging."  Right?



STEVE:  I'm bailing water, Leo.



LEO:  "A mind forever voyaging through strange seas of thought."  That's what William Wordsworth said about Sir Isaac Newton.  Alone.  I left off the most important part.  I think this is not for you.  You can put it on your tombstone maybe.  "A mind forever voyaging through strange seas of thought alone."  No, that's not Steve.



STEVE:  No.  Community is a good thing.



LEO:  Yeah, exactly.



STEVE:  Community's a good thing.



LEO:  We have the show as well on our website.  We have audio and video at TWiT.tv/sn.  You can download it there.  If you want to watch us do it live, like you want the very freshest copy of Security Now!, there is a live stream every Tuesday about 1:30, between 1:30 and 2:00 p.m. Pacific, 4:30 and 5:00 p.m. Eastern, 20:30 UTC.  That's at live.twit.tv.  You can chat there live with us at irc.twit.tv or in the Club TWiT Discord.



We also have on-demand versions, as I mentioned, on the website.  And there's a YouTube channel.  Of course the easiest thing to do, whether you use Steve's feed or ours doesn't really matter, is subscribe in your favorite podcast player.  And that way you get it automatically.  You don't have to think about it.  You just know you've got Security Now!.  Start your collection now.  You only have 883 other episodes to get.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#885

DATE:		AUGUST 23, 2022	

TITLE:		THE BUMBLEBEE LOADER

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-885.MP3

LENGTH:	92 MINUTES



DESCRIPTION:  This week we'll start off with a bit of fun over the most tweeted by far wacky tech news item.  We then get serious with a very worrisome flaw which very likely exists in the WAN interface of the routers that many of us probably own.  DDoS attacks have broken another record by a large margin.  Both Chrome and Apple deal with, if not emergency, then at least high-priority software updates.  We also have another major software repository tightening up its security against supply chain attacks.  Then, after sharing just a few, but powerful, bits of feedback, we're going to step through the blow-by-blow operation and actions of the newest and meanest kid on the block with the emergence of a powerful malware loader that gets its name from the DLL it first loads:  Bumblebee.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Oh, we've got some good stuff to talk about, a very worrisome flaw in the WAN interface of the routers many of us own.  That's probably not where you want that problem.  Another record DDoS attack and how Google mitigated it.  And then a blow-by-blow, step-by-step, list of how the newest and meanest kid on the block works, the Bumblebee Malware Loader.  It's all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 885, recorded Tuesday, August 23rd, 2022:  The Bumblebee Loader.



It's time for Security Now!, the show where we protect you online with this guy right here, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.



LEO:  Good to see you.



STEVE:  As we start into our 18th year.



LEO:  OMG.  Wow.  Who would have ever thought that?  Not me.



STEVE:  I have the hang of it now.  So we've got Episode 885 for August 23rd.  This one, I think a lot of our listeners are going to find this one interesting, I hope, because a security firm did a complete step-by-step, this is what we saw happening with a brand new piece of malware which is called Bumblebee, due to the name of the first DLL which contains it, which it arranges to get loaded.  And as we'll see, this is not a Bumblebee that you want your enterprise to get stung with.  Basically, this thing is taking over the previous means for getting malware onto people's, like, some poor unwitting person who clicked a phishing email in an enterprise.  This thing gets in, and it never lets go.



And it's one thing just to kind of wave our arms around and say, oh, you know, malware.  But the reason I wanted to take our listeners through this is that you really - it sort of gives you chills when you think about what this thing does.  And it is also just a textbook-perfect example of the new approach which is being called "living off the land," where rather than bringing a bunch of stuff in, which has the danger of tripping security alarms, increasingly we're seeing advanced malware using and abusing existing code on machines, thus the term "living off the land."  Anyway, that's at the end.  We're going to start off with a bit of fun over the most tweeted by far wacky news item ever.  Maybe.  Well, okay, we've had some wacky ones, so probably not ever.  But still, definitely it swamped my Twitter feed.



LEO:  Now I'm trying to think what could it because there have been so many wacky stories this week.



STEVE:  Oh, you will know.  I heard you talking about it, I think maybe on Sunday.  Anyway, we're then going to get serious with a very worrisome flaw which likely exists in the WAN-facing interface of the routers that many of us probably own.  We've got a new record having been broken for DDoS attacks, which Google managed to fend off.  And that's broken by a large margin.  We have both Chrome and Apple dealing with, if not emergency, then at least high-priority software updates to squash some zero-days that were in active exploit.  We've got another major software repository tightening up its security against software supply chain attacks.  And then after sharing just a few, but powerful, bits of feedback from our listeners, we're going to step through, as I said, the operation and actions of the newest and meanest kid on the block with the emergence of a powerful malware loader that is called Bumblebee.  And of course a great Picture of the Week, too.



LEO:  Nice.  Bumblebee bumblebee, fly away home.  Oh, no, that's ladybug ladybug.  That's a different one.  This is a Video of the Week this week, Steve.



STEVE:  It is.  I should say, though, that if you're not convinced yet that something like a Canary is what you need,  you will be by the end of this podcast



LEO:  Oh, boy.



STEVE:  Because it is exactly the sort of thing that the guys behind the Bumblebee loader would trip over.  And in fact the timeline, I was thinking about this already, that there's enough manual side to this that, if you did receive notification of early strange behavior, there would be time.



LEO:  Oh, that's good.  That's good.



STEVE:  And so anyway, it does play perfectly into today's topic.  So, yeah, we have a Video of the Week.  I had not seen this before.  I don't know, YouTube - oh, I was looking for something else, and it came up and said you might think this is interesting.  I thought, oh, it's a little freaky how correct you are, Google.  So we're playing it now.  And I played it for Lorrie, and she's like, what?



LEO:  Last year my trainer said, "I saw a UFO last night."  I said, "You did?  What did it look like?"  He described what you're seeing right now.  And I said, oh, that ain't no UFO.  What is it, Steve?



STEVE:  So it is - and describing it can't do it justice.  I mean, it would be - I guess we're sort of used to stuff happening near Earth, so maybe it wouldn't be that surprising.  But it's extremely cool.  What you're showing is a video of the light reflected, sunlight reflected off of a train of 53 Starlink satellites which had been - and this was taken a couple days ago - which had been launched from Florida on Friday.  And this has been going on for a while as Elon's company is getting the Starlink constellation of satellites up in the air.  But, I mean, the idea of - and this is just some random guy with a cell phone recording this.  I mean, and it is, like, it's a series of dots spaced somewhat evenly.  I mean, it's kind of perfect that they're not perfectly even because I guess these have been released from the transport vehicle that got then up into orbit.



LEO:  Yeah. They do this every time.  They fly in formation, and then they deploy to their locations.  But when they first come off of a - it's like a MIRV.  You know, when they first come off of the rocket they're all in formation.  Isn't that cool?



STEVE:  It's just too cool.



LEO:  It's a "train," they call it.



STEVE:  Yes.  Nothing to do with security, but just something very cool.



LEO:  Yeah.  And if you ever, you know, you can see it.  It's happening all the time.  They've done many, many launches.  So if you get a chance, it's worth seeing.



STEVE:  So since we're on the topic of nothing to do with security, I needed to respond to what was by far the most tweeted to me news item in a long time.  And our listeners who are naturally on top of their game felt about this pretty much as I do.  For more than 2.5 decades, the highly respected Microsoft engineer Raymond Chen has been blogging.



LEO:  Oh, I do know what you're talking about.  I love this.



STEVE:  I knew you would.  Last Tuesday he posted a blog entry that was just so weird that everyone picked up on it.  Raymond's posting was titled "Janet Jackson had the power to crash laptop computers."  Now, the fact that this was assigned a CVE number (CVE-2022-38392) has apparently lent it more credibility, or at least more notoriety, than I think it deserves.  And the fact that the CVE refers to Raymond's blog as its sole reference seems to be somewhat self-referential.  Raymond cites the CVE which cites Raymond.



And I'm actually wondering whether it might have been a slow blog week, and Raymond may have needed a bit of filler.  So his blog post opens with two lines:  "A colleague of mine shared a story from Windows XP product support."  Okay, well, that wasn't recent, presumably.  Anyway, he said:  "A major computer manufacturer discovered that playing the music video for Janet Jackson's 'Rhythm Nation' would crash certain models of laptops."



Okay.  So from the CVE we learn that this was "certain models of laptops" circa 2005, so 17 years ago.  The CVE's formal description says - because, you know, if it's a CVE you need a formal description.  It says:  "A certain unnamed 5400 RPM OEM hard drive, as shipped with laptop PCs in approximately 2005, allows physically proximate attackers to cause a denial of service," it says, "device malfunction and system crash via a resonance frequency attack with the audio signal from the 'Rhythm Nation' music video."  If this wasn't April, I mean, it's not April 1st; right?  So, really?  So we can see now why the tech press thought that this was just too wonderful to pass up.  On the other hand, we have a CVE that was apparently issued based upon what amounts to "a friend told me" rumor.  No mention of the make or model of the 5400 RPM OEM hard disk that should be kept away from discos.



So this begs the question of just how low the bar has been set for issuing CVEs.  This is not an attack, although, okay, there are a vocal group of people who feel that any playing of Janet Jackson's "Rhythm Nation" should qualify as a form of terrorism.  And neither is it a bug that needs to be fixed, nor malware that needs to be expunged.  There's no action that can or should be taken today.  It's from 17 years ago.  So why give this, you know, "heard it from an XP support guy" a CVE in 2022?  I have no idea.



And of course those who've been following this podcast will recall that video, which was also cited in some of the coverage of this "Rhythm Nation" hard drive DDoS attack, where somebody - and we showed this on the podcast; right? - was monitoring the dynamic throughput of an array of spinning hard disk drives while screaming at the array at the top of his lungs.  And sure enough, the throughput visibly dropped during the screaming.  And as we noted at the time, the throughput dropped because modern mechanical hard drives have crammed their tracks so closely together, I mean, actually they are now overlapping each other.  It's like this is what engineers do; right?  They engineer DRAM so tightly that neighboring rows interfere and cause bits to flip.  Well, they've also crammed hard drive tracks so closely together that they have become quite sensitive to any exogenous vibration.  And in fact, the way they're mounted in the server chassis can be critical.



Now, Raymond, of course, also referred to the famous video showing that 1940 collapse of the Tacoma Narrows Bridge.  In the same way that "Rhythm Nation" was able to rub some hard drives the wrong way back in 2005, the coincidentally timed gusts of wind through the Tacoma Narrows rubbed the bridge the wrong way, until it disintegrated.  Anyway, I felt that this podcast needed to at least acknowledge this story that everyone tweeted to me over the last week, and that most of the tech press had a lot of fun talking about, as did we here.



In a not-so-fun and much more relevant issue, during one of the recent DEF CON presentations in Las Vegas, a team of four Argentinean researchers from the cybersecurity company Faraday Security detailed their discovery of what was subsequently classified as a CVSS 9.8 zero-click remote code execution vulnerability in the interface stack which Realtek provides in the SDK for their hardware's use with the very popular open source eCos operating system.  Since they had previously responsibly disclosed their discovery, and since Realtek had patched the flaw in March, their presentation during DEF CON provided full disclosure of the all technical details needed to replicate the attack.  After all, it's been a few months.



Consequently, there is now exploit code released publicly for this critical security vulnerability affecting networking devices which use Realtek's RTL819x System on a Chip.  And those devices number, unfortunately, in the tens of millions.  Being of the turnkey consumer "plug it in and forget it" variety, there's little chance that most of these tens of millions of devices are ever going to be updated.  Many will have long since gone out of warranty.  Since this Realtek System on a Chip RTL819x is incredibly popular, we're talking about devices that many of us probably already have, since the chips are used by more than 60, six zero, vendors including ASUSTek, Belkin, Buffalo, D-Link, Edimax, TRENDnet, and Zyxel.



And again, zero-click on the WAN interface.  The vulnerability presents on the WAN interface.  And because it's a stack-based buffer overflow, it allows for no operator needed compromise of the host upon receiving a UDP packet from the public Internet.  The DEF CON presentation left nothing to the imagination.  The SIP, you know, SIP protocol, the ALG, the Application Layer Gateway function that rewrites SDP data has a stack-based buffer overflow.  This allows an attacker to remotely execute their code without authentication via a crafted SIP packet that contains malicious SDP data.  Which ends up getting written onto the stack and then executed.



We've spoken about the abuse of application layer gateways in the past.  Remember that ALGs are essentially enhancements to the baseline NAT routing functionality, which allows NAT to handle what would otherwise be NAT's interference with the details of specific NAT-unfriendly protocols.  The simplest example is the original FTP protocol where the client instructs the server which port it has opened to receive the reverse connection from the FTP server.  The router's application layer gateway monitors the outgoing data, sees the port being specified by the FTP client by looking in the packet as it's leaving the router, and then either opens that port in its WAN side interface so that the remote FTP server can connect in, or modifies the outbound port specification to a port it wishes to open.  The point being that it allows a NAT router to become transparent to the otherwise NAT-hostile FTP protocol.



Now, in this case the trouble exists in the application layer gateway logic for handling SIP, that's the session initiation protocol used in VoIP systems.  It's not clear whether disabling SIP's ALG, if that's even an option in the router, would help.



Johannes Ullrich, who's the Dean of Research at SANS, says that a remote attacker could exploit the vulnerability for the following actions.  They could crash the device, okay, that's easy; execute arbitrary code, a little more tricky; establish backdoors for persistence, that's what you want to do; reroute or intercept network traffic, you know, turning it into a proxy; basically, take over any vulnerable router.  And he warned that if an exploit for CVE-2022-27255 were turned into a worm, it could spread throughout the Internet in minutes.



Now, while Johannes is technically correct about a worm, as I've been saying recently, a massive worm attack no longer makes any sense.  They made sense back when email viruses just existed to see whether they could.  But in today's world, it's about money.  Anyone who's capable of writing a working worm would also be capable of using the router as a proxy to bounce malicious traffic; or to quietly mine cryptocurrency; or to enslave the router into the service of one of today's massive botnets, as we'll see in the next story; or to pivot into the network behind the router to see whether there might be something juicy worth attacking somewhere on the router's LAN.



For their part, the four security researchers said that devices using firmware built around Realtek eCOS SDK before March of 2022 are vulnerable.  Period.  Full stop.  Users are vulnerable even if they do not expose any admin interface functionality.  Attackers may use a single UDP packet to an arbitrary port to exploit the vulnerability.  And this vulnerability will likely affect routers the most, but some IoT devices built around Realtek's SDK may also be affected.



Realtek's own vulnerability report - I went looking for it and found it - is two pages and provides no guidance.  There's no list of manufacturers or makes and models of affected products, nothing.  There's no action that any responsible end user can take.  There's no obvious way to know whether any particular router or IoT device might be affected.  The only recourse would be to proactively verify that your router is running the latest firmware available for it from its vendor, and to hope that they care enough to update their firmware for the model of router you have.  If your system can run one of the alternative router firmware systems such as DD-WRT, I think that's what I would do.  That would be one way to move it to safety, into a platform that is being continuously kept up to date.



So many of these sorts of things have come out through the years, problems that are unlikely to ever be fixed, that it's possible to imagine what must exist, the sort of massive known vulnerability database that both nation states and sufficiently large criminal enterprises must now be maintaining.  You want to get into which organization?  What equipment do they have on their border?  Look up all of the known exploits over time that have been available against it and start working down through the list until you get in.



Seventeen years ago, back when we were launching this podcast, that scenario would have seemed like pure speculative fiction.  Today, I'd lay money down that such databases must now exist all over the world.  And it's difficult to see how this changes.  There are no plans in motion right now because anything would take a while to have any effect.  Nothing is going on that's going to like change the way the world is working now.  And the way the world is working now is deeply broken.



So those who can afford to be truly aware and concerned about security could choose to use a non-consumer router on their borders, such as something running pfSense, as I do at my locations.  But that still leaves the much larger majority of end-user consumers potentially vulnerable for decades to previous old vulnerabilities.  And every month more of these surface.  And routers are not being updated with nearly the speed or reliability that they should be.  And as I said, we don't seem to be taking any action.



Okay.  Forty-six - it's hard to say this.  Forty-six million requests per second.  After standing up to the largest-ever DDoS attack on behalf of one of its "Cloud Armor Adaptive Protection" customers, Google said it had blocked a record-breaking HTTPS-based DDoS query attack that hit, at its peak, a whopping 46 million requests per second.  That puts it 76% higher than the 26 million RPS attack which we talked about previously which had been mitigated by Cloudflare in June.  Google chose not to disclose the target of this attack, its customer, behind this protection, but said that it believes the attack was carried out with the help of the Meris botnet.



To put the scale of this attack in perspective, it's an HTTPS request rate equivalent to receiving all of the requests to the Wikipedia domain  which is one of the top traffic domains in the world  which Wikipedia would receive during one 24-hour period, take all of those requests and compress them into just 10 seconds.  It's that much traffic.  And this thing went on for quite a while.  Google's report of the attack - and Leo, thank you.  We got the chart showing the shape of the attack, peaking at 46 million RPS.  Google's report of the attack contains lots of interesting details.  Here's what they shared.



They said:  "Starting around 9:45 a.m. Pacific time on June 1st, an attack of more than 10,000 requests per second began targeting our customer's HTTP/S Load Balancer.  Eight minutes later, the attack grew to 100,000 requests per second."  They said:  "Cloud Armor Adaptive Protection detected the attack and generated an alert containing the attack signature by assessing the traffic across several dozen features and attributes.  The alert included a recommended rule to block on the malicious signature."



They said:  "Our customer's network security team deployed the Cloud Armor-recommended rule into their security policy, and it immediately started blocking the attack traffic.  In the two minutes that followed, the attack began to ramp up, growing from 100,000 requests per second to a peak of 46 million requests per second.  Since Cloud Armor was already blocking the attack traffic, the target workload continued to operate normally."  Meaning their site wasn't adversely affected.  It stayed on the air.  Everything was fine.  "Over the next few minutes, the attack started to decrease in size, ultimately ending 69 minutes later at 10:54 a.m.  Presumably, the attacker determined they were not having the desired impact while incurring significant expenses to execute the attack."  Now, I would argue that point.  I suspect that the attack cost the attackers exactly nothing other than the exposure of the IP addresses of their fleet of infected consumer routers hosting the Meris botnet.  But maybe not even that.



They said:  "In addition to its unexpectedly high volume of traffic, the attack had other noteworthy characteristics.  There were 5,256 source IPs from 132 countries contributing to the attack.  The top four countries - Brazil, India, Russia, and Indonesia - contributed approximately 31% of the total attack traffic.  The attack leveraged encrypted requests (HTTPS) which would have taken added computing resources to generate."



Again, Google appears to be deliberately missing the whole point.  If there were 5,256 observed source IPs, then that crypto burden will have been well distributed across the globe. And then we learn that HTTPS pipelining was also in use, further limiting the crypto overhead.  Google said:  "Although terminating the encryption was necessary to inspect the traffic and effectively mitigate the attack, the use of HTTP pipelining required Google to complete relatively few TLS handshakes."



Right, and thus also much less burden on the attackers.  It required them, the attackers, to similarly terminate relatively few TLS handshakes.  The attackers were establishing a single TLS connection, then attempting to flood that connection with pipelined HTTP requests.  There's no reason to believe that any IP that's flooding HTTPS requests down the pipe will ever generate a valid request.  So those IPs should have and hopefully were simply dynamically blacklisted.



They said:  "Approximately 22%" - which was 1,169 - "of the source IPs corresponded to Tor exit nodes, although the request volume coming from those nodes represented just 3% of the attack traffic."  Now, let's stop there for a second.  That's interesting.  22%, so just shy of one-fifth, of the total source IPs were coming from Tor, yet its traffic was just 3%, which is what we'd expect; right?  I mean, Tor incurs a huge latency burden and bandwidth burden on its user in return for giving you some hope for privacy on the Internet.  Anyway, sort of an interesting data point.



They said:  "While we believe Tor participation in the attack was incidental due to the nature of the vulnerable services, even at 3% of the peak" - which would have been greater than 1.3 million requests per second - "our analysis shows that Tor exit nodes can send a significant amount of unwelcome traffic to web applications and services."



And finally:  "The geographic distribution and types of unsecured services leveraged to generate the attack matches the Meris family of attacks."  And I'm sure they couldn't resist poking at a few of those IPs and confirming that, yup, in fact that was Meris.  Anyway, they said:  "Known for its massive attacks that have broken DDoS records, the Meris method abuses unsecured proxies to obfuscate the true origin of the attacks."



So yes, if it was Meris, then they were bouncing their traffic through proxies like routers that had been compromised.  Remember we've talked about how if a router exposes its plug-and-play port, or the plug-and-play service to the WAN interface, it's possible for someone to just set up a proxy and say, you know, send any traffic incoming to that IP and use it to reflect traffic.  Anyway, so we know that attackers were bouncing their botnet's traffic through intermediate proxies in order to protect the IPs of their actual bot agents, which they did not want revealed.



So I couldn't help - another just random point.  I couldn't help but note that in their redacted report because Google, in their posting of this, they had screen shots which were redacted to hide the identity of their customer.  They used text blurring to obscure the identity.



LEO:  Uh-oh.



STEVE:  Whoops.



LEO:  Uh-oh.



STEVE:  And we all know, remember, we covered it here.  In some beautiful work that we covered a while back, we learned about that blurring text is not secure.  We learned it doesn't work.  If someone is interested in learning what original text lies behind the blurred instance, they can identify the details of the typography from all of the examples of non-blurred text, then iteratively guess the text that's behind the blur, employ the same blurring of their guess text, and then compare the result of the two blurrings, one they control and one they do not.



LEO:  We'll leave this as an exercise for the listener.  



STEVE:  Yes.  Google's report then switches into marketing mode, bragging about their technology, which anyone would have to agree works.  So we're now living in a world where those whose Internet web services must remain online in the face of attacks will need to bear the added cost of the privilege of doing so by putting themselves behind Google or Cloudflare or one of these big pipe DDoS protector services because otherwise you just, I mean, aiming that much traffic at anyone else, I mean, it's just like, what's the point?



LEO:  Right.



STEVE:  It seems like just stomping on a gnat using a planet.  Just would be ridiculous.



And Leo, I'm going to take a sip of water.  Let's tell our listeners...



LEO:  Okay, let's do it.



STEVE:  ...why they're glad they're here.



LEO:  Yeah, we use Amazon for DDoS.  They also - anybody who has a lot of bandwidth can do that, DDoS protection.  Cloudflare does a great job.  And I think this is as much a war of press releases as anything else because I think Cloudflare just recently had an almost as big DDoS mitigation.



STEVE:  Yeah, in June.  



LEO:  In June, right, yeah.  So it's like, well, our DDoS was bigger than yours.



STEVE:  The entire entry point for this Bumblebee loader is spear phishing.



LEO:  Of course.  Of course.



STEVE:  And as we'll see, I wrote all this before I knew the advertising.



LEO:  You didn't know.  Yeah, yeah.



STEVE:  It is ultimately, as we'll see, that hapless person in the enterprise who clicks on an email and takes some actions which will clearly specify that begins this entire thing.  And it just must be keeping IT people up at night.



LEO:  Oh, and owners.  It's terrifying.



STEVE:  Yeah, yeah.  Okay.  So last Tuesday Google updated our Chrome browser for desktops to squash an actively exploited high-severity zero-day flaw in the wild.



It's tracked as CVE-2022-2856.  That's only four digits.  Interesting.  As we know, CVEs are allocated now in blocks, and so various people allocate them as they choose.  So the fact that it's four digits and a low number doesn't really tell us anything.  Anyway, the issue is a case of what they termed  insufficient validation of untrusted input, which is like Microsoft saying, yeah, that was a security bypass.  Okay.  Right.  Security researchers Ashley Shen and Christian Resell, both on Google's TAG team - remember, you know, their Threat Analysis Group - are credited with reporting the flaw last month on the 19th of July.



As usual, there's no upside for Google to sharing anything more with us beyond "Please be sure that your version of Chrome now ends in .102."  They did add that "Google is aware that an exploit for CVE-2022-2856 exists in the wild."  In addition to stomping on that 5th of the year actively exploited flaw, that update to blah blah blah .102 addressed 10 other security flaws, most of which relate to the most common use-after-free bugs that we just keep encountering in this code, which appear in various Chrome components.  They also fixed a heap buffer overflow in the downloads portion of Chrome.



So, this is number five this year.  Previously we had a - the first of the year was a use-after-free in animation.  We had two type confusion bugs in V8 and a heap buffer overflow in WebRTC.  So, and now with number five being a rather vague "insufficient validation of untrusted input," okay.  Anyway, so if you're using one of the non-Chrome Chromium siblings - Edge, Brave, Opera, or Vivaldi - just be sure to keep yourself updated there, too, because they would all be susceptible until they're updated with the latest update to Chromium, you know, their common core.



And not to be left behind, last Wednesday Apple released high-priority security updates for iOS, iPadOS, and macOS platforms.  This was to remediate a pair, two zero-day vulnerabilities which were being exploited by threat actors to compromise Apple's devices.  There was the CVE ending in 32893, which was an out-of-bounds bug in WebKit which could lead to the execution of arbitrary code by processing a specially crafted web content; and 32894, an out-of-bounds bug in the OS kernel that could be abused by a malicious application to execute arbitrary code with the highest privileges.



So again, these are not theoretical.  These were found being used to perpetrate malicious ends, so that was pushed quickly in all of our devices.  I got little notices everywhere.  So Apple clearly felt that this was worth getting out into the world.  They said that they had addressed the issues with improved bounds checking, so that's good because those were out-of-bounds bugs, so you want to do a little more bounds checking to keep them from going out of bounds.  And they said also that they were aware that the vulnerabilities "may have been actively exploited."  Uh-huh.  And please update immediately before you do anything else.



So as usual, they didn't disclose anything additional either regarding these attacks or the identities of the bad guys who may have been using them; although, as usual, it's almost certain that they were involved in targeted intrusions.  Again, you're not going to spray this around because you want to keep it quiet and get as much use out of it as you can.  And since we're counting zero-days so far this year, this latest update brings Apple's total of actively exploited zero-days to six for the year.  As with Chrome, we had four before these latest pair, and we've covered them all in the podcast in the past.  So anyway, two more added to the list, and iOS, iPadOS, and macOS Monterey all need to be updated.



As we know, RubyGems is the official package manager for the Ruby programming language.  And in a welcome response to the increasing threat and prevalence of supply chain attacks, the RubyGems repository has become the latest platform, following NPM and PyPI, to require multifactor authentication for its more popular package maintainers.  Specifically, owners of "gems," as they are referred to, having more than 180 million total downloads are now, as of last Monday, August 15th, required to enable multifactor authentication.



The RubyGems management said:  "Users in this category who do not have MFA enabled on the UI and API or UI and gem sign-in level will not be able to edit their profile on the web, perform privileged actions, for example, push and yank gems, or add and remove gem owners, or sign in on the command line until they configure MFA."  In other words, they'll log in, try to do anything, and they're going to get a nope, sorry, you're too popular.  You've got more than 180 million downloads.  You should be proud of that.  But just, you know, come on.  Let's do multifactor authentication here.



And as gem downloads approach that magic mandatory 180 million count, as soon as downloads pass 165 million cumulative, their maintainers will receive reminders to turn on multifactor authentication before the download count hits the magic 180 million, at which point they no longer have a choice.



So this is further welcome, of course, in the package ecosystem to improve the past's casual approach to software supply chain security, which no one took that seriously until we started discovering lots of malicious packages in our repositories.  So as we know, adversaries are increasingly setting their sights on open source code repositories, with attacks on NPM and PyPI having snowballed by a combined 289% since 2018.  Researchers from Checkmarx, Kaspersky, and Snyk have all uncovered a large number of malicious packages in PyPI that could be abused to conduct DDoS attacks and harvest browser passwords as well as Discord and Roblox credential and payment information.



So now RubyGems joins the ranks of NPM and PyPI which are all tightening their security.  And, you know, yay.  I mean, it's nice to see this happening.  It's clearly something that is easily done.  As I said before, we still don't have an answer to the IoT problem, to this problem that we've got very sophisticated devices packaged in $5 light switches and plugs, with no one standing behind them, where vulnerabilities are being found by researchers, and there's just no infrastructure in place to fix them.  And it's not like this is like slowing down.  The rate at which this stuff's being created is accelerating, and there's nothing on the horizon that suggests a way to fix this.  And even if there were, or once there is, it would still take decades for it to work its way through.  So, bad.



Okay.  We have some very neat closing-the-loop bits.  Thomas Tomchak tweeted.  He said:  "Hey, Steve.  When you register a domain, you do have the option to register a technical contact as well as the owner."  He said:  "When I have registered domains in the past for friends, I always make sure they are listed as the domain owner, and myself only as the technical contact."  He says:  "Yes, it's still under my registrar account, but at least that shows proof of ownership, and they could probably transfer it to a new account at the same or different registrar should I get hit by a bus unexpectedly."



So I'm so glad that Thomas thought to remind me and us of that.  And I want to acknowledge that several other of our listeners sent notes to the same effect, which I saw, and thank them.  Of course that's the case.  And it had completely slipped my mind.  Domain registration records provide for completely separate Owner, Administrative, and Technical contacts.  I'm so used to always pointing all three of those at myself that I completely forgot the power of the flexibility that they could provide.  Now, this of course begs the question, what would a domain registrar do if the assigned owner of a domain  which is, after all, just a name and email address  wished to take control of the domain in the event that the Admin or Technical contact was unresponsive?  Would that provide the degree of safety that we're looking for?



I don't know.  In an attempt to answer that question definitively, I found an ICANN FAQ, you know, an FAQ.  Question 12 that they asked themselves read:  "I can't access my domain name or my domain name management account because the domain name was registered by someone else, such as my web developer or administrative contact.  What now?"  And their answer, ICANN's answer is:  "You may not be able to access the domain name if you are not the administrative contact/registrant of record of the domain name.  You should contact the individual or entity who registered the domain name to obtain access credentials/details or update the domain name's administrative contact/registrant of record."



Then they said, in the second paragraph:  "You should contact your registrar right away if your domain name manager/administrative contact is unreachable, has gone out of business, et cetera, to update your information.  Once you're able to become the administrative contact/registrant of record, this will ensure that you have full control of managing your domain name and allow you to find someone else to help you manage your domain name, if you so choose.  It's a good idea to keep a record of your domain name management credentials at all times, even if you choose to outsource some administrative/management duties to a third party."



Okay.  So the formal names of the three things you can register, there's the Owner, there's Administrative, and there's Technical contact.  And in ICANN's response they keep referring to Administrative contact.  Well, okay, that's one of the three; right?  Administrative.  But there's also Owner.  So I dug around some more, and I couldn't find anything formal or official.  But anyway, I just wanted to put on the record that a bunch of our listeners said, "Hey, Gibson, did you forget about that?"  It's like, yeah, I did.  So thank you.



LEO:  I don't think it solves anything.  It's the same issue.  Because in effect ICANN's saying, well, if you can prove to your registrar that you're you, which is what it would require to change the administrative owner, well, then I guess we'll give you, I mean, it's putting it back on the registrar.  I don't think it solves anything.



STEVE:  So what I was suggesting last week was that we needed, like that the system needed to be upgraded to provide something like a next-in-kin sort of effect so that there would be some means for dealing with a sudden lack of management.  So our listeners said, hey, you know, you do have three things in a domain name registration record.  And so it would be possible for you to ask the person who's managing your domain name to point one or more of those at you, rather than all at him.  So it's something.  But again, you still don't have, like...



LEO:  There's no backdoor.



STEVE:  There won't be an access to the registration record itself.



LEO:  Plus you need the cooperation of this person who is presumptively not cooperating.



STEVE:  Right.



LEO:  That's the problem.  If they're cooperating - this is why you should register it yourself, I think.  But, you know, that's what I would say.  Do it yourself.  Don't trust them.



STEVE:  Okay.  So a listener, Dagan, he said - well, I should preface this with I am truly flattered, honored, and humbled that this podcast has had as much impact on people's lives through the years as it has.  I suppose that my true love for technology - and Leo, you share that love with me, you always have - and computing can be a bit infectious.  So in that sense I'm gratified that we've had the opportunity to infect so many of our listeners with this bug.



LEO:  Infect, okay, yeah, okay.



STEVE:  Yeah.



LEO:  All right.



STEVE:  And it's a bug that I've been in its grip throughout my entire life, so this tweet from Dagan really hit home.  So I asked its author if I could share it, and I received his permission.



So he wrote just to me by DM.  He said:  "Steve, I wanted to privately drop you a brief personal note of thanks.  I've been listening to Security Now! religiously since 2011, and I truly believe your weekly show has had a significant impact on my career and, as such, my life.  When I started listening, I was a happy nerd that loved the idea of security.  Nine years in, I discovered and was credited for identifying two CVEs in a Red Hat product.  Last year I was credited for two more CVEs in a SUSE product, and this year yet another vulnerability in a second SUSE product.



"And finally, just last weekend, I was fortunate enough to have the opportunity to present at DEF CON, where I demonstrated chaining three of five vulnerabilities together to fully compromise a Kubernetes cluster from the outside.  I will also be speaking at KubeCon this year, where I will talk about securing clusters from risks introduced by third-party applications."  He said:  "I'd happily buy you a nice bottle of wine to express my most sincere gratitude, but I'll settle for a SpinRite license instead."



LEO:  Aw, that's nice.



STEVE:  He said:  "Your show has changed my life, Steve.  Thanks."  Now, just this morning, you, Leo...



LEO:  Ah, I was wondering if you got that.  Okay.



STEVE:  Yes.  Forwarded a very heartwarming story from someone who said his career was catalyzed by this podcast.  So as we start into year 18, I want to share what he wrote as an example of what is possible if anyone else out there might be in need of a bit of a nudge.  So this person wrote:  "Leo, I must say I'm quite honored to have received a reply directly from you.  I would like to share a brief story you and Steve might like to hear.



"In 2015 I recognized Steve's name on a list of podcasts.  I knew it from back in the mid-to-late '90s when I used ShieldsUP! frequently.  I left IT after the dot com bubble around 2002-ish. I followed a path in science, but ultimately I couldn't find a good job in that field.  I was in poverty, on Medicaid, and in significant debt.  It wasn't just me  I met my wife and our son was just born during this time.  It was rough.  I listened to Security Now! and the ad for ITPro.TV.  I signed up, studied, and passed the CCNA.



"In a matter of days I had a good-paying job as a network engineer.  I have since moved on into cybersecurity, getting a CISSP.  I now work at a job I love, and my family is able to live debt-free with good healthcare and everything else that goes with this terrific career."



LEO:  Yay.



STEVE:  "I have you and Steve to thank for this."  Well, and of course his own initiative, too.  He says:  "Your show made this field approachable and fun.  Words cannot begin to express my gratitude and appreciation.  You guys change lives.  Please keep up the great work.  Michael."



LEO:  Thank you.  Thank you.  Yeah.  Yeah.



STEVE:  So thank you, Michael, for sharing your story.  You know, the field of cybersecurity truly is a growth industry.  There is a crying need for trained cybersecurity professionals.  And, boy, is it interesting and fun.



LEO:  Yeah, yeah.



STEVE:  So, you know, anybody else who's looking for something to do, you can follow in the footsteps of these guys.



LEO:  Okay.  I'm not crying.  You're crying.  Okay.



STEVE:  I know.  I got choked up the first time I read it.



LEO:  It was a beautiful - but, you know, I have to say, every time, and I know you know this, too.  But every time we go out, we meet people, I hear these stories again and again.  And so thank you for the job you do, Steve.  And I always tell them, it's not us, it's you.  But thank you for letting us be part of your life, and I'm glad we could help.



STEVE:  Wouldn't be happening, Leo, if it weren't for you.



LEO:  Okay.  What do you want to do now?



STEVE:  Let's tell our listeners about our last spot and see whether it also...



LEO:  Another inspiration.  All right.  You know, I tell you what, we don't do this for the money particularly.  We do it because it's a privilege and an honor, and it's something we really enjoy.  I'm don't think - I'm not saying anything you don't feel.  But it is always nice.  It is always nice to hear from people and know that we've made somewhat of a difference.  So thank you very much for those kind words.  I'm going to promote this new book from...



STEVE:  Dennis Taylor.



LEO:  Dennis Taylor, that's it.



STEVE:  We should mention that, because we were talking about it before we began recording.



LEO:  Be good for the book club, yeah.  Oh, you didn't mention it in the show?



STEVE:  No.



LEO:  Oh, it was before.  Yeah.



STEVE:  Yeah.  This is the guy who did the Bobiverse trilogy.



LEO:  Mr. Bobiverse, yes.



STEVE:  Of four books.  And I was planning to be reading something else, but one of our listeners said, uh, you know how much you like the Bobiverse.  He wrote something else called "The Singularity Trap."  And I was talking to Leo before the show.  I have not been able to put it down.  I've been reading it during the ad reads.



LEO:  No.  Oh, you're going to finish it before the show's over.



STEVE:  Well, I'm going to finish it tonight, unfortunately.



LEO:  Nice.  Oh, nice.



STEVE:  It is so fun.



LEO:  Fantastic.



STEVE:  So it's Kindle Unlimited, so if you are a reader it won't cost you anything, in the same way that the Bobiverse books didn't.



LEO:  Oh.  Oh, good.  Oh, nice.  Okay, good.



STEVE:  And you learned that it's got the same guy reading it  as who read the Bobiverse for Audible.



LEO:  Ray Porter's fantastic, yeah.



STEVE:  And everyone was raving about the Audible side of the Bobiverse.



LEO:  Yes.



STEVE:  So anyway, it's called "The Singularity Trap."  It is the same style, the same wit.  It's like really fun.  And it's a book where you have no idea what is going to happen next.  It's a whole new concept that we've - it's not a rehash of let's go attack the aliens thing.  It's a new idea, and really cool.  So this one I can, I mean, I'm at, sadly, at 93% or something of the book, and I'm going to be sad to have it end.  And then I went looking for more stuff, but he's not written anything else.  He's going to do some more Bender books on the Bobiverse side I guess next.



So anyway, okay.  The Bumblebee Loader.  It's recently become a big deal on the malware front.  Symantec in June wrote:  "Bumblebee, a recently developed malware loader, has quickly become a key component in a wide range of cybercrime attacks and appears to have replaced a number of older loaders, which suggests that it's the work of established actors and that the transition to Bumblebee was pre-planned.  By analysis of three other tools used in recent attacks involving Bumblebee, Symantec's Threat Hunter team has linked this tool to a number of ransomware operations including Conti, Quantum, and MountLocker.  The tactics, techniques, and procedures used in these older attacks support the hypothesis that Bumblebee may have been introduced as a replacement loader for Trickbot and BazarLoader, since there is some overlap between recent activity involving Bumblebee and older attacks linked to these loaders."



And even earlier than that, on the 7th of June, Cyble, the security firm we were just talking about last week, wrote:  "In March 2022, a new malware named Bumblebee was discovered and reportedly distributed via spam campaigns.  Researchers identified that Bumblebee is a replacement for BazarLoader malware, which has delivered Conti ransomware in the past.  Bumblebee acts as a downloader and delivers known attack frameworks and open source tools such as Cobalt Strike, Shellcode, Sliver, Meterpreter, et cetera.  It also downloads other types of malware such as ransomware, trojans, and more."



And last Wednesday, the global security firm Cybereason, based in Boston, Massachusetts with offices in London, Tel Aviv, Tokyo, France, Germany, South Africa and Singapore, published in their "Malicious Life" blog a detailed technical description of the operation of this extremely dangerous new entry onto the malware scene.  They titled their report:  "Bumblebee Loader - The High Road to Enterprise Domain Control."



And this podcast would be remiss if we didn't take some time to bring our listening audience up to speed about this emergent threat.  So Cybereason's report explains that they analyzed a case that involved a Bumblebee Loader infection which allowed them to describe in detail the attack chain from the initial Bumblebee infection to the compromise of the entire enterprise network.



Okay.  So let's begin with a couple of bullet points to set the stage.  "The majority of the infections with Bumblebee," they said, "we have observed started by end users executing LNK (.lnk) files" - still hasn't gone away - "which use a system binary to load the malware.  Distribution of the malware is done by phishing emails with an attachment or a link to the malicious archive containing Bumblebee."  And I'll be expanding on all of this here in a minute.



They said:  "Bumblebee operators conduct intensive reconnaissance activities and redirect the output of executed commands to files for exfiltration.  The attackers compromised Active Directory and leveraged confidential data such as users' logins and passwords for lateral movement.  The time it took between initial access and Active Directory compromise was less than two days."  And I'll be sharing a timeline breakdown in a minute.



"Cybereason GSOC" - that's their Global Security Operations Center - "has observed threat actors transitioning from BazarLoader, Trickbot, and IcedID to Bumblebee, which seems to be in active development and generally the loader of choice for many threat actors.  Attacks involving Bumblebee must be treated as critical.  Based on GSOC findings, the next step for the threat actors is ransomware deployment, and this loader is known for ransomware delivery."



Okay.  So let's take this step by step.  A spear phishing email is received containing an archive or a URL link to an external source to download the archive.  As we know, the malware is encapsulated in an archive to prevent the archive's contents from being tagged with the Mark of the Web (MOTW) which would complicate its execution.  The user extracts the archive and mounts the resulting ISO (.iso) image.  Newer releases of Windows will happily mount .ISO images, thus exposing the ISO's file system files.  The content of the mounted ISO image is a .LNK file executing the Bumblebee payload upon user interaction.



So the operators behind an instance of Bumblebee host malicious websites that implement a drive-by download.  To infect the system, an end user has to first manually decompress the archive containing the ISO file - on the other hand, if it's a ZIP, Windows will do that for you now, too - mount the file, and then execute the Windows shortcut LNK.  This is all done as part of a phishing email where the user fully believes that they are doing the right thing, that installing this or that is needed, or updating something that's needed before they can proceed.  So the user is without question unwittingly complicit in the success of this entire penetration and intrusion.  All of the other mechanics is about avoiding everything the user's enterprise security people have done to keep bad stuff out, despite what dumb stuff their users may do.



So the LNK file has an embedded command to load and execute the Bumblebee Dynamic Link Library, the Bumblebee DLL, using the already and always present odbcconf.exe in what has become the increasingly popular "living off the land" approach of using what's already available in the system.  And these days plenty is, in modern systems.  So in this context, odbcconf.exe is called a LOLBin.  A response, which has the extension .rsp, a response file is also used where some Bumblebee-specific name .rsp contains the reference to the Bumblebee DLL.  So specifically, the LNK file's target property contains the string odbcconf.exe space -f space, and then this Bumblebee-specific name ending in .rsp, the response file.  And the .rsp file contains a reference to, again, the Bumblebee-specific name .dll which is the Bumblebee payload.



Now, if anyone's curious, you can see this for yourself.  In any version of Windows, open a command prompt and type "odbcconf /?" and you'll receive a pop-up from ODBCCONF showing a list of its command-line options.  And sure enough, among them is /F which takes a response file as its argument.  Basically it's a command stream which is fed into ODBCCONF.  So in this case, this loads and runs the Bumblebee DLL, at which point all is lost because a hostile executable has made it into the user's system and has been started.



The Bumblebee DLL injects code into multiple running processes in order to establish a strong foothold on infected endpoints, and the newly launched odbcconf.exe process creates Windows Management Instrumentation calls to spawn two new processes from the wmiprvse.exe, which is the Windows Management Instrumentation Provider Service.  Once again, both of these newly spawned processes are existing Windows executables where malicious code is dynamically injected into their process space once they've been started.



So the first of the two is wabmig.exe.  That's the Microsoft Contacts import tool.  It's injected with Meterpreter agent code.  Meterpreter is a Metasploit attack payload that provides an interactive shell from which an attacker can explore the target machine and execute code.  It's deployed using in-memory DLL injection.  As a result, Meterpreter resides entirely in memory and writes nothing to disk.  The second existing Windows EXE that is spawned and then injected into is wab.exe.  That's  Microsoft's address book app.  After being launched, it receives an injection of the Cobalt Strike beacon, which we've covered.  We did a podcast on it a while ago.



Bumblebee performs privilege escalation by loading an exploit for CVE-2020-1472 - Zerologon, which we talked about at the time - into rundll32.exe.  Bumblebee uses a User Account Control (UAC) bypass technique to deploy post-exploitation tools with elevated privileges on infected machines.  Specifically, it uses an existing trusted binary - again, part of Windows - fodhelper.exe.  This prevents Windows from showing a UAC window when it's launched.  Fodhelper is the executable used by Windows to manage features in Windows settings.  Again, living off the land.  And it uses this to bypass any appearance of what's going on.



This fodhelper is exploited to run cmd.exe /c space and then rundll32.exe.  Then we give it the DLL, which is a path to the Cobalt Strike dll, comma, and then MainProc where the Cobalt Strike DLL is the Cobalt Strike framework beacon, and MainProc is the exported function which Cobalt Strike exports in its DLL in order to run.  As we know, Cobalt Strike is an adversary simulation framework used to assist in red team attack operations.  Unfortunately, bad guys use it to conduct actual post-intrusion malicious activities.  It's a powerful modular framework with an extensive set of features that are used to, you know, you can do command execution, process injection, credential theft, and more.



And speaking of credential theft, after obtaining its foothold and elevating itself to system privilege without any further user interaction or UAC permission, Bumblebee performs credential theft through two methods.  The first method is to trigger a memory dump of Windows' LSASS process.  LSASS is Windows Local Security Authority Subsystem Service.  Within LSASS's memory footprint are the keys to the kingdom, including both domain and local usernames and passwords.  They're all sitting in the memory space of the LSASS process.  So Bumblebee dumps the memory of this process using procdump64.exe, also living off the land, to obtain access to this sensitive information.



The second method of credential theft used by Bumblebee is registry hive extraction using good old reg.exe.  The HKLM SAM, which is the Security Account Manager database, is where Windows stores information about user accounts.  That's dumped.  HKLM Security, which is the Local Security Authority (LSA), stores user logins and their LSA secrets.  And HKLM System contains keys that could be used to decrypt and encrypt the LSA secret and SAM database.



Bumblebee issues three commands of the form reg.exe space save space hklm\sam and then a path to the program where that registry hive should be saved.  In this case they give the example  c:\ProgramData\sam.save.  Then the same command for \system and for \security, saving their dumps into system.save and security.save.  So that creates a trio of files containing the dump of those three system-critical registry hives.  Then the LSASS dump and those three registry hives are all compressed using 7z and exfiltrated back to command central.



At that point the human operators behind Bumblebee process the retrieved credentials offline, attempting to extract cleartext passwords.  The observed time between credential theft and the next activity is about three hours.  So this stuff all goes back to wherever.  Somebody ruminates on it, figures out what the username and passwords are, but basically reverse engineers cleartext, and then comes back three hours later.



After the attackers have gained a foothold within the organization's network, they gather information using tools such as nltest, ping, netview, tasklist, and ADFind to collect a wide range of information related to the organization.  They collect information such as the domain names, users, hosts, and domain controllers.  We talked about ADFind in episodes 789 and 790 back in October of 2020.  It is a powerful Active Directory exploration tool meant to aid in the administration of Active Directory systems by regular Active Directory admins.  Unfortunately, it's been turned against those administrators.



Bumblebee uses Cobalt Strike agent for lateral movement.  Their analysis, that is, the Cybereason's analysis, observed multiple connections from the Cobalt Strike process to internal addresses on RDP, Remote Desktop Protocol over TCP port 3389.  And of course now they've got credentials for all that stuff, so they're able to log in.  After lateral movement, the attacker persists on the organization's network using the commercial remote management software AnyDesk.



After the attacker has obtained a highly privileged username and password, they access the Volume Shadow Service Shadow Copy, which is, again, built in, Windows' built-in facility to create backup copies and snapshots of computer files or volumes while they are in use.  So Bumblebee accesses the remote Active Directory machines using the Windows Management Instrumentation command-line WMIC and creates a shadow copy using the vssadmin command.  In addition, the attacker steals the ntds.dit file from the domain controller.  Ntds.dit is a database that stores Active Directory data, including information about user objects, groups, and group membership.  And the file also stores the password hashes for all users in the domain.



In order to obtain maximum privileges on the Active Directory domain, the threat actor executes the following four steps:  Creates a shadow copy of the machine file's volume.  Lists all available shadow copies, storing the result in a file.  Copies the Active Directory database, that ntds.dit file, as well as registry hives containing credentials and sensitive data from the shadow copy.  Compresses the output directory for exfiltration.  And finally, the threat actor uses a domain administrator account obtained previously to move laterally across multiple systems.  After initial connection, they create a local user and exfiltrate data using the open source Rclone software.



Wikipedia describes Rclone:  "Rclone is an open source, multithreaded, command line computer program to manage or migrate content on cloud and other high-latency storage.  Its capabilities include sync, transfer, crypt, cache, union, compress, and mount.  The rclone website lists supported backends including S3, and Google Drive."  In the instance observed and monitored by Cybereason, the rclone.exe process transferred approximately 50GB of data to an endpoint with an IP address over TCP port 22 (SSH), located somewhere in the U.S.



So what does all this tell us?  The first and most obvious thing we've learned is that you do not want to have your enterprise stung by the Bumblebee loader.  It'll definitely ruin your whole day.  But speaking of "day," Cybereason compiled the entire event into a timeline.  Taking everything we've just stepped through, here's how it stretches out in time.  So we have T0 at initial access, when the unwitting user clicks the link, thinks they're doing the right thing, opens the archive, mounts the ISO, runs the program inside.  Actually it's the LNK file that'll be contained in the ISO.  Reconnaissance using nltest, net, and whoami commands, at T0 plus 30 minutes.  When we get to four hours, we're at command and control, loading the Meterpreter agent.  Also at four hours is privilege escalation using the Zerologon exploit.



Two hours later at T0 plus six we have command and control with Cobalt Strike beacon execution.  Also at the same time credential theft through the registry hive.  Reconnaissance 30 minutes later at six hours plus 30 using ADFind, the Active Directory find, ping, and curl.  Okay.  Now we have a big jump.  At T0 plus 19 hours is the credential theft and privilege escalation.  That's when we get the LSASS memory dump with procdump64.exe.  That was at T0 plus 19 hours.  At 22 hours, so three hours later, credential theft.  That's where ntds.dit exfiltration occurs with Active Directory full privilege.



Two hours later, now we're at one day later, 24 hours later than the initial T0, is lateral movement using Cobalt Strike SOCKS tunnel over RDP.  And data exfiltration using Rclone presumably finished at T0 plus three days.  So a full 72 hours from start to finish.  So this thing, as I said, isn't over before it starts.  It does take some time.  There is a human in the loop at that point where the registry hives and the LSASS dump have been compressed with 7z and sent back to headquarters.  Three hours goes by before they've got that figured out, and then they come back to do some real damage using all of the credentials that they've been able to obtain.



So one lesson we learn from all of this is why local privilege escalation vulnerabilities form such a crucial part of this and so many attack chains.  Remote Code Execution sounds like the worst possible nightmare.  And indeed it's not good.  But none of that was needed here.  If malware were truly constrained within a user's low-privilege account, much less damage could be done. But the old saying, "If wishes were horses, beggars would ride," reminds us that wishing won't make it so. With everything we've seen of the continuing and apparently worsening trouble Microsoft is having securing Windows, which they refuse to just leave as is and fix  because it's obvious they can't do both  there is virtually zero chance that once a single piece of malicious software gets loose inside a user's machine that the rest of the organization will not fall.



And this brings us back to that first fatal click, that innocent action taken by a user on the inside, that initiated the collapse of even the most carefully constructed enterprise's security.  Those in charge of their organization's security must be living in a state of constant terror over what any one of their employees might do next.



LEO:  Rightly so.



STEVE:  Yes.



LEO:  Yes.



STEVE:  And what we see here with Bumblebee loader is exactly what transpires.



LEO:  It's interesting.  It's educational once in a while to go through the step-by-step of how these things work.  It's fascinating.



STEVE:  I think it's important because it helps make it real for people.



LEO:  Yeah, yeah, yeah.



STEVE:  It's like, you know, oh, that happened to somebody else.  It's like, here's exactly how this could happen to you.



LEO:  Yeah.  Well, there you go. If you haven't seen the show notes, this would be a good time to get them and look at that step by step and really understand what the threat is.  The show notes are GRC.com, that's Steve's website.  You also can get of course a copy of the audio.  He's got 16Kb audio, 64Kb audio, and very nice transcripts, as well.  Plus the show notes.  I think the transcripts and show notes are going to give you all the additional detail you need.



While you're at GRC, pick up a copy of SpinRite, Steve's bread and butter, the world's best mass storage maintenance and recovery utility.  Version 6 is current, but if you buy 6 today, you get 6.1 when it comes out.  And Steve's working hard on that all the time, I know, except when he's reading this book.  And then, you know, all bets are off.  He's almost done.  He's a fast reader, don't worry.



STEVE:  I am, actually.



LEO:  After the fact you can also get on-demand versions at our site, TWiT.tv/sn.  We've got audio and video for you.  I don't know why you'd want video, but you could get it.  Well, I know why, so you could see the video of our Image of the Week, that's why.



STEVE:  Yeah, the satellites moving.  It's spooky.



LEO:  Yeah, it's a video.



STEVE:  It's spooky.



LEO:  You also, let's see, what else.  Oh, there's a YouTube channel you can watch.  In fact, a good way to share clips is on the YouTube channel because they have the feature to do the little clip thing.  And of course you can also subscribe in your favorite podcast player and get it automatically the minute it's available.  Just search for Security Now!.  It's been around, as you know, 18 years now.  Everybody should have a copy of it somewhere.  Keep it up.



That's actually how Michael first wrote to us.  He wrote me a note saying how come I only see the last 10 episodes in the feed?  And we just do that to keep the feed from getting big.  If we had 885 episodes in there it'd be hundreds and hundreds of megabytes.  And we don't want you to have to download that on a regular basis.  That's kind of how feeds work, you know, you download it each time.  So we keep it to 10 titles.  But all the titles are available at Steve's site and our site.



You want to watch us do it live, get the first edition?  The first edition, while the ink is still wet?  It's easy to do.  Just go to live.twit.tv of a Tuesday afternoon, 'round about 1:30 Pacific, 4:30 Eastern, 20:30 UTC, right after MacBreak Weekly.  People who are watching live often chat with us live at irc.twit.tv or in the Club TWiT Discord.  After the fact you can chat, and people often do, in our community forums.



Steve's got his own forums, GRC.com.  We also have ours at twit.community for all of the shows.  And we also have a Mastodon instance, which is a federated Twitter without all the security flaws.  That is at twit.social.  Both are free to join.  But I do approve everybody to keep the spammers out, so it might take a day or two for me to let you in.  But please do knock:  twit.community or twit.social.  Steve, have a great week.  Go finish your book.



STEVE:  Thank you, my friend.  Unfortunately it will not live out the night because I'm not going to be able to put it down, and I'm close enough to the end, although even at this point I'm on pins and needles.  I have no idea what's going to happen.



LEO:  You're almost done, and you still don't know what's going to happen.  That's a good sign.



STEVE:  It's really good.



LEO:  Exciting, yeah.  Well, I just bought it and downloaded it.  So I'll be listening soon.



STEVE:  Cool.



LEO:  Actually, as soon as I finish Stacey's Book Club, "Klara and the Sun," which is also excellent.  It's about an artificial intelligence, an artificial friend that you bring into your house to be your companion.  And it's told from her point of view, so it's really fascinating.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#886

DATE:		August 30, 2022

TITLE:		Wacky Data Exfiltration

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-886.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we begin by discussing the implications of last week's LastPass breach disclosure.  We look at some recent saber-rattling by the U.S.'s FTC and FCC over the disclosure of presumably private location data.  We share pieces of a fascinating conversation with a Russian ransomware operator, gaining some insight into the way he conducts attacks and the way he views the world.  We tell everyone about a new tracking-stripping and privacy-enforcing email forwarding service that's just come out of a yearlong beta from the DuckDuckGo people.  We have another big and widespread IoT update mess to share.  I have some welcome progress to report about my work on SpinRite, and some listener feedback.  Finally, we're going to look at some recent goings on at the Ben-Gurion University of the Negev, which never fails to entertain. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's got his take on the LastPass breach.  I know you wanted to hear all about that.  Then it's an interview with a hacker, some really interesting revelations from a ransomware hacker in  Russia.  Finally, he's going to talk about wacky ways to exfiltrate data from air-gapped computers.  Some really interesting ideas here.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 886, recorded Tuesday, August 30th, 2022:  Wacky Data Exfiltration.



It's time for Security Now!, the show where we cover the latest security news with the man, the myth, the legend:  Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Yo, Leo, great to be with you again for this last podcast of August.  Where has the year gone?



LEO:  Where has it gone, yeah.



STEVE:  Yeah, actually Lorrie and I were - we have an anniversary of our first date coming up here.  It's five years.



LEO:  What?



STEVE:  It's like, five years.



LEO:  That's great.



STEVE:  Where did that time go?



LEO:  Do you give - are there gifts for the first date?



STEVE:  Thank god, no.  She's so sane.  She's like, oh.



LEO:  You don't have to worry about that.



STEVE:  No, the reason we're married is that she's completely, like, not that way. 



LEO:  Low maintenance.



STEVE:  You know, I mean, like I have to tell her that it's Valentine's Day.  She says, "What?  Oh."  Okay.  Do you care?  "No."  Okay, good.  What's on TV?



LEO:  What are you guys watching these days?



STEVE:  Oh, well, we are really liking "The Old Man."



LEO:  Isn't that good?



STEVE:  Yeah.



LEO:  But, well, I won't say anything.  After you finish it, talk to me because it starts so well.



STEVE:  I totally agree.  And I think I just hit the rough spot you're talking about.



LEO:  Yeah.



STEVE:  And it's like, wait a minute.  This is not what I wanted to have happen.



LEO:  Bingo.



STEVE:  I just, yes, I loved the way it was progressing.  And then, you know, then Zoe does her...



LEO:  Yes, Zoe.  Exactly when that happens.



STEVE:  And it's like, this is, like, went off the rails, unfortunately.



LEO:  Round about Episode IV.  It's too bad because...



STEVE:  Uh-huh, that's exactly right.  And it was Roman numeral IV, Episode IV, in the apartment.  And it's like, oh, crap.



LEO:  I feel bad because I've recommended this to so many people, but I recommended it after the first three episodes.



STEVE:  Yes.  And the reveal that we got, that surprise?  Oh, goodness.



LEO:  Yeah.



STEVE:  You know?



LEO:  Yeah.



STEVE:  About his daughter.



LEO:  Yeah, yeah, that's a good twist.



STEVE:  And Lithgow is still in fine form after all these...



LEO:  I love John Lithgow.



STEVE:  Oh, my goodness.  So anyway.



LEO:  Great cast all around, actually.



STEVE:  Watching that, I think the next one is - I can't think of what the name is.  It's not the timeline, it's the something.



LEO:  Not "The Time Traveler's Wife."  I'm sure you're not going to watch that.



STEVE:  No, no, no, no.



LEO:  Although Lorrie might like it because the star is naked a lot, and he's very hunky.  You know, it's so funny because you can see the TV...



STEVE:  We won't be watching that.



LEO:  You can see the TV executives going, can he be naked?  Yeah, he can, yeah.  You know what I've been watching, and it's an old show that I discovered lately on Amazon Prime, it's called "Patriot."  And I think you guys would love it.  So make a note of that.



STEVE:  However, you were saying just briefly, and we should put it into the recording now since you hit the record button already...



LEO:  Yes, yes.



STEVE:  You started reading - thank you.  You started reading "The Singularity Trap."



LEO:  I did.  You were right.  This is the book that last week Steve almost didn't do the show because he had, like, four pages left, and he just couldn't put it down.  So I thought, well, that's pretty good.  And it's by Dennis E. Hamilton, who did the incredible Bobiverse.



STEVE:  Dennis Taylor.



LEO:  Taylor, I'm sorry.  Incredible Bobiverse saga.  And it is definitely that style.  Same reader, Ray Porter.



STEVE:  Yup.



LEO:  And I'm quite enjoying it.  Yeah, good pick.



STEVE:  And it's got wit, and it's written smart, you know, it's smartly written and, yeah.



LEO:  Yeah, I agree.  I agree.



STEVE:  Okay.  So we're going to talk about Wacky Data Exfiltration, brought to us by those amazing engineering students at the Ben Gurion University of the Negev, which never fails to entertain.  But first we have to discuss, because boy did my Twitter DM feed...



LEO:  I bet.



STEVE:  ...go overboard with the implications of last week's LastPass breach disclosure.  We then look at some recent saber-rattling by the U.S.'s Federal Trade Commission and Federal Communications Commission over the disclosure of presumably private location data, which turns out not to be such.  I want to share some pieces of a fascinating conversation with a Russian ransomware operator, which gains us some insight into the way he conducts attacks and the way he views the world, which is just a little jarring for me.



I also want to tell everyone about a new tracking-stripping and privacy-enforcing email forwarding service that's just come out of its yearlong beta from our friends at the - I wish he'd come up with a different name - DuckDuckGo.  We also have another big and widespread IoT update mess to share.  Then I've got a welcome progress report about my work on SpinRite and some listener feedback.  And then we're going to look, as I said, at two new wacky ways of exfiltrating data from air-gapped computer systems.



LEO:  Wow.



STEVE:  So I think another great podcast for our listeners.



LEO:  Wow.  Exfiltrating data.  Wacky ways.  There have been quite a few, come to think of it, over the years we've been doing the show.



STEVE:  Oh, look.  And I remind us at the beginning of talking about this about aiming the laser at the bag of potato chips.



LEO:  Right.



STEVE:  Okay.  So this Picture of the Week was tweeted to me.



LEO:  Yes.



STEVE:  And it was so cool that - and I apologize for wondering if it was authentic or not.  So I went back to the source, to the original tweet from the U.S. Army Chief of Cyber, who tweets from @armychiefcyber.  And apparently the slogan is "Defend, Attack, Exploit."



LEO:  Okay.



STEVE:  Yeah.  And so the tweet reads:  "Interested in becoming a nation-state hacker?  We will develop your skills in offensive and defensive cyber operations."



LEO:  Wow.



STEVE:  "Defend, Attack, Exploit."  And then there's a link.  And I've got the link in the show notes, and a link to the original tweet.  Now, Leo, I have to say that I've often wondered, if I were a youngster, like what would I do?  Well, even if the pay wasn't that great, the idea that you could actually, like, it would be legal for you to be attacking...



LEO:  Make portable dog killers?  What?  Is that what - wow.



STEVE:  No, I mean, this is so cool.  And props to them for just saying this is - look at him standing next to this big emblem there.  Defend, Attack, Exploit.



LEO:  It's defensive and offensive is what it is.



STEVE:  Sign me up, baby.  Oh, goodness.  Unfortunately...



LEO:  Yeah.  I'm not crazy about the uniform, though.



STEVE:  Well, and the problem is if you're really good you probably get moved into the bureaucracy.  At that point I would say, okay, I'm going to take all my skills that I've just sharpened and go somewhere else.



LEO:  This is very Jason Bourne, though.  You really do what to do this; right?



STEVE:  Oh, I do.  I mean, it's legal.  You can attack people.  Holy crap.



LEO:  And you're doing it for the good guys.  Yeah.



STEVE:  That's right.  That's right.  You know, we're giving the Russkies something back.  It's like, oh.



LEO:  The only thing I don't like, and I guess you can't expect more from the Army, is the word "cyber."  Just by itself I don't like "cyber."  Right?  But I guess that's, you know.



STEVE:  Yeah, I think that's with us.  We can thank William Gibson for that one.



LEO:  Well, I don't mind "cyber" in conjunction with another word.  But this guy is the U.S. Army Chief of Cyber.



STEVE:  Oh, I agree, that's a little awkward.  Yes, yes.



LEO:  You know why? 



STEVE:  Cyber what?



LEO:  Because they don't want to say "cyberwarfare."  They don't want to say "cyberwarfare."  But that's what it is; right?  They just don't want to say it.



STEVE:  Exactly.  Exactly.



LEO:  You could say "cyberdefense," but then it's exploit, as well.



STEVE:  So we've been talking about careers in IT and in hacking.  And, you know, if your particular, like, bent would suggest, I just wanted to make sure that everybody knew that this was actually happening.  So very, very, very cool.



LEO:  Very cool.  Very nice, yeah.



STEVE:  Okay.  So not so cool was the news of last week's LastPass breach announcement, which as I mentioned before overwhelmed my Twitter DMs.  So I wanted to lead with this because so many of our listeners, myself included, are using LastPass.  So I had, as a consequence, also received an email from LastPass.  The current LastPass CEO, and I say "current" because it's been jumping around somewhat recently, a guy named Karim Toubba had the following to say in their online blog posting which echoed the email that he went to everyone.



He said:  "I want to inform you of a development that we feel is important for us to share with our LastPass business and consumer community.  Two weeks ago, we detected some unusual activity within portions of the LastPass development environment.  After initiating an immediate investigation, we have seen no evidence that this incident involved any access to customer data or encrypted password vaults.  We've determined that an unauthorized party gained access to portions of the LastPass development environment through a single compromised developer account and took portions of source code and some proprietary LastPass technical information.



"In response to the incident, we've deployed containment and mitigation measures, and engaged a leading cybersecurity and forensics firm.  While our investigation is ongoing, we've achieved a state of containment, implemented additional enhanced security measures, and see no further evidence of unauthorized activity.  Based on what we have learned and implemented, we are evaluating further mitigation techniques to strengthen our environment.  We've included a brief FAQ below of what we anticipate will be the most pressing initial questions and concerns from you.  We will continue to update you with the transparency you deserve.  Thank you for your patience, understanding, and support."



LEO:  So note that there's not a categorical denial that anything like password vaults - it's just "no evidence of."



STEVE:  Right.



LEO:  So I feel like we're not completely out of the woods, that I'd like to know that there is in fact not merely no evidence of, but it didn't happen.  



STEVE:  Okay.  Yes.



LEO:  I'm curious what you think about that.  The other thing is I think this is part of the Twilio breach, that this was a follow-on on the Twilio hack, which turned out to really be problematic.



STEVE:  It was pretty deep, yes.



LEO:  Because so many people use Twilio for authentication and other, you know, texting and so forth.



STEVE:  So of course we have the problem of proving a negative.  So lack of evidence isn't evidence of lack and so forth.



LEO:  True, right.



STEVE:  Okay.  So the short version of the FAQ, I'm not bothering to share it all, but it was basically that they believe there is to be zero impact upon LastPass users.  You know, no need to change passwords, do anything, or take any action of any kind.  And I'm sure they're unhappy that this occurred since I'm sure that they hold their proprietary information in high regard and don't attackers snooping around in it.



But we've always known, since I first checked out the technology that Joe Siegrist originally designed, is that so long as the LastPass code that runs our local browser vault is not itself compromised - and that's the key, I mean, that's the golden goose there is the script in our browser that knows how to decrypt the local copy of the vault.  As long as that's not compromised, the only thing we're providing to LastPass, the only thing they have of ours to lose is a very well-protected encrypted blob of entropy, one from each of their users.  That's what they hold for us in the cloud which allows them to link all of our devices together.  And I'm sure this is no longer unique technology.  I don't know that it was back then.  But though I haven't looked, I would imagine and hope that's what every other password manager also does because it's the only way to do what we all want safely.



We know that LastPass uses a strong, many-iteration PBKDF, you know, a password-based key derivation function which runs in our local browser to encrypt all of our password data before it ever leaves our local machine.  So you need to have a good strong password to protect your vault.  If you have that, you're as safe as you could be.  And presumably, adding any of their other security measures such as multifactor authentication, hardware dongles, et cetera, only strengthens things from there.



But this leaves us with the question:  With LastPass having admitted to having one of their developer accounts breached, should we change password managers?  I was asked that directly by many of our listeners.  And it's a worthwhile question.  Lacking any additional information  and no additional information is available at this point  I think that's an emotional decision rather than a rational decision.  Which is not to discount it.  I mean, you could argue that the human race is here because of the result of emotional decisions.



LEO:  Well, you could argue that Trust No One is an emotional decision, too, I guess; right?



STEVE:  Yes, yes.  So the reason I think that, that is, that we need a rational decision, is that because there's no factual basis currently for knowing about what matters.  To make an informed decision it would be necessary to deeply understand the company's policies and procedures, like as an insider, and to know exactly how this particular breach occurred.  They're not saying.  Their policies and procedures would tell us how they have set up the barriers, which hopefully exist, between their developer resources and their production services.



LEO:  Yeah, you hate to think that it's so easy that all we have to do is social engineer one person, and then it's all gone; right?



STEVE:  Well, yes.  And Leo, just look at what we just learned about the way Twitter operates.



LEO:  Yeah, yeah.



STEVE:  It's like crap.  Okay.  But then you would also need to know that same thing about the password manager you were considering switching to.  Again, an emotional decision needs no justification, whereas a rational decision is only about justification.  Now, I've always been careful to draw a clear distinction between policies and mistakes.  Policies are deliberate; mistakes, well, they're mistakes.  When you're an employer, for example, and this is the example you and I have often used, Leo, and an employee screws up, do you fire them because they screwed up?  Or do you consider that they made a mistake and have learned a valuable lesson from it?  If, as a consequence of having made a mistake, they're now a better and more valuable employee, why give them to your competition?



So unfortunately we don't know enough about the inner workings of LastPass to make an informed decision about switching.  Should we now be more or less afraid?  How does their actual policy and behavioral security after this incident compare to the actual security available elsewhere?



LEO:  And there's an interesting comparison because it's believed that the same nation-state hacker who did the Twilio attack, we know DoorDash was attacked by the same guy.  They say yes.



STEVE:  Yup.



LEO:  But Okta, Signal, and LastPass, all breached roughly the same time using similar social engineering attacks.  But the one who wasn't, but was attacked, was Cloudflare.  Remember this?  You had this story last week, I think.  They use YubiKeys.  And because they use strong security, even though the social engineering attack worked, it didn't compromise them.



STEVE:  Yeah.



LEO:  So that's the kind of thing I'd like to see from LastPass, yes.  



STEVE:  Right.  And in his note he was noncommittal.  I mean, he wasn't specific.  He talked about increasing their security and tightening their boundaries and things.  It's like, okay.  Again, so we have an example.  But again, to make a change you need to know about where you're changing to, just as much as you need to know about where you're changing from.  So if LastPass learned a valuable lesson, that's great.  But I have no idea, and neither does anyone else.  Their track record is all we really have to go on.  And it's been good so far because the security architecture is good, and it's the security architecture that I'm relying upon.  At the same time, as I said, presumably everybody else's security architecture is equally sound, because none of this should be rocket science anymore.



LEO:  Would you recommend changing your LastPass password at this point?  Would that be a reasonable response, rather than changing your password manager?



STEVE:  No, no, no.  I don't see how that has any effect because it's the password which is used only locally to encrypt the blob which we send there.



LEO:  They don't have access to that.  Nor do they need it.



STEVE:  They never have.  They don't want it.  And that was Joe's original concept.  So if I were starting out today, all other things being equal, I would probably choose Bitwarden.



LEO:  Our sponsors, we've got to say.



STEVE:  Yes.



LEO:  That's not why you're choosing them, I'm sure.



STEVE:  No.  In fact, being open source, I'd be able to do the same sort of security architecture vetting that I once did with LastPass's designer, Joe Siegrist.  As we all know, and as you just said and reminded us, Bitwarden is currently a sponsor of the TWiT network, and I think that's great, though it's worth noting that LastPass had never been a sponsor here at the time I chose them.



LEO:  Yes.  In fact, it was because you chose them I think many years later that they came to us.



STEVE:  I figured it was.



LEO:  Yeah.



STEVE:  Yeah.  I chose them because Joe was more open than everyone else, which allowed me to understand exactly how their system worked and why it was the proper design.



LEO:  It's kind of ironic because if in fact what the bad guys got from LastPass is the source code, Bitwarden's open source.  They got that already.  Right?



STEVE:  Right.  And in a properly designed system it shouldn't matter.  Yes.



LEO:  It's okay.  It shouldn't matter.  Exactly, yeah.



STEVE:  Yeah.  So anyway, many of the flood of DMs I received last Thursday asked whether I was still using LastPass; and, if so, whether I was now planning to change.  Security Now! Podcast #256 - I love that it was 2^8 - was dated July 9th, 2010, and it was titled "LastPass Security." The little summary description for it on TWiT says:  "Steve thoroughly evaluates LastPass, explains why high-security passwords are necessary, and tells us how LastPass makes storing those passwords secure."



So it looks like I've been using LastPass for the past 12 years, and I still am.  If they ever give me a rational reason to change, I will, in a heartbeat.  And whether or not Bitwarden is still a sponsor of the TWiT network at the time, I would probably go there because openness matters.  But so does inertia, and the devil you know.  So anyway, I'm still using them.  I don't see any reason to change.  Subject to additional information coming to light, there's never been a breach that affected our stored security because of the way it's designed. 



LEO:  Yeah.  That's what counts.



STEVE:  And that's really what counts.  And then it's a matter of looking at the pricing and the features and what suits your model best.  I just never have a problem with it.  So it's not...



LEO:  No, no, no reason, yeah.



STEVE:  It's not irritating me.  



LEO:  I have a very soft spot in my heart for LastPass, not only because of your support, and I used them for many, many years, but when they became the studio sponsor a few years ago they kept us on the air through COVID.  If it weren't for LastPass, I don't know if we'd still be on the air.  So I have a very soft spot for LastPass.  I do use Bitwarden.  I like the idea of open source.  But I think there's pretty much feature parity between most password managers at this point.



STEVE:  Yeah.  And really it's just inertia.  It's like there's no good reason for me to leave because it works.  And when there is, yeah, I'll be out of there in a hot second.  But so far, so good.



Okay.  What's not so far so good is that just yesterday the U.S. Federal Trade Commission - well, maybe this is good, actually - filed a lawsuit against the large data broker known as Kochava, K-O-C-H-A-V-A.  Probably most of you can go to their website.  I can't, but I'll tell you why in a minute.  The lawsuit's complaint, that is, the Federal Trade Commission, U.S. Federal Trade Commission's lawsuit complaint alleges that the company Kochava offered for sale the precise geolocation data of hundreds of millions of mobile devices  and one wonders where they got it, we'll get to that in a second - revealing potentially sensitive information in what the agency says amounted to an unfair or deceptive consumer practice.



According to the FTC's complaint, as part of its operations - these guys are in Idaho - Kochava "collects a wealth of information" about people and their mobile devices, including by purchasing it from other data brokers, and sells customized feeds.



The FTC explained that among the information it sells is precise geolocation information associated with a unique marketing ID that can be used to reveal visits to sensitive locations, such as places of worship and healthcare providers.  Such data can also be relatively easily tied back to an individual by observing patterns, such as regular sleep and work locations.  Samuel Levine, the Director of the FTC's Bureau of Consumer Protection, said in a press release announcing the suit:  "Where consumers seek out healthcare, receive counseling, or celebrate their faith is private information and should not be sold to the highest bidder.  The FTC is taking Kochava to court to protect people's privacy and halt the sale of their sensitive geolocation information."



When asked about the suit, Kochava's legal representatives did not immediately respond to a request for comment.  Kochava charges clients $25,000 - and I don't know if that's monthly or what, but I saw that number - for access to its location feed, like where is so-and-so right now, I don't know - and until recently offered free samples.  Kochava attempted to preempt the action by suing the FTC earlier this month, alleging overreach in proposed complaints the agency shared in July and August.  Like, okay, this is the way we're getting ready to complain about you.  Got any comment?  And so they got sued.  Shortly before suing the FTC, the company also announced a new capability called "privacy block" which it said should assuage the agency's concerns by removing "health services location data from the Kochava Collective marketplace."



So, okay.  We know what's behind this; right?  This is all being allowed to occur, well, first of all, the tracking is only being allowed to occur, only because it's invisible to the consumer.  If tracking was apparent, it would never have grown so out of control.  As we know, Apple started requiring their apps on the iOS platform to obtain consumers' explicit permission to track them outside of the app, and the result was a resounding "No!"  So I'm glad that slimy companies like this Kochava are finally being put under the spotlight.  It's annoying that it took the Supreme Court's overturning of their previous decision in Roe to bring this to the forefront, but better late than never.



And as I noted before, when I attempted to go to https://www.kochava.com to see what they were bragging about, Chrome told me that the domain was unknown.  I got back "DNS_PROBE_FINISHED_NXDOMAIN."



LEO:  What?



STEVE:  Of course NXDOMAIN is the error for there's no DNS listing for this.



LEO:  What?



STEVE:  Then I smiled when I realized that was because I took your suggestion, Leo, last week and decided to experiment with NextDNS...



LEO:  Oh, bravo.



STEVE:  ...as an advertising and tracking blocker.  Obviously, those guys already know about Kochava.  And they're saying, uh, no.  So I'm very impressed with what I've seen so far.  If I was curious, I could have quickly whitelisted Kochava.com and then gone to their site and poked around, or changed my DNS and then changed back, whatever.



LEO:  You ain't missing anything.  I was going to tell you.  Incidentally, they offer a product to help you improve your Apple search ad performance, as well.  They know a lot.  They know a lot.  Wow.



STEVE:  Yeah.  So this is the world we're in.  This is the data broker.  And the question is, where are they buying this information?  Which brings us to the Federal Communications Commission.  Not to be left out, though not that it appears to matter much.  The U.S.'s Federal Communications Commission has launched an investigation into mobile carriers' geolocation data practices.



LEO:  Oh, yeah, sure.



STEVE:  Uh-huh.  Last Thursday, the FCC shared responses from mobile carriers to a probe into how they handle geolocation data, and announced a new investigation into carrier compliance with the Commission's rules about disclosing how much data is stored and shared.  Okay.  So the FCC's Chairwoman Jessica Rosenworcel said in a press release:  "Our mobile phones know a lot about us.  That means carriers know who we are, who we call, and where we are at any given moment."



LEO:  And where we are at any given moment.



STEVE:  Oh, thank goodness.



LEO:  Yes.  I was going to say, "Where we at?  Where are we at?  Where we at?"



STEVE:  Where we are, thank you, at any given moment.  I couldn't believe that was actually going to be an official press release.



LEO:  They must use Grammarly.  I'm sure they fixed it up for them, yeah.



STEVE:  "That's why the FCC is taking steps to ensure," she says, "this data is protected."  Except they're not.  Anyway, good luck with that.  Though I suppose this might answer the question of where slimeball Kochava obtained the information they're now aggravating, yeah, aggravating aggregating.



LEO:  I like that.  Aggravating, yeah.



STEVE:  Aggravating and reselling.  Okay.  So the Commission, the FCC, sent inquiries to 15 carriers, including AT&T, T-Mobile, Verizon, Google-Fi and others last month asking them to spell out their policies around geolocation data, including how long information was retained, as well as how and why, what circumstances, like how much do they pay you, it might be shared with third parties.  The FCC requires mobile companies - get this - to get consumer consent for sharing information, unless such sharing is necessary to complete a service or required by law.  Which is the biggest loophole ever written; right?  Oh, no, we have to retain that information in order to make our cell towers work.  So great, we can, we're allowed to. 



Anyway, unfortunately, aside from that, has anyone ever actually succeeded in reading the fine print of the agreement that you click on with any of these companies?  And what is one to do if the terms turn out to be onerous?  All the other carriers use the same fine print.



Two years ago, as evidence of their lack of ability to actually do anything, in 2020 the FCC proposed more than $200 million in what they described as still-pending fines for major carriers for selling user location data without consent or appropriate safeguards.  Jessica Rosenworcel, whose grammar is correct, tasked the FCC's Enforcement Bureau with the new investigation into the companies' compliance with rules requiring them "to fully disclose to consumers how they are using and sharing geolocation data."  Again, a lot of good that'll do since they seem unable to collect $200 million in still-pending fines from two years ago.  I would just say pull the plug and get their attention.  But that'll never happen.



Justin Brookman, the head of tech policy at Consumer Reports, said that nevertheless, "The quality and specificity of answers" - that the FCC received as a result of their inquiry - "definitely ranges among the respondents; but there's some interesting, concrete information in there, especially on data retention periods."  However, not surprisingly, in some cases the responses simply referred to their dense, publicly available privacy policies.  You know, it's like, yeah, oh, go see the fine print.  That's what everybody already checked.  But some others did answer questions directly point by point.



Justin, this Consumer Reports guy, agrees with me that transparency isn't enough.  He said:  "People have no choice but to share very sensitive data like geolocation with mobile carriers for those products to work.  There should be substantive constraints on what they do with that information and for how long they keep it."  So what I wonder is, in a world where that information can be sold to third parties in real time, that is, it's not like it's being typed up or printed out and emailed in boxes somewhere, that information can flow out the moment it's captured.  And where its timeliness makes it valuable, it's unclear to me whether "retention time" anymore matters at all.  That sounds like, you know, pre-communications is free sort of timescale.  



Harold Feld, who's the Vice President with a company called Public Knowledge, also called for regulatory action, saying the FCC should "set new rules of the road" for mobile carriers' privacy.  He said:  "These letters show that, despite the constant invocation of 'industry standards' and 'best practices,' carrier geolocation data practices are all over the map."  For example, the length of time carriers retained location data, as determined by proximity to cell towers, ranged widely, and as long as five years in the case of AT&T.



So my feeling is, unless we make them delete it, they're not going to.  But again, if they are allowed to sell it immediately to third parties, retention time no longer matters at all.  They claim that they must keep it for business purposes and to maintain the health of their networks.  Fine.  Simply outlaw its sales to any other entity, period.  But that's not going to happen.  The FCC appears to be toothless to me.  They completely ceded control over broadband privacy during Trump's administration.  And while the FCC still theoretically has substantial regulatory authority over mobile phone carriers, the carriers appear to simply be ignoring the FCC.  So I don't know.  It would be nice to have something happen at the federal level.  Maybe it's going to be at the state level.



Which brings me to California.  In very late-breaking news, only a couple hours ago, Techdirt's Mike Masnick reported...



LEO:  Ooh, is he mad.  Ooh, baby is he mad.



STEVE:  Yes.



LEO:  And I don't blame him.



STEVE:  Yes, he is.  The California State Senate just passed what he described as three horrific new Internet regulation laws, apparently written by bureaucrats who have no idea how the Internet works.  Since this just happened, I haven't had any chance to look into it.  But if it's interesting, we'll talk about it next week.



LEO:  Oh, it is, and we'll be talking about...



STEVE:  Do you know more, Leo?



LEO:  Yeah, I know all about it.  We'll be talking about it tomorrow on TWiG.  It is one of those things where it sounds on the surface to be very good.  New York Times published an article about it, you know, essentially saying, yeah, it's good to have privacy.  It doesn't understand how the Internet works.  It's not going to solve the problems it's intended to solve.  And in fact it's probably going to make them worse.  And it's certainly going to be an onerous burden for us because any, well, one of the laws is to protect children, people under 18.  But the COPPA, the Child Online Privacy and Prevention Act, protects kids under 13, but against sites that are aimed at kids.  This affects all sites.  So if you think an 18-year-old might visit your site - well, guess what, they do, they do - then you have to do a number of things to protect them, including age verification.



STEVE:  What?



LEO:  You have to make sure that nobody is under 18 who's visiting your site; or, if they are, that you mitigate any hazards to them.  Well, Mike's point is, well, you're asking us to figure out how people are when they visit the site?



STEVE:  Every single visitor.



LEO:  Every single - and collect that information.  That's not exactly privacy forward.



STEVE:  No.



LEO:  Nor does it protect anybody.  So this is - I think your summation is exactly write, written by bureaucrats who have no idea how the Internet works.  It's politically probably very popular because it looks good.  Looks like it protects children and protects privacy.  Does not.  Anyway, we'll talk about this tomorrow.



STEVE:  And 100% of the Senate voted for that.



LEO:  33 to nothing.



STEVE:  Yeah. 



LEO:  Nobody voted against it in California.  Now, the last buttress against this will be the governor.  And what Mike's hoping, I think a lot of people are hoping, is that Governor Newsom will hear from people like Mike Masnick and say, oh, yeah.



STEVE:  And maybe that he's tech savvy enough to understand what this means.



LEO:  Right.  What people like The New York Times say is, well, the giants don't like it because of course they want to collect more information about us.  But imagine.  Thing is, Facebook already knows how old you are.  Google can easily figure that out.  But Security Now!, GRC.com?  TWiT.tv?  Do we want to start collecting age information about everybody who visits?



STEVE:  No.  And in fact when I set up GRC's forums, I explicitly removed that from the signup sheet and from any criteria because I didn't want to ask for it.  I don't want to know.  I don't care.



LEO:  The legislature's response to Mike is, well, you know, the AG, the California Attorney General, gets to decide who's prosecuted.  He's not going to look at TWiT and say, oh, yeah, this is a hazard to 18 year olds.  But Mike's point is, oh, great, so now you give the AG a tool that if he doesn't like somebody, he can attack them.  That seems like...



STEVE:  On the basis that you're not collecting visitor age information.



LEO:  Right, right.



STEVE:  Yeah, we'll just add that to the cookie banner, Leo.  We'll just add another field.



LEO:  Oh yeah, just like that.



STEVE:  You know, what year were you born?



LEO:  By the way, all of a sudden, if you start collecting age information, now a whole range of GDPR regulations apply to you because you are collecting personally identifiable information.  Right?  So it opens you up to this whole can of worms.  Anyway, don't get me started.  I agree with Mike.



STEVE:  Lorrie's been talking about New Zealand of late.



LEO:  Yeah, sounds better and better.  And by the way, no Internet presence.  Just stay off the Internet.  It's a bad idea.



STEVE:  Back when I was writing the TechTalk column for InfoWorld, I, as all columnists, had a copy editor.



LEO:  Yes.



STEVE:  Mine was a great guy named Michael Miller.



LEO:  Oh, I know Michael, yeah.



STEVE:  Yup.  And he once said to me something that, it was like, what, 30-some years ago?  And it's just stuck with me ever since.  He said, "Well, Steve, you know, mostly I just go through your columns searching for the word 'which,' and I change them to 'that.'  Because you do that."  And I go, oh.  And so of course I've been self-conscious about it ever since.  I like the word "which."  And sometimes it seems better to me than "that."



LEO:  You can overuse "which."  I don't know if it's the same Michael Miller, but I think it is.  He became Editor in Chief of PC Magazine.  We were talking about him this morning, as a matter of fact.



STEVE:  Yup, that's Michael Miller.  That's my Michael Miller.



LEO:  Great guy.  Really like him.  And he's done very well since.  I don't think it says in here "Former copy editor for Steve Gibson."



STEVE:  Oh, he'll know.  Eight years he had to go through with his - thank god he had copy and replace, yeah, find and replace.



LEO:  He was a great guy.  Or he is a great guy.  I really like Michael, yeah.



STEVE:  Yup.  So the guys over at the publication The Record had a lengthy conversation with a Russian ransomware attacker by the name of Mikhail Matveev.



LEO:  Interesting.



STEVE:  And although I didn't think that much of the conversation, which revolved a lot of the squabblings among adversarial ransomware groups, would be that much interesting to our listeners, Mikhail's answers to a couple of the questions were interesting.  So I've selected a few bits out of that longer conversation to share.  And I should mention that this is a translation from Russian, because the conversation was held in Russian, so the semantics will be a bit non-English.  And I've also edited it a bit since this young man's choice of descriptive language was a little bit blue.  So it was not safe for work.



LEO:  Well, he is a hacker.  I mean, come on.



STEVE:  Yes, he is.  I didn't have a problem with it, but we've got a large listening audience.



LEO:  Yes, no, I appreciate that.  We don't want to have to scan your face to find out how old you are.



STEVE:  No.  Dimitry from The Record asks:  "How often do people from different affiliate programs compete in the same network to extort victims?  Have you had such situations?"  In other words, Dimitry was asking, are there ever collisions among different attackers?  And Mikhail says:  "This happens often."  What?  "Especially when several people own the exploit, or pour logs from the same traffic market if we are talking about extracting initial access credentials with a stealer."



He says:  "I took some source codes, so-called 'proof of concept,' from GitHub and modified them.  If you remember, there was a well-known CVE for the Fortinet VPN.  We found it with one programmer from the forum.  Based on the list of IP addresses, we got approximately 48,000 entry points. I was very surprised then, really shocked.  But we did not even work through 3% of this list.  Not enough time.



"And when others  well, let's say our competitors  began to use this vulnerability, there were intersections across networks.  I often went into a network already locked" - and by the way, when he says "locked," that's his term for encrypted.  So he went in, and everything was, all of the servers were already encrypted.  So he says:  "I often went into a network already locked by someone and didn't touch them because it's not my job to encrypt for the second time.  But some guys over-locked networks.  They come in and see that it is encrypted.  And so that nobody gets it, they encrypt it again.  There were cases where the guys and I just crossed paths on the network during development" - that is, development of their presence in the network - "exchanged contacts, and somehow discussed what to do next.  We basically always agreed.



"And it even happened that we then jointly did some other projects.  In the summer of 2022" - that is, this summer, he says - "this happens all the time because everyone is hungry for the material.  How can we get to the initial access?  Actually, there aren't many options.  There are vulnerabilities, such as RCE [Remote Code Execution] in various products of VPN devices, everything that can give access to the network.  Or a network access login from stealers.  But basically, everyone is now flooded from traffic exchanges, and there is little unique traffic.  And those who have it, they pour just for themselves or are already working in some teams, so it's absolutely normal that there is a conflict of interest on the networks, and now it will be even more."



Okay.  So I thought Mikhail's comments that they were only able to exploit 3% of the list of 48,000 Fortinet VPNs because there's not enough time.  In other words, he's saying there really is an active race when a new patch drops and a proof of concept is made available for something like a critical remote access vulnerability.  So these cretins are actively watching everything, waiting for the first glimmer of a newly discovered problem.  And they realize that there is going to be a lot of the systems patched quickly, so they're not wasting any time. 



And significantly, they are not finding any of these problems themselves.  These are not high-end security researchers gone bad.  They're living off of the interval in the delay to patch.  They're not good enough to find the trouble themselves.  But they are good enough to quickly weaponize a working proof of concept when it's posted to GitHub, then immediately turn around and employ it to gain entry wherever they can.  And basically what he's saying is there's now a lot of them, all basically competing for access to opportunities that appear whenever they do, and so it's who can get in there first.



So Dimitry asks:  "Tell me about some attacks that stood out to you.  Which was the fastest?  How long did it take from the first penetration into the network to receiving the payment?"  So Mikhail says:  "There were many interesting ones.  But I would like to sum it up, before talking about the attacks.  There are small networks, there are medium networks, and there are very large networks.  And I'll tell you, it's much easier to work with a network of an organization with $1 billion of revenue than in a network of an organization that has income of $9 million.



"I'll tell you why.  There are many more computers that are easier to hide on and easier to navigate than in a small network where you are limited.  You have to move very fast.  And when I started my career" - I love the word "career."  It's like, this is a career.  Okay.  "When I started my career, I started with BlueKeep  a vulnerability in Microsoft Remote Desktop.  I hacked five small networks per day because I had to go in and do it right away.  But, as I progressed, the time I spent on the hacks increased."



He says:  "My longest development, probably everyone has heard about the Capcom company.  I got there through a Fortinet vulnerability.  As a matter of fact, when I went there, I was a little surprised that everything was in Japanese.  There is no hierarchy, there's no division into departments, and they have everything in a big heap.  I found a dead domain admin.  That is how the name Babuk" - which is one of his monikers - "appeared."  He said:  "Capcom had an admin Babak, or Bambook.  And when I found this administrator, I realized that no one uses him, but he was an enterprise type."  Okay, so Mikhail is explaining that he found an abandoned active administrative account which he was able to use.  And he took "Babuk" as one of his several aliases from them on.



He said:  "The fastest attack in my life happened as soon as I got the ProxyLogon vulnerability.  At that time, I had a programmer on a grant who was finalizing the exploit.  One of the interesting networks was a logistics company in the Netherlands.  Large warehouse.  Very large warehouse.  I got in and immediately obtained the domain admin tokens.  These guys weren't very security conscious and didn't worry about anything.  I remember I went there at 8:00 p.m. Moscow time, and at about 4:00 a.m. Moscow time" - so that was eight hours later, he says - "it was already all locked up."  Meaning he'd encrypted the works.  He said:  "From 6 a.m., the administrator wrote to us in a panic, to which I told him, 'Bro, wait for the supervisor.'"



So anyway, he's saying that somebody realized something was wrong, and Mikhail didn't want to talk to an underling.  So he said:  "Looking around the network, everything seems to be simple and clear.  They have an administrator's domain for us.  The password was the same for everything."



LEO:  Oh, boy.



STEVE:  Uh-huh, "On hypervisors, on a backup server, in the work group, everything.  After analyzing the network, I found a WIM Windows backup system.  I could get all the passwords from it."  Right?  Because it's an offline backup.  He says:  "And thereby got all the backups, although their backups were so bad."  He said:  "They just backed up to the NAS."  He says:  "I went to the NAS and formatted it.  Went to ESXi, encrypted, and then after about an hour he wrote to us."



So he says:  "The admin wrote right at midnight.  He said, 'I would like to resolve the issue.'  I said that the issue could not be resolved because he was not a boss.  In the morning" - this is Mikhail.  "In the morning I had to fly to another city.  I remember sitting at the airport.  The company writes to me:  '$2 million.  Transferring $2 million.'"  He said:  "I have never had such an amount in my wallet.  I get on the plane, realizing that I have a laptop with $2 million."  He says:  "Well, I gave them decryptors, and when I arrived, I opened the chat."



He says:  "Damn it, something is not right there.  They just yell 'You destroyed VMDK.'  He says, of course, that's the file format, right, for ESXI virtual machines.  So they're screaming at him:  "You destroyed VMDK."  He said:  "I tried to figure it out and asked for VMDK samples.  But the VMDK files are 0KB."



LEO:  Oh, that's not good.



STEVE:  "So," he writes, "everything is screwed.  I am writing to this developer who created the exploit for me, 'How could this happen?'  He says, 'Well, I don't know,' he said, 'something broke.'"



LEO:  Nice.  These are quality people we're talking here, I'll tell you.



STEVE:  And they asked, he says:  "And they asked to return the money.  Well," he said, "we had no choice but to block them."



LEO:  Great.



STEVE:  "So," he said, "we scammed them for this money.  I still blame myself for this.  It was the fastest and most solvent attack I've ever done."



LEO:  Besides the fact this guy is an absolute scum bucket, the thing that really strikes me is how invulnerable he feels.  He's confessing to at least two major crimes, Capcom and this Dutch warehouse, with absolute impunity.  The Russians don't care.



STEVE:  Yeah.  And he says something in a minute that I just - okay.  So Dimitry says:  "How do you see the ransomware industry in three years?  Will ransomware remain the best monetization model for cybercriminals, or will they move on to something else?"  Mikhail says:  "It's like how carding used to be popular, and there was a lot of money in it, but now it's dead.  And ransomware will soon die, not in three years," he says, "but sooner."  He says, and I disagree with that, but we'll see.  He says:  "Literally everything" - now, this is interesting.  "Literally everything has changed over the last six months."  And remember where he is.  He's in Russia.  He says:  "Since the beginning of the special operation in Ukraine, almost everyone has refused to pay."



LEO:  Oh, good.  Good.



STEVE:  He says:  "I often encountered people who wrote to me in the chat, 'You are a Russian occupier.  Be content with $10K, and we won't give you more.  At least take that.'"  So he says:  "Return on investment has completely fallen in the last six months."



LEO:  Aww.  Aww.



STEVE:  Oh, boo-hoo.



LEO:  What is this?  Geez.



STEVE:  I know.  "Return on investment has completely fallen in the last six months."  He says:  "It became difficult to work in general."  Poor baby.  He says:  "If it dies" - meaning ransomware - "it dies.  You need to come up with something new.  But ransomware is worse than heroin.  I haven't tried heroin, but I've seen people who are on it, and I'll tell you this:  Ransomware is worse than drug addiction.  There is no such money anywhere as there is in ransomware."



LEO:  [Sigh]



STEVE:  I know.  "I even compared it to drug dealers from Hydra, the world's largest dark net marketplace, which was shut down this year.  They earn less than we do."  Okay.  So he's calling this "earning money."  He finishes:  "But at the moment, ransomware remains the leader in monetization.  There are no other schemes on the Internet that would carry more monetization, or I don't know about them yet."



LEO:  Mm-hmm.



STEVE:  So as I read that, I'm struck by how casual Mikhail is about being a criminal.  There's an utter lack of morality.  He did appear to feel badly that his decryptor didn't reverse the encryption of the large warehouse's VMDK files.  So he got $2 million without returning their data.



LEO:  Aw, yeah.  He feels bad.



STEVE:  Yeah.  Well, and he had to block them because they were screaming at him.



LEO:  Yeah, hmm.



STEVE:  So but what seems to be utterly absent is the idea that extortion itself is wrong.



LEO:  Right.



STEVE:  He talks about it as a "career."  Like it's a legitimate profession that he's in.  Like his parents would be proud.  As though, if you have leverage over someone, using that leverage for your own personal gain at their loss is acceptable.



LEO:  Horrible.



STEVE:  So anyway, I thought that everyone would find this interesting.  These guys are not geniuses.  They're computer savvy.  They use other people's tools to force those abroad  meaning as not in Russia  to give them money.



LEO:  Yeah.  They're like criminals anywhere.  They have zero moral compass and I'm sure justify it in their mind.  And, oh, it's horrible.



STEVE:  Yeah.  So, and I just don't cover the continuing ransomware problems.  But, I mean, because I know our listeners are like, yeah, yeah, yeah.  But, I mean, I skip over story after story after story.



LEO:  Oh, yeah.  Same with breaches.  I don't even bother talking about breaches anymore.  It's nonstop.



STEVE:  Yeah.  Yeah.  So a bit of good news.  The privacy-centric DuckDuckGo has had what it calls an "Email Protection" service in beta since July of last year, so more than a year.  But they've just opened it to the public.  It looks like a very useful and completely free service.  So our listeners might want to jump over and grab their name or their favorite handle quickly before it's taken.  So I'll explain how to do that first.  Then I'll tell you why this seems like a nifty service.



To register, go in a web browser to duckduckgo.com/email.  If you don't, as you probably won't, currently have their browser extension installed, you'll need to do that first.  You can remove it later since it's not required to use the service once it's set up.  Although you do need it to manage the service, and there are some cool management stuff I'll explain in a second.  So you'll be asked to provide a username which has not yet been taken.  And it will become sort of your base or default @duck.com email address.  So whatever name or handle you choose @duck.com will be your default email address.  You also provide an email address which will receive cleaned and formatted and forwarded email.



Okay.  Oh, and so after you install the browser extension, go back to DuckDuckGo.com/email.  Now it'll say, ah, and then you'll be able to set up your account.



So what does this all get you?  Their Email Protection is DuckDuckGo's dedicated email forwarding system which strips advertising and profiling trackers from email  links, scripts, images, media, all that crap  before forwarding them to your registered forwarding email.  When you receive the forwarded email you'll also see a short report which has been added to it of how many trackers were removed, which companies were responsible for their injection into your email, and more.  DuckDuckGo says that after a year of running the beta program, 85% of all emails on their beta testers' communications, contained trackers of one form or another.



So at that point anything sent to "yourusername@duck.com" will be forwarded after being cleaned and reformatted.  And in a very cool feature, Email Protection also provides users with unlimited disposable and dynamically manageable private addresses to use on sites you want to supply with a per-site or not your primary email address.  These can later be deactivated if spamming to that address becomes a problem.  You can ask for as many of these throwaway email addresses as you need.  And of course using unique email addresses confers some of the benefits of using unique passwords on sites.  In the event of a website's data breach, the linkability of your identity to any other of your identities online will be dramatically reduced.



So messages passing through DuckDuckGo are never stored by DuckDuckGo.  They make that very clear, while what small amount of accounting and forwarding information is kept for operational reasons is deleted within 30 days after the account's closure.  So if you close the account, within a month it's gone.  No long-term footprint.  And even though email is forwarded to your real email address, it's still possible to reply to those emails, which will then come from a Duck.com address.  So this can be useful where anonymity would be important.



So it's 100% free.  It has a user-friendly dashboard for quickly configuring forwarding addresses and making on-the-fly changes.  You can manage account settings and all that.  In addition to a browser extension, there are apps for Android and iOS which allow for the same sort of dashboard management in those apps.



So, okay, I still don't like the name, but the service seems pretty cool.  Since I run my own server at GRC.com I'm able to, and I do, create tons of email addresses, I mean email aliases, exactly for this purpose.  So I'm giving somebody I'm not sure about an alias.  And if I ever start getting spam there, well, first of all, I know where it came from, and I'm able to terminate the alias.  So this is that sort of a service which in addition to that is filtering email to remove tracking crap from it.  So anyway, seems like a neat deal.  We know DuckDuckGo and that they really are privacy centric.  So I wanted to make sure everybody knew that this new service had just come out of beta.



Okay.  Another big IoT mess.  And each one of these that we talk about is a lesson.  I'm not going to get preachy because I know that can get tiresome.  But the state of the IoT industry brings my blood close to a boil every time.  Here's what's going on this time.  The cybersecurity firm CYFIRMA, C-Y-F-I-R-M-A, recently published a report describing a long and still outstanding security threat created by insecure - and these are like commercial grade - Internet surveillance cameras produced by a U.S.-based firm Hikvision, H-I-K-V-I-S-I-O-N.  They're located down here near me in Southern California, I think City of Industry.  Anyway, one year ago, a year ago in September 2021, in response to a discovery by security researchers which was given CVE-2021-36260, Hikvision did the responsible thing:  They published a firmware update to correct a serious vulnerability.



Unfortunately, it's for a camera.  Okay.  The researchers discovered that the Hikvision cameras were vulnerable to a critical command injection flaw that's easily exploitable via specially crafted messages sent to the camera's vulnerable web server, which is what it exposes to the Internet.



Okay.  So that was then.  Today, CYFIRMA analyzed a sample of 285,000 Internet-facing Hikvision camera web servers. They found that today, a year later, roughly 80,000 are still vulnerable to exploitation.  And this, of course, is the problem.  Some contractor you've hired purchases a camera, or 50, and installs them.  They set them up, send you an invoice, and move on to their next job.  Meanwhile, you have some number of web servers on your network which can be taken over remotely.  And the takeover is not theoretical.



There have been two known public exploits for this CVE-2021-36260.  One was published in October of 2021, and the second in February of 2022.  So that's known public exploits published, meaning this is exactly what Mikhail is sitting around in Russia waiting to see pop up on the Internet, and he jumps on it in competition with all of the others of his ilk.  In December 2021, a Mirai-based botnet called "Moobot" used one of those two publicly published exploits to spread aggressively and enlist those cameras into DDoS swarms.  What's generating those record-breaking DDoS attacks which now force everyone who needs to remain online to move behind and pay for DDoS protection?  Exactly these kinds of IoT devices.  It may include thousands of compromised Hikvision cameras.  There are 80,000 available today.



In January of this year, CISA alerted that that CVE-2021-36260 was among the actively exploited bugs in its list.  CISA warned organizations that attackers could take control of devices and that they should be patched immediately.  Yeah, they should have been patched last September.  How'd that work out?  As I noted, those 80,000 still-vulnerable surveillance cameras were just recently enumerated.  The cameras are very popular, and they appear to be industrial grade, as I said.  Hikvision has an impressive-looking website.  In CYFIRMA's report they tracked those 80,000 vulnerable IPs back to 2,300 organizations across 100 countries.  None have applied the security update which is now nearly a year old.



CYFIRMA's report notes that Russian-speaking hacking forums often sell network entrance points relying on exploitable Hikvision cameras that can be used either for botnetting or lateral movement to gain entrance into the organizations where they're deployed.  Like I said, just what Mikhail is looking for.  I have a chart in the show notes showing the geographic breakdown of the cameras' locations.  Most of them are located in China.



LEO:  Because it's a Chinese company.  So not surprising, yeah.



STEVE:  Yes.  There are 12,690 of them.  And the United States has the second most, 10,611.  While Vietnam, the U.K., Ukraine, Thailand, South Africa, France, the Netherlands, and Romania all count between 7,000 and 2,000 vulnerable endpoints each.  So, wow, lots of vulnerabilities in those companies.



LEO:  You might be interested to know that it is controlled by the Chinese government.  It is not, I mean, it's a private company, but the majority of shares are controlled by the CCP.  And it is used in many police surveillance systems all over the world.  



STEVE:  I was wondering about that, too, because we don't even talk about if you compromise that, obviously you can see whatever those cameras are seeing.



LEO:  That's the least of it, though; right? 



STEVE:  Yeah.



LEO:  Yeah.  Leading the future of AIOT, they proclaim.



STEVE:  I know.  Yeah.  So the credential lists for those cameras pop up in hacking forums often.  And I still doubt that the public at large understands the danger that's represented by the casual attachment of high-tech devices to their network.  As long as that remains true, purchasers won't know that they need to consider the operational life cycle of such devices.  We've all been trained now about OS and Smartphone updates. But there's just no awareness that your thermostat needs to be updated, if some third party is not taking care of that for you.  So we really are in the Wild West of Internet IoT.



Okay.  Over the past weekend I posted two status updates to the grc.spinrite.dev newsgroup.  The first posting had the subject "Friday Night Update."  And I wrote:  "Gang, I just finished the complete read-through of SpinRite's DynaStat system."  It's like the core of its data recovery stuff.  I wrote:  "I've been slogging my way through it for the past week or so.  It's extremely involved, and it was working once.  I wanted to be certain that I hadn't done anything to break it with all of the changes I've made  the I/O driver abstraction and the relocation of several working buffers into high memory.  Since they did affect the DynaStat code deeply, I've had to work my way through every code path.  It's still going to need extensive testing, but that will be joyful since it will mean that SpinRite is essentially working and just needs to have the final bits of debris eliminated.



"With this done, I now need to finish the comparatively trivial task of updating the rest of SpinRite's main processing loop, the data inversion media testing, et cetera.  And then it will be ready for the thorough testing of all of its main data recovery loop.  But we're definitely getting tantalizingly close."



So that was Friday night.  Then "Saturday Night Update."  I wrote:  "Okay, I'm done.  This is not to say that I have any" - I have mail.  Thank you very much.  "This is not to say," I wrote, "that I have illusions that it could possibly run yet.  There's no possibility.  But I have finished working through all of the code, and now it'll be up to SpinRite to show me where it's not yet ready for primetime.



"What I plan to do next is to get it actually running so that it would appear to the casual observer to be working.  That'll still be a chunk of work since I've deliberately not allowed it to begin execution.  It's certain to explode fabulously.  But before long, it won't be exploding anymore when it runs.  At that point, when there's no longer anything obviously wrong, I'll verify that it's actually doing something useful and that all of the various data recovery paths, several of them new, are working as they're designed to.  And then it'll be done."



So I just wanted to share, with everyone here who is not following along with the blow-by-blow in the SpinRite development group, where things stand.  Tonight, after the podcast, I will begin running SpinRite and fixing everything that doesn't run, since as far as I know it all should.  Once everything appears to be running, I'll then begin the work of carefully inducing various sorts of media read-and-write failures and carefully watch SpinRite deal with each type of problem to make sure it's doing the right thing.



And by amazing coincidence, a listener of ours, Ameel Khan, sent me a Twitter DM which I saw this morning.  He said:  "Hi Steve.  Love the show.  Been a regular listener for 16 years now.  Check out this video of John Carmack talking about the importance of using a debugger while you code."  So the YouTube video that Ameel linked to is an interesting 15-minute conversation with the of course legendary coder John Carmack.  I have the link in the show notes and it's our GRC shortcut of the week, so grc.sc/886.  That'll bounce you over to the YouTube video.



What I learned by watching the video, to my surprise, is that John and I code in exactly the same way, that is, with exactly the same philosophy.  Our listeners will remember that at the beginning of my return to working on SpinRite, everyone heard me talking about setting up a comfortable and smooth debugging environment before I did anything else.  And you've heard me mention it over and over since then.  My wife Lorrie lived with me grumbling about that for several months while I struggled to get everything working exactly the way I wanted.  In my case it was challenging because my target environment for the debugging was MS-DOS; and to do the sort of debugging I wanted to do, I needed a real-time link between a state-of-the-art 64-bit Windows machine and a 16-bit real mode DOS machine.  And that has become much more tricky as the years have separated these two worlds.



Anyway, I thought it was interesting that John's code writing philosophy and mine are the same.  Rather than trying to guess what's going on, rather than attempting to debug in our heads, we both immediately go to the debugger to watch the code execute step by step.  As I've often noted here, something about the programmer's ego prevents us from seeing what the code actually does.  We see what we want it and expect it to do, right up until the debugger slams our face into the reality.  At one point John notes that tools that are easy to use get used, whereas tools that are difficult or cumbersome tend to only be used as a last resort.  He and I have apparently both learned the lesson that having a comfortable and easy-to-use debugging environment is the way to get the best possible code written.



So, thank you, Ameel, for sharing the link to that conversation.  And for any of our listeners who are interested, grc.sc/886.



Okay.  Oh, and a couple more little bits of closing the loop.  Vlad Jirasek tweeted:  "Hi Steve, I have an update on this."  Actually it was on an update from last week.  He said:  "I pressed Cybereason to clarify whether the escalation of privileges would have been successful if the users were not part of the local administrator group, and they confirmed it."  And then he sent me a LinkedIn link to their dialogue.  And he said:  "Might be good to mention on next Security Now!.  Even Microsoft is saying that removing admin privileges makes over 90% of attacks ineffective."



Anyway, so what this guy sent to Cybereason, who we were talking about last week, he said:  "Very nice report, thank you.  However, may I ask why you do not mention recommendation for computer users not to be assigned administrator privileges as one of the key controls protecting them against the escalation of the attack?  If an attacker is not a member of local administrator group, then running fodhelper.exe" - which we talked about when we did this whole walkthrough of the analysis of the attack - "will not give attackers the administrative privileges by bypassing UAC.  Am I correct?" he asked.



Cybereason, to their credit, replied.  They said:  "Thank you.  We should have previously addressed that the point of the article is not to be exhaustive in terms of recommendations.  In the case involving Bumblebee, users were already in administrator group, and UAC bypass worked.  But you are correct, users need to be in administrator group.  The article is focusing on post-exploitation.  The recommendations list is not exhaustive."



So I thought that was interesting.  Remember that we've talked about the way Microsoft has basically compromised the whole problem of running as a non-privileged user, but not making it burdensome to get root or admin privileges.  In a traditional Unix or Linux environment, you are typically running not as the root user.  You can do lots of things.  But there are certainly low-level admin things where you need to logoff as the user or upgrade your rights using Linux and Unix commands to the root privilege in order to get something done.  And you're able to run a single program under those privileges.



The way Windows does it is they developed this notion of a split token where you actually have two different security tokens.  You're running with one with lesser privileges usually.  But that's what the UAC does, the User Access Control, is to switch you dynamically to the admin privilege token.  So his point is, and it's really worth remembering and highlighting, it's not - it would be interesting to try running Windows without the ability to elevate.  I know you can, but the typical end user would probably find it more annoying than was worth the trouble or worth the added security.  Maybe not so the enterprise user, where they're not supposed to be making deep changes to their system.  So removing the ability to elevate by taking them out of the admin group, that's worth remembering as a possible way of mitigating, as Microsoft has said, nine out of 10 of the attacks, which do require admin elevation.



And finally, Ed McKiver, whose Twitter handle is @OhWellDamn2010, I don't know why.  He said:  "Hi, Steve.  FYI, I canceled my LastPass Premium subscription today," he said, "(due to the recent close-call security breach)."  He said:  "I've had LastPass since they were a sponsor on TWIT, you gave your thumbs-up to their software/encryption, and before LogMeIn purchased the company.  I'm trying to limit my exposure with my password managers" - plural, and we'll see why in a minute - "now to just one.  I've used Passwords Plus from DataViz since v1.0 when it was sold..."



LEO:  Oh, my god.



STEVE:  "...on a 5" floppy disk."



LEO:  It used ROT13 for password protection, I believe.



STEVE:  Did we have passwords back then?



LEO:  There was only five.



STEVE:  Leo, that's when you went, well, we know that yours was...



LEO:  DataViz.



STEVE:  We know that yours was monkey.



LEO:  Yeah.



STEVE:  When DataViz was v1.0.



LEO:  Wow.



STEVE:  I didn't know we had passwords, but I guess we did.  Anyway, he said:  "They recently stopped all support for their product" - okay - "and their CEO decided not to move over to a subscription option in order to keep it profitable."



LEO:  Good, good.



STEVE:  So people stopped using DataViz, I guess.  And really, where are you going to stick a 5" floppy these days?  That's going to be a problem.



LEO:  I can give you some ideas, but okay.



STEVE:  Yeah.  You've got to roll it up first.



LEO:  Yeah.



STEVE:  Anyway, he said:  "I tried mSecure Premium as the recommended password manager to replace Passwords Plus, but decided to cancel that password manager today, too."  I guess he was in the mood to cancel these managers.  He says:  "As I found their tech support severely lacking."  Oh, yeah, you don't want to ever talk to anyone's tech support.  He said:  "It seemed to me that mSecure was a one- or two-man operation."  Okay, I'm kind of, well, we've got Greg, and we've got Sue.  So I guess that's three people.



LEO:  Two and a half.



STEVE:  Yeah.  He says:  "I'm sticking with Bitwarden as they have the best options, prices, and they also support the YubiKey.  Thank you as always for your great work on Security Now!.  I'm looking forward to SpinRite 6.1 since I've been a subscriber since v1.0."  And yes, it also had a 5" floppy disk in the beginning.  Thank you, Ed, and congratulations on no longer using 20 different password managers, whittling it down to just one.



LEO:  Wow, yeah.



STEVE:  Yeah.  So, okay.



LEO:  If you want a break, why don't you take a little break before we launch into this; yeah?  You've been talking for a long time.  And I will take over for a minute.  I can feel the tension building in your throat.



I just want to do a plug for Club TWiT.  This is how we're kind of smoothing out the ups and downs in advertising.  This was an idea that came to Lisa during the pandemic, and it's really been a boon to us.  What do you get in Club TWiT?  You get, for $7 a month, that's all, ad-free versions of all our shows, this show plus everything else we do.  You get access to a really fun social media site, I think, a Discord.  Discord is where you can chat about the shows, but also about anything else on your mind, every geek subject under the sun.  We even have our own Minecraft servers.  We have a trivia contest going on in there.



Plus we have some Discord-only shows, which you will also get access to, either live or with the TWiT Plus feed, which is a separate feed just for TWiT Club members.  We do a lot of stuff in the Discord, a number of shows.  For instance, Hands-On Mac, Mikah Sargent's show is for club members only right now.  As it grows and we get advertisers, it will certainly become public eventually.  But same with Hands-On Windows.  No?  Never?  Okay.  Never, Lisa says.  We also - so subscribe; right?  That's the idea.  We want to make it desirable.  We also have the Untitled Linux Show.  We do a lot of events.  We just did Stacey's Book Club.  We're going to do that again.  Our community manager Ant Pruitt is planning more events in the future.  So there's a lot of reasons to join, and for 7 bucks a month there's hardly any reason not to join.



Now, I should mention, if you just want Security Now!, I know some of you do, you can buy that from the iTunes Apple podcast for 2.99 a month.  That'll give you the ad-free version.  I think for a few bucks more getting the whole thing is a great thing.  And it really helps us out.  So go to  TWiT.tv/clubtwit.  There's a yearly plan, as well.  There's enterprise plans.  Check it out.  And that's where we also have information about buying individual shows, including you can buy the Hands-On Mac or Hands-On Windows show.  Club TWiT is at TWiT.tv/clubtwit.



And now, return to Steve Gibson and our topic.



STEVE:  I just looked up Michael Miller on LinkedIn.  And not surprisingly, we have a couple of mutual connections, Scott Mace and Evan Katz.



LEO:  There you go.  And by the way, Michael Miller still works for Ziff Davis, of all things.  He works for their investment arm as their CISO.  So he's still in the biz.  But great guy.



STEVE:  Cool.



LEO:  All right.  Now time to tell us what Wacky Data Exfiltration is.



STEVE:  Oh, and thank you for that pause to refresh.



LEO:  I thought, you know, you've been going a long time.  I thought I should.



STEVE:  I did need it.  Okay.  So through the years we've had fun considering all the various ways that Dr. Mordechai Guri and his student researchers at Israel's Cyber Security Research Center of the Ben-Gurion University of the Negev, which I also love saying every time I can, have come up with for secreting information from air-gapped computer equipment.  That's like a hobby of theirs.



So we'll all recall picking up the vibrations from the surface of a bag of potato chips sitting unnoticed in a conference room.  There was also, in a party setting, the balloons were known to be vibrating to the conversations being held around them.  And remember there was a plant whose leaf was vibrating.  And so, yeah, all that.  Anyway, there have been many of these such inventions, all of which they developed and actually pulled off in order to determine the feasibility and the achievable information transmission rate.  So in the past week we have their reports of two additional covert information leakage channels.



The first is actually one that we've discussed in the past.  That's the blinking LEDs on network interface cards.  Now, I was quick to discount that since the LEDs, as anyone knows who's actually looked at them, don't actually blink in time with the data.  Although there's no hard and fast standard for the way they do blink, I notice on my equipment they're blinking for the same data in different ways, you know, rates and just different styles, in general they just show a flash when there's data activity on the line in either direction, and the light is on enough for you to be able to see it.  But knowing that did not deter these intrepid Israeli researchers.



In their paper entitled "ETHERLED" - E-T-H-E-R-L-E-D.  It's titled "ETHERLED:  Sending Covert Morse Signals from Air-Gapped Devices via Network Card (NIC) LEDs."  And they explain:  "Highly secure devices are often isolated from the Internet or other public networks due to the confidential information they process.  This level of isolation is referred to as an 'air-gap.'  In this paper, we present a new technique named ETHERLED, allowing attackers to leak data from air-gapped networked devices such as PCs, printers, network cameras, embedded controllers, and servers.  Networked devices have an integrated network interface controller (NIC) that includes status and activity indicator LEDs.



"We show that malware installed on the device can control the status LEDs by blinking and alternating colors, using documented methods or undocumented firmware commands.  Information can be encoded via simple encoding such as Morse code and modulated over these optical signals."  I wouldn't use Morse code.  I'd use the encoding used hard disk drives because that's serial also.  But anyway.  "An attacker can intercept and decode these signals from tens to hundreds of meters away.  We show an evaluation and discuss defensive and preventative countermeasures for this exfiltration attack."



Okay.  So in a sense they're cheating.  Or at least they're modifying the rules in a Kobayashi Maru-like way.  They're allowing for malware to rewrite the NIC's firmware to take control over the LEDs.  In that case it would indeed be possible to hugely increase the rate at which data could be exfiltrated from an air-gapped network which has no other means of communicating, but those NIC cards can be seen.



What I appreciate I think most about these guys is that in every case they really do wrestle to the ground whatever wacky topic and method they are researching.  They really do the work.  For example, in this case their eight-page paper described the three methods which can be employed to control the LEDs of NIC interfaces.  They said, okay, first, driver/firmware control.  They said:  "In this method, the LED-controlling code runs as a kernel driver or within the NIC firmware.  Changing the LED state/color requires direct access to low-level registers or special non-volatile memory addresses.  This method enables the highest degree of control over the LEDs, but is very hardware-specific and mostly undocumented.



"For example, documentation discusses how to control the Ethernet LEDs in an Intel NUC PC.  It can be done from a kernel driver or by writing to specific addresses in flash memory at word 0x18, which holds the LED's configuration.  For embedded controllers, the control of the NIC is typically performed via internal bus or USB interfaces.  For example, sample code for LAN915X Ethernet controllers programs the corresponding LED register via USB commands."  So that's the first way, and the best if you can get it.



Way number two:  Link status control.  They say:  "In this method, only the status LED can be controlled.  The malicious code can intentionally change the link speed, which in turn causes the network adapter to change the status LED.  For example, setting the link speed to 10Mb, 100Mb, and 1Gb will set the status LED to off, green, and amber, respectively.  Selecting the link speed can be done by interacting with the NIC driver.  For example, the ethtool command-line tool in Linux enables to change the link speed of the Ethernet controller.  The same is possible in the Windows OS via the netsh command.



"Note that setting the link speed requires root/admin privileges in both the Linux and Windows.  Technically, the link speed is determined through the auto-negotiation procedure.  In this procedure, which occurs in the physical layer, the connected devices share their capabilities regarding supported parameters such as transmission rate, half/full duplex, et cetera.  The link speed of a network NIC can be determined from the computer's OS."



And third, user LED control.  They say:  "In this method, the user directly turns the status LEDs on and off by enabling and disabling the Ethernet interface using API or tools such as the ethtool or eth command.  The user directly turns the status LEDs on and off by enabling and disabling the Ethernet interface.  Another technique to blink the status LED is using the 'test' or 'identify' functionality, enabling the operator to identify the adapter by visual indication.  These operations can be triggered programmatically or via low-level tools such as ethtool."



Okay.  So as they always do, some of them, some group of them, whomever, really looked at this, and wrung out every detail, and actually implemented these strategies.  They consider the cameras needed to receive the information, and the camera frame rates and interactions thereof, the maximum distances at which cameras can be focused upon LEDs on NIC adapters, and basically what can be done to make the entire thing work.  Then they finally get down to the effective bitrates which are achievable through each of these three methods.



They have the driver/firmware control, that first and best one.  They have in their table OOK, which is their short for on/off keying.  And they say blink frequency and colors allows them to achieve 100 bits per second.  So they talk about being able to use that to exchange text files, usernames/passwords, encryption keys, and PIN codes.  The second approach is link status control.  They can get 1 bit per second there.  So you're not going to exchange anything really long, or at least not quickly.  And then the final, the user LED control.  They're claiming 2 bits per sec.  So you could do keylogging, usernames and passwords, credentials, encryption keys, and so forth.



So that's the first, this blinking of NIC LEDs.  You know, again, I discounted it because, again, it's not the actual data that's moving through the lines.  But yeah, if you had control over the link, if you got software into the system in the first place, in a system that was unable to communicate with the outside world, but you were able to briefly infiltrate it, then you could clearly, given enough time, exfiltrate data.  And really, if you could cheat the firmware and get 100 bits per second, then you could clearly do some damage.



But it's worth remembering that for many important secrets, you do not need a lot of bandwidth.  Some of the very best kept secrets are also very short.  A server's elliptic curve private key might only be 256 bits long.  And even a larger RSA key is still only 2 or 4Kb. So even at a measly 1 bit per second, sluggishly bringing a LAN link up and down, a 2048Kb key can be transmitted in only a little over half an hour, about 34 minutes.  So it's possible, if you wanted to do it.



Okay.  So that's the first wacky idea.  Wacky Idea #2, and arguably somewhat less wacky, their recently published 11-page paper is titled "GAIROSCOPE," and they spelled it weird, but understandably.  They spelled it G-A-I-R-O scope, as in air-gapped, gyro, G-A-I-R-O scope.  They said:  "Injecting Data from Air-Gapped Computers to Nearby Gyroscopes."  And you might think "Gyroscopes?  What?"  But they're in every one of our smartphones.



The paper's abstract explains.  They said:  "It's known that malware can leak data from isolated, air-gapped computers to nearby smartphones using ultrasonic waves.  However" - and that was like from speaker to microphone; right?  We talked about that little deal, and that was theirs, years ago.  So "It's known that malware can leak data from isolated air-gapped computers to nearby smartphones using ultrasonic waves.  However, this covert channel requires access to the smartphone's microphone, which is highly protected in Android OS and iOS, and might be non-accessible, disabled, or blocked.



"In this paper we present 'GAIROSCOPE,' an ultrasonic covert channel that does not require a microphone on the receiving side.  Our malware generates ultrasonic tones in the resonance frequencies of the MEMS gyroscope."  MEMS is the abbreviation for Micro-Electro-Mechanical System, which is what these little itty-bitty, well, electromechanical systems are.  So they said:  "These inaudible frequencies produce tiny mechanical oscillations within the smartphone's gyroscope, which can be demodulated into binary information.  Notably, the gyroscope in smartphones is considered to be a 'safe' sensor that can be used legitimately from mobile apps and javascript.  We introduce the adversarial attack model and present related work.  We provide the relevant technical background and show the design and implementation of GAIROSCOPE.



"We present the evaluation results and discuss a set of countermeasures to this threat.  Our experiments show that attackers can exfiltrate sensitive information from air-gapped computers to smartphones located a few" - actually up to eight - "meters away" - more than 24 feet away - "via a Speakers-to-Gyroscope covert channel."



So this one is more serious and interesting.  We were just talking about resonances last week with the Janet Jackson Rhythm Nation video and the Tacoma Narrows bridge.  What's special about resonance is that a relatively small signal  like a gust of wind up the Tacoma Narrows  which would be entirely harmless in isolation, can sum into the power of successive properly-timed bits of energy to result in a significant signal.  That's the effect these guys have taken advantage of here.



They explain what's going on in the MEMS gyroscopes.  They said:  "It is known that acoustic tones degrade MEMS sensors in a frequency range known as the 'resonance frequencies.'  This ultrasonic input produces erroneously low-frequency angular velocity readings in the X, Y, or Z directions.  The vulnerability of MEMS sensors to ultrasonic corruption is due to the mechanical structure of a MEMS gyroscope.  The misalignment between the driving and sensing axes is one of the main causes of the fault output generated by the gyroscope.



"The phenomenon and its physical and mechanical roots are discussed in relevant literature.  It was observed in the previous works that the typical resonance frequencies of MEMS are within a fragmented band in the ultrasonic frequencies mainly above 18 kHz.  The frequency of the resulting vibrations within the sensor is determined by the structure of the MEMS gyroscope, its positioning, and the distance from the sound source."



As always, they do all the footwork, actually develop and implement a full attack from the software in the PC to run the speaker as an ultrasonic sound source, and determine how far away they're able to position the smartphone and what data rates they're able to get.  They determine the natural resonance frequencies for a number of the MEMS gyroscopes and a number of different smartphones.  And they demonstrate the ability to send 8 bits per second of binary data  completely covertly and obviously silently because it's above our ability to hear  from a standard PC speaker to a smartphone located up to 8 meters away.  Actually I said 24.  It's actually 26 feet.  So that's pretty slick.



As always when I talk about the work that these guys are doing, I'm left thinking that it would be a blast to be in this professor's class, being asked to actually make these out-of-the-box attacks work.  That would be an awful lot of fun.  If you're not working for the U.S. government doing attack and defend and infiltrate or whatever that was.



LEO:  We call it "cyber."



STEVE:  Cyber, yes.  If you're not in the U.S. Cyber.



LEO:  I just think of Donald Trump saying his son Baron was "excellent in the cyber."  That's what I think of when I think of that.  It would be a good class to be in, coming up with this stuff.



STEVE:  Lot of fun.



LEO:  You could do it yourself.  Maybe you should - let's think of a new way to do it.  We've done glass windows.  You said potato chip bags.  That's for listening to audio.  Exfiltrating from computers.  Oh, there's got to be lots of ways.  What about the sounds of the hard drive?



STEVE:  Oh, well, I mean, remember we even had the sounds of the power supply at one point.



LEO:  Yeah.



STEVE:  And getting fans to spin at different speeds so like it speeds up and slows down.



LEO:  Exactly.



STEVE:  And you can hear that at a, I mean, there's lots of ways.  I mean, and remember the original, what was the military where they were able to, from a distance they were able to look at the electromagnetic noise coming off of a CRT screen.



LEO:  Right.  That's Tempest, yeah.



STEVE:  Tempest, that's the thing.  Yup, Tempest.



LEO:  They can do it through walls.  Unbelievable.  Unbelievable.  Well, a fascinating story.  Thank you for bringing that up.  I appreciate it.  This is why you listen to Security Now!; right?  Mm-hmm.  Get all the security news and some mind meat.



STEVE:  Mind candy.



LEO:  Mind candy.  Better than meat.  Steve does Security Now! every Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  Watch us do it live at live.twit.tv.  There's audio and video streams there.  Chat with us live at irc.twit.tv or in the Discord chat room.  But you don't have to watch live.  I mean, that's the whole point of, you know, we record these.  And we carefully craft a podcast out of them.  We take this clay, and we shape it, and then we put the podcast up on the website.



Now,  Steve's site is GRC.com.  He has 16Kb versions of the show, that's a unique version, for the bandwidth-impaired.  We do 64Kb audio, as well.  He does it, and we do it.  He also has transcripts, which really are handy for searching or for reading along as you listen, thanks to Elaine Farris, who does those for you every week, Steve.  All of that at GRC.com.  While you're there, you might want to take a look at SpinRite, the world's best mass storage maintenance and recovery utility.  



STEVE:  Getting there.



LEO:  Getting there is the word.  SpinRite 6 is the current version.  But if you buy that, you'll automatically get upgraded to 6.1, and you can participate in the rapidly winding down process of developing 6.1.



STEVE:  There actually will be a long beta period.



LEO:  Oh, okay.



STEVE:  Because I'll have the DOS side code nailed well before I have everything packaged up as the Windows app that's able to produce the bootable USB sticks and everything else.  So there will be something to owning 6.0 and being able to get the beta of 6.1.



LEO:  Excellent.  So it really is worth joining, then, or buying.  Go to GRC.com.  You can leave feedback for him there, GRC.com/feedback.  You can also Twitter him.  He is the tweeter guy, @SGgrc.  We have copies of the show at our website, which is TWiT.tv/sn, 64Kb audio and video.  That's our unique format, since we record video of this.



After the fact, it'll be up there usually a couple of hours, maybe three hours after the show.  And you could also get it on YouTube.  There's a dedicated YouTube channel to this show and all of our shows.  And probably the easiest thing is subscribe in your favorite podcast player.  That way you'll get it automatically, as soon as it's available.  Discussions continue.  Obviously chat is a little late, you know, if you're listening to a download.  But we have a great forum.  Steve has his forums at GRC.com.  We have the TWiT Community Forums at TWiT.community.  There's also a TWiT Mastodon.  So if you want Twitter without the tweets, all the toots, none of the tweets, it's at TWiT.social is our Mastodon instance.



That concludes this thrilling, gripping edition of Security Now!.  Thank you, Steve.  Have a great week.



STEVE:  Thank you, my friend.  I will see you in September.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#887

DATE:		September 6, 2022

TITLE:		Embedding AWS Credentials

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-887.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Google's just announced and launched open source software vulnerability rewards program.  We ask the question whether TikTok leaked more than two billion of their users' records.  We look at Chrome's urgent update to close its sixth zero-day of 2022 and at a worrisome "feature"  I think it a bug!  in Chrome.  A somewhat hidden autorun facility in PyPI's pip tool used for downloading and installing Python packages is being used to run malware.  And we examine a recent anti-quantum computing opinion from an Oxford university quantum physicist.  Then I have two bits of miscellany, three pieces of listener feedback, a fun SpinRite video discovery, and my discovery of a wonderful and blessedly prolific science fiction author.  And after all that, we look at the result of Symantec's recent research into their discovery of more than 1,800 mobile apps which they found to be leaking critical AWS cloud credentials, primarily due to carelessness in the use of today's software supply chain.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Did that TikTok hack really happen?  Steve says, yeah, probably not.  There's a new Chrome zero-day you definitely will want to patch.  An Oxford University physicist says quantum computing is bogus.  And then Steve will reveal his brand new favorite science fiction author and a new 20-volume series, plus a look at why people are embedding AWS credentials in their apps.  It's a bad idea.  All coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 887, recorded Tuesday, September 6th, 2022:  Embedded  AWS Credentials.



It's time for Security Now!.  Get ready to protect yourself.  Put on your hard hat.  We've got some construction to do with Mr. Steve Gibson of GRC.com.



STEVE GIBSON:  Put your hard hat on your hard head.



LEO:  A hard hat with a propeller, if you have it.



STEVE:  You're listening to this podcast, you're not soft.



LEO:  Line it with tinfoil, you'll be set.



STEVE:  So here we are at the beginning of September, Security Now! Episode 887.  And, okay.  So I didn't have a topic for most of the setup for this until I ran across a report that Symantec just published about their findings - okay, I should finish that thought - their findings for the cloud security of mobile apps.  And I was a little confused because they were talking about supply chain.  And I thought, how is mobile app security about supply chain?  And now I get it, and it's really interesting.  So it became - so I moved things around.  I was going to start talking about this grumbly physicist from Oxford who doesn't think that - he's a quantum physicist who doesn't think that this quantum computing stuff is ever going to come to anything.



LEO:  Oh, music to my ears.  I've been grumbling about this for a long time.



STEVE:  I know.



LEO:  And I'm no physicist, so that's - yeah, yeah.



STEVE:  No.  But Leo, when factoring 33 is an achievement...



LEO:  Yeah, exactly.  Exactly, yeah.



STEVE:  Okay.  Anyway, so that got moved to the end of our discussion of all the other stuff, and Symantec's story now takes the lead, which puts it at the far end.  But we've got a lot of other cool things to talk about first.



So we are going to take a look at Google's just announced and launched latest open source software vulnerability rewards program.  We ask the question whether TikTok leaked more than two billion of their users' records.  We look at Chrome's urgent update to close its sixth zero-day of 2022, and at a worrisome, uh, feature?  I think it's a bug in Chrome.  I have a little demo for our listeners which is a little bit unnerving.  Everyone can do it.  I also have a somewhat, well, news of a somewhat hidden autorun facility in PyPI's pip tool, which is used for downloading and installing Python packages.  That feature is being used to run malware, to the surprise of people using pip.  And as I said, we're going to examine a recent anti-quantum computing, we'll call it an "opinion" - it borders on a rant - from an Oxford University quantum physicist.



Then I've got two bits of miscellany, three pieces of listener feedback, a fun SpinRite video discovery from this morning, and I'm most frankly excited about this than anything else:  my discovery of a wonderful and blessedly prolific sci-fi author.  And after all that, we're going to look at, as I said, Symantec's research into their discovery of more than 1,800 mobile apps which they found to be leaking critical AWS cloud credentials, primarily due to the carelessness in the use of today's software supply chain.  So I don't think our listeners are going to be bored.



LEO:  No.  In fact, I've been waiting for this show all week.  There's so much to talk about.  The funniest thing happened on the radio show.  On Sunday, somebody called and said...



STEVE:  Oh.



LEO:  You already know what I'm talking about.



STEVE:  The false positive?



LEO:  Yes.



STEVE:  Oh, no.



LEO:  And I said, what?  Yeah, because apparently a lot of Windows users suddenly thought they were, in fact all Windows users suddenly thought they were infected.  Anyway, just so many stories, so little time.



STEVE:  Okay.  So this is just so classically xkcd, I love it.  It's titled "General Physics Safety Tip."  And it's a very small flow chart, very simple flow chart which is designed to help you decide where to stand.  It answers the question and poses it, "Should I stand near this thing?"



LEO:  It's a simple question, but an important question.



STEVE:  Oh, my god.  Well, yes.  And then so it poses that question in the top box, which feeds into the decision triangle, or I'm sorry, the decision diamond, where the way to answer the question "should I stand near this thing" is to ask, well, "Are physicists excited about it?"  And if the physicians are not excited about it, then maybe you should stand near it.  If physicists are excited about it, then, no.  Do not get...



LEO:  No.  No, definitely not.  Get away.  Run.  Uh-oh.  Steve froze.  "In general," says Retcon5, "avoid exposure to any temperatures, pressures, particle energies, or states of matter that physicists think are neat."  I love it, Steve.  That's awesome.  Randall is so good.  You know, I interviewed him on a Triangulation some years ago.



STEVE:  I'm so glad you did.  And I was thinking, like, this is the kind of stuff that we would never have were it not for the Internet. 



LEO:  Yes.



STEVE:  I mean, the Internet brings all kinds of crap.



LEO:  God bless the nerds, yes.



STEVE:  You know, it brings all kinds of crap along with it.  That's just inevitable.  But like this, before the Internet we had magazines that were monthly.



LEO:  Right, right.  But they weren't up to date.



STEVE:  No.  And the New Yorker would have some great cartoons that would like stand out.  But you didn't have like a - you couldn't get an IV of this stuff.  This is just...



LEO:  Yeah.  Randall Munroe, it was back in 2019, Triangulation 412, if you haven't heard it yet.  Great interview with the creator of xkcd.  Yeah, I agree.  Thank goodness the Internet's given us some real nice things, and that's one of them.



STEVE:  And you think about it, like, the bounds and the depth of his creativity, like where did this come from?  Right?  I mean, like he could easily think, well, I don't have any good ideas, and then here's like this, which is just wonderful.



LEO:  Right.  And in days gone by, where would you go?  Would you go to a newspaper and say I'd like to publish this comic?  And they go, no, no one will understand this.  



STEVE:  Right.



LEO:  This guy with his genius capabilities probably would be underemployed and no one would know about him.  So, yeah, yeah, it's great.



STEVE:  It's why when people have asked me, like, for some career advice, I've said to them, I would specialize.  That is, become, invest however many tens of thousands of hours are necessary to become the best something.  And it doesn't matter what.  Well, I mean, it has to have some application.  But the point is...



LEO:  Well, not now, with the Internet, not even then.  Not even then.



STEVE:  No, that's true.



LEO:  You know?  If a thousand other people might be interested, that's enough.



STEVE:  Yes.  And the idea is that, if you were like just the unbelievable best plumber, well, in the old days, well, all of the toilets in your city would be working now.



LEO:  You'd own them, yes.



STEVE:  But you wouldn't have much marketing reach beyond that.  Now, you know, the Vatican might hire you because they really need your help.



LEO:  Exactly.  I think that's brilliant, Steve.  Yes.



STEVE:  And they could find you.  That's the point.



LEO:  Exactly.



STEVE:  They would be able to find - they'd just put into Google, who is the best plumber?  And up would come Maury Mergensteen.  And they'd go find him and say, Maury, we need some help here with the Pope's commode.



LEO:  Actually, you just told the story of Father Robert Ballecer.  That's, in a nutshell, the secret to his success.  So there you go.



STEVE:  Yeah.  Okay.  So last week Google announced and launched a new Google-targeted Open Source Software Vulnerability Rewards Program.  I say it's Google targeted because it's for their open source software project, but okay.  Their money's good.  So this is what they said.  And yes, it's a little bit commercial, but it's still interesting.  And after all, they're giving away money.



So they said:  "Today, we are launching Google's Open Source Software Vulnerability Rewards Program" - which, because it's a mouthful, you can shorten as OSS VRP - "to reward discoveries of vulnerabilities in Google's open source projects.  As the maintainer of major projects such as Golang, Angular, and Fuchsia, Google is among the largest contributors," they write, "and users of open source in the world.  With the addition of Google's OSS VRP to our family of VRPs, researchers can now be rewarded for finding bugs that could potentially impact the entire open source ecosystem.



"Google," they write, "has been committed to supporting security researchers and bug hunters for over a decade.  The original VRP program" - or just VRP - "established to compensate and thank those who help make Google's code more secure, was one of the first in the world and is now approaching its 12th anniversary.  Over time, our VRP lineup has expanded to include programs focused on Chrome, Android, and other areas.  Collectively, these programs have rewarded more than 13,000 submissions, totaling over $38 million paid."  That's pretty good, on average.



They said:  "The addition of this new program addresses the ever more prevalent reality of rising supply chain compromises."  And actually we're going to be winding up talking about supply chain compromises of a different ilk.  They said:  "Last year saw a 650% year-over-year increase in attacks targeting the open source software supply chain, including headliner incidents like Codecov and of course Log4j vulnerability that showed the destructive potential of a single open source vulnerability.  Google's OSS VRP is part of our $10 billion commitment to improving cybersecurity, including securing the supply chain against these types of attacks for both Google's users and open source consumers worldwide."



And finally, "Google's OSS VRP encourages researchers to report vulnerabilities with the greatest real and potential impact on open source software under the Google portfolio.  The program focuses on all up-to-date versions of open source software, including repository settings, stored in the public repositories of Google-owned GitHub organizations" - so Google, GoogleAPIs, GoogleCloudPlatform, and so forth.  And, and this I thought was surprising and significant, "those projects' third-party dependencies, with prior notification to the affected dependency required before submission to Google's OSS VRP."



So if you find a problem not only in Google's own stuff, but in something they're pulling into their project and are dependent upon, that qualifies, too.  So you fix it there, and then you say hey, Google, I found something that just helped you.  Pay up.  So, and they finish:  "The top rewards go to vulnerabilities found in the most sensitive projects."  Oh, I did get a kick out of this.  "Depending on the severity of the vulnerability and the project's importance, rewards will range from $100 to $31,337."  And of course 1337 is hacker-esque, it's LEET upside down.  So, yes, a little tip of the hat to the hacker community.



Anyway, so I've got links to where to go to find out more information.  It's generally at bughunters.google.com.  You can start there and then browse around.  They've got rules for qualifications and so forth.  But overall, what occurred to me as I was reading through this and choosing to include it to start off today's podcast, is the degree to which bug bounties have become a part of today's modern software ecosystem.  It's no longer a surprise for a company to be offering a bug bounty for the discovery and responsible reporting of problems found in their software.



But it wasn't so long ago that this was unheard of, or that it was an enlightened exception.  Today it's the way large companies do business.  They figure if we can't find all of the important bugs ourselves, and what we're seeing is that apparently no one can, then you reward the white hats who do.  And more significantly, as we've seen, this sort of bug hunting, while not guaranteeing a steady paycheck, at least not at the start, could increasingly be considered a valid and workable career of sorts.



So very cool that, I mean, I'm glad to see Google's doing this.  They've got the money to do it.  They're going to get the benefit.  And again, they've obviously got a very capable stable of internal security researchers and bug hunters.  Yet even so, they're saying we'll take any help we can get, which is really the mature thing to do in this day and age.



LEO:  It's also a defensive move because, as we've talked about many times, there's places like Zerodium offering big bucks for  Google flaws, as well.



STEVE:  Ah, and not fixing the packages, but selling them to, yes...



LEO:  To bad guys, yeah.



STEVE:  To the state-sponsored, yes, bad guys.  Yeah.  Good point.  So did TikTok leak 2.05 billion, with a "b," user records?  TikTok says no, but other independent researchers are not so sure.  Every week while I'm looking through the past week's news, half, really half of what I see are breach reports.  You know, this or that company reports that it was breached, and bad guys may have obtained the data for typically a couple hundred thousand of their users, you know, more or less.  And okay, that's not good.  But there's generally not much more to say about it.  It's a bit like this or that company got hit by ransomware.  Not good.  We're sorry.  Hope you recover.  But the potential exposure of more than two billion user records by TikTok, well, we're talking about this one because of who it is, and the size, scale, and impact of the possible breach.



When news of this appeared last week, TikTok pressed their "Respond to the press" button, and out popped a statement reading:  "TikTok prioritizes the privacy and security of our users' data.  Our security team investigated these claims and found no evidence of a security breach."  Then the button popped back out.  But TikTok's canned-sounding denial follows reports of a hack that surfaced on the Breach Forums message board Saturday, with the threat actor noting that the server holds 2.05 billion records in a single humongous 790GB database.



The hacking group known as BlueHornet, also known as AgainstTheWest, or ATW, tweeted:  "Who would have thought that TikTok would decide to store all their internal backend source code on one Alibaba Cloud instance using a trashy password?"  Bob Diachenko, who's known as the open database hunter, and he's a threat intelligence researcher at Security Discovery, he said the breach is "real," in quotes, I mean, like that's exactly what he said, "real," and that the data is likely to have originated from "Hangzhou Julun Network Technology Co., Ltd. rather than TikTok."  But Troy Hunt wasn't yet convinced.  Troy tweeted:  "This is so far pretty inconclusive.  Some data matches production info, albeit publicly accessible info.  Some data is junk, but it could be non-production or test data.  It's a bit of a mixed bag so far."



And then just before I put everything together, I checked both of their Twitter feeds for any updates.  Bob Diachenko's feed had two updates.  The first said "Update:  While there is definitely a breach, it is still work in progress to confirm the origin of data, could be a third party."  That was followed up sometime later with "Update 2:  Okay, #TikTokBreach is real. Our team analyzed publicly exposed repos to confirm partial users' data leak."



But then Troy retweeted a tweet and added:  "The thread on the hacking forum with the samples of alleged TikTok data has been deleted and the user banned for 'lying about data breaches.'" The group that was banned, as explained in the tweet that Troy retweeted, was that same BlueHornet/AgainstTheWest who made the original allegation.  So it appears that the whole thing was, as TikTok originally claimed, not a breach at all.



Though this story sort of cancels itself out, I wanted to share this because it's a great example of what is continually going on behind the scenes in the security industry.  Not all players are on the up and up.  And not everything - surprise - that's tweeted is factual.  And Troy, for whom this is certainly not his first rodeo, knows to remain skeptical until all the facts are in and proven.  So...



LEO:  Yeah.  You know, we have to deal with this all the time.  A lot of other publications hoping to get the links will go with this.  When I saw the story, one of the first people I looked at was Troy Hunt.



STEVE:  Yup.



LEO:  His skepticism immediately stopped me cold.  It's very easy for some jerk to claim this, get a lot of attention.  And worse, a lot of publications will just jump on it because they know that's how you drive traffic.  So as any journalist, you've really got to be - we're very skeptical about all this stuff until it's proven to be the case.  And we do not jump on these things.



STEVE:  Yeah.  And in fact even Brian Krebs, you know, who is a well-known great journalism reporter within this whole infosec...



LEO:  I'm not going to say "great" anymore.  Go ahead.  Keep going.



STEVE:  Okay.  Well, what I was going to say...



LEO:  Because he got in big trouble with this Ubiquiti thing.



STEVE:  And that's exactly what I was going to say, was that he did, like, say, you know, I had a single source. I went with it.  And I apologize to Ubiquiti and had to retract it.



LEO:  Because he got sued, and it's pretty much widely believed that this was part of the settlement with Ubiquiti.  And in fact he would have lost because he knew the guy was a hacker and continued to run the story when he should have immediately said, "I was wrong.  This guy was the guy who perpetrated the hack.  He used me to, basically, to extort Ubiquiti."  And in fact, you know, yeah.  So I think Brian, I'm sorry, but Brian's got a black mark in my book now.  I'm not sure I'm going to fully trust him going forward.



STEVE:  Yeah, little bit too much inertia behind the story, I guess.



LEO:  Again, a case of I think somebody very anxious to get clicks, and as a result not being very thoughtful.



STEVE:  And unfortunately, you know, if clicks are your model, then...



LEO:  You have to.



STEVE:  ...it's really tough to say, uh, whoops.  That's not what happened.



LEO:  Yeah.  So Krebs has fully retracted now, taken all those stories down.  But in the kind of anodyne language that tells me this is part of the settlement, you know.



STEVE:  I see.  So not a huge mea culpa.



LEO:  No.



STEVE:  Just, okay.  Well, an urgent Chrome update required and got an urgent patch.  Last Friday, Chrome bumped up to version blah blah blah ending in .102 to urgently close a vulnerability that was being actively exploited in the wild.  As usual, little more is being said of CVE-2022-3075, which involves, all we know is insufficient data validating in Mojo, which is a library of routines to provide platform-agnostic interprocess communication.  Google credited an anonymous researcher with their report of the high-severity flaw on August 30th.  And I'm again impressed by the Chromium team's three-day incident response.  To learn about it on the 30th and push it out on Friday the 2nd, that's great.



So this is zero-day number six for the year in Chrome.  And while it's not likely to be an emergency for everyone, everyone as always is advised to be sure they're running that version ending in .102.  The update applies to the desktop versions of Chrome for Windows, macOS, and Linux.  And as always, the users of Chromium-based browsers Edge, Brave, Opera, and Vivaldi are also advised to look around for updates for theirs when they're available.



Okay, now, Leo.  If you've got a version of Chrome around, you want to go to webplatform.news, W-E-B-P-L-A-T-F-O-R-M dot news.  And what comes up is an innocuous-looking page.  But it just puts something on your clipboard without your permission.  So you now open like Notepad or something and hit CTRL+V to paste.  And you will see a message reading:  "Hello.  This message is in your clipboard because you visited the website Web Platform News in a browser that allows websites to write to the clipboard without the user's permission.  Sorry for the inconvenience.  For more information about this..."



LEO:  Wow.



STEVE:  Yeah, huh.



LEO:  No, it did not happen to me.  I'm not using Chrome.  Does this happen on all browsers?



STEVE:  I tried it on Edge; and, yes, it's got to be a Chromium-based browser.



LEO:  Ah.  Let me try it on Edge, okay.  I was using Firefox.



STEVE:  Not Firefox.  Ah, good for you because Firefox - now, Firefox has a related problem, and this wasn't clear to me as I was tracking this down.  So again, to all of our listeners, in a Chromium-based browser, Chrome or Edge, Brave, Opera, Vivaldi that I was just talking about, webplatform.news.  And then, like, open Notepad and hit CTRL+V to paste.  And you'll get a  happy little message planted onto your clipboard.



LEO:  Oh.  Right you are, my friend.



STEVE:  Yeah.  So hopefully to our listening audience it's needless to say this is not safe.



LEO:  Yeah.



STEVE:  And more than being unsafe, some consider it to be a major security issue.  The problem is that the browser's interaction with the clipboard is somewhat tricky.  And web developers have been tinkering around with it, considering the deliberate addition of some non-interactive access.  You know, it seems pretty clear to me that a user should have to clearly highlight and mark something on a web page, then explicitly issue some form of clipboard copy command for their browser to be given permission to modify their system's clipboard.



Now, and what it feels like to me is the web designers must be saying, hey, well, other native first-party OS apps are able to put something on the clipboard if they want to.  Why shouldn't a browser?  Why should it not be equally entitled?  Well, the answer is, you know, browsers are out hitting random pages, pulling ads in from god knows where, running scripts from who knows who.  And all of that should have the ability to put stuff on your desktop clipboard without your knowledge or permission?  I don't think so.



So, okay.  So, you know, I could see the benefits of having a web page announce that something has already been placed on the user's clipboard, you know, if that was like clearly in their benefit; if they really wanted that to happen.  But unfortunately it offers bad guys far too much opportunity for carnage.  And there appears to be some lack of clarity on this front.  Web developer Jeff Johnson said that what he's calling the "clipboard poisoning attack" was accidentally introduced in Chrome version 104.  Okay, and I don't know what 104 he's talking about.  We just got 102, and that's what I'm running, and it's in there.



But looking over the pertinent Chromium discussion thread actually kind of muddies the water.  I have a link in the show notes to the discussion thread.  But, for example, last Monday, that is, Monday before last, not yesterday, eight days ago, from Microsoft on August 29th, a Microsoft Edge person using the Chromium Engine says it's pri-3, which I guess means Priority 3, because this behavior has been there since we shipped async clipboard APIs.  It is not a regression.  However, I agree that this should be fixed.  What?  Okay.  So "not a regression" means it isn't something that we broke.  But this guy is saying I agree this should be fixed, and we should send a breaking change email to blink-dev to figure out the right process to add the transient user activation restriction to the APIs.



And then he says:  "I guess pri-1 makes sense since Firefox and Safari are also considering adding the transient user activation," and then "(instead of a user gesture requirement)," which is what they have now.  The reason, Leo, it didn't happen to you under Firefox is due to the deliberate presence of something they're calling the "user gesture requirement."  Meaning you have to do something in order to, like, enable this event.



LEO:  Now, it is writing, not reading.  I mean, reading the clipboard...



STEVE:  Correct, correct.



LEO:  ...would be very problematic.



STEVE:  Correct.



LEO:  What is the hazard of writing to my clipboard?



STEVE:  Okay.  So...



LEO:  I can see the usefulness of it.



STEVE:  Yes.



LEO:  But I don't know what the problem is.



STEVE:  So Jeff says, for example, while the problem exists in Apple, Safari, and Mozilla Firefox, as well, what makes the issue more severe in Chrome, this is a requirement for a user gesture to copy content to the clipboard is currently broken.  Jeff appears to be - okay.  So I had this here somewhere.  He says, oh, the idea that the danger of this is not glaringly apparent to the web developers is a bit surprising.  So, okay, I sort of got thrown off here.



LEO:  I apologize.  I shouldn't interject.  I'm sorry.



STEVE:  Because I did, yeah, I have that covered here.



LEO:  Okay.  Go back to your - you can answer - put a pin in it and get to that.



STEVE:  So we're going to get to your question.



LEO:  Thank you.



STEVE:  So last Tuesday on August 30th the Chromium guy in this thread said, "To be clear, READING" - he had it in all caps - "from the clipboard always requires a permission," he says, "very much like geolocation, microphone, et cetera."  And he said, "To see what this looks like, check out this demo site."  And there's a link to async-clipboard-api.glitch.me.  Then he says:  "Writing plain text or images to the clipboard can currently be accomplished without a permission or user gesture, although the site and tab in question must be foregrounded."  He says:  "We are looking to tighten up the security model here."



And he says:  "Neither one of these behaviors has changed recently, nor does the new tab page test rely on permission-less, gesture-less clipboard access."  Anyway, so at this point the web developer who thinks this is an issue, Jeff Johnson, says that Safari and Firefox are considering making this smoother, not requiring there to be a gesture.  So Jeff appears to be saying that Safari and Firefox require the user to have some interaction with a page, though not necessarily a clipboard copy.



He does say that clicking a link or pressing the arrow key to scroll down gives the website permission to overwrite your system clipboard.  So you don't even need to do - you just need to have any interaction with the page in order for that permission currently in Safari and Firefox to be given.  And they're considering synchronizing themselves with Chromium where even that's not necessary.  So although the Chromium guys are saying, uh, and even the Microsoft guy is saying, maybe we'd better rethink this. 



So the idea that the danger of this is - okay.  So blah blah blah blah blah.  Okay.  So aside from being annoying and worrying, we have the ability of any web page to replace my system's clipboard data without my permission.  Okay.  If I were using my computer, and I had something on the clipboard, and then I went to paste it somewhere else, and I got some message, I got some, like, image or text that I had never seen before, I would be sure that my machine had been infected with malware of some kind that had messed with my clipboard without my permission.  Because I would be freaked out.



LEO:  That's somebody who doesn't understand how the clipboard works because it happens all the time.  Password managers wipe the clipboard so your clipboard can't be read with the password on it.  It's not at all unusual for clipboards to have multiple different versions of the content, depending on where you're pasting, and do content-aware paste.  So you might get an image in some case, you might get a text in another case.  The clipboard is often manipulated by the OS.  This is not at all unusual.



STEVE:  So you're saying if you pasted the contents of your clipboard, and it was something you had never seen before, and you had never pasted into the clipboard, that wouldn't concern you?



LEO:  Well, yeah, I mean...



STEVE:  Come on.



LEO:  I guess it shouldn't be something random.  But clipboards are often manipulated by the operating system.  That's not unusual.



STEVE:  By you.  By user hitting edit copy.



LEO:  No, no, no, no, no.  No, no, no, no, no.  I mean, LastPass wipes the - how many times do you have a password on your clipboard that gets wiped after 10 seconds, or you set the time.  It gets wiped automatically.  There's things happen to your clipboard all the time.  And as I said, clipboards have, at least on a Mac, probably not on Windows...



STEVE:  Okay.  So you're saying you would have no problem if random pages that you visit are writing to your desktop clipboard.



LEO:  Well, usually they do that for utility; right?  So they'll paste something in there that you're going to want - a link or something that you're going to want to paste somewhere else.  So they'll try to do it for utility.  That's why this feature exists.  All right, so it's scaring you.  I got it.  Is it hazardous?



STEVE:  Certainly.  You could imagine that a site or page manipulates you so that you have a cryptocurrency address on your clipboard, which the web page changes behind your back without you knowing it.



LEO:  That's a good - there's a good malicious use.  But we say all the time you shouldn't trust your clipboard; right?  I mean, we say that all the time.



STEVE:  Okay.  So the issue is no user permission.  I mean, no user action at all.  You go to The New York Times, and an ad on The New York Times...



LEO:  Is put on your clipboard, yeah, yeah.



STEVE:  No, puts anything it wants.



LEO:  Right.



STEVE:  Not the ad.  It puts anything it wants in your clipboard.  It just, to me, I mean, the good news is, now that this has come to light, the web designers are saying, oh, okay, we need to do something.  I'm sure that they thought it was cool that you'd be interacting with a web app, and the web app would say, okay, we're all finished doing what you wanted.  The results are waiting for you on your clipboard.  I don't have any problem in using Google Docs and marking something and then hitting CTRL+V, and it gets pasted to my clipboard.  That's what I want.



But I have a problem if I go to some website, and without knowing - because I'm like you, Leo.  I mean, I'm using my computer.  I know what's on my clipboard.  And typically I'm copying and pasting things.  And yes, I've often had the experience, sometimes it's annoying, in fact, that Last Pass has erased a password before I was able to paste it somewhere.



LEO:  It's always annoying.  Right.  But it's for good reason.



STEVE:  Yeah.  And so I go, okay, yeah, fine.



LEO:  Yeah, no, I think, okay, yeah, I can see the hazard here.  That was a perfect example of, you know, because you can't really - a crypto account number is so long, you might not remember the one you had cut, and instead put a different one in.  That would be a big problem.



STEVE:  And normally you don't even attempt, it's like a long password, you don't even attempt to memorize it.



LEO:  To type it, yeah.



STEVE:  And then make sure that it...



LEO:  So we use copy-and-paste for something like that all the time, yeah, that's a good point, yeah.



STEVE:  Right.  So anyway, the good news is that these guys are looking at it.  I just thought it was a little jarring to just go to a page and have it change our clipboard without us giving permission.  And so to me it's - just because, I mean, yes, it's true that any app that we have on our desktop could do this.  But we would consider it a misbehaving app if it was changing our clipboard in a way that didn't benefit us.  And we know what's on the Internet, you know.  I would say way less than half of it benefits us.  So I don't want, you know, pages that I happen to encounter or components of pages like an ad to run some script that changes my clipboard.  That's just like, hands off my clipboard.



LEO:  I think there's utility, and that's why they put it in.



STEVE:  Yes.



LEO:  But I can see that potential hazard, yeah.



STEVE:  And, yeah, they're wanting it to be a first-class citizen of the desktop.  And my only reservation is, not everything that lands in my browser is something that I want to trust my desktop with.



LEO:  You actually raised probably the real reason why Google did this:  Google Docs.  Which is a browser-only experience, but they want it to be like a desktop app; right?  Or Gmail.



STEVE:  Yup, yup, yup.  And I use the crap out of the Docs cross-clipboard ability.  In fact, oh, one of my biggest peeves is that you cannot copy an image out of Google Docs and move it somewhere else.  It, like, refuses to let you have it for some reason.  It just drives me nuts.  But you have to jump through hoops to do that.  But anyway.



Speaking of unwanted features - oh, actually, speaking of my throat being dry.



LEO:  I've got a wanted feature.  Another fine commercial.



STEVE:  We weren't speaking of that because I can't speak.  PyPI is the Python language's popular Package Index repository, inviting alliteration.  Python Package Index, thus PyPI.  And in another discovery that would expose developers to increased risk of a supply chain attack, it was found that nearly one-third of the packages in PyPI trigger an automatic code execution upon downloading them.  Now, sort of as with auto-copying to the user's clipboard, if it's what the user wants, then fine.  Having a Python script autorun after downloading a well-designed and benign package, well, that's just convenience; right?  It just, like, if you say "pip install," you want it to do that.



But a researcher at Checkmarx noted in a technical report they published last week that a worrying feature in pip's command allows code to automatically run when developers are merely downloading, not necessarily installing, a package.  He added that the feature's alarming because, he said, "a great deal of malicious packages we are finding in the wild use this feature of code execution upon installation, or download, to achieve higher infection rates."  Yeah, that would follow.



Anyway, one of the ways by which packages can be installed for Python is by executing the "pip install" command, which in turn invokes a file called "setup.py" which comes bundled along with the module.  "Setup.py," as its name implies, is a setup script that's used to specify metadata associated with the package, including its dependencies.  It provides some of the welcome automation that makes package management convenient in this environment.  And in what amounts to a documentation flaw, meaning that it was undocumented and actually surprising, a cautious user might opt to use the safer appearing pip download command, since its documentation states:  "Pip download does the same resolution and downloading as pip install; but instead of installing the dependencies, it collects the downloaded distributions into the directory provided, which defaults to the current directory."



In other words, the command can be used to download a Python package without having it installed on the system and all of its dependencies.  But as it turns out, executing the download command also runs the embedded "setup.py" script, resulting in the execution of whatever malicious code it might contain.  It turns out that "setup.py" autorunning only occurs when the package contains a tar.gz file instead of a wheel file, which is .whl.  Although pip defaults to using wheels instead of tar.gz files, attackers take advantage of this behavior to intentionally publish Python packages, you know, malicious Python packages, obviously, without a .whl file, leading to the execution of the malicious code present in the setup script.  The Checkmarx report noted that:  "When a user downloads a Python package from PyPI, pip will preferentially use the .whl file, but will fall back to the tar.gz file if the .whl file is not present."



At the moment, there's not much that Python users can do.  If pip is used to either install or download a PyPI package, bad guys can arrange to run their "script.py" on the user's machine.  And given all the recent troubles with supply chain attacks, that's a bit nerve-wracking.  So anyway, I just wanted to bring that to our listeners' attention.  Python is popular and only becoming more so for many good reasons.  And so this looks like a problem that needs to get addressed somehow.



LEO:  Yeah.  This is also a problem with some Linux distros.  Arch, for instance, has a user repository that if you - you don't have to, but many people use an Arch User Repository installer that will run the script.  And the better ones say, no, no, you've got to look at the script before you run it.  And there are plenty of sites you go to, and you install, you know Homebrew is one for the Mac and Linux.  It's a package manager.  And you install it with a curl command.  You know?



STEVE:  Right.  Ohhh.



LEO:  And, you know, they even say, you know, we know this is a terrible way to do it.  But, you know.



STEVE:  But here's the command.



LEO:  Here's the command.



STEVE:  It's easy.



LEO:  It's a lot easier.  



STEVE:  Oh, goodness.  Yeah.  We had some fun years ago talking about dangers of curl.  Whoa.  Well, and there have been some vulnerabilities in it, too.  Okay.  So we've been talking about quantum computing recently.  Even though the capability to break our current public key cryptographic security protocols remains purely theoretical, and I mean entirely theoretical, the existence of technology that could do so could render the asymmetric cryptography which we depend upon to manage our symmetric keys useless for that purpose.



In practice, the security of anything and everything we currently protect with certificates, and the public handshakes we make to negotiate secret keys, would be open to circumvention and abuse.  Astonishing to me as it is, although bombs are not flying through the air between hostile superpowers, there is clearly very active continuous and slowly escalating cyberwarfare being conducted among and between the world's hostile superpowers.  The only thing keeping this under control and at parity is that none of these superpowers has meaningful superiority in cyberspace or, as Leo would never say...



LEO:  Don't do it.  Don't do it.  Don't do it.



STEVE:  In cyber.



LEO:  <yelping>



STEVE:  If quantum computing's promise is realized, someone will have it first.  And that someone will have a massive destabilizing power over the entire rest of the world.  The degree to which we depend upon the stabilizing force of today's status quo should not be overstated.  I don't think you can overstate it.



And it's for this reason that researchers in cryptography, armed with an understanding of what a future working quantum computer might be able to do, they've already been at work, as we know, we've talked about it recently, for several years on the design and implementation of next-generation so-called "post-quantum" replacement cryptography.



The website "Futurism" runs a column called "The Byte," and last Saturday's title was:  "Oxford physicist unloads on quantum computing industry, says it's basically a hype bubble."  Now, it would be a full-time job to know everything about what's going on in quantum computing.  And I suppose that's this guy's job.  And that's good because I'm already overbooked.  At the same time, I don't know anything about what biases this guy might have.  He certainly seems disgruntled.  You know, maybe he applied for some grant and didn't get it.  I don't know.  But I found the synopsis of what he wrote to be interesting and worth sharing.



So this was in "The Byte" column on futurism.  They wrote:  "Oxford quantum physicist Nikita Gourianov tore into the quantum computing industry this week, comparing the 'fanfare'" - they have in quotes because that was his word - "around the tech to be a financial bubble in a searing commentary piece for the Financial Times."  Now, Financial Times is behind a high paywall or I'd have gone to the source.  But I tried, and I couldn't get there.  They said:  "In other words, he wrote, it's far more hype than substance.  It's a scathing, but also perhaps insightful, analysis of a burgeoning field that, at the very least, still has a lot to prove.  Despite billions of dollars being poured into quantum computing, Gourianov argues, the industry has yet to develop a single product that's actually capable of solving practical problems."



And, now, he doesn't even mean crypto.  Crypto turns out to be an extremely high bar because you can't have any fuzziness.  And fuzziness seems to be a side effect of quantum, at least now.  Anyway, he says:  "That means these firms are collecting orders of magnitude more in financing than they're able to earn in actual revenue," he says, "a growing bubble that could eventually burst."  Gourianov wrote for the Financial Times:  "The little revenue they generate mostly comes from consulting missions aimed at teaching other companies about 'how quantum computers will help their business,' as opposed to genuinely harnessing any advantages that quantum computers have over classical computers."



Okay, now, for my part, while I think this is an interesting opinion from someone whom others apparently believe knows something about quantum physics, I'll just note as a counterpoint that something is impossible, right up until the time it isn't.  And this doesn't guarantee that that something will ever not be impossible.  But when the stakes are as high as they are, this sort of tax-deductible research is easy to justify to a company's board of directors.



Anyway, Nikita Gourianov went on to say:  "Contemporary quantum computers are 'so error-prone that any information one tries to process with them will almost instantly degenerate into noise,' which scientists have been trying to overcome for years."  He also took aim at other assumptions about the field, arguing that fears over quantum computers being able to crack even the most secure cryptographic schemes are overblown.  And notably, Gourianov's rant in the Financial Times comes just weeks after a group of researchers found that a conventional computer was able to rival Google's Sycamore quantum computer, undermined Google's claim in 2019 of having achieved "quantum supremacy."



Recalling the sentiment "There's gold in them thar hills," despite the industry's lackluster results to date, investors are still funneling untold sums into quantum computing ventures.  You know, yeah, because what if?



Gourianov said:  "In essence, the quantum computing industry has yet to demonstrate any practical utility, despite the fanfare."  He says:  "Why then is so much money flowing in?  Well, it's mainly due to the fanfare."  The money, he argues, is coming from investors who typically don't have "any understanding of quantum physics," while "taking senior positions in companies and focusing solely on generating fanfare."  So in short, Gourianov believes it's only a matter of time until the quantum bubble will pop, and then the funding will dry up.



So anyway, I wanted to share this presumably informed perspective, since many tech media outlets covered this rant in the Financial Times - the Financial Times is well regarded - because this Oxford University quantum physicist may know what he's talking about, because on some level it does appear to fit the evidence, after all, and because it serves as an interesting counterpoint to what does indeed, despite huge expenditures, still seem to be quite a long ways off, if it is even ever practical.  So will this quantum crypto panic ultimately turn out to have been misplaced?  Maybe.  But the stakes are clearly so high that a great deal of wealth is being transferred.



And Leo, as I said at the top, if factoring the number 33 is considered a huge achievement just recently, and if as Gourianov appears to believe this particular branch of quantum physics is not about to be visited by a breakthrough, then the crypto industry probably has time to get its post-quantum crypto right the first time.  And I think that's good.  Even if "quantum" never happens, the replacement of our aging quantum-unsafe crypto only makes sense.  You know?  Why not do it?



And since the replacement of everything we have now will take significant time, I'm very glad we're already working to determine what that replacement will be.  And it'll be interesting to see whether it is the replacement of pre-quantum crypto with post-quantum crypto that finally bursts the quantum hype bubble.  Maybe if like no one suddenly any longer has any cause to worry about achieving a crack in current crypto, then it's like, oh, well, okay, isn't weather forecasting already good enough?



LEO:  I mean, yeah, it doesn't hurt to come up with better crypto techniques.  I'm using...



STEVE:  No, exactly.  And we know how long it's going to take.  It's going to take forever.



LEO:  Yeah.  I'm using ECC and, you know, some other - like I use that, what is it, that 25519 now for my SSH keys.



STEVE:  Yup, yup.



LEO:  Doesn't hurt; right?



STEVE:  Well, those are all crackable. 



LEO:  Oh, right.  That's right.



STEVE:  Those are all, I mean, anything public, anything public key.



LEO:  Today.  



STEVE:  So elliptic curve is public key.  25519 is public key.  Anything public key today.  And so that's why I think we do need to, like...



LEO:  Yeah.



STEVE:  And we know.  We know how long it's going to take.  It's going to take forever to replace what we have.



LEO:  Right.  This is a common problem in general for people who cover technology.  Fusion's hard. 



STEVE:  Yes.



LEO:  Quantum's hard.



STEVE:  Like the easy problems have been solved.



LEO:  Yeah.  AI is hard.  Self-driving vehicles are hard.  Does it mean they're not going to happen?  Not necessarily.  But maybe not.  I think it's good to be skeptical.  Dvorak taught me that.  He thought everything was crap.  But you know what, that's actually, if you're going to pick a default position,  that's the most likely correct because most stuff is crap.  But then you're going to miss a few jewels, like he thought the mouse was a terrible idea.  And, you know, everybody remembers that.  Nobody remembers the 101 other things that he thought were crap that went away.  So it's something I deal with a lot.



You know, we're talking a lot now about augmented reality.  And I think I was right when I said 3D in movies and TVs was a terrible idea and was a gimmick.  I'm not sure about - VR I think is a gimmick.  AR I'm not sure about.  Quantum, you know, the real problem is, as you point out, there's a gold rush because governments are throwing money at scientists.  So of course they're going to say, oh, yeah.



STEVE:  Oh, you know why they are, too.  I mean, oh my god.



LEO:  Yeah.  If somebody does come up with it.  I would like, and I think they're starting to, but I would like to see them throw as much money at fusion.  Fusion could solve so many of our energy.  It'd solve all of our energy issues.  Today.



STEVE:  Yeah, yeah.



LEO:  But it's hard.  So is it never going to happen?  I don't know.  We don't know.  It's an interesting conundrum for the tech journalist.



STEVE:  So I have a couple bits of miscellany.  Black Hat, the organization, is somewhat notorious for being quite slow to release the materials after their conferences.  So an enterprising researcher has put them all up on Google Drive with open public sharing access.  He tweeted a link to the collection, and I've captured it in this week's show notes.  And to make it easy for those who don't want to track it down in this week's show notes, I also gave it a GRC shortcut of BH, for Black Hat, obviously, BH2022.  So if you go to grc.sc/bh2022, that'll bounce you over to Google Drive, and you will see a ton of PDFs containing all of the slides for all of the presentations for Black Hat 2022, which I just thought was cool.



LEO:  Yeah, these are great, too.  That's great.



STEVE:  Yeah.



LEO:  Thank you, Black Hat.



STEVE:  Okay.  Csurf NPM, the package manager library, has a mistake.  So the NPM-hosted JavaScript library named Csurf is an JavaScript library designed to protect applications from Cross-Site Request Forgery attacks, known as CSRFs.  You can see where the name Csurf, C-S-U-R-F, got its name.  So these are cross-site request forgery attacks.  Library protects JavaScript apps from that.  Unfortunately, researchers at the security company Fortbridge discovered a CSRF vulnerability in Csurf.  And unfortunately, the package apparently cannot be used to protect itself.  Since the project's authors have apparently decided it's not worth repairing, they chose instead to mark Csurf as "vulnerable and deprecated."  And I suppose, at the same time, as evidence of the need for it.  So there.



I didn't think I was going to have anything new to share about SpinRite today.  Work is proceeding well.  It's running, and I almost have all of the obvious problems fixed, which I created by basically rewriting most of it, except the UI stuff.  I may rearrange its real-time activities screen to make better use of its real estate as a consequence of other things that I've had to change on that screen.  You know, lots of field lengths had to change in order to make room for the number of sectors that drives now have and so forth.



But in any event, this morning I encountered a Twitter direct message from Matt Foot, one of our listeners with whom I exchange notes on Twitter.  He explained that he was searching for "How SpinRite works," and he encountered a surprisingly recent 2021 YouTube video showing a full demo walkthrough of SpinRite II, which used Roman numerals back then, running on a 20MB Seagate ST-225 drive.  I hadn't seen SpinRite II run in quite a while, so I wound up watching the entire 13-minute, well-narrated video.  The drive was initially actually in pretty bad shape, but SpinRite fixed it.  For anyone who's interested, it's this week's shortcut of the week, so grc.sc/887, which will bounce you over to a 13-minute YouTube video which is sort of interesting.  And thanks to Matt.



A little bit of closing the loop with our listeners.  Tyson Moore said:  "Hi, Steve.  Just listened to SN-886."  So that was last week.  He said:  "I have a different take on NIC LEDs" - you know, Network Interface Card or Controller LEDs - "you might find interesting."  He said:  "I worked for a large Canadian telco that had disabled activity lights on almost all of their switches and routers" - okay, which makes me shudder.  But he said - can you imagine that, Leo?  Cutting all the lights?



LEO:  Must have been very dark in there.



STEVE:  Oh, my god.  Well, you would think the power had failed.



LEO:  Right.



STEVE:  I mean...



LEO:  Oh, I use my lights all the time.



STEVE:  Oh.



LEO:  I need those act lights because I need - yeah.'



STEVE:  Yeah.  Yeah.  He says:  "...from small 1U 16-port access switches to 30U" - that's like 30...



LEO:  It's a whole rack.



STEVE:  19" rack U height, "...multi-terabit routers."



LEO:  Wow.



STEVE:  He said:  "On many devices, this was done through undocumented commands or special firmware."



LEO:  Wow.



STEVE:  He says:  "Though it made troubleshooting difficult" - yeah, do you think? - "the idea is that an adversary wouldn't be able to distinguish dormant or lightly used links from busy ones."



LEO:  Ah.  Okay.



STEVE:  Okay, "especially when faced with hundreds or thousands of options in a large telephone exchange."  He says:  "I was never fully sold on the usefulness of this measure, but as a defense-in-depth strategy I figure it couldn't hurt."  And I guess my feeling is, if you could like turn them on when you need them or, like, you know, have, like - then, you know...



LEO:  Yeah.  Because I look at the activity light to know if the printer's talking...



STEVE:  Oh, I can't imagine not having those little flickering lights.



LEO:  I know.



STEVE:  You absolutely need them.



LEO:  And the router and the modem you really need them.



STEVE:  Yes, to know what the hell is going on.



LEO:  Am I connected?  Am I not?  Yeah.



STEVE:  Yeah.



LEO:  I turn them off, though, on the WiFi access points because I don't like those little green lights all over the house.  But other than that, you know.



STEVE:  That's an interesting point.



LEO:  Yeah.  I like all those LEDs everywhere.



STEVE:  Okay.  So Tyson, thank you.  Also, okay, now, I removed this person's name because I felt I needed to be maybe a little critical, and I didn't want to embarrass him.  He said:  "Hi, Steve.  I enjoyed your discussion of the LastPass breach.  Whilst users' vaults are strongly encrypted and therefore presumably fairly safe, do you think there could be another risk in that conceivably the hackers could have introduced a backdoor into the LastPass code?  How can we know the code itself is still safe, given that hackers have had access to it for an unspecified amount of time?"



Okay, well, I think this is where the meaning of words matters. Since everyone is on the outside looking in, we don't know precisely what happened, and I doubt we ever will.  So we can choose to take their CEO's statement as fact or not.  If we do choose to accept it as fact, then exactly what he said matters.  He said, this is the CEO who I quoted last week:  "We have determined that an unauthorized party gained access to portions of the LastPass development environment through a single compromised developer account and took portions of source code and some proprietary LastPass technical information."  Okay, certainly "taking source code" is a world different from "may have modified source code."  We must assume that they have well-established source code controls in place, and that verifying the extent of the intrusion would have been their top priority.



LEO:  Yeah.



STEVE:  So again, if we choose to trust the CEO's statement, then there's no danger to us, and we can hope that they will respond to this by making anything like this from ever happening again much less likely.  And if we choose not to trust what the CEO said, then nothing he said matters anyway.



LEO:  And people may not know that a code repository isn't just a bunch of text files that you can change, and then they just...



STEVE:  Nobody will notice.



LEO:  Nobody will know.  There's a whole process, you know.



STEVE:  Right, right.



LEO:  And of course there's a complete log of everything that's been done.  They even, I think it's nice, they called the command "blame."  And you can see who is to blame for a particular commit.



STEVE:  Well, and how many times have we talked about like there's a problem, and then they'll go back, and they'll go, oh, that was Johnny when we were having that bad week in 1987 or something.  It's like, oh.



LEO:  Yeah.  Yeah.  You can see in any - blame is a git command. But almost in any...



STEVE:  There is an audit trail.



LEO:  Yeah, always you can see which lines were changed, who changed them, when.  All of that is completely transparent.



STEVE:  Right.  And on Saturday, Leo, Bill Crahen tweeted:  "Hey, Steve.  Just set up my Sandsara last night."  He said:  "Smaller than I imagined, but still fascinating to watch."  I don't have mine yet, but that means that they have shipped them.



LEO:  Woohoo.



STEVE:  And presumably I'll have a box on my porch, and you will, too.  I think I added, I think I asked for the addition of black sand, and apparently that really freaked out customs for some reasons.



LEO:  Yeah, because it looks like gunpowder.  I can see why.



STEVE:  Or like, you know, microcaviar or something.  I don't know.



LEO:  Can I have yellowcake on mine?  I think that would be the best.



STEVE:  And finally, one of our listeners tweeted:  "Just listened to SN-886.  I've been in the foo@duck.com beta for about a week now."  Meaning the email filtering that we talked about last week.  He says:  "It has worked as advertised and as you described.  I'm not seeing 85% of my email with trackers, more like 50 to 60.  It's equally surprising to me who is and who isn't using trackers."  And he didn't say more, but now I'm curious, like, oh.  I think I need to start routing some stuff through it and find out because that seems really cool to find out.



Okay.  And now the one thing I am more excited about than anything else.



LEO:  What?



STEVE:  Yes.



LEO:  This must be good.



STEVE:  I have a new sci-fi author, Leo.  Thanks to one of our listeners whose Twitter handle is "Laforge129" - and Laforge, thank you, thank you, thank you - I'm very excited to announce the discovery of a completely new and blessedly prolific science fiction author.  This author's name is pronounced Scott Jucha, which is spelled J-U-C-H-A.  His website domain is his name, so  scottjucha.com.



Okay.  So first of all, Scott can write, which is an endangered talent these days.  And he has a pleasantly expansive working vocabulary which is a joy to encounter.  He writes stories containing well-formed characters who, at every turn, do what you hope they're going to do, and then surprise you by exceeding your expectations.  Many of his Amazon reviewers, in their review, like their final review after they've finished the final book in his The Silver Ships series, state that this is the best space opera series they've ever read.  It would take a lot in my mind to compete with Ryk Brown's "Frontiers Saga," but there's room for a top two.



I received Laforge's tweet a week ago today, and since then I've read the first book.



LEO:  Oh, I know why you like this.  There's 20 of them.



STEVE:  Yes.



LEO:  Oh, my god.



STEVE:  Actually 24 because he did a short little four-book series that branches off after book 13.  So I needed, as I said, Leo, he's prolific.



LEO:  Yes.



STEVE:  He's also on Audible narrated by Grover Gardner.



LEO:  Oh, I love Grover Gardner.  Oh, good.  All right.



STEVE:  Okay.  So, okay.  So I read the first book.  Then I needed to see what was about to happen next, so I read just enough of the second book to relieve that pressure.  Then I went back and reread/skimmed the first third of the first book in order just to relive its key parts because now I knew what the author had in mind.  And at one point my wife Lorrie asked where I was in my reading, and I explained that I was re-reading book one.  She just shook her head and said:  "We are so different that way."  And so I explained/asked, I said to her,  "Haven't you ever seen a movie that was so good that you immediately watched it again, able to enjoy it even more since you knew how everything fit together?"  And that generated some more headshaking.



LEO:  I'm thinking Lorrie says life is too short to read more than once.  I'm guessing, but I don't know.



STEVE:  Yeah, exactly.  We actually encountered another couple, neighbor friend of ours, out on our evening walk yesterday.  And one of them said that, when I was explaining this, that she goes to the end of the book to see how it ends.



LEO:  And simplifies it.



STEVE:  And it's like, oh, my god.  Whoa.



LEO:  Just cut to the chase.



STEVE:  So anyway, 10 years ago, in 2012, Scott began writing.  In April of last year he finished his 24-novel series which he calls The Silver Ships.  All of the first five novels in the series were three times awarded Amazon's #1 Best Selling Sci-Fi book.  And they also won the #2 spot twice across multiple science fiction categories of first contact, spaceflight, and alien invasion.



LEO:  Well, my three favorite topics.  And there's asteroid mining, as well, just to round it out.



STEVE:  Oh, baby.  Actually the main character, Alex, is an asteroid miner in the beginning.  So the story is set in the far future.  It follows a number of Earth-descended colonies which encounter a very non-human, quite powerful, and quite hostile alien race.  Ever since I caught up with Ryk Brown's Frontiers Saga series, I've been casting about, looking for something next, while Ryk continues working on his books.  So I am absolutely certain that I've found what I've been looking for. I have no idea what's going to happen, but I love what has happened so far.



LEO:  Oh, boy.



STEVE:  The books are all on Amazon.  They're all available through their Kindle Unlimited program and, as I mentioned, also on Audible, narrated by Grover Gardner.  Look for The Silver Ships series.



LEO:  Adding it to my list.



STEVE:  And oh, Leo, oh.



LEO:  I like it that it's Kindle Unlimited because that means I can dip into it and just see if I like it.



STEVE:  Yes.



LEO:  Good.



STEVE:  I already told JammerB.  John is in the middle of a trilogy, but he can't wait.



LEO:  I'm just finishing "The Singularity Trap," so I've got a ways to go.



STEVE:  Yeah.



LEO:  Only a few more chapters on that.



STEVE:  Okay.  Let's take our last break.



LEO:  Yes.



STEVE:  And then we're going to talk about what Symantec found in more than 1,800 mobile apps.



LEO:  Wow.  Have you ever attempted to write a sci-fi novel yourself?



STEVE:  It'd just be a bad use of my time.



LEO:  Yeah, no, it wouldn't be now.  Maybe in retirement, whenever that is.  I wish I could because I would love to be a writer.  That seems like the perfect occupation; right?  It's kind of like coding; right?



STEVE:  I would love it, too.



LEO:  Yeah.



STEVE:  I mean, if I - yeah.



LEO:  But I can never think of any good stories, so I don't think it's going to happen.



STEVE:  Apparently Lorrie is listening to us.



LEO:  Uh-oh.



STEVE:  Because she sent me a text, and she said that she had watched some movies multiple times, she said, the Harry Potter movies.  She said, "Big fan of the Harry Potter movies."



LEO:  There you go.



STEVE:  And for me, I think probably "The Matrix," I'm sure I watched it a second time.  "Terminator," the first time I saw that I would have had to, like, watch it again.  So, you know, there are just some that are really good.



LEO:  All right.  Yeah, I'm not a big movie re-watcher.  I try because I foolishly have bought movies in the past, and that's a waste if you're not going to watch it again.



STEVE:  That's a good point, yeah.



LEO:  Yeah.



STEVE:  Okay.  Embedding AWS Credentials.  Last Thursday, Kevin Watkins, a security researcher with Symantec, revealed the results of Symantec's sobering research into a previously unappreciated, or at least grossly under-appreciated, serious weakness and vulnerability created by the way today's increasingly powerful mobile applications are being developed. The problem surrounds the collision of the increasingly ubiquitous use of "The Cloud" by mobile apps with the merging of out-sourced code libraries and SDKs which use "The Cloud" to contain their sometimes massive databases.  Another problem appears to arise from a failure to follow the old adage which carries the well-known abbreviation:  RTFM.  We know what that stands for.



The data belonging to both users and the enterprises hosting these dangerous ill-designed apps are thus put at risk, as is the data of all the customers of these enterprises.  The problem is big.  So I wanted to get specific and put a sharp point on this, fleshing it out by sharing what Symantec's research revealed.  They open this report with a punchline:  "Over three-quarters of the apps Symantec analyzed contained valid AWS access tokens that allowed access to private AWS cloud services."  Okay.  Now, that was 1,859 iOS and Android apps which were found to be leaking actionable AWS cloud credentials that MUST be kept private.



So here's how Symantec framed the problem and explained what they found.  They said:  "Most of us by now have been impacted in some way by supply chain issues.  An increase in the price of fuel and other items, delivery delays, and a lack of product availability are just some of the consequences of supply chain issues stemming from recent events around the world."  Okay, well, that's not - doesn't apply to us.



They said:  "However, in the context of software and technology infrastructure, the consequences resulting from supply chain issues are very different.  Mobile apps, for example, can contain vulnerabilities introduced in the supply chain that can potentially lead to the exposure of sensitive information, which in turn could be used by threat actors for other attacks."  They said:  "Mobile app supply chain vulnerabilities are often added by app developers, both knowingly and unknowingly, who are likely unaware of the downstream security impacts putting not only the app users' privacy at risk, but sometimes putting their company and employer's privacy and data at risk, too."



Okay.  So in other words, this is what I would call the "modern modular software component assembly dilemma," where it's too easy to plug this library into that library and have this API calling into that API, and where everything just appears to work, but without the developers ever obtaining a full in-depth working understanding of exactly what's going on.  And of course that's the whole point of using a plug-in modular library and its APIs is that you don't need to learn everything about what the serving library is doing.  The trouble is, this is also the way implementation mistakes happen.  And in the case of AWS, the mistakes have huge consequences.



So they said:  "Similar to the supply chain for material goods, mobile application software development undergoes a process that includes the collection of materials, such as software libraries and software development kits (SDKs); manufacturing or developing the mobile application; and shipping the end result to the customer, often using mobile app stores.  This research," they said, "examined the type of upstream supply chain issues that can make their way into mobile apps, making them vulnerable.  The issues include mobile app developers unknowingly using vulnerable external software libraries and SDKs; companies outsourcing the development of their mobile apps, which then end up with vulnerabilities that put them at risk; and companies, often large ones, developing multiple apps across teams, using cross-team vulnerable libraries in their apps."



They said:  "In order to better understand the prevalence and scope of these supply chain vulnerabilities, we took a look at publicly available apps in our global app collection that contained hard-coded Amazon Web Services credentials.  Hard-coded cloud credentials is a type of vulnerability," they said, "we've been looking at for years and have extensively covered in the past.  This time, in order to get to the bottom of the supply chain impacts caused by the issue, we've looked into why app developers hard-code cloud credentials inside apps; where the hard-coded credentials are located in the apps tracking the sequence, or chain of events leading to the vulnerability; and, finally, the size of the problem and its impact."



They said:  "We identified 1,859 publicly available apps, both Android and iOS, containing hard-coded AWS credentials.  Almost all were iOS apps, 98%" - which is really a curious number - and, they said, "a trend and difference between the platforms we've been tracking for years, possibly linked to different app store vetting practices and policies."  And I don't understand, or maybe the application market is just that different between iOS and Android.



LEO:  Oh, it is, it is.



STEVE:  Yeah.  That would be my guess.  And they said:  "In any case, we examined the scope and extent of the risks involved when AWS credentials were found embedded inside apps.  We found the following:  Over three-quarters (77%) of the apps contained valid AWS access tokens allowing access to private AWS cloud services."



LEO:  Geez, Louise.



STEVE:  I know, Leo.



LEO:  They're like hard-coded in?



STEVE:  Yes.  Yes, it's like, you know, we've talked about how lame it is for routers to hard-code in a username and password for like some backdoor.  Cisco spent years recovering from that practice.



LEO:  Is this because like they're using AWS as a server for images or something, and so they have to do that?



STEVE:  That may be one of the reasons.



LEO:  Maybe it's not private stuff, it's like parts of the app.



STEVE:  Well, yeah, but this is access to private AWS cloud services.



LEO:  Oh.  Seems like a bad idea.



STEVE:  So sounds bad.  And then they also said:  "Close to half (47%) of those apps contained valid AWS tokens that also gave full access to numerous, often millions, of private files via the Amazon Simple Storage Service."



LEO:  Okay, that's definitely bad.



STEVE:  That's definitely bad.  And they're going to get much more specific about this in a minute.  So then they said:  "We will explore the type of private data exposed in the examples discussed later in this blog, but the message is clear:  Apps with hard-coded AWS access tokens are a vulnerable, active, and present risk."  They said, okay, well, and present a serious risk, they said.



So I should explain that it would be entirely possible for apps not to embed - and Leo, you already get this - not to embed static AWS access tokens into their code.  And again, how many times have we talked about the insanity of a Cisco router, for example, embedding some backdoor access username and password into its firmware where it's ripe for discovery?  It's just malpractice and laziness.  In the case of well-connected mobile apps, it would be trivial to have apps reach out to obtain the AWS token on the fly over a secure encrypted and authenticated connection.  That would have the added flexibility of allowing the app's developers to change AWS credentials on the fly, if some access right problems, such as we'll be discussing in a minute, were to be found.



In any event, Symantec continues.  They said:  "We then looked into why and where exactly the AWS access tokens were inside the apps, and if they were found in other apps.  We discovered" - get this - "that over half (53%) of the apps were using the same AWS access tokens found in other apps.  Interestingly, these apps were often from different app developers and companies.  This pointed to an upstream supply chain vulnerability, and that's exactly what we found," they wrote.  "The AWS access tokens could be traced to a shared library, third-party SDK, or other shared component used in developing the apps.



"As for the remaining question of why app developers are using hard-coded access keys" - Leo, to your point - they said:  "We found the reasons to include downloading or uploading assets and resources required for the app, usually large media files, recordings, or images; accessing configuration files for the app and/or registering the device and collecting device information, storing it in the cloud; accessing cloud services that require authentication, such as translation services, for example; or no specific reason, dead code, and/or used for testing and never removed."



They said:  "If an access key only has permission to access a specific cloud service or asset, for example accessing public image files from the corporate Amazon S3 service, the impact may be minimal.  Some app developers may be assuming this is the case when they embed and use hard-coded AWS access tokens to access a single bucket or file in Amazon S3.  The problem is often that the same AWS access token exposes all files and buckets in the Amazon S3 cloud, often corporate files, infrastructure files and components, database backups, et cetera.  Not to mention cloud services beyond Amazon S3 that are accessible using the same AWS access token."



They said:  "Imagine a business-to-business company providing access to its service using a third-party SDK and embedding an AWS hard-coded access key, exposing not only the private data of the app using the third-party SDK, but also the private data of all apps using the third-party component.  Unfortunately, this is not an uncommon occurrence, as you can see in the following case study examples."



Okay.  So we've got I think three here, and they kept them anonymous, not to embarrass the actual provider.  But these are specific iOS apps.  They said:  "We found, in an Intranet platform SDK, we found a business-to-business company providing an Intranet and communication platform that had also provided a mobile SDK that its customers could use to access the platform.  Unfortunately, the SDK also contained the business-to-business company's cloud infrastructure keys..."



LEO:  Oh, please.



STEVE:  Oh, "...exposing all of its customers' private data on the business-to-business company's platform.  Their customers' corporate data, financial records, and employees' private data was exposed.  All the files the company used on its Intranet for over 15,000 medium- to large-sized companies were also exposed.  Why did the company hard-code the AWS access token?  In order to access..."



LEO:  Because they didn't know better.



STEVE:  Exactly, yes.  "In order to access the AWS translation service."



LEO:  What?  Oh, please.  Oh, please.



STEVE:  "Instead of limiting the hard-coded access token for use with the translation cloud service, anyone with the token had full unfettered access to all the business-to-business company's AWS cloud services and uses."



LEO:  Wow.  I'm starting to think that, just as we license drivers before they're allowed to go on the road, we should have some sort of minimum competency standard for programmers before they're allowed to publish software.



STEVE:  I know.  



LEO:  I mean...



STEVE:  I know, and it's completely lacking.  Doctors and lawyers have to go through extra school and get certified and pass tests to demonstrate that they have, in the case of lawyers, a basic understanding of the way to do their job.  Programmers, not so much.



LEO:  Wow.



STEVE:  I mean, not at all.  And, you know, there are, as we know, ITProTV produces certifications.  Those exist.  But you don't have to have one in order to write code. 



LEO:  Yeah.



STEVE:  We have another instance, a digital identity and authentication.  They said:  "We discovered several popular banking apps on iOS that rely on the same vulnerable third-party AI Digital Identity SDK.  Outsourcing the digital identity and authentication component of an app is a common development pattern as the complexities of providing different forms of authentication, maintaining the secure infrastructure, and accessing and managing the identities can incur a high cost and requires expertise in order to do it right.  Unfortunately, in this case, things were not done right.



"Embedded in the SDK" - which, again, was shared by several popular banking apps on iOS.  "Embedded in the SDK were cloud credentials that could place entire infrastructures at risk.  The credentials could expose private authentication data and keys belonging to every banking and financial app using the SDK.  Furthermore, users' biometric digital fingerprints used for authentication, along with users' personal data (names, dates of birth, et cetera), were all exposed in the cloud.



"In addition, the access key exposed the infrastructure server and blueprints, including the API source code and AI models, used for the mobile operation.  In total, over 300,000 biometric digital fingerprints were leaked across five mobile banking apps" - five mobile banking apps - "using the SDK."



And finally, online gaming.  They said:  "Often, already established companies rely on outsourcing, or partnering, with other business-to-business companies for their digital and online services.  This allows them to quickly move their brand online without having to build and support the underlying technology platform.  At the same time, by relying on the outsourced company to run the technology platform, they often have to give exclusive access to their business data.  Furthermore, they have to trust that the outsourced company will protect the online private data, not to mention the reputation of the brand overall."  Boy, talk about skating on thin ice.



They said:  "We found a large hospitality and entertainment company depending on another company for their technology platform, even forming a sports betting joint venture with the company.  With a highly regulated sports betting market, the complexities of building and supporting infrastructure for online gambling cannot be underestimated.  Unfortunately, by giving the joint venture company exclusive access to that part of its business, the company also exposed its gaming operations, business data, and customer data to the world.



"In total, 16 different online gambling apps using the vulnerable library exposed full infrastructure and cloud services across all AWS cloud services with full read/write root account credentials."  And they said:  "All of the organizations whose vulnerable apps were discussed in these case studies have been notified about the issues we uncovered."



LEO:  Now you know why there are so many S3 bucket exploits.



STEVE:  Yes.



LEO:  I mean, this happens, this is probably the most common kind of breach.



STEVE:  Yes.



LEO:  It seems like Amazon maybe isn't doing a good job of explaining permissions or something.  Like maybe Amazon can fix this.



STEVE:  Yeah.  I agree with you.  It is - I think what happens is that it's so easy to use Amazon S3 buckets.  I have and maintain a bunch of my own, like every time I upload the edited audio to Elaine, copies of the podcast go into an S3 bucket, just to have it in the cloud so that it's archived archived.  The problem is I think that developers get it going, like without any access controls, and then they just forget to go back and lock it down and, like, restrict it so that it's not just left as public because, unfortunately, that's the way it is by default.  It's public unless you make it private.  And that seems to be the mistake that's being made.  Crazy.



LEO:  Wow.



STEVE:  And it's also, maybe it's because it's remoted that somehow you don't have the same level of visibility into access controls that you do for something that is local.  Or just they didn't RTFM.



LEO:  Yeah.  I mean, I seem to remember having set up quite a few of these.  They kind of - this is like the password.  This gives you access.  They even hide it; right?  You have to reveal it.  It seems to me that - I don't know what's going on.  Lazy.



STEVE:  It's not good.



LEO:  Or, yeah, or something.



STEVE:  Well, and the problem is, and this was the point that Symantec was making, is that we've got a difficult, clearly a difficult-to-secure or too often not secured cloud resource, which because of the way business to business are now beginning to contract with each other for big chunks of responsibility, that cross-business contracting is what Symantec is calling a new form of supply chain.  And if your provider of these services isn't careful with the design of their product, its inherent security appears to be lacking.  There's no other way to explain why 16 different, entirely separate gambling environments which all use a common package, would have had all of their data cross-exposed if it weren't for the mis-design of that package.



LEO:  Yeah.  Unbelievable.  Yeah, that's the other problem.  People just kind of willy-nilly just pasting packages in there, yeah.



STEVE:  Yeah.  It's like, oh.  Well, and because who wants to learn all that.



LEO:  I don't want to write all that code.  Oh, look, somebody wrote it.



STEVE:  We can get it from Joe.  Great.



LEO:  Yeah, yeah.



STEVE:  Import.  Import.  Press the Import button.



LEO:  Yeah.  Well, that, I'm kind of sympathetic to that.  It's really up to Joe to make that secure.  Joe's got to do a better job.  Wow.  Well, another great Security Now!, as always.  Thank you, Steve.  Steve Gibson lives at GRC.com.  There's all sorts of great stuff there.  Of course SpinRite, the world's finest hard drive recovery and mass storage recovery and maintenance utility.  And you can pick up a copy right now, 6.0.  6.1 is in process, will be out sometime, and you'll get it for free if you buy today.  And he's working on 7.0, apparently, I just found out.  So, busy man.



STEVE:  Plans.  Plans for 7.0.



LEO:  Planning.  You've got to think about it first.



STEVE:  I know what it's going to be.



LEO:  You've got to know what S3 buckets to stick it in.



STEVE:  That's right.



LEO:  That's one good thing about assembly language.  Probably not a lot of S3 tokens.



STEVE:  Hopefully it's not a leaky bucket.



LEO:  Yes.



STEVE:  And there are no one else's modules that I'm importing.



LEO:  Nobody else makes them.  Right.



STEVE:  That's right.



LEO:  Where are you going to go to find those modules?  Look, we've got a whole library of assembly language routines you can use.  Aggressively non-cross-platform.  That's Steve.  Right there.  In a nutshell.  Nope, 8086, that's it.  But, boy, I tell you what, it works good.  He also has of course the copies of this show.  Now, he has a 64Kb audio, which is kind of the standard audio podcast version.  But he also has a 16Kb version for people who really don't want to spend the bandwidth.  And he's got transcripts.  Elaine Farris writes those out after every show, takes a few days.  That's really nice to read along as you listen or to use for reference or to search.  I bet you, I know somewhere out there there's somebody with three shelves' worth of three-ring binders with all the transcripts printed out and indexed.  Don't you think, Steve?  Somebody's done that.



STEVE:  Yeah.



LEO:  The Magnum Opus.  All of that's free at GRC.com.  All you've got to do is buy SpinRite.  Everything else takes care of itself.  Lots of other stuff, as well.  You can leave him a question or a comment or suggestion at GRC.com/feedback.  Probably the better way to do it is to go to his Twitter account, @SGgrc.  You can DM him there.  His DMs are open.  After the fact we have the show, as well, at our website, TWiT.tv/sn.  We've got the 64Kb audio, and we also have video, which is weird, but we've got it.  [Beeping]  Oh, time to go.  Show's over.  I heard, I swear, and I must be dreaming, wind chimes.  Do you have wind chimes in your studio, in your office?



STEVE:  That was Lorrie's iMessage coming in.  Yes, because I do have sounds associated with everything.



LEO:  Nice.  So she's very relaxing wind chimes.



STEVE:  Nice sound, yeah.



LEO:  I thought I must have died and gone to heaven.  Steve Gibson has wind chimes at the GRC Labs.



STEVE:  There's no wind here except what comes out of my mouth.



LEO:  A little hot air, but no wind, yeah.  Hot air here today, I'll tell you.  We also invite you to watch live, if you want.  You could.  We stream it live as we do with all our shows.  We just open up the cameras in the studio and let you watch at live.twit.tv.  I's usually right after MacBreak Weekly, that's about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  If you're watching live, you can chat with us live in IRC.  That IRC site is irc.twit.tv.  You can go there with a browser.  It has the deets on how you could use an IRC client if you prefer.  If you do it more than once, you should probably get an IRC client.  Of course the Discord folks are also chatting behind the scenes, if you're a member of Club TWiT.



YouTube Channel also, dedicated to Security Now!.  That's most useful, I think, well, if you watch a lot of YouTube it's great.  But also it's a great way to send just a little clip to somebody because that's easy to do on YouTube.  So if you heard something you thought, I've got to send this to Joey, he keeps putting our S3 bucket credentials in the app, you could just snip that part, send it to Joey, let Steve explain why it's a bad idea.  Oh, of course the best way to get it would be subscribe.  That way you can add, to your six-foot shelf of binders, you could add the complete Security Now!.  We are now in our 18th year.  19th year.  18th year.



STEVE:  18th, yeah.



LEO:  Episode 887.  So collect all 887.  We only do 10 at a time in the feed.  So if you want to go past 877 you'll have to go to the website, either Steve's or ours, and download all of them.  But they're all up there on the website.  You can get them all.



Steve, have a great week.  I am going to be reading this new Silver Ships.  I'm excited about it.



STEVE:  Oh, I think you're going to like it.  It's just so pleasant.	



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#888

DATE:		September 13, 2022

TITLE:		The EvilProxy Service

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-888.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at an unusual and disturbing escalation of a cyberattack.  I also note that crypto heists have become so pervasive that I'm not mentioning them much anymore.  The White House conducted a "listening session" to dump on today's powerful tech platforms, and a government regulator in The Netherlands quit his position and tells us why.  There's another QNAP mess which is bad enough to exceed my already quite high QNAP mess threshold, and D-Link routers need to be sure they are running their very latest firmware.  I have another comment about my latest sci-fi author discovery, and two quick bits of feedback from our listeners.  Then we're going to examine EvilProxy, the conceptual cousin to Ransomware-as-a-Service.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  The White House has a listening session that drives my blood pressure up.  We'll find out why one Netherlands regulator quit his job in protest.  Another QNAP mess.  And finally, it had to happen someday, Phishing-as-a-Service.	It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 888, recorded 13 September, 2022:  The EvilProxy Service.



It's time for Security Now!.  Yes, he's here, ladies and gentlemen, Steve Gibson.  He didn't sleep well last night, but he's going to sleep like a baby tonight because it's Security Now! day.  Hello, Steve.



STEVE GIBSON:  That's right.



LEO:  He told me he sleeps better when the show's done.  Like you put a lot of stress into this; right?



STEVE:  Well, okay.  So I monitor the phases of my sleep.  Remember the Zeos that I had you get?



LEO:  Yeah, yeah, yeah.



STEVE:  Like years ago?



LEO:  I still have it.



STEVE:  Well, it may come in handy.  It turns out that the amount of slow wave sleep we get, which is the deepest level of sleep, it correlates with - basically it's a response from the previous day's cognitive memorization-related work.



LEO:  Hmm.



STEVE:  And they've done tests where they've taken two groups of people who are well mixed and had them both spend the day doing two different types of task, one which involves memorization, and the other also a mental task but did not require memory.  And the amount of slow wave sleep obtained by the group who were trying to memorize things is significantly larger than the group that did an equal amount of work, but didn't require memory.  It's during slow wave sleep that memories are transferred from temporary storage into permanent storage.  And what's interesting is that that's also the only cycle, the only phase of our sleep where the toxic proteins, the amyloid betas and the tau proteins, are swept out of our brain.



So one wonders whether the old adage about like learning a language or exposing yourself to novelty, if the reason that tends to keep your brain healthy is that our brain's response to that load we're putting on it, like a memorization load, learning something new, is to give us more slow wave sleep, which has the effect of clearing out the metabolic debris from the previous day's work.



LEO:  Hence, you should never stop doing the show.  This show is good for you.



STEVE:  Oh, hence I need to keep finding authors like the new author that I've found.  Oh, my.



LEO:  Oh, good books will work, too.



STEVE:  Oh.  But anyway.



LEO:  Are you still reading him?  You're crazing about him?  We'll talk about him later.



STEVE:  Oh, I've got a section of our show notes to talk about him.



LEO:  Oh, good.  Can't wait.  



STEVE:  Yes.



LEO:  What else we going to cover?



STEVE:  So we're at Episode 888 for the 13th of September.  This one's titled The EvilProxy Service.  And oh, boy.  Bruce Schneier's words about attacks never getting worse, they only ever get better or stronger, they really ring true here.  Anyway, we've got something really interesting and upsetting to talk about.  But first we're going to look at an unusual and disturbing escalation of a cyberattack.  I hope not an indicator of things to come.  I also note that crypto heists have become so pervasive that I'm not mentioning them much anymore.  They're just - it's ridiculous.  We'll talk about that.



LEO:  Like breaches.  It's the same thing.  It's just so many.



STEVE:  Yes.



LEO:  Yes.



STEVE:  Also the White House last Thursday conducted a "listening session," as they called it, to dump on today's powerful tech platforms.  Some of what came from that was interesting.  And a government regulator in the Netherlands quit his position and then told us why.  Also there's another QNAP mess which is bad enough to exceed my already quite high QNAP mess discussion threshold.  Normally I just don't talk about it anymore because it's like, okay, yeah, again.  Anyway, also D-Link routers need to be very sure that they are now running the latest firmware.  I'll explain why.  I've got, as I mentioned, another comment about my latest sci-fi author discovery; two quick bits of feedback from our listeners, and then we're going to examine this - essentially Phishing-as-a-Service...



LEO:  Oh, golly.



STEVE:  ...has happened.



LEO:  Inevitable.  Inevitable.



STEVE:  In the same way - yes.  It is exactly that, Leo.  In the same way we had Ransomware-as-a-Service, now we have Phishing-as-a-Service.  And this service can bypass all of our multifactor authentication safeguards.



LEO:  Oh, boy.  That's not good.  Oy.



STEVE:  So it is essentially the conceptual cousin of Ransomware-as-a-Service.  And, oh, do we have a Picture of the Week.



LEO:  I burst out laughing when I saw it.  It's good.  It's good.  We get to see that thing that made me laugh out loud.



STEVE:  So the residential version of this is that old story - you'd have to be our age or older, probably, Leo, to remember when fuses were screw-in base in homes; right?  The actual fuse, there weren't circuit breakers so much back then, they were very much like a lamp socket, but you would screw a fuse which was round and would have a little piece of copper there in the middle.  And of course the point was that if something downstream of the fuse was drawing so much current that the little fuse, thus it's called a "fuse," would overheat and melt.  Well, you wanted that to happen because the melting opened the circuit and...



LEO:  Protected you, yes.



STEVE:  ...turned off the current.  And so as an engineer, you know, as a technical person, the idea - and what people would do, of course, is like, you know, things would happen.  The fuses could be pulled; right?



LEO:  You don't have a fuse, yeah.



STEVE:  It could just sort of, yeah, you don't have a fuse handy because you used them up.  You used a fuse the last time it blew.  That was your last fuse.  And so you forgot to go get some more.  But the lights are off.  So what are you going to do?  Well, you get a copper penny, and you stick it in the socket, and then you screw the burned-out fuse on top of it, and oh, look, the lights come back on.



LEO:  Yeah.  And you just, you know, and you'd better hope the next surge is so powerful it melts the penny.



STEVE:  And again.  So, okay.  All this by way of introducing our Picture of the Week, which is the equivalent on an industrial scale.



LEO:  Oh, my god.



STEVE:  If anyone has ever seen like a fuse box that would be protecting a huge, like, a woodworking shop with a bunch of equipment in it, or something that's drawing like an oil derrick or something, where the fuses are cylinders with big thick copper blades on each end.  And you stick them in, and the blades are grabbed by receptacles on either end, and there's a pair of them for the hot and the cold line.  Anyway, in this picture, apparently because something similar happened, they ran out of fuses, maybe they were stolen, maybe they kept blowing out.  Well, first of all, if your fuses keep blowing out...



LEO:  That's not good.



STEVE:  ...there's something wrong.



LEO:  Yes.



STEVE:  Anyway, these industrious people decided, okay, well, you know, these pesky fuses keep blowing out.  So they took two very large screwdrivers and just stuck them in in place of these fuses.  And boy, I tell you, if these fuses blow...



LEO:  You're in trouble.



STEVE:  You really have some problems.



LEO:  Yeah, wow.  That's hysterical.  I have those screwdrivers.  Now that I know I can use them as fuses, I'm set.  That's good.



STEVE:  Actually, I own the one on the right.



LEO:  Yeah, me, too.  Who doesn't?  That's a Stanley.



STEVE:  Yes, that thing must be a popular screwdriver because both of us have one.  Okay.  So Albania versus Iran.  Risky Business News headlined their story this way.  They said:  "Albania cuts diplomatic ties with Iran in the first-ever cyber-related escalation."  You know, I don't have a strong emotional tie to either Albania or Iran, though it's worth noting that Albania is a member of NATO.  Fortunately, at this time, cyberwar mostly amounts to, you know, transient inconveniences; right?  Like, you know, some office can't process green cards or something.  But what's so worrisome about this is that it feels as though it might be predictive of worse things to come, and eventually perhaps involving global-scale adversaries.



Okay.  Anyway, so here's what happened:  The Albanian government announced last Wednesday the 7th that it would be cutting all diplomatic relations with Iran in the aftermath of a major cyberattack.  And this marks the first time ever that a cyberattack has escalated this severely in the political realm.  In a recorded video statement published on YouTube - for anyone who's interested I have the link in the show notes - Albania's Prime Minister Edi Rama said that after concluding an investigation into the incident, they found "indisputable evidence" that Iranian state-sponsored hackers were behind the cyberattack that took place nearly two months prior on July 15th.  So they didn't just jump at this immediately.  They did some investigating.  In fact, they involved Microsoft.  That cyberattack crippled multiple Albanian government IT systems.



Rama gave Iranian diplomats one day, 24 hours, to close their embassy and clear out.  While the Iranian government naturally denied any involvement in the attack, NATO, the U.S. White House, and the U.K. government all published statements in support of the Albanian government and supported its attribution of the attack to the Tehran regime.  The U.S. called Iran's attack on its NATO ally a "troubling precedent" and promised to "take further action to hold Iran accountable."  And I did see subsequently, but I didn't track it down, that the U.S. had announced sanctions on Iran specifically due to this attack, this cyberattack on Albania.



And of course, although Iranian officials may deny their involvement, the proof lies in the malware used, which was discovered in the July 15th attack.  Both Mandiant and Microsoft have linked back to multiple past instances of Iranian cyber-espionage operations and tooling using the same stuff.  Microsoft, which has participated, as I said, in the Albanian government's response to the incident, said it was able to link the incident to four different Iranian APTs, advanced persistent threat groups, and detailed how these four groups have been working together to breach Albanian government networks at least since last year to establish the proverbial foothold.  Then finally in July, under the auspices of the Iranian government, which apparently decided it was time to act, the attack was launched.  



Microsoft says the four groups appear to work under the guidance and control of the Iran Ministry of Intelligence and Security (MOIS).  The four groups with numerical designations, there's DEV-0842 which deployed the ransomware and the wiper malware.  DEV-0861, a different group, gained initial access and exfiltrated data, so now we're seeing specialization among individually identified groups.  We have DEV-0166, which exfiltrated the data; and DEV-0133, the group which probed the victims' infrastructure initially.



So both Mandiant - which, by the way, Google, remember, purchased in March for $5.4 billion - and Microsoft concur in their statement that the Iranian attack is directly connected to the Albanian government's harboring thousands of Iranian dissidents, part of an exiled opposition party named the People's, how do you say that, Mujahideen, something like that.  Anyway, I meant to look up the pronunciation before the podcast, and I forgot.  Anyway, also known as MEK, which I like to say much more easily.



At the request of the U.S. government, MEK was given shelter in Albania in 2016, after the Iranian regime declared the group a terrorist organization and started hunting its members.  MEK members were planning to hold an annual summit on July 21st.  But that summit, which was titled The Free Iran World Summit, was canceled because of terrorist and bomb threats.  Microsoft says that the threats and the July 15th cyberattacks were part of a broader effort from the Iranian government to go after the group and its host country.



So whereas past operations typically involved coordinated social media campaigns, data leaks, vague threats, and declarations from Iranian officials, the deployment of a data wiper and ransomware appears to have crossed a line which Albanian and NATO officials are not taking quietly.  Though Albania's prime minister tried to play down the aftermath of the July 15 attack and said the government systems were now restored, the attack crippled government operations and official websites for weeks.  And in fact, moments after Iranian officials left the embassy, Albanian police raided the building, which is unusual, in search of any incriminating evidence that might have survived the typical hard-drive bashing and document-burning practices of fleeing diplomats.



Conducting this raid was seen as extreme, but the general sentiment is that NATO partners backed and pushed Albania into this action as a way to signal to other cyber-aggressive countries that a line is being crossed when entire government IT networks are being wiped just because someone wants to attack a dissident group that they're annoyed with and of course attack those who are harboring the group.



LEO:  I think that's appropriate.  I really do.  Don't you?



STEVE:  Which?



LEO:  You have to draw a line in the sand.  No, no, to defend.



STEVE:  Oh, yes.  Oh, yes, appropriate to draw the line, absolutely.



LEO:  Yeah, you know, this shall not pass.



STEVE:  And Leo, I mean, the thing that's worrisome about this, as I started off saying, is that what if this is a harbinger?  Like what if, I mean, we've talked about how weird it is that like the U.S. and China are apparently right now involved in percolating kind of going on in the background cyberattacks against each other.  Well, the problem is cyber is becoming - sorry to use that term in isolation, Leo - the cyber world is becoming...



LEO:  Cyberspace, man.



STEVE:  Yeah.  It is, you know, it's becoming where the world operates.



LEO:  Yeah.



STEVE:  And so attacks there are real attacks, increasingly.



LEO:  Yes, yes, absolutely.  And they can be deadly.  I mean, it's no reason to treat it any less seriously than a rocket, a mortar attack, I think.  You know?



STEVE:  Right, right.  And so I agree with you completely.  I think that it is good...



LEO:  Proportional response.



STEVE:  Yes, that the world said, okay, this is not all right.



LEO:  Right.



STEVE:  You know, we know it was you, Iran.  We know why you did it.  We know you're not happy.  We don't agree with your unhappiness, and you've just attacked a member of NATO.



LEO:  Sorry, yeah.



STEVE:  Even if it was a cyberattack.



LEO:  Yeah.  Now, I understand the risk is that it will escalate into a worse and worse back-and-forth.  But I don't see any way out of that.  This is the whole, you know, this is the whole issue of any military force.  There are bullies.  And so you need a defense.  You can't just let bullies be bullies.



STEVE:  Oh, and wait till you get to the fourth book in this...



LEO:  Oh, I can't wait.  I can't wait.



STEVE:  Ohhh.  You want some bullies, oh, baby, I've got your bullies.  And it ended up being a bit of a back-and-forth.  Last Wednesday the diplomats were given one day to clear out and close the embassy.  Two days later, last Friday the 9th, Albania was hit by another major cyberattack which has officials once again pointing the finger at Iran.  The attack hit Albania's Total Information Management System, as it's called, TIMS, which is an IT platform belonging to Albania's Ministry of Interior, used to keep track of people entering and leaving the country.



According to a series of tweets from Albania's Minister of the Interior, six border crossing points were impacted and experienced border crossing stoppages and delays for at least two days.  This included five land crossings at Greece, Kosovo, and several in Montenegro; and at the airport near Albania's capital.  Ministry officials blamed the attack on "the same hand," as they put it, that hit Albania's IT network in July, in other words, Iran.  So let's hope that the world is watching and recognizes that cyberattacks are not going to be treated like anything less than the attack that they really are, especially when they seriously impact government infrastructure.



Okay.  So I feel that I should note something else that I'm seeing constantly which I just skip over, typically without comment on this podcast.  And that's crypto heists of this or that also-ran cryptocurrency...



LEO:  They're nonstop.  They're nonstop.  They're constant.



STEVE:  Oh, my god.  From this or that random exchange that no one's ever heard of before, or random newbies being crypto-scammed.  So this week I'll give everyone three perfect typical examples, Leo, of what we're both talking about so everyone has a feeling for what they're normally not missing.



Okay.  First, get this, the New Free DAO, that's NFD token, whatever the...



LEO:  It's a DAO.



STEVE:  Right, the DAO.



LEO:  Autonomous organization, yeah.



STEVE:  Right, the New Free DAO token lost 99% of its value after a threat actor used a flash loan attack to steal more than $1.5 million worth of crypto from the platform.  According to blockchain security firm CertiK, the hacker appears to be the same attacker who also hit DeFi platform New Order four months ago.  I know.



LEO:  I shouldn't laugh.



STEVE:  Gripping news, I know.



LEO:  I shouldn't laugh because...



STEVE:  No, but Leo.



LEO:  The sad thing is, I don't mind if some bitcoin bro loses his shirt.  That's fine.  But probably a lot of these people are just suckers, normal people.



STEVE:  Yes, unfortunately.  Also the operators of the Gera cryptocurrency suspended operations last week after a hacker gained control over the platform's "smart contract," which is the name of it, which apparently wasn't so smart, after developers leaked the private key.  According to the Gera team, the attacker minted $1.5 million worth of crypto, which they later transferred to their own Ethereum address.  The platform has not yet resumed operations.  Okay, boo hoo.



And third, Romanian law enforcement raided two penthouses - I got a kick out of the fact that they were in penthouses - two penthouses in Bucharest and detained three suspects.  According to a joint investigation with the U.K.'s National Crime Agency, the NCA, the suspects would contact victims - get this, Leo, you're going to love it - the suspects would contact victims of cryptocurrency fraud and defraud them again.



LEO:  Oh, that's terrible.



STEVE:  By posing as financial fraud recovery specialists, and ask for a substantial fee to recover their initial losses.



LEO:  Once a sucker, you know.  You've got the sucker hat on, they're going to come at you.



STEVE:  Ohhh.



LEO:  You're wearing it.



STEVE:  Just so everyone knows, there is now a more or less constant flux of these sorts of heists.  I mean, cryptocurrency, no one seems to be able to hold onto it.  It's just constant.



LEO:  This is a great website from Molly White called "Web3 Is Going Just Great," and she has a little counter in the lower right-hand corner of how much money has been lost to crypto fraud.  And it is, it's kind of stunning.  It's just nonstop.



STEVE:  What does it show, 10 point something billion dollars?



LEO:  $10.669 billion.



STEVE:  Oh, my lord.  Wow.



LEO:  I mean, this is a classic, this is a typical headline.  "Algorand Foundation discloses $35 million exposure to Hodlnaut."



STEVE:  Well, you don't want to expose your Hodlnaut, Leo.



LEO:  Well, both of these were legit.  Hodlnaut was a crypto wallet that halted withdrawals on August 8th.  Algorand is a proof-of-stake blockchain, and they foolishly put $35 million into Hodlnaut.  And then Hodlnaut was heavily exposed to Terra, which collapsed in May.  So Hodlnaut halted withdrawals.  Because there's no regulation.



STEVE:  Because what could possibly go wrong?



LEO:  Yeah.  No regulation.



STEVE:  [Crosstalk] running the bank.



LEO:  Incredible.



STEVE:  Oh, goodness.



LEO:  Incredible.



STEVE:  Yes, I, you know, if we titled our podcasts after the show the way you do for MacBreak Weekly, it would be "Never Expose Your Hodlnaut."



LEO:  Yeah.  And, by the way, I'm laughing only because it's so horrible.  And again, if it were bitcoin bros, fine, you know, throw your ill-gotten gains away.



STEVE:  The Winklevoss brothers or whoever they are.



LEO:  Let them lose all the bits in their coin.  But it's not, it's sad to say, it's people who are being suckered by NFTs and crypto.  Don't be fooled, kids.



STEVE:  We have some guy, a neighbor in our neighborhood, who's all NFT hopped up.  And I just, you know.  And some other neighbors who know I'm kind of a computer guy say, should we do anything?  I said no.  Stay away from...



LEO:  Stay as far away as you can.



STEVE:  Stay away from that guy.



LEO:  Yeah, yeah.



STEVE:  Yeah.  I don't, you know, I mean, apparently, if you're Kevin Rose, and you can have a little, what is it, a zombie he's got?



LEO:  Oh, he's got - he owns some zombies.  But then, and I love Kevin, but I think this is a little sketch, he created his own owls, Moonbirds they call them, and has been selling them, sold them within the first week, $50 million worth of them.



STEVE:  Ohhh, what...



LEO:  And so many that he, you know, it's he and a bunch of other people called The Proof Collective.  But it's really - he's one of the big names.  There's three people is involved in stuff.  So because it's big names in this area, NFT area, people bought in to the tune of $50 million.  All of it's speculation.  You only buy an NFT because you think someday some sucker's going to come along and buy it for twice as much.  Then Kevin, realizing he made a lot of money here, put out a YouTube video saying, no, no, we're going to do good things with the money.  Then last week it was announced that Marc Andreessen, Andreessen Horowitz, just put another 50 million into it as an investment.  So I think the only thing that...



STEVE:  That was a good YouTube video.



LEO:  The only thing we did wrong, Steve...



STEVE:  He cannot stop making money, this guy.



LEO:  I know.  The only thing we did wrong, Steve, is not issuing an NFT early on.  That and throwing away the hard drive.



STEVE:  Ohhh.



LEO:  I can only say a hundred times, you know, stop, don't, it's not, you know, this is - Bill Murray's NFT charity auction, that's $185,000, which is immediately stolen.  Hours after the auction, a hacker gained access to Murray's crypto wallet and snagged the 119 ETH for themselves.  And on and on and on.



STEVE:  What were those bulbs that were once so popular?



LEO:  That was tulip bulbs.



STEVE:  Tulip bulbs, yeah.



LEO:  I just thought that was going to be a good investment.



STEVE:  That was going to be a big deal.



LEO:  I've got them next to my Beanie Babies in the closet.



STEVE:  Okay.  So the White House held a Tech Platform Accountability listening session last Thursday.  In a nation founded on the principle of a right to free and open public speech and a free and open press, neither being under the thumb of the government, the question is what responsibility do our social media platforms have, and to what degree, if any, about the content their users publish and which they subsequently host and our search engines find and index.  Certainly a good question.



Now, I looked through the list and the titles of the 16 attendees who were invited to participate in this "listening session" last week.  If it were possible for bureaucracy to reach a critical mass where its own gravitational attraction would cause it to collapse in upon itself, putting this group into a single room would be inadvisable.  Boy, I mean, the titles, you need a line wrap in order to see them.  Nevertheless, the listening session occurred, and everyone appears to have survived.



I suppose that a session titled "Tech Platform Accountability" would tend toward the negative.  But boy, did this group dump on today's social media offerings.  The White House started everyone off with a negative tone, and the meeting's participants appear to have willingly added fuel.  The summary of the event is not long, and I think it's worth sharing.



Here's the White House's summary.  They said:  "Although tech platforms can help keep us connected, create a vibrant marketplace of ideas, and open up new opportunities for bringing products and services to market" - and okay, just so everyone knows, that's the end of the good news part of the summary.  They continued:  "They can also divide us and wreak serious real-world harms.  The rise of tech platforms has introduced new and difficult challenges, from the tragic acts of violence linked to toxic online cultures, to deteriorating mental health and wellbeing, to basic rights of Americans and communities worldwide suffering from the rise of tech platforms big and small."



They said:  "Today, the White House convened a listening session with experts and practitioners on the harms that tech platforms cause and the need for greater accountability.  In the meeting, experts and practitioners identified concerns in six key areas:  competition; privacy; youth mental health; misinformation and disinformation; illegal and abusive conduct, including sexual exploitation; and algorithmic discrimination and lack of transparency."  And for what it's worth, I mean, I know we are all sympathetic to the problem that we have.  There are certainly problems here.



They said:  "One participant explained the effects of anti-competitive conduct by large platforms on small and mid-size businesses and entrepreneurs, including restrictions that large platforms place on how their products operate and potential innovation.  Another participant highlighted that large platforms can use their market power to engage in rent-seeking," as the term is, "which can influence consumer prices.



"Several participants raised concerns about the rampant collection of vast troves of personal data by tech platforms.  Some experts tied this to problems of misinformation and disinformation on platforms, explaining that social media platforms maximize 'user engagement' for profit by using personal data to display content tailored to keep users' attention, content that is often sensational, extreme, and polarizing.



"Other participants sounded the alarm about risks for reproductive rights and individual safety associated with companies collecting sensitive personal information, from where their users are physically located to their medical histories and choices.  Another participant explained why mere self-help technological protections for privacy are insufficient.  And participants highlighted the risks to public safety that can stem from information recommended by platforms that promote radicalization, mobilization, and incitement to violence.



"Multiple experts explained that technology now plays a central role in access to critical opportunities like job openings, home sales, and credit offers, but that too often companies' algorithms display these opportunities unequally or discriminatorily target some communities with predatory products.  The experts also explained that the lack of transparency means that the algorithms cannot be scrutinized by anyone outside the platforms themselves, creating a barrier to meaningful accountability.



"One expert explained the risks of social media use for the health and wellbeing of young people, explaining that while for some, technology provides benefits of social connection, there are also significant adverse clinical effects of prolonged social media use on many children and teens' mental health, as well as concerns about the amount of data collected from apps used by children, and the need for better guardrails to protect children's privacy and prevent addictive use and exposure to detrimental content.  Experts also highlighted a magnitude of illegal and abusive conduct hosted or disseminated by platforms, but for which they are currently shielded from being held liable and lack adequate incentive to reasonably address, such as child sexual exploitation, cyberstalking, and the non-consensual distribution of intimate images of adults."



LEO:  Ugh.



STEVE:  I know.  "The White House officials closed the meeting by thanking the experts and practitioners for sharing their concerns.  They explained that the administration will continue to work to address the harms caused by a lack of sufficient accountability for technology platforms.  They further stated that they will continue working with Congress and stakeholders to make bipartisan progress on these issues, and that President Biden has long called for fundamental legislative reforms to address the issues."



So it seems clear that, much as with the argument over cryptography, and privacy which creates an inherent lack of accountability when it can be used by criminals for criminal ends, that there's a tension there which I find fascinating because it's created by technology.  Well, there's obviously another set of tensions here, you know, that is being created by the technology and, frankly, by the willful conduct of these major tech platforms.



So it seems clear that sooner or later we're going to be subjected to legislation of some form as our various governments attempt to somehow, you know, it's going to come down to micromanaging this incredibly slippery terrain which at least in the United States also employs constitutionally protected freedoms.  So I imagine there'll be some time spent in the courts, as well.



Anyway, I wanted to finish by sharing the six bullet point, sort of the takeaways, the targets which were cited as the main focuses, the core principles for reform.  The first is "Promote competition in the technology sector."  They said:  "The American information technology sector has long been an engine of innovation and growth, and the U.S. has led the world in the development of the Internet economy.



Today, however, a small number of dominant Internet platforms use their power to exclude market entrants, to engage in rent-seeking, and to gather intimate personal information that they can use for their own advantage.  We need clear rules of the road to ensure small and mid-size businesses and entrepreneurs can compete on a level playing field, which will promote innovation for American consumers and ensure continued U.S. leadership in global technology.  We are encouraged to see bipartisan interest in Congress in passing legislation to address the power of tech platforms through antitrust legislation."



Second:  "Provide robust federal protections for Americans' privacy."  They said:  "There should be clear limits on the ability to collect, use, transfer, and maintain our personal data, including limits on targeted advertising.  These limits should put the burden on platforms to minimize how much information they collect, rather than burdening Americans with reading fine print.  We especially need strong protections for particularly sensitive data such as geolocation and health information, including information related to reproductive health.  We're encouraged again to see bipartisan interest in Congress in passing legislation to protect privacy."



Third:  "Protect our kids by putting in place even stronger privacy and online protections for them, including prioritizing safety by design standards and practices for online platforms, products, and services."  They said:  "Children, adolescents, and teens are especially vulnerable to harm.  Platforms and other interactive digital service providers should be required to prioritize the safety and wellbeing of young people above profit and revenue in their product design, including by restricting excessive data collection and targeted advertising to young people."  And I for one, I don't have any young kids, never had to raise them in this Internet age.  But it would be a terrifying prospect, I think.



LEO:  Yeah.  The problem - I agree with all of these sort of in principle.  The problem's the implementation.



STEVE:  I'm with you completely, Leo.  And I heard you talking last week, it wasn't here, it was in the Techdirt guys...



LEO:  Yeah, my dialogue with TWiG, yeah.



STEVE:  Yes.  About some of the ideas that California's legislators have come up with.



LEO:  It passed, by the way.  It's in law.



STEVE:  Right.



LEO:  And it should be terrifying to you.



STEVE:  Yes.



LEO:  Because you have potentially 18 year olds and under using your site.  You want them to.  That means you have to design your site, and everybody has to design their site, for the lowest common denominator.  And that's ridiculous.  That's just absurd.  That's not how you protect kids.  So, you know, well, let's change the Internet and make it safe for kids.  You mean all of it?  Yeah, all of it.  Okay.



STEVE:  Yeah.  It's very much like the overreach of the grant permission for cookies.  It's like, oh, my goodness.  Let's fix them rather than make everyone agree to them.



LEO:  There are some things I completely agree with.  You know, we need to work on privacy protections.  I agree.  But then they intermix this with this to protect the children, the design of websites, you know, that was all code earlier on about social networks to overturn Section 230 of the DMCA, which is vital to the Internet. 



STEVE:  Yes.



LEO:  And it's just it's a fundamental misunderstanding of how it works.  And it's politics, and it's very shameful.  I'm very disappointed, frankly.



STEVE:  Yeah.  So I'll just skip a lot, just summarize the last three points.  There was remove special legal protections for large tech platforms.  And here we come to Section 230, just as you were saying, Leo.  It's like, you know, how can we make open platforms actually responsible for everything that their participants post.  I mean, again, it's a problem that they're not.  It's a problem what people are posting.  But it's impractical to make them responsible.



LEO:  Section 230 makes it possible for them to moderate.  The problem is you have some politicians who don't like to be moderated.  They call it "deplatforming."  And if you make it impossible to moderate, well, that's not good.  Get ready.  It's going to be bad news.



STEVE:  Yup.  Number five is increased transparency about platforms' algorithms and content moderation decisions.  They said:  "Despite the central role in American life, tech platforms are notoriously opaque.  Their decisions about what content to display to a given user and when and how to remove content from their sites affect Americans' lives and American society in profound ways.  However, platforms are failing to provide sufficient transparency to allow the public and researchers to understand how and why such decisions are made, their potential effects on users, and the very real dangers these decisions may pose."



LEO:  So California passed a law on this, too, just the other day, which Mike Masnick calls "The Spammers Protection Act."  Because it essentially says make it public how you block spam, how you clear stuff out that's bad.



STEVE:  Decide which is not.



LEO:  Yeah.



STEVE:  What is not.



LEO:  Make your algorithms public.  And oh, by the way, you can't change them unless you have a period of publishing it and stuff.  So all this does is tell people how to game the system.  It achieves nothing.  I'm sorry.  Go ahead.  Do your show.  You get me all heated up.



STEVE:  No, no, no, Leo, we like to hear from you.



LEO:  I'm all het up now.



STEVE:  And number six:  "Stop discriminatory algorithmic decision-making."  They said:  "We need strong protections to ensure algorithms do not discriminate against protected groups, such as by failing to share key opportunities equally by discriminatorily exposing vulnerable communities to risky products, or through persistent surveillance."  So, yeah.



You know, what was the famous line?  We're the government, and we're here to help?



LEO:  But, you know, honestly, I don't think we should depend on the tech platforms to regulate themselves.



STEVE:  No.



LEO:  Because they won't.



STEVE:  No, no.



LEO:  So government needs to, but it needs to do so intelligently, not stupidly, and not with a political axe to grind.



STEVE:  I mean, capitalism has a lot going for it.  One of the problems it has, however, is it does tend to form monopolies.  It naturally forms monopolies.  Someone or group will get bigger than others, and they will use the power of their bigness to continue to accelerate.  So that creates positive rather than negative feedback.  And it's unstable.  So it's a good system, but it needs management.



LEO:  Yup, yup, yup.



STEVE:  And we've got something like that here happening.



LEO:  I agree.  And so we do need regulation, but boy.  Maybe we need people under the age of 50 to do it.  Perhaps that's the problem.  They just...



STEVE:  Well, and you know, in the case of the Internet, we also have a single global network carrying services which straddle nations whose governments grant their citizenry widely differing rights and which restrict the behavior of their enterprises in widely different ways.  So how does, I mean, presumably they've been trying to so far.  How does a single Facebook, Twitter, Instagram, or Google simultaneously satisfy the widely differing requirements of different geographical regions of the globe?  You know, I mean, these are hard problems.



Oh, before I leave the subject of governments, Bert Hubert - and you know, Mr. and Mrs. Hubert...



LEO:  Why?  Why Bert?  Why Hubert?



STEVE:  I don't know.  Really?  Did you really have to name your son Bert?



LEO:  Do you think his real name is Hubert Hubert?  Humbert Humbert?



STEVE:  Oh.  Maybe.  Anyway, he's a member, or actually he was, of TIB, the Dutch government board that checks the legality and approves communications interception warrants for the Dutch intelligence and security services.  Well, as I said, he was, because he resigned last week.  The automatic English translation of Bert's blog posting explaining his decision was so atrocious, he said, that he wrote an English version himself.  And I'm glad he did because, if I were serving in a government that I believed in, I'd hire this guy in a second based on what he wrote.  So here's what he said.



He said:  "If either of the civil or the military intelligence and security services of the Netherlands want to use a lawful intercept, SIGINT, or hacking, or some other legal powers, they must first convince their own jurists, then their ministry, and finally the TIB.  The TIB then studies if the warrant is legal, and that decision is binding."



He said:  "When I joined the regulatory commission, I was very happy to find that the Dutch intelligence and security services were doing precisely the kinds of things you'd expect such services to do.  I also found that our regulatory mechanisms worked as intended.  If anything was found to be amiss, the services would actually stop doing that.  If the ex ante regulator," meaning upfront in advance, he says:  "(i.e., my board) ruled a permission to do something was unlawful, it would indeed not happen."  He says:  "I think it is important to affirm this in public.



"Over the past two years, however, there have been several attempts to change or amend the Dutch intelligence law.  The most recent attempt has now cleared several legislative hurdles and looks set to be passed by parliament."  He said:  "Under this new law, my specific role, technical risk analysis, would mostly be eliminated.  In addition, the Dutch SIGINT bulk interception powers would be stripped of a lot of regulatory requirements.  Furthermore, there are new powers, like using algorithmic analysis on bulk intercepted data without a requirement to get external approval.  Finally, significant parts of the oversight would move from up front ('ex ante') to ongoing or afterwards ('ex post').



"Doing upfront authorization of powers," he says, "is relatively efficient, and is also pleasingly self-regulating.  If an agency overloads or confuses its ex ante regulator, they simply won't get permission to do things.  This provides a strong incentive for clear and concise requests to the regulator.  A regulator that has to investigate ongoing affairs, however, is in a difficult position.  It can easily become overloaded, especially if it's unable to recruit sufficient technical experts.  In the current labor market, it is unlikely that a regulator will be able to swiftly recruit sufficient numbers of highly skilled computer experts able to do ongoing investigations of sophisticated hacking campaigns and bulk interception projects.  An overloaded regulator does not provide good coverage.  It is also vulnerable to starve the beast tactics."



He said:  "Once it became clear the intended law would likely pass parliament, I knew I would have to resign anyhow, since I don't agree with the new expanded powers and the changes in oversight.  As a member of the regulatory board, I could not share my worries about the new law.  The regulatory board itself is staffed with excellent people, but by design the board only operates within the existing law.  It is not responsible for formulating or even criticizing any new laws.



"Instead of waiting out the likely passing of the new law, I've decided to leave now.  This enables me to speak my mind on what is wrong with the new law.  It may not help, but at least it's better than watching democratic backtracking in silence.  It has been a great honor to have been part of the regulatory powers board.  Its staff and members are an impressive bunch, and I wish them the best of luck with their ongoing and important work.  On a final note, if anyone is looking for a government regulator with a proven track record of resigning when things go wrong, know that I'm available."



LEO:  That's great.  You're hired.  Wow.  Good for Bert Bert, or whatever his name is.  That's great.  That's great.  Wow.



STEVE:  Yeah.  And it's also worth noting, although Bert didn't mention it in his blog posting, that his TIB flagged several cases of abuse last year that targeted journalists and several cases of broad warrants that intercept bulk traffic over entire global Internet cables.



LEO:  Wow.



STEVE:  So his term for what he sees happening I thought was great.  He called it "democratic backtracking."  And I thought this was worth sharing since it shows the way democracy will decay if it's not fully understood and continually reinforced.  As I said before, it's not an inherently stable system since it is subject to creeping manipulation.  You know, just think of the U.S. Tax Code if you need another example of creeping manipulation.



LEO:  Oh, yeah.



STEVE:  You know?  Some group of right-minded people originally established the operation of the Dutch regulatory commission to work the way it does today for a reason, for at least some of the reasons Bert has explained.  But who knows?  Maybe those who did this are now out of power, and those being regulated had been chafing at the limitations the current system deliberately imposes upon them.  Yes, it's inconvenient and annoying.  It's meant to be.  Surveillance of a free and democratic people should not be the default.  It should be the exception.  And it does seem that initiating the surveillance first and asking for permission, either concurrently or afterward, is far more likely to lead to abuse.  Again, the question is what principles do we want to support?  So bravo, Bert.



LEO:  Bravo, Bert.



STEVE:  I'm sorry you needed to resign.  But that's what people have to do, if they see things happening around them that they cannot participate in, in good conscience.



LEO:  Awesome.  And I'm glad you gave him a forum.  You know?



STEVE:  Yeah, yeah.  Okay.  Another near-constant event that I choose to only cover periodically - actually, you and I talked about it after we stopped recording last week, Leo - is horrendous problems occurring in QNAP NAS software.  You know, it's just constant.  Since it's entirely possible to run a non-QNAP OS on their QNAP hardware, I dearly hope that anyone listening to this podcast will have switched out QNAP's constantly disappointing firmware for any of the Linux or Unix alternatives that are known to run on the hardware.  In fact, QNAP's own platform is a Linux derivative.  So you can do that.  And again, Google will show you how.  And if you do need to remain with QNAP, please by all means protect it from the public Internet.  We've talked about many ways to do that in the past.  Even now QNAP themselves has told their own users not to expose their devices to the Internet, despite the fact that they're network storage.



LEO:  That's the whole point.



STEVE:  I know.  



LEO:  Okay.



STEVE:  I know.  Okay.  So Deadbolt is both a ransomware and a ransomware group that has been plaguing QNAP users and their devices throughout 2022, all year.  Since January, thousands of QNAP customers have reported being attacked by the DeadBolt ransomware group.  The group demands a ransom of 0.03 bitcoin, currently around $1,100, for the decryption key.



After the initial attacks affected about 3,600 devices last January, the group continued to resurface with campaigns in March, May, and June of this year.  They're a persistent bunch.  Reddit and other message boards have been flooded with customers lamenting the loss of files that included family photo albums, wedding videos, and more.  You know, irreplaceable things.  Dozens of users took to Reddit to complain that they were among those attacked in the latest campaign.



In a note to QNAP, the hackers demanded 5 bitcoin, which would be about just shy of $94,000, to reveal details about the alleged zero-day vulnerabilities they initially used to attack its users; and another, whoo, 50 bitcoin, which is just shy of a million dollars, to release a master decryption key that would unlock all of their victims', their users' victim files.



Now, QNAP would not say whether it has considered paying the ransom for the universal decryption key, which is to say, uh, no.  But we can be pretty sure that that's not going to happen also when a spokesman said the company's research - so this is the QNAP spokesman said the company's research has shown that the Deadbolt group is attacking legacy versions with known vulnerabilities which have security updates available.  Okay.  Sounds logical, reasonable.  Maybe true.  In other words, they're saying it's the users' fault, not theirs, so they should pay if they want their data back.



Now, some users have disputed QNAP's insistence that only devices that have not been updated are being attacked.  And it kind of seems reasonable since the group behind this, the Deadbolt group, are coming up with new ways to do this all the time.  And here's a little bit of a gotcha.  If ransom is paid, the key provided by Deadbolt may not work.  So the security company we've talked of several times, Emsisoft, released its own version of a DeadBolt decryptor after several victims reported having issues with the one they received in exchange for paying a ransom.  However, it's not any sort of universal decryptor.  It only works with a decryption key supplied by the operators of the DeadBolt ransomware through a ransom payment.



Emsisoft's Fabian Wosar tweeted:  "QNAP users who got hit by DeadBolt and paid the ransom are now struggling to decrypt their data because a forced firmware update issued by QNAP removed the payload that is required for decryption."  Okay.  So this got so bad finally that QNAP took matters into their own hands and forced a firmware update onto their customers, which broke the ability for the ransomware payment, after receiving the decryption key, to function.  So, wow.  Emsisoft came along and said, okay, we'll fix that for you.  And they did.  What a mess.



Earlier this year, the security company Censys who runs that IoT search engine - remember we've often talked about Shodan.  There's now another kid in town, Censys.  And they're doing neat things.  Anyway, they have a search engine that goes wide and deep for IoT stuff.  They reported - and a QNAP NAS is considered an IoT device.  They reported that of the total 130,000 QNAP NAS devices sold, 4,988, so just shy of 5,000 of those servers, exhibited the telltale signs of this specific piece of ransomware.  So about 5,000 compromises.



Censys also managed to track the bitcoin wallet transactions associated with an infection and found that of the previous batch of victims, 132 paid ransoms totaling about $188,000.  So this is making money for someone who is saying we'll give you all of the stuff you've lost on your NAS for $1,000 in bitcoin.  132 people in that particular batch did so.  Censys also created a dashboard to track the number of victims around the world.  The majority of the most recent infections are taking place in the U.S., Germany, and the U.K.  And it's not over.



Since all of that, and this is really what finally caused this to rise above my QNAP threshold, Censys observed that the number of QNAP NAS devices infected by the same Deadbolt ransomware spiked from 2,144, which was the count on July 9th, to 19,029 on September 4th, which was Sunday before last.  The spike arose because the ever-industrious Deadbolt gang exploited, yes, another new zero-day vulnerability in the Photo Station app which is installed on most QNAP NAS systems.  So they're finding more new ways in.  Again, if you have a QNAP NAS with QNAP software, get it off the Internet.  And if you can, put in a replacement software set.  And it is possible.  There are third-party solutions for QNAP.



Oh, and before we leave the Censys Internet scanning company, it's worth noting that they recently published a "2022 State of the Internet Report" which observed that misconfigurations accounted for 60% of the issues they observed across all Internet-exposed services globally.



LEO:  Holy cow.  That's a good number.  Wow.



STEVE:  Yeah.  They found that software problems only accounted for 12% of all observed problems.  That is, software vulnerabilities.  So all of these problems are misconfigurations.  Now, it's unclear whether placing a QNAP NAS onto the Internet would inherently be considered a misconfiguration of those devices, but it seems pretty clear that it should be.  You do not want to put one of those things on the 'Net.



Also, one last IoT note.  D-Link is currently being taken over by MooBot, M-O-O-B-O-T.  Palo Alto Networks' Unit 42 has identified a three-year-old Mirai botnet variant known as MooBot.  It's rapidly finding and co-opting any remaining vulnerable D-Link routers into another army of denial-of-service bots by taking advantage of multiple old and two new, but all patched, exploits.  Last Tuesday Unit 42 said:  "If the devices are compromised, they will be fully controlled by attackers, who could utilize those devices to further conduct attacks such as distributed denial-of-service."



Okay.  So MooBot, which was first identified and disclosed by the Chinese group, the Qihoo 360's Netlab team, back in September of 2019, so three years ago, MooBot has previously targeted LILIN digital video recorders and those Hikvision video surveillance products we were talking about a couple of weeks ago.  In the latest wave of attacks discovered by Unit 42 early last month, as many as four different highly critical flaws in D-Link devices are being used in the development of MooBot samples.  So the four flaws, the oldest, believe it or not, CVE-2015-2051, carries a CVSS, you know, got to love this one, of 10.0.  I mean, it must just be that you connect to the D-Link router and say, "Please, sir, may I enter," and it says, "Oh, by all means, make yourself at home."  How do you get a 10.0?



LEO:  I don't know.  Wow.



STEVE:  So that one is - it's called the HNAP SOAPAction Header Command Execution Vulnerability.  You probably just put a command in the header, and it runs it for you.  CVE-2018-6530, so back from year 2018, that's got a CVSS of 9.8.  Still right up there with the best of them.  This one is the D-Link SOAP Interface Remote Code Execution Vulnerability, sounds kind of generic, but okay.  Then the other two are 2022, this year's CVEs, both also carrying scores of 9.8.  D-Link goes big.  They are both also Remote Command Execution Vulnerabilities.



So as I said, successful exploitation of any one of those four flaws, which all have very low attack complexities, so we're told, is used to remotely launch a WGET command which retrieves the MooBot payload from a remote host.  MooBot then, after it's started, parses instructions from a command-and-control server to launch DDoS attacks in ways we're all too familiar with.  So although the oldest vulnerability is from 2015, and the next oldest is from 2018, those other two, which are 9.8 remote code execution vulnerabilities, known and patched, were only fixed this year.  So anyone who knows anyone who uses a D-Link router should be certain that they have updated recently because these Deadbolt guys are on the prowl, and they're looking for all the routers that they can get themselves into.  Okay.  Leo?



LEO:  Would you like me to do something?



STEVE:  No.  I'm good.



LEO:  No?  Okay.



STEVE:  But thank you.  Shortly.



LEO:  I'm here when you need me, man.  I'm just - I just hear your voice.



STEVE:  Last week...



LEO:  Oh, I want to hear about this, yes.



STEVE:  I introduced our listeners to my latest science fiction reading discovery, Scott Jucha's "The Silver Ships," thanks to one of our listeners.  I have been having so much fun ever since.



LEO:  Oh, now you've got me.  Now I've got to have it.



STEVE:  Leo, I'm now halfway through the fourth book.  Actually I'm at 71%.  And I can assert that this is the most engaging and satisfying series of novels I have read in a long, long time.  And just wait until you meet the Swei Swee.  For those of you who prefer to have books read to them, I'm so glad, Leo, that you said that the reader of this series is someone you know and enjoys listening to.



LEO:  Yeah, he's good, yeah.



STEVE:  Because I wouldn't want anything to spoil the experience of Audible's listeners.  My initial mild concern after only the first book was that Scott Jucha's character development might be overly focused upon his story's central character, a young asteroid miner by the name of Alex Racine.  Well, that concern has dissolved completely.  We now have a broad cast of wonderful characters.  And this guy writes so well.



I was trying to put into context for myself how good this book series is.  I'm just giddy reading it.  I know that there have been times in the past when I have been this thrilled over a science fiction storyline - the Honor Harrington novels, Michael McCollum's Gibraltar Stars trilogy.  And I'm sure that some of Peter Hamilton's stories did this, although there was always a lot to wade through with his major work.  And there must be others since it is a familiar feeling for me to be so satisfied when I'm reading the inventions of a skilled storyteller who really knows how to weave a yarn and who has come up with a bunch of great new sci-fi technologies, and people, both human and non.



Another thing I've noticed is that, like the best serialized stories, a lot happens in every installment.  So far there has been no sense of Scott stringing us along.  Even when the action slows down for a while, as almost has to happen from time to time, there turns out to be a real purpose in the way we were spending that time.



And you know, the best books always cause those of us who love to read to immediately wish for amnesia so that the story can be experienced again new.  Anyway, this series is the equal of any I've read.  And, oh, boy, prepare yourself for a huge surprise at the start of book three.



LEO:  Oh, boy.



STEVE:  So anyway, it is just - it is so good.



LEO:  I can't start it till September 22nd.



STEVE:  I understand.



LEO:  I need my Audible credit.



STEVE:  I know you've got a backlog.  I know you have a backlog.  Oh, speaking of which, funny you should mention that.



LEO:  Yes.



STEVE:  The tweet from x4jw.  He said:  "Hi, Steve.  Thanks for the recommendation of The Silver Ships series of books.  Went to purchase Book 1 on my Audible account and was surprised/delighted to see that Books 2, 3, 4, 5, 7, 8, 9 are all included for free as part of the Audible Plus membership, and I just had to click ADD TO LIBRARY to grab those."  He said:  "I can understand why Book 1 is not free.  Not sure why Book 6 isn't."  So again, it's you've got to buy Book 1, then 2, 3, 4, 5, 7, 8, 9 are all free.



LEO:  Nice.



STEVE:  And then he said:  "Books 10-20 are also purchase-to-own titles on Audible."  He said:  "Anyway, just thought 'readers'" - he has in quotes - "using Audible might like to know that they can get 35% of the series for free as part of the Plus subscription service."



LEO:  And by the time you're hooked at Book 11, you go buy the rest of them.



STEVE:  Leo, as I said, I'm at 71% in Book 4.  I'm astonished by what this guy has done.  It's just, you know, we were talking about bullies.  And, oh, in Book 4 there are some people you just want to have get what's coming to them.  And, oh, do they ever.



LEO:  Oh, I might have to buy this one ahead of time just to get started.  I love it that I get so many of them for free.  That's good.



STEVE:  Yeah.



LEO:  You know, so the guy who reads it, Grover Gardner, I've spent some time with because he read Steven King's "The Stand."  So I've spent at least 48 hours listening to Grover Gardner.  Actually listened to several of his audiobooks.  He's got, you know, you might at first say, well, I don't like his voice.  Just bear with him.  I think for something like this he's a good choice because he's a very clear, simple, easy to understand reader.  And it sounds like there's a lot of content here.  So this'll be good.



STEVE:  Ohhh.



LEO:  Yeah, I really like Grover Gardner.



STEVE:  Okay.



LEO:  Fun.  Thank you.



STEVE:  Okay.  Let's take our break, and then we're going to talk about the EvilProxy Service.



LEO:  I appreciate your recommendations, Steve.



STEVE:  So far.  I get a lot of feedback from our listeners saying, you know, they've liked everything I've suggested.



LEO:  Oh, it's always great to get them, you know.  And we have Stacey's book club, which is a sci-fi book club.  I get some, you know, between this show and This Week in Google, that's why  my list is so long on Audible, and I'm so far behind.  I get so many good recommendations.  But I think I might start this sooner than later.  You're really selling me.  Let's talk about this EvilProxy.



STEVE:  Oh, boy.  So, wow.  As you said, and you hit it exactly right, Leo, in retrospect it was obvious that this was going to happen.  Last Monday the security research group called Resecurity published their findings about a recently appearing, just last May, new, fully functional, turnkey, Phishing-as-a-Service system known as EvilProxy.  Key among the many powerful features of this new underground service debuting on the Dark Web is its effortless ability to intercept SMS, OAuth, and one-time token, multifactor, you know, time-based token, multi-factor authentication flows.  As a result, the "Login with some other website" like Google or Facebook, or enter the SMS code we just sent to your phone, or enter the six-digit code displayed on your authenticator, are all effortlessly bypassed and rendered ineffective.



Okay.  This is all accomplished by streaming the actual target website, where the naive user believes they are logging in, through a transparent reverse proxy, which I'll explain further in a minute.  They're not actually where they think they are.  And unless they are scrupulously attentive to the URL being displayed in their browser's URL bar, they will be unwittingly providing their full authentication credentials, including any form of multifactor authentication, to a malicious third-party who will intercept their successful login session token to obtain a full secondary login to their account, with all the rights that arise from that.



Okay.  So this sort of proxying is one of the inherent Achilles heels of the way the web works.  I remember clearly one summer, when I was deep into the work on SQRL, I brought my work to a halt in order to completely wrap my head around this whole problem of spoofing because it's a tough one.  I felt as though I still didn't have an absolutely crystal clear understanding of exactly where the problem arose, and I needed SQRL to solve it, if it was possible.  Anyway, I figured it out.  And the result for SQRL was something called CPS, Client Provided Session.  And it does indeed, once and for all, completely solve this problem.



Since I'm at work on SpinRite 6.1 I haven't taken the time to determine whether the FIDO2 and the WebAuthn folks also solved this problem.  And I guess it doesn't matter whether it does solve it or not, since the Passkeys system is what we'll eventually be getting.  In fact, it's live in iOS 16, which is now on iOS devices that are able to take it.  But for what it's worth, it is possible to completely solve the problem, and that's another one of the things that SQRL does.



Anyway, remember the wonderful observation which we credit to Bruce Schneier.  I mentioned it at the top of the show.  He said something to the effect of "Attacks never get weaker, they only ever get stronger."  And we're about to see an example of Bruce's observation on steroids.  The thing that is so chilling about this new EvilProxy service is exactly that:  It's a service.  That horrifying, we thought, Log4j Java vulnerability which began the year is certainly a problem.  But as we've previously described, it turned out not to be the end of the world for one reason:  It was not a slam-dunk, drop-and-go, easy-to-use vulnerability.  Every specific instance of its use needed to be deliberately engineered for the specific target where that potential vulnerability might be exploited.  And the industry learned an important lesson from that:  It matters far less whether something is possible than whether it's easy.



Which brings me to why this new EvilProxy Phishing-as-a-Service facility is so horrifying.  The service providers have created an astonishingly powerful, simple-to-use, point-and-click web interface for their service.  Through this interface, powerful phishing campaigns can be created by filling out some fields, selecting the required features, and pressing a Create Campaign button.  If the Log4j vulnerability never exploded because it was difficult to use, this EvilProxy service promises to be an instant hit because it could hardly be any easier to use.



So that everyone can see for themselves, this week's GRC shortcut of the week, so that's grc.sc/888, will bounce its user's browser to a four-minute Vimeo video.  That's Vimeo's number 746 020 364.  But you can just put in grc.sc/888. That will show you a video which the EvilProxy service provider uses to market and demo the ease of use of their tool.



Okay.  So now let's back up a bit for a bit of a broader overview of Resecurity's discovery from their coverage of this.  The title of their report was "EvilProxy Phishing-as-a-Service with MFA Bypass Emerged in Dark Web."  They said:  "Following the recent Twilio hack leading to the leakage of two-factor authentication one-time-password codes, cybercriminals continue to upgrade their attack arsenal to orchestrate advanced phishing campaigns targeting users worldwide.  Resecurity has recently identified a new Phishing-as-a-Service" - and then they have PhaaS, in fact, in the same way that Ransomware-as-a-Service is RaaS.



LEO:  PhaaS.



STEVE:  Yes.  So PhaaS - "called EvilProxy advertised in the Dark Web.  On some sources the alternative name is Moloch (M-O-L-O-C-H)."



LEO:  Yeah, evil, Moloch.



STEVE:  "Moloch, which has" - you want some Moloch? - "which has some connection to a phishing kit developed by several notable underground actors who targeted the financial institutions and ecommerce sector previously.  



"While the incident with Twilio is solely related to the supply chain, cybersecurity risks obviously lead to attacks against downstream targets, the productized underground service like EvilProxy enables threat actors to attack users with enabled multifactor authentication on the largest scale without the need to hack upstream services."



They said:  "EvilProxy actors are using Reverse Proxy and Cookie Injection methods to bypass multifactor authentication - proxyfying victim's session.  Previously such methods have been seen in targeted campaigns of advance persistent threat and cyberespionage groups; however, now these methods have been successfully productized in EvilProxy, which highlights the significance of growth in attacks against online services and multifactor authorization mechanisms.



"Based on the ongoing investigation surrounding the result of attacks against multiple employees from Fortune 500 companies, Resecurity was able to obtain successful knowledge about EvilProxy including its structure, modules, functions, and the network infrastructure used to conduct malicious activity.  Early occurrences of EvilProxy have been initially identified in connection to attacks against Google and Microsoft customers who have enabled multifactor authentication on their accounts, either with SMS or Application Tokens."  In other words, you know, authenticators.



They said:  "The first mention of EvilProxy was detected early May 2022.  This is when the actors running it released a demonstration video detailing how it could be used to deliver advanced phishing links with the intention to compromise consumer accounts belonging to major brands such as Apple, Facebook, GoDaddy, GitHub, Google, Dropbox, Instagram, Microsoft, Twitter, Yahoo, Yandex, and others."



LEO:  Was it on YouTube so we could all enjoy it?  Geez, Louise.



STEVE:  I know.  And Leo, if you scroll down in the notes, look at some of the screenshots I've attached.  I'll get to them in a second.



Then they finished:  "Notably, EvilProxy also supports phishing attacks against Python Package Index," which we were just talking about, PyPI.  Okay.  So in their report these guys embed a screenshot from the EvilProxy control panel showing the entry and options for proxying PyPI login and authentication.  It shows that login, password, and session cookies are supported, meaning that they're captured, and the user can choose to have the service running for 10 days for $150, 20 days for $250, or 31 days for $400.  So your typical quantity discount schedule.



LEO:  God.



STEVE:  Up at the top of the page we see a .onion URL, so this is all being hosted by a hidden Tor Project Onion Service.  And below is the control panel page selector showing a shopping cart icon labeled "Available Services & Prices" next to a circled dollar sign icon labeled "Account Balance."  Conveniently, on the left, is an expandable dropdown labeled "Campaign URLs," and underneath that is "Create Campaign."



The Resecurity guys addressed the point of targeting software repositories.  They said:  "The official software repository for the Python language (Python Package Index PyPI) said last week that project contributors were subject to a phishing attack that attempted to trick them into divulging their account login credentials.  The attack leveraged JuiceStealer as the final payload after the initial compromise and, according to Resecurity's Hunter team findings, related to EvilProxy actors who added this function not long before the attack was conducted," suggesting strongly that EvilProxy was the reason that the PyPI system was attacked in a phishing attack.



"Besides PyPI, the functionality of EvilProxy also supports GitHub and npmjs [of course], the JavaScript Package Manager which is widely used by over 11 million developers worldwide, which enables supply chain attacks via advanced phishing campaigns.  It's highly likely the actors aim to target software developers and IT engineers to gain access to their repositories."



And again, remember, this is not the EvilProxy people doing the attack.  EvilProxy is merely now a service in the same way that ransomware attacks were being conducted by affiliates using the Ransomware-as-a-Service service.  So what we have is we have random cybercriminals now starting to leverage the EvilProxy service to launch sophisticated phishing attacks using that service.  So we're already seeing evidence of the EvilProxy service in use.



Okay.  So how does all this work?  As I mentioned before, the Internet, and the World Wide Web specifically, have an inherent problem which is created by the web's brilliantly flexible and powerful underlying technologies.  The URL itself, the URL as a thing, was originally intended to be fully human readable, even human typeable.  But as we've seen, and we've all watched the evolution of web-hosted services through the past few decades, we've watched the readability  and certainly the typeability  of URLs virtually disappear.  As I'm typing this text into Google Docs, I look up, and I see a URL that appears to be mostly random character gobbledygook.



And significantly, I opened and have been editing this document at this point for the past three hours, yet that was the first time my eyes fell upon this page's URL.  Why did I have any reason to believe I was at the right place?  I was sure I was because the page looked the way I expected it to look.  I never had any doubt.  So I never sought or received any further confirmation beyond the composition of the page I'm visiting.



I'm one of the hundreds of thousands of people listening to this podcast.  I'm one of us.  How do we imagine that a normal Internet user regards all of the utterly indecipherable things that their web browser does?  And we've added all of this script-driven automation to the user's experience, too.  When a user clicks on a link in a search engine, on a social media site, or in email, they may have noticed their URL bar flickering rapidly as their browser dances among all of today's various third-party link tracking services.  Everyone wants to get in there for a piece of the action.  So we've fully eliminated any sense from even an unusually savvy user that they should worry about the details of what's going on there.  That's just the way things are today.



EvilProxy leverages the "reverse proxy" principle which is made possible by all of this inherent flexibility we've built into the web.  Conceptually, the way it works is simple:  The bad guys lead their intended victim to a phishing page.  We've talked about phishing extensively in the past; right?  You know, it's popal.com.  That page uses what's known as a reverse proxy to fetch and display from the legitimate page all of the legitimate content the user expects to see, including login pages, and it sniffs their traffic as it passes through the proxy.  It's a classic man in the middle.  This "in the middle" position allows the middleman to harvest the valid web browser session cookies which are eventually passed back to the victim user, thus using the victim as an authentication mule to provide the usernames, passwords, and even two-factor authentication tokens.



Remember also that, while the man in the middle is able to intercept and forward one-time tokens for their one-time use, they also intercept and obtain the resulting session authentication cookies because the reverse proxy terminates TLS encryption in each direction.  It sees everything in the clear.  This means that anyone not using some form of additional one-time multifactor authentication will have their username and password stolen in the clear for future use.



The Resecurity guys obtained videos released by the EvilProxy service providers demonstrating the use of their point-and-click setup to steal the victim's session and successfully authenticate through Microsoft two-factor authentication and Google's email services to gain access to the target account.  The more you see, the more chilling it all is.  I've included the link to Resecurity's full report which embeds additional Vimeo videos for anyone who wants to become even more frightened.



As I noted above, EvilProxy's services are offered on a prepaid account basis.  When the end user cybercriminal chooses a service of interest to target  Facebook, LinkedIn, whatever  the activation will be for a specific period of time, as I said, 10, 20, or 31 days described in the plan's itemized description.  And Leo, there's another screenshot further down.  Yup, there it is.  One of the key actors, using the moniker "John_Malkovich," acts as gatekeeper administrator to vet all new customers.  The service is represented on all major underground communities including XSS and Exploit, both of which we've talked about before, and Breached.



Payments for EvilProxy are arranged manually via an operator on Telegram.  Once the funds for the subscription are received, they're deposited into the account in the customer portal hosted in TOR.  Use of the service is available for $400 per month in the Dark Web hosted in the TOR network.  And in the show notes and on the screen in the video we see the options for creating campaigns where Dropbox is used as the phishing target, RubyGems, Yandex, Yahoo, Microsoft, and the list - that looks like the list is about maybe half of the scroll length based on the scroll thumb that we see over on the right.  So, and more services are being added continually.  And in fact for the Microsoft box we see Xbox.com, Skype.com, OneNote.com, Office.com, MicrosoftOnline.com, Microsoft.com, Live.com, and Bing.com.  So you get to choose your target of the phishing attack.



The EvilProxy portal contains tutorials and interactive videos explaining and demonstrating the use of the service and configuration tips.  So the bad guys have done a state-of-the-art job in terms of the service usability and configurability of new campaigns, traffic flows, and data collection.  After activation, the operator will be asked to provide SSH credentials to further deploy a Docker container and a set of scripts.  This approach was likely borrowed from a previous Phishing-as-a-Service called Frappo which the Resecurity guys identified earlier this year.



So what does this all mean?  While access to the EvilProxy service requires individual customer/client vetting, cybercriminals now have a cost-effective and scalable point-and-click solution which provides them with all the backend machinery required to enable them to run advanced phishing attack campaigns on their own while having no skill whatsoever about how to actually do the technology.  That's all now turnkey, provided for them, just as Ransomware-as-a-Service was.  And that includes bypassing state-of-the-art multifactor authentication, which is no protection against any of these.



The appearance of such a service on the Dark Web will undoubtedly lead to a significant increase in account takeover business email compromise activity and cyberattacks targeting the identity of end users, where MFA may now be easily bypassed with the help of tools like this one.  And EvilProxy has no corner on the market.  All they really did was to fully automate an already existing aspect of advanced cybercrime.  They have made it trivial to do.  They clearly got the idea from the preceding Ransomware-as-a-Service control panels, which act just the same.  And as we know, those have been way too successful for exactly the same reason that EvilProxy promises to be.



And we know what'll happen next.  Other cretins will see it and decide to compete with it.  Once multiple such services exist, competition will drive continued evolution in the features and will also drive down the cost to use them.  We built a very powerful and capable World Wide Web whose features are increasingly being used against us.  The creation of reverse proxy exploitation, followed by an easy-to-use turnkey service, well, it was probably inevitable, but it's certainly not good news.



LEO:  Wow.  PhaaS.



STEVE:  Yes, PhaaS.  Phishing-as-a-Service.  And so now the script kiddies...



LEO:  Anybody can do it, yeah.



STEVE:  Yup.  Anybody can do it.



LEO:  This is the problem I have with things like the WiFi Pineapple, too.  It's like, oh, well, it's a proof of concept.  Or you could use it for pentesting.  But you really just make it easy for people with malintention and no skill to act out.  And that just means more people can do it.  Oh, well.  I don't understand - it's funny, you and I, and I'm sure all of our listeners have a moral compass and just can't fathom how somebody could do this.



STEVE:  No, it's why that job offer from the government was so appealing.  It's like, wait.  You mean I could do this for the U.S. government?  And get a paycheck?



LEO:  Still get to do it, yeah.



STEVE:  You know?



LEO:  Yeah.



STEVE:  No, I agree with you.  It's, well, and I told you that I have turned down some solicitations in the past.



LEO:  Sure.



STEVE:  They knew I was able to do these things, and they said, I mean, and this was our government, said we'd like you to do this.  And I couldn't even do that because...



LEO:  I'll say this to anybody who might be teetering on the edge, thinking, well, I could really use the money.  Maybe I'm, you know, my family needs the money or whatever.  If you have a moral compass, follow it.  You will never go wrong.  And at any point when you don't, you will regret it, and it isn't a good thing.  You know, just stick with your moral compass.  Stick with your moral tenets, your deeply held beliefs.  Do what's right.  Don't be tempted by what's wrong.



STEVE:  In the meantime, as a takeaway to our listeners, and to everybody we are talking to knows and loves, be so careful.



LEO:  Oh, gosh, yes, yes.



STEVE:  About clicking links in email.  That's the way this happens.  That's the starting point for all of this is the innocent-looking, seductive-looking, expected-looking, whatever it is, I mean, this is not the Nigerian prince anymore.  Nor is it your car insurance needs to be renewed.



LEO:  It's clever.  It's very believable.  It's very believable.



STEVE:  Yes.  And the problem is this means that we're going to see a dramatic increase in the amount of these sorts of attempts to get us to click something.  And it's going to be believable, exactly as you said, Leo.  But again, it's a matter of scale.  And unfortunately this is going to cut loose a jump in the scale at which these sorts of campaigns occur.  I mean, it's probably going to get to the point where smart people refuse to click anything in email.



LEO:  Yeah.  Or believe anything you see in a text message.  I mean, I don't know about you, but I get text messages every day from Amazon and my bank and other companies that aren't my bank, saying, you know, oh, you've got to act now.  Something's gone wrong.  Quick, call this number.  And it's very easy to fall for this.  I really, I spend almost I think now every radio show 10 minutes talking about this because people need to hear it and really need to gird their, you know, prepare themselves for battle when they go out on the Internet.



STEVE:  It's sad.



LEO:  Put your armor on.  It is sad.



STEVE:  True.



LEO:  It's very sad.  You know what's not sad?  That this guy here gets a better night's sleep tonight because he did this show.  Thanks for listening.  Thanks for getting him to do it.  Episode 888, which is the super lucky episode.  You know that; right?  Eight is a very lucky number.  Eight eight eight.



STEVE:  Yes, happy to have it.



LEO:  Yeah.  You'll find copies of this show in a couple of places.  Steve has them.  In fact, he has two unique forms of this show at his website, GRC.com.  He's got the 16Kb audio, which admittedly is not, you know, hi-fi.  But it is a small form factor for people who are bandwidth-impaired.  Also those wonderful transcripts Elaine Farris does for every single show, which allow you to read along as you listen, to search for the part you're looking for, all of that plus 64Kb high-quality audio available at GRC.com.



While you're there, check out SpinRite, the for the last 20-some years, more than that, 30 years, the world's finest mass storage maintenance and recovery utility.  If you have storage, hard drive or SSD, you need SpinRite.  That's Steve's bread and butter, so go there and get a copy.  If you buy 6.0 now, you'll get 6.1 as soon as it's done.



STEVE:  Only slightly delayed due to this new author.



LEO:  Damn you, Silver Ships.  Now that I know that, I'm not going to read that book.  Only slight.  Just a little bit.  Just a little bit.



STEVE:  Because I'm a fast reader, and I'm still working on SpinRite.



LEO:  Yeah, yeah.  Steve's never going to stop working.  Just taking, it's good, you need a little break, little down time for the brain.  Let the steam cool off a little bit, you know, at the ears, get the smoke out of the ears.  You can also find so much other stuff at Steve's site, it's worth visiting, GRC.com.  Do leave him feedback there, if you want, GRC.com/feedback.  Or on his Twitter account.  His DMs are open at @SGgrc.



We have 64Kb audio and video of the show at our website, TWiT.tv/sn.  There is a Security Now! YouTube channel, which is probably the easiest way, if you want to share like a little tidbit with a co-worker or a friend or a boss.  You just snip it out of the YouTube, makes it very easy.  So look for the Security Now! YouTube channel.  And of course, you know, the thing most people will end up doing is getting a podcast player and subscribing, because that way you get it automatically every Tuesday afternoon, the minute it's done being polished up and edited.



We do the show live, if you want to get it the soonest, you can watch us do it live at live.twit.tv.  Supposed to be 1:30 Pacific, 4:30 Eastern, 20:30 UTC, depending how long MacBreak Weekly goes.  Sometimes it's delayed by half an hour or so.  Be patient.  It will show up on the stream.  If you're watching live, chat live at irc.twit.tv.  That's Tuesdays.  I think that concludes this thrilling, gripping edition of Security Now!.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#889

DATE:		September 20, 2022

TITLE:		Spell-Jacking

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-889.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at last week's Patch Tuesday and at the changing cyber insurance landscape.  We visit and revisit a collection of major network breaches at Uber, Rockstar Games and LastPass.  We look at another significant problem facing 280,000 WordPress users and at a recommended mitigation for the future.  We examine the cost to processing performance of the most recent Retbleed security mitigations, and look at Google's very welcome use-after-free vulnerability technology.  And after sharing a few pieces of feedback from our listeners, we examine a somewhat surprising consequence of enabling Chrome's enhanced spell check and provide some mitigations.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, as always, the week jam-packed with security news.  We'll talk about the breaches at Uber and Grand Theft Auto and whether they're related.  We'll also look at Google's very welcome use-after-free vulnerability technology.  And as long as we're talking Google, a very important setting you'll want to turn off in Chrome.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 889, recorded Tuesday, September 20th, 2022:  Spell-Jacking.



It's time for Security Now!, the show where we cover the latest security news, the latest breaches, ransomware.  And this guy right here's in charge, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo. 



LEO:  Good to see you.



STEVE:  Good to be with you again.  We were discussing fezzes before we began recording.  The years, I mean, I have collected them since I guess you've stuck them on my head when I've come by TWiT in previous years.



LEO:  Right, right.



STEVE:  And I, you know...



LEO:  A fez is actually a nice hat because - but it is brimless.  So I'm not sure exactly its intent.



STEVE:  Yeah.  I'm a Kangol hat wearer.



LEO:  Yeah, because at least a little brim over your eyes to cover your eye...



STEVE:  Yeah, that's right.



LEO:  Yeah.



STEVE:  So for Episode 889, we're going to talk about something which I don't mean to like over-alarm anyone.  This is not a big problem.  But it's an interesting information leakage which people should be aware of, if nothing else.  And it was named by the people who stumbled upon it "Spell-Jacking."  But we've got a lot to talk about.  We've got last week's Patch Tuesday.  Also the changing landscape of cyber insurance as a consequence of all of the attacks that the world is now being subjected to.  And of course we've sort of seen the writing on the wall there.  We're also going to revisit sort of a collection of recent major network breaches.  Of course, Uber made the headlines everywhere.  But also Rockstar Games got hit by the same cretin.  And then we have the final update from LastPass about what happened there, which I know that a lot of our listeners had been holding their breath about.



We're going to look at another significant problem facing 280,000 WordPress sites and at probably a useful recommendation for future mitigations of similar things.  We're going to examine the cost of processing performance for the most recent Retbleed security mitigations.  Someone did, actually an engineer at VMware, they have a department called Performance Engineering at VMware.  And he lived up to his department's title.  Then we're going to look at Google's very welcome use-after-free new vulnerability mitigation technology, you know, all of these problems that we're finding in browsers, virtually, are use-after-free problems.  And to their credit, they've tackled this thing, like this seemingly intractable problem.  And then after sharing a few pieces of listener feedback, we're going to take a look at a surprising consequence of enabling Chrome's enhanced spellchecking and talk about some mitigations there.



LEO:  Nice.



STEVE:  So I think we have a great podcast for our listeners.  And a sad but sort of like, okay, well, of course, Picture of the Week.



LEO:  Aww.  Aww.



STEVE:  Yeah.



LEO:  Yeah, sad, but a sign of the times, shall we say?



STEVE:  It is a sign, indeed.  The signage of the times.



LEO:  The signage of the times.  



STEVE:  So our Picture of the Week is poignant.  We've often shown pictures of the big automated electronic signage which is showing something, well, basically demonstrating that behind it is a unhappy Windows system.  And that turned out to be the case last week.  This was a large sign, like a roadway-style sign, just acknowledging Her Majesty the Queen, showing 1926-2022.  And unfortunately the looks like Windows 10 system that was driving the signage was running low on disk space, so popped up on the sign, again, sort of apropos of what the sign was acknowledging, it says "Low disk space.  You're running out of space on this PC.  Manage storage," and blah blah blah.  Anyway, I just thought it was an interesting coincidence.  Life does that to us sometimes.



This is - I decided we have a new name, Leo, for the third Tuesday of every month.



LEO:  Oh?



STEVE:  And that's "Patch News Day."



LEO:  Patch Aftermath Day.



STEVE:  That's right.  So last Tuesday Microsoft updated their range of software to resolve a total of 63 flaws, including, well, either one or two publicly disclosed zero-day vulnerabilities, depending upon whether you use Microsoft's more liberal definition of zero-day, which does not depend upon having a vulnerability in active use.  But either way, one of the vulnerabilities was being actively exploited.  In fact, it was being so widely exploited that researchers with DBAPPSecurity, Mandiant, CrowdStrike, and Zscaler all encountered it in the wild and reported their findings of it to Microsoft.



So its CVE designation is 2022-37969, and it is a "Windows Common Log File System Driver Elevation of Privilege."  And given that it's a file system driver which runs in the kernel, any of those many attackers who were apparently using this and having some fun with it were obtaining full system root-level privileges on the machines they were attacking.  So the good news is that was happening to unwitting Windows users, and presumably in targeted attacks.  So not widespread, but it did come to the attention of four different security firms.



Other than that, there were 30 remote code execution vulnerabilities; 18 elevation of privilege vulnerabilities.  So the most problems solved were the two worst classes of problems you can have, which are remote code execution and elevation of privilege.  Then there were 16 fixes for the Edge browser, you know, Chromium vulnerabilities; seven information of disclosure vulnerabilities; seven denial of service vulnerabilities meaning you could crash something; and then one of these oh-so-generic security feature bypass vulnerabilities.  Okay.  And there were some admins reporting problems with group policy management and settings after installing last Tuesday's problems.  So Microsoft didn't get away completely unscathed.  But there were no reports of anything widespread affecting typical Windows users.  So, you know, a quieter Patch Tuesday than we've been seeing recently.



Lloyd's of London Ltd., of course the famous insurer, has told its global network of insurer groups that new or renewed cyber insurance coverage policies must exclude nation-state attacks as of March 31st, 2023.  So about six months from now.  Lloyd's cited systemic risk to the insurance market as a whole as a reason for the change, also adding that policies must also exclude losses from war unless there is a separate exclusion for this type of, I guess an exclusion for the exclusion.



And it's not surprising; right?  This sort of dialing back on cyber insurance coverage is what we've been expecting.  And it's here.  Insurance firms are seeking ways to get control of the spiraling cost that they're seeing driven by recent increases in cybercrime, especially ransomware.  Nation-state attacks are often most targeted and more about espionage than just casual theft or causing damage.  But the consequences do sometimes spill over to do considerable damage to other organizations.



And the NotPetya incident of 2017 appears to be the primary factor driving this decision.  We talked about this a couple times.  There was a protracted legal battle between Merck and its fleet of insurers over, get this, $1.4 billion that Merck was claiming in damages caused by that attack.  And remember, Leo, we talked about it must be that they were just like trying to replace all the PCs that they had.  I mean, $1.4 billion.



LEO:  There's also loss of business revenue, too, though, remember.  I mean, if you're down for five weeks, that could be a billion dollars in a company.



STEVE:  That could hurt you, yeah.  So anyway, as we've noted before, cyber insurance coverage had previously been relying on an "acts of war" exclusion to address incidents such as these, but last year's ruling - oh, I forgot to mention that Merck won their battle.  They got their $1.4 billion from their insurers.  So basically in response to this, the insurers are saying, okay, let's rethink contracts moving forward here.  So there's now an explicit nation-state exclusion.  The invasion of Ukraine has stoked fears among insurers that similar cyber exchanges may slip their containment and cause, like, ancillary damage.  There's also been at least one smaller incident of this nature, the AcidRain malware that was aimed at Ukraine's Viasat service at the start of the war.  We talked about this at the time back in February, or I guess it was March.  But that also ended up hitting and affecting, basically it sort of lost containment.  It also affected a large wind turbine system in Germany.  And it was insured, and so they had to pay up.



So insurers are looking to pull back on risk as companies are increasing their demand for cyber insurance coverage, I mean, like, companies are saying, oh, we never really thought about cyber insurance coverage before, but that seems like a good thing.  And Lloyd's has been planning a change of this sort for some time.  They've been drafting an assortment of contractual clauses throughout last year, working to clarify when cyber attacks can be considered acts of war and catastrophically damaging enough to be excepted from coverage.



So anyway, I just wanted to mention that this is happening, that costs are going up.  It's now estimated, in fact, that the amount of money that insurers are going to be asking for for the coverage that companies wish will be expensive enough that half of companies will say, you know, that's just too much for us.  We're going to go without.  So insurance is always tough, right, because you're paying for it whether you need it or not until you do need it, and then you sure wish you had it.  So anyway, we are seeing a general tightening in the insurance market, basically a raising of operating costs for corporations as a consequence of so many that have been insured and forced major payouts.



Okay.  So then, as you mentioned, Leo, we also had a number of major breeches occurring recently.  Uber got a lot of attention, mostly I think because the attacker was so brazen.  The attacker was just plastering the fact of this attack everywhere.  So it wasn't like Uber had any choice, any opportunity to keep it quiet.  They suffered, as a consequence, a significant embarrassing network intrusion.  Last - I got a kick out of their first posting.  Last Thursday, let's see, it was in the evening, 6:25 p.m., they just tweeted, this came from Uber_Comms, saying:  "We are currently responding to a cybersecurity incident.  We are in touch with law enforcement," which they've actually been, like, talking that up a lot as if maybe that's going to scare people, I guess.  And they said they would be posting additional updates as they become available.  So that was Thursday.



The day after that, last Friday, in the interest of keeping the lines of communication open, although they still didn't have lots of information after what is arguably a very short time, they said:  "While our investigation and response efforts are ongoing, here is a further update on yesterday's incident."  They said four things:  "We have no evidence that the incident involved access to sensitive user data, like for example trip history.  Second, all of our services including Uber, Uber Eats, Uber Freight, and the Uber Driver app are operational.  Third, as we shared yesterday, we have notified law enforcement."  As I said, they keep talking about that.  It's like, okay.  "Fourth, internal software tools that we took down as a precaution yesterday are coming back online this morning."  So, good that they're keeping lines open.



And then, finally, three days after that, which brings us to yesterday the 19th, we get a significantly more comprehensive update.  They said:  "While our investigation is still ongoing, we are providing an update on our response to last week's security incident."  They said:  "An Uber external contractor had their account compromised by an attacker.  It's likely that the attacker purchased the contractor's Uber corporate password on the dark web after the contractor's personal device had been infected with malware, exposing those credentials."  And then listen, get this:  "The attacker then repeatedly tried to log in using the contractor's Uber account.  Each time, the contractor received a two-factor login approval request, which initially blocked access.  Eventually, however, the contractor accepted one, and the attacker successfully logged in."



So as I understand this, and there's some confusion in the wording of this and in the reporting, but it sounds like because they also later said that they've tightened their multifactor authentication parameters, this appears to have been a brute-force multifactor authentication bypass.  And we've seen that happen in the past since the typical multifactor authentication uses six digits, you know, and six digits is a clear compromise between convenience and security.  We talked about this years ago when this first surfaced.  There is literally for a single challenge a one in a million chance of correctly guessing a given multifactor authentication challenge.  But if nothing stops someone from making as many guesses as they wish, as often as they wish, 100,000 guesses would yield a 10% chance of guessing, you know, of getting one correct.



Anyway, so that appears to be what happened is that they acquired username and password credentials, but they were stopped from an easy authentication by multifactor authentication.  But because Uber had not configured strong lockout policies, which I'll talk about in a second, that guy was still able to get in.  So they said:  "From there, the attacker accessed several other employee accounts which ultimately gave the attacker elevated permissions to a number of tools, including G-Suite and Slack.  The attacker then posted a message on a company-wide Slack channel" - which they're posting - "which many of you saw, and reconfigured Uber's OpenDNS to display a graphic image to employees on some internal sites."



LEO:  Oh, boy, that's mean.  You can guess what that image was.



STEVE:  Uh-huh.



LEO:  Oh, lord.



STEVE:  A graphic image.  So they said:  "Our existing security monitoring processes allowed our teams to quickly identify the issue and move to respond.  Our top priorities were to make sure the attacker no longer had access to our systems; to ensure user data was secure and that Uber services were not affected; and then to investigate the scope and impact of the incident.  Here are some of the key actions we took and continue to take."  And they list six.



"We identified any employee accounts that were compromised or potentially compromised and either blocked their access to Uber systems or required a password reset.  Second, we disabled many affected or potentially affected internal tools.  Third, we rotated keys, effectively resetting access, to many of our internal services."  Meaning, you know, updated their passwords.  "Fourth, we locked down our codebase, preventing any new code changes.  Fifth, when restoring access to internal tools, we required employees to re-authenticate.  We are also further strengthening our multifactor authentication policies.  And finally, we added additional monitoring of our internal environment to keep an even closer eye on any further suspicious activity."  You know, and this could sound like an ad for the idea of sticking canaries within a network in order to catch somebody doing something as quickly as possible.



Anyway, they said:  "The attacker accessed several internal systems, and our investigation has focused on determining whether there was any material impact.  While the investigation is still ongoing, we do have some details of our current findings that we can share.  



"First and foremost, we've not seen that the attacker accessed the production, i.e., public-facing systems that power our apps; any user accounts; or the databases we use to store sensitive user information like credit card numbers, user bank account info, or trip history.  We also encrypt credit card information and personal health data, offering a further layer of protection.  We reviewed our codebase and have not found that the attacker made any changes.  We also have not found that the attacker accessed any customer or user data stored by our cloud provider, AWS S3.



"It does appear that the attacker downloaded some internal Slack messages, as well as accessed or downloaded information from an internal tool our finance team uses to manage some invoices.  We're currently analyzing those downloads.  The attacker was able to access our dashboard at HackerOne, where security researchers report bugs and vulnerabilities."  And of course we know HackerOne well.  They said:  "However, any bug reports the attacker was able to access have been remediated."  So no access to anything sensitive there.  "Throughout," they wrote, "we were able to keep all our public-facing Uber, Uber Eats, Uber Freight services operational and running smoothly.  Because we took down some internal tools, customer support operations were minimally impacted and are now back to normal."



And finally:  "We believe this attacker, or attackers, are affiliated with a hacking group called Lapsus$, which has been increasingly active over the last year or so.  This group typically uses similar techniques to target technology companies, and in 2022 alone" - that is, this year - "has breached Microsoft, Cisco, Samsung, Nvidia, and Okta, among others.  There are also reports over the weekend that this same actor breached videogame maker Rockstar Games.  We are in close coordination with the FBI and U.S. Department of Justice on this matter and will continue to support their efforts.  We're working with several leading digital forensics firms as part of the investigation.  We'll also take this opportunity to continue to strengthen our policies, practices, and technology to further protect Uber against future attacks."



So I find nothing to fault Uber here.  Well, except they acknowledged their multifactor authentication, if that was in fact basically brute forced in order to allow a bad guy in, could have been strengthened, and it now is.  Their response was immediate, their communication has been swift and balanced, and I would imagine that their forensics team is likely glad to be able to get some sleep after a handful of probably very sleepless nights.  That they are users of HackerOne speaks well of them, and they appear to have a well-running security component to their IT systems since their continuous auditing of systems were able to provide them with a lot of relevant information when those audits were queried.



They also identified a weakness, as I mentioned, in their multifactor authentication configuration, and they're tightening up.  So I guess a takeaway for everyone would be to think about the fact that just as username and password authentication should lock out for some period of time after a reasonable number of failed attempts, the same remains true even after multifactor authentication has been added.  That is, there's really no good reason for an authentic user to fail five times in a short period of time to properly authenticate themselves.  Clearly, if they've got multifactor authentication, they've got something which is generating six digits for them to enter.  And, you know, you could understand a typo or a timeout once or twice.  But not many times in a short period of time.  So the lesson may be here that just adding multifactor authentication doesn't mean that you can then decide you no longer need other lines of defense like short, like some time-limited auto resetting lockout of an account.



And Rockstar Games.  Of course they are famously the publishers of Grand Theft Auto.  And what happened to them was a massive leak of videos for the not-yet-released Grand Theft Auto 6, which I guess is like a highly anticipated superhot topic.  Uber said:  "There are also reports over the weekend that this same actor breached videogame maker Rockstar Games."  And it's certainly believable that this is the same guy or gang, although I wasn't able to ascertain why they believe that.



LEO:  Oh, because when he released the videos, he used the handle, something like "Uberhacker."



STEVE:  Okay.



LEO:  "I hacked Uber" or something like that.  Which, you know, that's not probative.  But, you know.



STEVE:  Yeah, that's it.  Well, but it is - what I was going to say was that he does seem to be liking making a splash.



LEO:  As Lapsus$ is.  I mean, that's what Lapsus$'s motivation mostly seems to be is getting attention.



STEVE:  So this guy put out more than 90, nine zero, videos  showing game play from the upcoming GTA 6 on Sunday.  Reports indicated that he's believed to be a teenager, but I was also unable to learn what backs that up.  I just saw that in passing.  And in this instance he was using the handle "Teapot."  And he said he plans to leak more game play and even some of the game's source code.  Although I wonder about that because he doesn't seem like the kind of person who holds anything back.  And so maybe he doesn't have source code.  Maybe he does.  We'll see.  Anyway, Rockstar Games confirmed that the videos were authentic to Bloomberg's main games reporter, but has not commented on the news of the hack or if the hacker did indeed steal the game's source code.



So there's breach number two.  Breach number three is coming back to update from our previous coverage.  And that is LastPass.  Last Thursday LastPass published their final official port-mortem, exactly three weeks following their initial breach disclosure, which of course left some aspects of the attack unknown because it was unknown at that time.  So here's the final word from their CEO.



He said:  "On August 25th, 2022, we notified you about a security incident that was limited to the LastPass Development environment in which some of our source code and technical information was taken."  He said:  "I wanted to update you on the conclusion of our investigation to provide some transparency and peace of mind to our consumer and business communities.



"We have completed the investigation and forensics process in partnership with Mandiant.  Our investigation revealed that the threat actor's activity was limited to a four-day period in August 2022.  During this timeframe, the LastPass security team detected the threat actor's activity and then contained the incident.  There's no evidence of any threat actor activity beyond the established timeline.  We can also confirm that there's no evidence that this incident involved any access to customer data or encrypted password vaults.  Our investigation determined that the threat actor gained access to the Development environment using a developer's compromised endpoint.  While the method used for the initial endpoint compromise is inconclusive, the threat actor utilized their persistent access to impersonate the developer once the developer had successfully authenticated using multifactor authentication."



Okay, now, that's interesting.  It appears that we have another incidence of multifactor authentication bypass, essentially.  We don't know enough about the way LastPass has set things up.  But it must just be that the bad guys obtained an authenticated session token from a successfully logged-in endpoint.  At least that's sort of what it feels like.  It does remind us, though, that simply adding multifactor authentication isn't any sort of universal cure.



So they said, or he continues:  "Although the threat actor was able to access the Development environment, our system design and controls prevented the threat actor from accessing any customer data or encrypted password vaults."  And of course that's what we were hoping to hear, and it looks like we are.  "First," he said, "the LastPass Development environment is physically separated from, and has no direct connectivity to, our Production environment."  So that was one big question we had when we talked about this three weeks ago is was the Production environment ever in danger from a breach of the Development environment?  And he's saying no.  They are deliberately physically separated.  So that's just great.  That's exactly what we want.  Great news.



He says:  "Secondly, the Development environment does not contain any customer data or encrypted vaults.  Third, LastPass does not have any access to the master passwords of our customer vaults.  Without the master password, it's not possible for anyone other than the owner of a vault to decrypt vault data as part of our Zero Knowledge security model," he says.  So that's what we would hope.  And that's what we were talking about before.  He's making the point that I made three weeks ago, which is that mistakes can happen to anyone.  But as long as the security architecture of the system is designed for "Trust No One" operation, all of their users will be completely protected.



And providing another example of proper design, the CEO explained.  He said:  "In order to validate code integrity, we conducted an analysis of our source code and production builds and confirm that we see no evidence of attempts of code-poisoning or malicious code injection."  And he said:  "Developers do not have the ability to push source code from the Development environment into Production."  Again, really good isolation there.  He said:  "This capability is limited to a separate Build Release team and can only happen after the completion of rigorous code review, testing, and validation processes."  So from what he's saying, it really does sound like they have built this system properly, in a significant chain.



He said:  "As part of our risk management program, we have also partnered with a leading cyber security firm to further enhance our existing source code safety practices which includes secure software development life cycle processes, threat modeling, vulnerability management, and bug bounty programs.  Further, we have adopted enhanced security controls, including additional endpoint security controls and monitoring.  We've also deployed additional threat intelligence capabilities, as well as enhanced detection and prevention technologies in both our Development and Production environments."



And again, this is what I was saying last time we talked about it was all of that is what we would like to have happen.  That is, their existing architecture was good.  They realized it could be made better.  And that's what they have done.  So I think that, you know, those who have chosen to remain with LastPass should have every reason to feel that LastPass is a competent caretaker of their users', of their own pre-Internet encrypted data.  As I was thinking about that, I realized that our long-term listeners will recall that early acronym we developed.  You know, remember PIE.



LEO:  The first one was PEE.  I'm glad you changed it for PIE.  Do you remember that?



STEVE:  Oh, yeah.  That was Pre-Encryption Environment or something?  You're right.  Good memory, Leo.



LEO:  Yeah, Pre-Entry Encryption or something.  And you realized that PIE might be better, yeah.



STEVE:  PIE was better than PEE, indeed.



LEO:  Pre-Egress Encryption maybe.



STEVE:  That's, whoa, thank you whoever was listening.



LEO:  That was me.  Pre-Egress Encryption.



STEVE:  Oh, no kidding.  You remembered.



LEO:  You know, I know that I don't remember anything that happened recently.  But boy, I've got a good memory for that old stuff.



STEVE:  Welcome to the club.  So we have a CVSS of 9.8 for WordPress.  Last time we talked about a big vulnerability hitting WordPress I came away suggesting that anyone who's using WordPress in anything other than its barest out-of-the-box essential configuration  that is, anyone who has added any of the gazillion tantalizing WordPress add-ons  ought to give serious thought to running a third-party application firewall on their site.  There are three or four of those, but one stands out, and that's WordFence.  It's the one that we keep referring to since they appear to be most on top of this particular chunk of the industry.  They're the ones who are discovering problems more than any of the others.  I remember I went, like, looking for other WordPress add-on companies, and I found that there were others, but they weren't nearly as active as these guys.  Anyway, as we know, WordPress matters; right?  It's just shy of 40% of the Internet's websites.



So to that end, WordFence last week put out a report which serves as a perfect case in point.  The report is titled "PSA," as in Public Service Announcement, "Zero-Day Vulnerability in WPGateway Actively Exploited in the Wild."  And they said:  "On September 8th, 2022, the Wordfence Threat Intelligence team became aware of an actively exploited zero-day vulnerability being used to add a malicious administrator user to sites running the WPGateway plugin."  They said:  "We released a firewall rule to Wordfence Premium, Wordfence Care, and Wordfence Response customers to block the exploit on the same day, September 8th."  I don't know what those three things are, but they have some sort of a product lineup.



They said:  "Sites still running the free version of Wordfence will receive the same protection 30 days later."  Well, you might as well not have it if you don't get it for a month.  Anyway, the Wordfence firewall, they said, has successfully blocked over 4.6 million attacks targeting this vulnerability against more than 280,000 sites in the past 30 days.



Okay.  So this WPGateway plugin is a premium plugin tied to the WPGateway cloud service, which offers its users a way to setup and manage WordPress sites from a single dashboard.  Part of the plugin's functionality exposes a vulnerability that allows unauthenticated attackers to insert a malicious administrator into the site.



So they said:  "We obtained a current copy of the plugin on September 9th and determined that it is vulnerable."  Okay.  So their model is they've got these application firewalls deployed.  They see something happening.  And in WordPress, since it's all PHP based, they're going to be seeing some odd-looking query that's being made.  So that brings it to their attention.  They look at what plugin the PHP query is targeting, and in this case the next day they got a copy of the plugin and analyzed it to see what was going on.  So they said:  "We determined that it is vulnerable, at which time we contacted the plugin vendor with our initial disclosure."  They said:  "We've reserved vulnerability identifier CVE-2022-3180 for this issue."



So they said:  "This is an actively exploited zero-day vulnerability, and attackers are already aware of the mechanism required to exploit it."  Clearly, because that's how it came to their attention.  They said:  "We're releasing this public service announcement to all of our users.  We are intentionally withholding certain details to prevent further exploitation.  As a reminder, an attacker with administrator privileges has effectively achieved a complete site takeover."



They said:  "If you are working to determine whether a site has been compromised using this vulnerability, the most common indicator of compromise is a malicious administrator with the username of 'rangex,'" R-A-N-G-E-X.  They said:  "If you see this user added to your dashboard, it means your site has been compromised."  So they also mentioned that, if you're unable to get an update to this WPGateway, remove it from your system, or take some action to prevent it from being accessed because that's the way these bad guys are compromising so many WordPress sites.



So personally, I'm no longer running any WordPress sites.  I was for a while to maintain a blog, which I infrequently posted to.  But if I were doing so today, I would - first of all, I tend not to be adding lots of bells and whistles to my site.  I'm happy with more of the bare minimum functionality.  But if you had lots of sites that you're using this cloud-based dashboard in order to manage them, you should know that that's a problem.  I'd take a look at these WordFence people.  They seem to be good folks.  Although again, as I said, there are a bunch of similar offerings because this sort of add-on is useful.  And in fact, as we'll see, it's exactly a web application firewall which led the people who created it to today's podcast topic.



Okay.  This I loved.  Two months ago, on July 19th, Security Now! Episode 880 was titled "Retbleed."  As we discussed at the time, it's another of the ongoing and ever-evolving speculative attacks against the Intel and AMD microarchitectures.  Until security researchers began looking closely and developed the Spectre and Meltdown attacks, Intel and AMD were both happily inventing and incorporating all sorts of clever tweaks into the execution path of their processors for the sake of improving their performance.  In essence, all of these performance-tuning tweaks involve having the processor's microarchitecture learning something about the code it's executing.



Unfortunately, well, the good news is that allows it to correctly anticipate what's likely to happen again.  But it allows the microarchitecture to be probed by software that understands that there is basically a trail of breadcrumbs following behind the code.  When this all works, it allows the code to execute much more smoothly.  But the big question we always ask and ponder, but so far we haven't received much of a clear answer to because Intel doesn't want to tell us, is what's the performance impact of turning off these features?  What happens if we disable this microarchitectural optimization in the interest of enhanced security?



Friday before last, on September 9th, a VMware engineer in VMware's Performance Engineering department posted the results of his analysis of exactly this into the Linux Kernel Archive list under the subject "Performance Regression in Linux Kernel 5.19."  Which is the, you know, just recently updated and released kernel.  And it's the one that incorporates treatment for Retbleed.  He wrote:  "As part of VMware's performance regression testing for Linux Kernel upstream releases, we have evaluated the performance of Linux kernel 5.19 against the 5.18 release, and we have noticed performance regressions in Linux VMs on ESXi as shown below."



Get this:  Computing performance has fallen 70, seven zero, percent.  Network throughput, down 30%.  Storage bandwidth down 13%.  He said:  "After performing the bisect between kernel 5.18 and 5.19, we identified the root cause to be the enablement of IBRS mitigation for spectre_v2 vulnerability by commit," and then he's got the hex number of the commit that added that to the kernel.  And it's titled "x86/bugs:  Report Intel Retbleed vulnerability."  He said:  "To confirm this, we have disabled the above security mitigation through kernel boot parameter" - and it's spectre_v2=off - "in 5.19 and re-ran our tests and confirmed that the performance was on par with 5.18 release."



Okay.  So the IBRS to which the engineer refers stands for "Indirect Branch Restricted Speculation," which Intel describes as an indirect branch control mechanism that restricts speculation for indirect branches.  Doing that is necessary, as our Retbleed podcast two months ago explained in detail, to prevent a surprising rate of data exfiltration from otherwise completely secure operating systems.  Consequently, since that would be bad, the Linux Kernel starting with 5.19 does so by default unless it's prevented from doing so with a kernel boot override parameter.



And to their dismay, what the VMware Performance Engineering folks discovered was that, compared to the immediate preceding Linux kernel 5.18, 5.19, as I said, sees a 70% reduction in the performance of compute intensive tasks, a 30% reduction in network throughout, and 13% reduction in mass storage performance.  I have a link in the show notes to VMware's Linux Kernel Archive posting for any Linux aficionados among us who will want all the details on this and on the specific benchmarks which were run, and how it was done.  The guy who posted this from VMware provided complete details.



My feeling about all of this has not changed from its first appearance.  And Leo, it's the opinion you and I had from the beginning.  End users have never had much, if anything, to worry about from any of these subtle microarchitectural attacks.  It's the big datacenter cloud server guys running many different virtual machines across a heterogeneous client population that does have cause to worry.  So if I were a Linux hotshot, I'd be disabling all of these Spectre-ish mitigations and free up my processors to run with as much "intuition" about the code they are running as possible.  The only danger you would face would be a cross-process information bleed.  And in order to have a cross-process information bleed, you've got to have something running in your machine which is already running in your machine which is able to perform this sort of operation.



Now, having said that, I do recall that we talked about Retbleed being operable from code in a browser.  And of course that sort of blurs this boundary.  But typically it has to run for a long time to get any information, and it's got to find out where the information is, blah blah blah.  So again, my sense is end users really don't have much of a concern.  If you've got no problem at all under the latest Linux kernel from a performance standpoint, then obviously leave the stuff enabled.  But recognize, depending upon which distro you're using and which Linux kernel it's got, and what mitigations it has by default, there is a huge difference in performance when you turn these Spectre mitigations on versus off.  And we finally have some what look like very good numbers to support that.  So again, I guess I would say it's an individual preference, but you need to understand that there is some serious performance hanging in the balance.



And I also wanted to tell everyone about some very encouraging news from Google's Chromium team.  Okay.  So the trouble with my doing that is that it really gets down into some technical weeds, and I did not want to devote an entire podcast to tackling a single complex topic that most of our listeners won't care that much about.  Mostly you're going to want the headline.  So this involves pointer reference counting and what they call poisoning quarantined pointers before the pointers' release.  It is some seriously cool, but also complex, pure computer science. But again, all we really need to know is that the single most troublesome aspect of the Chromium code base, that is, those ubiquitous use-after-free errors and their exploitation, are finally going to be resolved.  So I do want to share some of what Google explained.



So here's how they begin their explanation.  They said:  "Memory safety bugs are the most numerous category of Chrome security issues; and we're continuing to investigate many solutions, both in C++ and in new programming languages.  The most common type of memory safety bug is the use-after-free.  We recently posted about an exciting series of technologies designed to prevent these.  Those technologies, collectively *Scan" - star as in a wild card scan - "are very powerful, but likely require hardware support for sufficient performance."  In other words, we can't have that today.



They said:  "Today we're going to talk about a different approach to solving the same type of bugs.  It's hard, if not impossible," they wrote, "to avoid use-after-frees in a non-trivial codebase.  It's rarely a mistake by a single programmer.  Instead, one programmer makes reasonable assumptions about how a piece of code will work, then a later change invalidates those assumptions.  Suddenly, the data isn't valid as long as the original programmer expected, and an exploitable bug results."



They said:  "These bugs have real consequences.  For example, according to Google's Threat Analysis Group" - their TAG team - "a use-after-free in the Chrome HTML engine was exploited earlier this year by North Korea.  And as shown in the percentage bar chart below, half of all known exploitable bugs in Chrome are use-after-frees."  And I have a chart in the show notes which I grabbed from their blog posting where these blue bars in this percentage bar chart are shown pretty much, especially later on, that's every quarter from the second quarter of 2015 through the first quarter of 2021.  And certainly from around 2019 on, those blue bars have represented about half of the total bugs that Chrome has seen.  And of course we're talking about them all the time on the podcast.



Okay.  So then they introduce the concept of what they call their MiraclePtr, you know, Ptr.  They said:  "MiraclePtr is [modestly named] a technology to prevent exploitation of use-after-free bugs.  Unlike aforementioned *Scan technologies that offer a non-invasive approach to this problem, but would require hardware that doesn't yet exist, MiraclePtr relies on rewriting the codebase to use a new smart pointer type, which is raw_ptr.  There are multiple ways to implement MiraclePtr.  We came up with around 10 algorithms and compared the pros and cons of each.  After analyzing their performance overhead, memory overhead, security protection guarantees, developer ergonomics, et cetera, we concluded that the BackupRefPtr was the most promising solution."  So that was the one from around 10 that they end up choosing as the implementation for their so-called MiraclePtr.



They said:  "The BackupRefPtr algorithm is based on reference counting.  It uses support of Chrome's own heap allocator, known as PartitionAlloc, which carves out a little extra space for a hidden reference count for each allocation.  Raw_ptr increments or decrements the reference count when it's constructed, destroyed, or modified.  When the application calls free or delete, and the reference count is greater than zero, PartitionAlloc quarantines that memory region instead of immediately releasing it.  The memory region is then only made available for reuse once the reference count reaches zero.  Quarantined memory is poisoned to further reduce the likelihood that use-after-free accesses will result in exploitable conditions."



In other words, typically what we've seen is the pointer which has been released is pointing to something useful like still into the operating stack, where real damage could be done.  So they are deliberately writing something illegal into that pointer so that if someone had access to it, it wouldn't be pointing to anything useful.  And they said:  "In the hope that future accesses lead to an easy-to-debug crash, turning these security issues into less dangerous ones."



And, they said:  "We successfully rewrote more than 15,000 raw pointers in the Chrome codebase into raw_ptr, then enabled BackupRefPtr for the browser process on Windows and Android, both 64 bit and 32 bit, in Chrome 102 Stable."  And note that we're all running 105 now.  So it's been in there for a while.  They said:  "We anticipate that MiraclePtr meaningfully reduces the browser process attack surface of Chrome by protecting around 50% of use-after-free issues against exploitation.  We are now working on enabling BackupRefPtr in the network, utility, and GPU processes, and for other platforms.  In the end state, our goal is to enable BackupRefPtr on all platforms because that ensures that a given pointer is protected for all users of Chrome."



So after that they continue, that's about like - that's just the first piece of this blog posting because they get into a deep discussion of implementation overhead, which comes down to the overhead of a four-byte reference count for each pointer.  But the bottom line is hats off to the Chromium folks for not simply chasing their tails endlessly under the fantasy and fallacy that they're eventually going to find all of the use-after-free bugs.  We all know that as new code is being written, and as old code is being modified, as many new bugs are being introduced as old bugs are being found and eliminated.



That's, you know, just take a look at that chart.  You can see that there's actually no progress being made along those lines.  If anything, there's more blue today than there was back in 2015 when they began doing this.  So yay to them actually tackling this and developing some new technology.  The only people who will be made unhappy by this news are those who've been making their living off of discovering new ways to break into Chrome.  Life promises to be much more difficult for them, which is wonderful.



I have a couple little bits of Closing the Loop from our listeners.  Oh.  Peter G. Chase, and actually a handful of our sharp-eyed listeners, looked at that Picture of the Week.  Well, I'll say what Peter said first.  He said:  "The Picture of the Week looks a lot like they're actually bypassing the meter.  That looks like a meter box."  And that hadn't occurred to me.  But it did to a number of our listeners.  And of course that's even more interesting; right?  So the idea would be that one of those big round-faced electric meters has essentially the same sort of feet sticking out of it, prongs, that two bar fuses would.  And so what's actually happened is that the people who are using that power - thanks for having it on the screen, Leo, right.  The people who are using that power have bypassed the meter, which would normally be plugged into the front of that, thus monitoring the amount of electricity being used by those people.  So if so, it looks like they're getting free electricity.  Anyway, thank you, all of our listeners, for pointing that out. 



Dan Taylor said:  "Hi, Steve.  Years ago you mentioned PEGASYS TMPGEnc."  And I recall that.  He says:  "As the video editing software you like and use.  It's been at least 10 years since my first purchase.  I have three different packages from them.  The latest is Video Mastering Works 7," he says, "and I love it! I just thought I'd say thanks for steering me in their direction so long ago."  And it's funny, Leo, I mean, you and I were messing with media back then.  I remember that TMPGEnc, for me, I didn't have any video editing software, but that was my go-to MPEG-2 encoder back when MPEG-2 encoders were like littered with all kinds of bells and whistles in the UI that allowed you to, like, tune it in order to get the best compression and image appearance.



LEO:  I probably wasn't using Windows at the time.



STEVE:  Yeah, maybe not, although...



LEO:  I wouldn't know.



STEVE:  And I didn't know.  But they're still there. 



LEO:  Nice.



STEVE:  As Dan says.  And they've got a bunch of nice-looking software.



LEO:  Nice.



STEVE:  Ed, oh, boy, Grigoleit.



LEO:  Grigoleit.  I don't know.



STEVE:  He says:  "I'm a long-time Security Now! listener, but I do not recall hearing you mention the following benefit in the past, and it may be useful to some of your listeners.  I am a CISSP, and I must maintain my certification by completing CPE credits.  I've been submitting my Security Now! attendance as a qualifying Webinar (1.5 CPEs) for several years, and these submissions have always been approved.  Best.  EdG."  So Ed, thank you for sharing that, for any of our listeners for whom that may not have occurred and who may also be needing CPE credits.  Cool that the podcast is a source for those.



LEO:  Yeah, that's great.



STEVE:  Bob O'Brien said:  "In an alternate universe where everyone used SQRL, would that stop/thwart phishing attacks because they wouldn't have one's master key or encrypt the login handshake?"  So, okay, Bob.  SQRL, I realized that when I talked about this last week I didn't explain why or how SQRL solves the problem.  It solves it in a couple ways.  First of all, the key being generated by SQRL is tied to the domain name.  So if you're at a phishing site with a bogus domain name, SQRL will generate a key.  But it won't be the right key.  It'll be a SQRL key for the phishing site, rather than the SQRL key for the site you think you're logging onto.  So you automatically get that protection.



But that client-provided session, CPS feature that I mentioned, that's actually an additional layer of protection.  The reason that all of these technologies are prone to man-in-the-middle attacks is that the man in the middle is literally monitoring all of the traffic in both directions, unencrypting it, sniffing it and then reencrypting it.  The way SQRL's client-provided session system works is that the SQRL client itself, which is separate from the browser, initiates a connection to the server in order to negotiate credentials.  So it automatically bypasses a man in the middle, going around it, talking to the proper server, even if it had the proper domain, which in a phishing scenario it would not.  So you sort of get multiple layers of protection from that.  Anyway, thanks for the question and for pointing out that I hadn't really explained it.



And Leo, you're going to love this one.  Get a kick out of this.  A tweet from our favorite Dutch ex-regulator, Bert Hubert.



LEO:  No.  He heard us?



STEVE:  Yup.  He said...



LEO:  I'm sorry, Bert.



STEVE:  Bert said:  "International attention for my resignation as regulator of the Dutch intelligence and security services in the Security Now! podcast at minute 58:40."



LEO:  Wow.



STEVE:  "Thank you, Steve Gibson @SGgrc and Leo Laporte."



LEO:  I didn't mean to make fun of your name, I'm sorry.



STEVE:  No, I think he really appreciated that...



LEO:  He needed the attention.  That's the whole point of resigning; right?



STEVE:  Exactly.  And the reason he blogged about it was to say this is my beef about this legislation and the problem it has.



LEO:  Good.



STEVE:  And so we helped him shine a big spotlight on it.  So Bert, you're welcome.  And one last bit.



LEO:  Yes.



STEVE:  Bill Sempf, he tweeted:  "Another winner, Steve.  Second time now that you have recommended a book series that my wife loves."



LEO:  Is that Silver Ships?



STEVE:  He's clearly talking about the Silver Ships.  And there is, in the third book, there's something I need Lorrie to read, my wife.  And I think she'll enjoy getting to that point.  But there's an alien which this author creates.  Oh, they are so wonderful, these aliens.  So enough said.



LEO:  Can't wait.  I'm reading William Gibson's "The Peripheral" because they're making a TV show out of it, which is coming next month.  And I thought, I've had this book for ages.  I should read it.  Because I love William Gibson.  It's not a good audiobook.  I can't follow it at all.  I guess his stuff is so visual that you kind of have to see it in your head.  And it's just not working for me.  So two days from now, Silver Ships.



STEVE:  Good.  Good, good, good.  You won't regret it.  Is "The Peripheral" going to be a movie or a cable...



LEO:  A series.  A series on, I want to say HBO or Apple TV.  I can't remember which.



STEVE:  Oh, good, good, good.  So we don't have to have commercials.



LEO:  Yeah.  Did you read that?  I know you're a William Gibson fan.  He wrote "Neuromancer," which is the cyberpunk novel.



STEVE:  Of course.  Of course.



LEO:  And that one I really loved.  But I think it might be better for me to watch the show.



STEVE:  Yeah, I think I'll do that, too, because my reading has been hijacked. 



LEO:  Yeah, no kidding.  



STEVE:  Oh, it's so good.



LEO:  Yeah.  You're going to be on Silver Ships for the rest of your life.  "The Peripheral" is Amazon Prime Video.  And I'm just looking at it right now.  Looks pretty good.  Looks like it'll be a good show, anyway.  Great, high sci-fi.  Lot of VR.



STEVE:  We need some.  We need some.



LEO:  Yeah, very futuristic.  So it should be interesting.  Okay, Steve.  I am ready to talk about Spell-Jacking.



STEVE:  Okay.  So again, I don't want to overstate this, but this is something I know a percentage of our listeners will really care about.  So it comes from a company named Otto, O-T-T-O.  Actually it's O-T-T-O hyphen J-S dotcom, as in Otto JavaScript.  It's not a company we're talked about before.  And they look like an interesting group.  They describe themselves with a tag line which is kind of interesting:  "Built on innovating cybersecurity and protecting modern freedom."  Okay.



LEO:  What?



STEVE:  Well, that's a mix.  I know.  And they explain.  They said:  "Otto was created by a team of innovators with 30 years of combined expertise in cybersecurity, third-party JavaScript, and MarTech/AdTech.  Before launching Otto we worked on and created solutions for some of the world's largest banks, media companies, and even the U.S. government."



Okay.  So what they appear to have is some powerful add-on technology for helping to manage what exactly a sophisticated modern website is doing, which when you think about it may not always be obvious or necessarily under control of a site's designers.  Consider the challenges when all sorts of third-party libraries with their own complex dependency chains are being pulled into a website visitor's browser on the fly.  And then to that mess add advertising, which may also be bringing along its own scripting.



Anyway, the bullet points for them says:  "Stop Client-Side Attacks.  Plug Otto into your application security suite and protect your supply chain.  Traditional WAFs (Web Application  Firewalls), API security, DDoS, and bot protections are all essential components of your AppSec suite, but they don't protect the client-side gap in your third-party supply chain."



And so they said:  "Visibility.  Otto provides continuous monitoring and analysis of first, third, and Nth-party script behavior and vulnerabilities," which is like - it's what I was talking about before where who knows what scripts the scripts are loading the scripts for.



Also "Protection:  Advanced Malware Guard and Script Shield defend your website from trojans, phishing, malicious code injections, Magecart, and client-side attacks with real-time integration."  And "Control:  Take control over client-side application security with precision Script Policy and Dynamic CSP automation."



Anyway, so given that these guys are deep into watching and analyzing exactly what a user's browser is doing, it's not surprising that they found, that they would have been the ones to catch some unexpected and unwanted behavior from Chrome and Chromium-based browsers, specifically Chrome and Microsoft's Edge.  So what they found:  Google's Chrome and Microsoft's Edge Editor, which is of course Chromium-based, the Enhanced Spellcheck features are phoning home to expose their users' in-the-clear passwords, usernames, email addresses, dates of birth, Social Security numbers, and so forth.  Basically anything entered into form fields that is not recognized locally by their spellcheckers is sent back, as entered, in the clear to Google or Microsoft to request spelling suggestions from their remote recommendation engines.



Now, that's obvious; right?  I mean, that's what Enhanced Spellcheck is.  Except that there by default are no limits on what is being sent back.  And so it includes anything not in the local dictionary, which certainly better be your passwords.  So again, it's not the end of the world.  But in an environment where we want and expect our browsers to locally hash our passwords so that they never leave our browsers in the clear, sending every one of our pre-hashed passwords to Google or Microsoft without our knowledge or permission seems like something that someone should have thought about and should be preventing.  And if the passwords, as I said, that we're using are present in our spellchecker's local dictionary, so that they're not being sent to Google or Microsoft, well, then we have bigger problems because you don't want to be using passwords that are in your own local language.



So I have a screenshot in the show notes of the Chrome browser's POSTed query and Google's reply when a user was logging into Alibaba with the test password "sharepassword*123."  And you can see that being sent in a little JSON blob on the left.  And Google replied with the spellcheck suggestion "share password."  Which is what you would expect it would recommend.  But in the process the user's password, with no encryption of any sort, went to Google, and they got it.  And the same thing happens if you're using Microsoft Edge.  So again, not the end of the world, but not a good look.  Also you can argue that it's never useful to run someone's password or email or Social Security number through spellcheck.  All it's going to do is it's going to, like if it did correct your password, it would mess it up.



LEO:  It would be wrong, yeah.



STEVE:  So, yeah.



LEO:  That's terrible.



STEVE:  So Otto-js's co-founder and CTO Josh Summitt, he discovered the spellcheck leak while he was testing the company's scripts behavior.  And he explained, he said:  "If 'show password' is enabled, the feature sends your password to their third-party servers.  While researching for data leaks in different browsers, we found a combination of features that, once enabled, will unnecessarily expose sensitive data to third parties like Google and Microsoft.  What's concerning is how easy these features are to enable and that most users will enable these features without realizing what's happening in the background."  And as we'll see in a minute, I was guilty of having done so at some point in the past.  I checked, it was like, oops.  Ouch.



So this unsuspected and inadvertent leakage could obviously lead to serious trouble for consumers and major industries when it comes to privacy, data protection, and client-side security.  Not to mention that it's a clear violation of HIPAA and similar privacy regulations which rigorously restrict third-party access to sensitive private information.  In principle, when these features are active, any terminology, such as medical conditions, which are not known to the local spellchecker, will be shared with Google or Microsoft.  And if your browser is also logged into either company's websites, since the POST query is being made to their servers, your logged-in session ID cookie will accompany the aberrant spellcheck query.  So they also know who you are.  Not that there's anything wrong with Google or Microsoft receiving this.  Not that there's anything, any reason to believe they're logging it or collecting it or doing anything.  But they're receiving it from you. 



So Otto-js noted that five websites and services of concern were - and they didn't do an extensive test, but they found out that they noticed that Office 365 does this.  Alibaba's Cloud Service does this.  Google Cloud Secret Manager site does this.  AWS Secrets Manager did this.  They added an update that AWS had already mitigated the issue.  And LastPass site was doing this when they were notified.  They noted that both AWS and LastPass had immediately and already fully mitigated the issue.  The Otto guys said that LastPass was the first to respond to outreach and first to fully mitigate the risk.  They quoted LastPass's Christopher Hoff, who's their Chief Secure Technology Officer, saying:  "It is disconcerting that customers can inadvertently expose confidential data by enabling innocuous browser features and not understand that anything they type  including passwords  could result in that data being sent to third parties."



And in the show notes I grabbed a snippet of the LastPass login page now, after they made the fix.  And you can see highlighted there in the body tag, it opens the body tag.  It says spellcheck="false" in double quotes.



LEO:  Yay.



STEVE:  That's all that's necessary to shut this behavior down on that page.  But nobody is doing it.  The Otto-js researchers created a demonstration video to illustrate how spell-jacking could easily expose a company's cloud infrastructure - servers, databases, corporate email accounts, and password managers.  In the video, an employee had enabled enhanced spellcheck features when he was using that to create a document.  But that feature remains enabled for all sites.  And the user then visits after that, goes to his enterprise database credentials, and shows them being spell-jacked and being sent to Google when he clicks on the Show Password button in order to verify that he entered his password correctly.



So the video uses a common scenario in the workplace to illustrate how easy it is to enable the browser-enhanced spellcheck features, and how an employee could inadvertently expose their company without ever realizing it.  Most CISOs would be extremely alarmed to learn that their company's administrative credentials were unwittingly shared in cleartext with a third party, even one they generally trust such as Google or Microsoft.



Otto-js tested more than 50 websites and sorted 30 of those into a control group spanning six different categories of websites which people use frequently and which have access to highly sensitive Personally Identifiable Information (PII) data.  Okay.  So remember that by default, all data entered into forms that's not recognized by the browser's local spellcheck dictionary will be sent to retrieve remote suggestions.  But passwords will not be sent until and unless the user clicks the "show password" button.  So that's some relief there.



In Otto's testing, five websites per category were selected based on top ranking in each industry.  They were testing to create some benchmark for how much exposure might be occurring.  So the six categories they selected were online banking, cloud office tools, healthcare, government, social media, and eCommerce.  Of the reference group of 30 websites tested, 96.7% did send Personally Identifiable Information back to Google and Microsoft.  Only one did not.  73% sent passwords when "show password" was clicked.  But those not sending passwords only didn't because they lacked the "show password" feature on their site.  So otherwise they would have.  



And interestingly, the only control group website that had mitigated the issue, there was one out of those total of, what was it, 30 sites, yeah, one out of 30, thus 96.7.  The one that had was Google.  So Google was aware of this and didn't want that Personally Identifiable Information and possibly passwords even being sent back to them.  Though Google did mitigate the issue for email and some services, they have not mitigated it for some of their other services like Google's Cloud Secret Manager.  Also, Auth0, a popular single-sign-on service, was not in the control group, but was the only website other than Google which they found that had correctly mitigated the issue.  So props to Auth0.



Anyway, so the point is the knowledge of this is out there, but it has not yet been receiving wide attention, which is one of the reasons I wanted to put it on everyone's radar today.  As I noted with the example of LastPass's mitigation, companies can mitigate the risk of sharing their customers' Personally Identifiable Information by adding spellcheck="false" to the containing page, or to all input fields, although this might create problems for users who want spellcheck, I suppose.  Alternately, that override could just be added to the form fields that might contain sensitive data like username, password, and so forth.  Or please describe your medical condition.



Fortunately, the enhanced spellcheck feature is not enabled by default.  But once it's been enabled, it remains so.  I was curious about my own settings.  So I went to Settings in Chrome and entered "enhanced spell," that's as much as I needed to put into the search bar.  And what do you know?  It turns out I have enhanced spellcheck on.



LEO:  Of course you do.



STEVE:  I'm sure I turned it on.



LEO:  It's just, if you added all your passwords to the dictionary, then it would always be okay; right?



STEVE:  That's right.  Then it will never, yes, it will never need to go ask Google or Microsoft...



LEO:  There's another solution.



STEVE:  If they have a suggestion for an improvement.



LEO:  Because it's always spelled right.



STEVE:  You mistyped your password.  It looks like gibberish.  So it was enabled in my instance of Chrome.  I have no idea when I may have turned it on.  But it's been on ever since.  Google makes it clear what's going on.  They say right there under the option:  "Text you type in the browser is sent to Google."  But you'd sort of think they wouldn't send your password field data.  But they do.  Anyway...



LEO:  Well, that's up to the people, as you showed, who have password fields to make sure spellcheck="false."



STEVE:  Yes.  Yes.



LEO:  I think, I would hope everybody would now go, oh, yeah, I guess we need to do that; right?



STEVE:  Let's hope that happens.



LEO:  I'm looking at Firefox, and it says check your spelling as you type.  And I have it checked.  So it probably is default.



STEVE:  Yeah.



LEO:  In all likelihood.



STEVE:  So while simply turning off enhanced spellcheck will resolve all concerns, Otto-js does offer a free Chrome extension that will alert users when they are visiting a website that has the risk of data leaks caused by enhanced spellcheck.  Now, the problem is all websites do, like, virtually.  So it may be a little annoying.  Maybe it only pops up when you're on a password page, and so it's just going to remind you of that.



LEO:  I wonder where Firefox sends my password.



STEVE:  Yeah, that's a good question.



LEO:  Another question; right?  I mean...



STEVE:  Yeah.



LEO:  This might be something you want to turn off there, as well.  The operating system does spellchecking, most operating systems.  Right?  So you probably don't need the browser to do it.



STEVE:  True.  Although it makes sense that Chrome would be bringing along their own just because they're Chrome.



LEO:  Yeah, yeah, yeah.



STEVE:  And, you know, and they want it cross-platform and the same for everything and so forth.



LEO:  Right.



STEVE:  So anyway, for what it's worth, I've got the links at the very end of the show notes for today.  There's something called Otto-js ShopSecure, which they call "free browser protection for shoppers."  I don't know why it's not for everybody.  And then also Otto-js Developer tools, which they said is "free runtime script testing tools," which might be of use or interest to our more techie users.  So I've not looked at either of those.  But this just popped up on my radar that personally identifiable information was by default going to Google and Microsoft.  May not be something you care about.  Maybe not be something you're exposed to if you never turned on enhanced spellcheck.  But I'm sure there's a bunch of listeners who are saying, ooh, crap, I didn't know.



LEO:  As far as I can tell, Firefox brings its dictionary with it.  So this is a very Google-y thing to do.  Oh, no, no, no.  We're not going to have an on-disk dictionary to do spellcheck.  We're going to send it back to the server and let them do it.  I think Firefox just is not a problem because they use their on-disk dictionary.  So they wouldn't be sending it back to the home office.



STEVE:  So long as they don't reach out and check, if something's not in the dictionary, make a query to see.



LEO:  I think Google sees that as a feature.  That's like, you know, when you do a Google search, and you mistype it, this is actually a way I know some people check their spelling.  They type it in Google search field to get the right spelling.



STEVE:  I do it all the time, as a matter of fact.



LEO:  Yeah.  Yeah.  So that's kind of a feature of Google's.  Turns out not to be such a good one.



STEVE:  Not when it's in your browser, and not when you don't know it's there.



LEO:  Yeah, yeah.



STEVE:  Now all of our listeners know.



LEO:  Good job, Steve, once again.  This is why you listen to this show; right?  Valuable, valuable stuff.  You can get this show in a couple of ways.  You can always watch us do it live, if you're in a hurry.  I've got to know what happens today.  Just we do this show every Tuesday about right after MacBreak Weekly, 1:30 to 2:00 p.m. Pacific.  That's 4:30 Eastern time.  It's 20:30 UTC.  The livestream is at live.twit.tv so you can just go there, there's live audio or video, and listen along.  And if you're doing that, chat with us:  irc.twit.tv.  Actually I'd appreciate if you did because the people in there, they're not talking about Security Now!.  I think we have a lot of people who aren't - you're sometimes a little over their head.  So come on in there and raise the IQ a little bit.  We'd appreciate it.



STEVE:  Sometimes when my wife and I are out walking, some neighbors will say, so you do a podcast?  Should I listen to that?



LEO:  No.



STEVE:  No.  No.



LEO:  It's for a very special person.  You, it's for you.  The Discord, also a very good place to chat if you're already a Club TWiT member.  After the fact, on-demand versions of the show are always available.  Merely go to the website GRC.com.  That's Steve's site.  You can pick up a copy there.  He has some unique formats.  He has the 64Kb audio, same as we do at TWiT.tv.  But he also has 16Kb audio for the bandwidth-impaired.  He also has transcripts written by Elaine Farris, so that's really nice if you like to read along while you listen, or you want to search.  And every show has a transcript, so you can search those transcripts and find whatever you're looking for.  That's a very nice feature at GRC.com.



While you're there, support Steve.  Pick up a copy of his bread and butter, SpinRite, the world's best mass storage maintenance and recovery utility.  Currently 6.0; 6.1's coming.  And you'll be getting it for free if you buy it today.  You can also leave feedback there at GRC.com/feedback.  And there's lots of other free stuff.  It's well worth checking out, including ShieldsUP!, which of course is the first thing everybody should do when they get a new router is test it on ShieldsUP!.



We have copies of the video as well as the audio at our website, TWiT.tv/sn.  There's a dedicated YouTube channel to Security Now!.  That's a good way to share clips from people.  Just go to YouTube.com/securitynow, and you can just make a little clip and share it that way.  That's a great way to do that.  Of course if you have a podcast player, you can subscribe.  We've been around for 18 years.  If it doesn't have Security Now! in its directory, I don't know what they're doing, what they're playing at.  Just subscribe.  That way you'll get it the minute it's available of a Tuesday evening.



If you are watching or listening after the fact, and you still want to interact, we also have TWiT Forums at TWiT.community.  Those are open to all.  And a Mastodon instance, which is like Twitter only better.  It's federated.  And that's at TWiT.social.  Again, open to all.  So please join both of them.  Love to have you in both places.  Steve, I think we've done everything we possibly can do to save the world.	



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#890

DATE:		September 27, 2022

TITLE:		DarkNet Politics

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-890.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine Europol's desire to retain data on non-criminal EU citizens, and we look at the fourth EU nation to declare that the use of Google Analytics is an illegal breach of the GDPR.  Has Teapot been caught?  Seems like.  And Mozilla says it's no fair that operating systems bundle their own browsers.  Here we go again.  Meanwhile, Chrome's forthcoming V3 Manifest threatens add-on adblocker extensions, and past Chrome vulnerabilities are leaving embedded browsers vulnerable.  Windows 11 actually gets a useful feature, and some U.S. legislation proposes to improve open source software security.  We revisit the Iran-Albanian cyber-conflict now that we know how Iran got into Albania's networks.  And after one important and interesting bit of listener feedback about multifactor authentication fatigue and a quick SpinRite update, we look at some new trends in the dark underworld with the leak of another major piece of cybercrime malware.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a lot planned for you.  We'll talk about yet another country joining the growing list of countries prohibiting Google Analytics.  What's Google going to do about it?  There are some proposals.  We'll also talk about Google's V3 Manifest for Chrome.  It blocks the blockers.  What are you going to do about that?  And an inside look at Lockbit, the number one ransom program.  They just got hacked.  Aww.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 890, recorded Tuesday, September 27th, 2022:  DarkNet Politics.



It's time for Security Now!, the show where you get really down and dirty with the technology.  



STEVE GIBSON:  That's right.



LEO:  Well, not in that way.  You know what I mean.



STEVE:  That's right.



LEO:  We get really into it with this guy right here, Steve Gibson.  He is the technology wizard we all look to when it comes to understanding better what's going on in the digital world.  Hi, Steve.	



STEVE:  Hello, Leo.  Great to be with you for our last episode of September.  What happened?



LEO:  It's so quick.  It's just so quick.



STEVE:  So now we're starting into the fourth quarter of 2022, for Episode 890.  So a lot of stuff to talk about.  It was a busy week in the security world.  We're going to examine Europol's, which is sort of the policing force, the enforcement side of the EU government, their desire to retain data on non-criminal EU citizens, which is not technically legal.  And we look at the fourth EU nation, speaking of not legal, to declare that the use of Google Analytics is an illegal breach of the GDPR.  We're going to look at the question of whether Teapot has been caught.  Seems like.  And Mozilla says it's no fair that operating systems bundle their own browsers.  So here we go again.



Meanwhile, Chrome's forthcoming V3 Manifest threatens add-on adblocker extensions.  And past Chrome vulnerabilities are leaving embedded browsers vulnerable, which is an aspect of Chrome we've never talked about before, or Chromium, rather, you know, the engine.  Windows 11 actually gets a useful feature.



LEO:  No.



STEVE:  Yeah, I know, it happened.



LEO:  No.



STEVE:  Really.  That's amazing.  And some U.S. legislation proposes to improve open source software security.  We revisit the Iranian/Albanian cyber conflict, now that we know how Iran got into Albania's networks.  And after one important and interesting bit of listener feedback about multifactor authentication fatigue, and a quick SpinRite update, we're going to look at some new trends in the dark underworld with the leak of another major piece of cybercrime malware.  Thus today's podcast is titled "DarkNet Politics."



LEO:  Ooh.  Back to the Picture of the Week, Mr. Gibson.



STEVE:  So this one was just one I've had in the queue for a while.  It shows a fishing line descending down into the frame from off-frame.  At the end of it is a hook with a worm stuck there.  And we've got two fish that are sort of eyeing this, looking like easy prey.  And the smaller one of the two is saying, "Be careful.  It could be an online scam."



LEO:  Yikes.  It's a "fishing" scam.



STEVE:  And yes indeed, it's a "fishing" scam.



LEO:  Ohhh.



STEVE:  So, okay.  So an interesting conundrum caught my eye last week.  The European Data Protection Supervisor - who we'll just say EDPS for short, European Data Protection Supervisor - which is - it's the European Union's independent supervisory authority chartered with monitoring European institutions and bodies to assure that they respect citizens' rights to privacy and obey their own data protection rules.  This EDPS filed a lawsuit with the European Court of Justice against the European Union and Europol.  And as I mentioned at the top of the show, Europol is the law enforcement or policing division.



So going back a few months to January of this year, EDPS, that supervisory agency, published the results of a three-year investigation.  They said that they had found that Europol, this law enforcement agency of the EU, had secretly collected - secretly collected - large troves of personal information on EU citizens dating back years, even if those persons had not committed any crimes or were under any investigation of any kind.  In other words, data collection without cause or oversight.  Which is a violation of privacy laws in Europe.



So the EDPS used its regulatory powers - this is after it learned this back in January - to order Europol to filter its database, deleting any and all information they had on European Union citizens that had not committed any crimes over the past six months.  It ordered Europol to scrub this database, all of its databases, by January of 2023.



Okay.  So now the reason for the lawsuit, which was filed just last week, was that the EDPS said that EU lawmakers went behind its back and passed new legislation in June to allow Europol to retroactively keep all its previously collected information.  In reaction to that action in June, the EDPS said it had "strong doubts as to the legality of this retroactive authorization."  And now the EDPS says that this new development actively subverts its independence and authority and wants the court to invalidate the new legal amendments and stay its original decision.



Because of this legislative and enforcement infighting, the EDPS investigation and lawsuit are highly controversial topics among law enforcement officials.  In an official response in January, defending its massive data collection, Europol said, and they're probably right about this, that deleting this data will "impact its ability to analyze complex and large datasets at the request of EU law enforcement," and that it would hinder the EU's ability to detect and respond to threats such as terrorism, cybercrime, international drug trafficking, and of course you know what's coming next, child abuse, and others, many of which involve transnational investigations at a very large scale.  And as I said, they're arguably correct in that assumption.  I mean, the problem is of course abuse of this massive collection and dataset.  



So the reason these are difficult problems is that both sides of the dispute can be correct, each from their own perspective.  21st-century crime fighting will be enhanced by massive machine learning datasets used for data analysis.  But it's also true that enormous databases of sensitive and personally identifiable information need, at the very least, robust safeguards.  And there's no better safeguard than the deletion of all such data for non-criminal citizens.



So anyway, I just thought it was interesting that they're basically fighting with themselves over what it is that they should do.  Even with governments being well intentioned, remember our friend Bert Hubert, who resigned from his posting in the Netherlands because a branch of his government was trying to push to or past the limits of the safeguards and boundaries that had previously been put in place for a reason.



LEO:  You know, so they said if you haven't committed a crime in the last six months, no information about you.  But what about a fingerprint database or a DNA database?  What if you committed a crime two years ago, and we've got your fingerprints on record?



STEVE:  Right.



LEO:  And you're saying, no, you're not allowed to have any DNA?  You're not allowed to have any fingerprints if I haven't committed a crime in the last six months?  I think there's a reason to have an archive.



STEVE:  You have to imagine that criminals who have been convicted are permanently in a system and not subject to that six-month deletion.



LEO:  Okay.  But then it should say nobody who's ever been convicted of a crime.



STEVE:  Right.



LEO:  If you've never been convicted of a crime, it shouldn't be in there.  I agree with that.  That absolutely I agree with.  But it says six months, for the last six months.



STEVE:  It does.



LEO:  Which implies that if you did it seven months ago, oh, no, you've still got to delete it.  And I think that actually does hinder police work unreasonably.



STEVE:  Yeah, I agree.  So, I mean, there is - and so again it's one of these dilemmas where we could do anything we want.  We just have to decide what we want to do.  And there are opposing forces that have arguments in both directions.



LEO:  Yeah, I understand.  Privacy is great, but - and maybe that's the rule.  If you haven't committed a serious, maybe it should be serious crime, not a petty theft, but a serious misdemeanor or felony, then we have the right to keep your fingerprints and DNA for as long as we want.  Right?



STEVE:  Yeah.  Well, and I liked - a couple weeks ago the way I phrased this about privacy and encryption was that if you had absolute privacy, then that would allow individuals to absolutely escape responsibility.  And so it's the - like the system we have in the U.S. with a search warrant is conditional privacy.  We're protected against illegal search and seizure, as the phrase goes.  But if you convince a judge that there is reason to suspect that the interests of the people will be served by incrementally breaching some privacy, you know, a search warrant, then it can be granted.  And in that instance, within the limits of that warrant, an individual's privacy, which is not absolute, is then removed for the sake of enforcement.



Anyway, while we're in the neighborhood, Denmark has become the fourth EU member, joining Austria, France, and Italy, to rule that the use of Google Analytics is illegal in Denmark.  The Danish Data Protection Agency ruled this week, actually it was last week, that the use of Google Analytics inside the country is not compliant with the GDPR.  The agency told local companies to either adjust the tool for increased privacy - and actually there are no useful adjustments, as we'll see in a second - or stop using it.



The beginning of an explanation published last Wednesday said:  "In January 2022, the Austrian Data Protection Authority issued a decision on the use of Google Analytics by an Austrian organization."  So that was January of this year.  "Since then, the Austrian Data Protection Authority has issued another decision on the use of the tool, and several decisions have also been issued by the French Data Protection Authority.  Most recently, in June, the Italian Data Protection Authority issued a decision on the use of the tool," the "tool" meaning Google Analytics.  "In all of these cases, the supervisory authorities found that the use of Google Analytics under the given circumstances was unlawful."



The Senior Legal Advisor at the Danish Data Protection Agency said:  "The GDPR is made to protect the privacy of European citizens.  This means, among other things, that you should be able to visit a website without your data ending up in the wrong hands.  We've carefully reviewed the possible settings of Google Analytics and have come to the conclusion that you cannot use the tool in its current form without implementing supplementary measures.  Since the decisions by our European colleagues, we have looked into the tool and the specific settings available to you when you intend to use Google Analytics.  This has been particularly relevant as Google, following the first Austrian decision, has begun to provide additional settings in relation to what data can be collected by the tool.  However, our conclusion is that the tool still cannot, without more, be used lawfully."



Okay.  So organizations in Denmark that employ Google Analytics, which is on so many websites...



LEO:  Including ours, I might add.



STEVE:  Yeah.



LEO:  Does this mean I can't have anymore Denmark listeners?



STEVE:  Okay.  So that's really a question, right, is like the fact that the GDPR technically reaches us; right?  And so we have to be compliant if EU citizens come to websites in the U.S.  So anyway, its use is widespread.  And organizations in Denmark, after this order, must assess whether their possible continued use of the tool takes place in compliance with data protection law.  And the ruling here says it can't.  And if it's not the case that they can be compliant, the organization must either bring its use of the tool into compliance or, if necessary, discontinue using the tool.  So there are now four countries which have all said it's not possible for the tool to be used in compliance regardless of its settings.



The Senior Legal Advisor said:  "A very important task for the Danish Data Protection Agency is to give guidance to citizens about their rights, and to give guidance to Danish organizations in how they comply with data protection law.  As is the case with data protection law, we at the Danish Data Protection Agency are neutral to technology, and therefore have no interest in either approving or banning certain products.  We are not at all empowered to do so.  Following the decisions of our European colleagues, however, we've experienced a great demand for guidance in relation to specifically Google Analytics, and we have therefore made an effort to look into this specific tool more closely."



Okay.  So the message from the Danish Data Protection Agency is that any enterprise's websites that are within Danish jurisdiction - or, again, actually within the reach of the EU's GDPR - which use Google Analytics must put in place a plan to bring their use into compliance by implementing supplementary measures.  We'll get to that in one second.  And they said:  "If it is not possible to implement effective supplementary measures, you must stop using the tool and, if necessary, find another tool that can provide web analytics and allows for compliance with data protection law."  Boy, Google must be really getting some headaches with all this.  "For example," they said, "by not transferring personal data about visitors to 'unsafe' third countries."



Okay.  So what are these "supplementary measures" that could be taken?  Well, it turns out that France, the second of the four EU countries to object to Google's Analytics, invested some technical resources to provide a document which answers the question.  The document, dated July 20th of this year, a little over two months ago, is titled "Google Analytics and Data Transfers:  How to Make Your Analytics Tool Compliant with the GDPR."



Okay.  So the French document explains.  It says:  "The Court of Justice of the European Union (CJEU), in its ruling 16 July 2020, invalidated the Privacy Shield, a mechanism that provided a framework for transfers of personal data between the European Union and the United States.  The U.S. legislation does not offer sufficient guarantees in the face of the risk of access by the authorities, particularly intelligence services, to the personal data of European residents.



"Following these formal notices, many actors have sought to identify the technical settings and measures that can allow to maintain the use of Google Analytics while respecting the privacy of Internet users.  However, simply changing the processing settings of the IP address is not sufficient to meet the requirements of the CJEU, especially as these continue to be transferred to the U.S.  Another idea often put forward is the use of encryption of the identifier generated by Google Analytics, or replacing it with an identifier generated by the site operator.  However, in practice, this provides little to no additional guarantee against possible re-identification of data subjects, mainly due to the persistent processing of the IP address by Google.



"The fundamental problem that prevents these measures from addressing the issue of access of data by non-European authorities is that of direct contact, via an HTTPS connection, between the individual's" - now, they call it a "terminal," but we know that's PC and browser or device and browser - "the individual's terminal and servers managed by Google.



"The resulting requests allow these servers to obtain the IP address of the Internet user as well as a lot of information about his terminal.  This information may realistically allow the user to be re-identified and, consequently, to access his or her browsing on all sites using Google Analytics."  And of course that's 100% true.  100% technically accurate.



LEO:  Same thing happens when you do a Google search, but okay.



STEVE:  Yeah, yeah.  They said:  "Only solutions allowing to break this contact between the terminal and the server can address the issue."



LEO:  So they should ban Google.  Seriously.



STEVE:  Well, yeah, yeah.



LEO:  So I don't understand.  If the same thing happens when I do a Google search, and you don't want it to happen with Analytics, well, just ban Google.  Let's see what happens then.



STEVE:  So, okay.  "So beyond the case of Google Analytics," they said, "this type of solution could also make it possible to reconcile the use of other analytics tools with the GDPR rules on data transfer."  Okay.  So that all makes sense.  The issue is that the user's machine and web browser, or "terminal" as they say here, is posting its analytics directly to a Google domain.  So its incoming IP address is always known to Google.  To resolve this, the French recommendations, and this is in this formal document that they published, are that a proxy server would be a possible solution.



They say:  "In view of the criteria mentioned above, one possible solution is the use of a proxy server to avoid any direct contact between the Internet user's terminal and the servers of the analytics tool, in this case Google.  However, it must be ensured that this server fulfills a set of criteria in order to be able to consider that this additional measure is in line with what is presented by the EDPB" - whoever they are - "in his recommendations of 18 June 2021.  Indeed, such a process would correspond to the use case of pseudonymization before data export."



They said:  "As stated in these recommendations, such an export is only possible if the controller has established, through a thorough analysis, that the pseudonymized personal data cannot be attributed to an identified or identifiable individual, even if cross-checked with other information.  It's therefore necessary, beyond the simple absence of a request from the user's terminal to the servers of the analytics tool, to ensure that all of the information transmitted does not in any way allow the person to be re-identified, even when considering the considerable means available to the authorities likely to carry out such re-identification."



So in other words, they're talking about really being serious about erecting a barrier between the EU citizens and anyone downstream of this barrier.  So, and they specify what is entailed.  They said:  "The server carrying out the proxification must therefore implement a set of measures to limit the data transferred.  The CNIL" - which is the group that created this document - "considers, in principle, the following is necessary:  The absence of transfer of the IP address to the servers of the analytics tool.  If a location is transmitted to the servers of the measurement tool, it must be carried out by the proxy server, and the level of precision must ensure that this information does not allow the person to be re-identified,  for example, by using a geographical mesh ensuring a minimum number of Internet users per cell."  So they're just giving some samples.



And the replacement of the user identifier by the proxy server:   "To ensure effective pseudonymization, the algorithm performing the replacement should ensure a sufficient level of collision, i.e., a sufficient probability that two different identifiers will give an identical result after a hash."  Okay.  Now we start to have a problem because, if you do this, then you're breaking the point of analytics, which is to identify the activity of the site, although you are keeping the specific user who's visiting the site secret, so you are getting pseudonymized per user data still.



Also they specified the removal of referrer information from the site.  That's a problem for analytics because analytics wants to know where you are, like where you are on the site, which is what the referrer information in the query header provides.  Also the removal of any parameters contained in the collected URLs, that is, you know, URL tails - UTMs, also URL parameters which may also cause a leakage of information.  Also reprocessing of information, they said, that can be used to generate a fingerprint, such as user-agents, to remove the rarest configurations that can lead to re-identification.  So make those all look the same.  The absence of collection of cross-site or lasting identifiers, a CRM ID or any sort of a unique ID.  And the deletion of any other data that could lead to re-identification.  In other words, it is a daunting task.



They go on to say:  "The proxy server must also be hosted in conditions that ensure that the data it processes will not be transferred outside the European Union to a country that does not provide a level of protection substantially equivalent to that provided within the European Economic Area."  So, wow.  More and more what we're seeing here is kind of the way they would like the Internet to be in the EU, coming into head-to-head collision with the current operation of the Internet, and asking for huge changes.  Establishing a proxy would be a lot to ask for the typical website that just wants to use analytics because it was two lines of JavaScript code, and they got all this amazing information for free.



LEO:  Yeah, we want to know how many people visit our site.



STEVE:  Yeah, and which pages, and what search terms brought them there, you know, and all this cool stuff.  A Google Analytics proxying service could be set up somewhere in the EU.  Then EU websites would point their Google Analytics JavaScript to that service's domain instead of to analytics.google.com.  In that way, the visitors to any of these GDPR-compliant Google Analytics-using websites would have their browsers query the proxy on their behalf.  Since the proxy would be terminating their TLS connections, it would be able to strip identifying information from the query, make any changes it wanted to, insert some randomization to confuse fingerprinters, and so on.



So we have another example here of the growing tension between privacy and commerce.  And what they're asking for is feasible.  But, boy, you know, are they going to enforce it?  Are they going to require it?  And it would require setting up a local proxy within the jurisdiction of any country or countries that wanted to enforce this level of anonymization through something like Google Analytics and route all the queries through it before it then goes to Google for their data correction.



LEO:  And who runs the proxy and sees the information on the proxy?



STEVE:  I know.  Well, so they're saying...  



LEO:  Sounds like a data grab.



STEVE:  That would need to be in the EU.  And, yeah, it's going to be another centralization.



LEO:  Set a third eyeball to watching this and see what happens.



STEVE:  But their point, of course, is that it's not leaving the EU for the U.S.  And so, you know, it is a solution to the problem.  But, boy, it's a heavy lift in order to change the operation of something which has been in place already, what, Leo, like 15 years or 20 years?



LEO:  Yeah.  And it's going to slow it down dramatically.



STEVE:  Yup.



LEO:  We don't get, you know, when you use Google Analytics, I don't get any individual information about visitors at all.



STEVE:  Right.  You don't, but Google does, and that's their complaint.  



LEO:  The presumption is that Google does, and does something with it, yeah.  By the way, if I were to run a local analytics program, I would get all of that information.  I'd get all the IP addresses and everything.  This just seems so wrong-headed to me.  I don't understand.



STEVE:  I know.



LEO:  I understand what they want.  I don't understand how they think this is going to get them there.



STEVE:  Yeah, well, and again, it's like now we have to agree to cookies wherever we go.  Thank you very much.



LEO:  Oh, yeah, that's really worked.  Oh, boy, has that solved that problem.



STEVE:  Okay.  In last week's network breach review, we were just talking about the Uber and Rockstar Game breaches and the belief that both quite public intrusions were perpetrated by the same teenager.  So I wanted to just note for the record that last Thursday the City of London police detained a 17 year old from Oxfordshire on hacking-related charges.



LEO:  And it ain't his first rodeo.



STEVE:  While U.K. officials have not released the suspect's name or other details about his arrest, the teen is widely suspected of being Teapot, a member of the Lapsus$ gang, who recently breached Uber and Rockstar Games.  And I would love to know how this kid was tracked down.  As I mentioned when we talked about this before, he seemed to be extremely braggadocious about these breaches.  And the more one struts around crowing, the more clues you inadvertently leave behind.



LEO:  Well, and also this is the same kid that already has been arrested for the Microsoft hack, the earlier Lapsus$ hack.  That's what they're saying.  So this kid, not only is he braggadocious, but he keeps...



STEVE:  He's not learning a lesson.



LEO:  It's like, he's currently on parole for that.  Or probation, I think, not parole.



STEVE:  Probation, yeah.



LEO:  On probation for that.



STEVE:  It's only probably the fact that he's a minor that I saving him at this point.



LEO:  Yes, right.  That's right.  That's right.  Well, I wonder if Lapsus$...



STEVE:  I mean, these are felonies.  These are felonies.



LEO:  I wonder if Lapsus$ is a gang, or just this guy, frankly.



STEVE:  This is felony cyber intrusion.



LEO:  Yeah.



STEVE:  So, wow.



LEO:  And he's using, you know, in every case he's used social engineering, you know, posing as somebody, and give me your two-factor kind of thing.



STEVE:  Okay.  So let's take a break, and then we're going to talk about Mozilla saying it's no fair.



LEO:  It's no fair.



STEVE:  It's no fair.



LEO:  It's not fair, Mark.  Okay.  Moving right along, I'm still kind of trying to figure out what we're going to do at TWiT about these GDPR things because that's now, what, four or five countries that won't allow Analytics.



STEVE:  Everybody who is asked to rule on it rules the way they have to, which is it is a breach of the GDPR.  



LEO:  Yeah.  It requires - and it's I think mostly because GDPR considers IP addresses IIP; right?  PII, rather, Personally Identifiable Information.



STEVE:  Yes.  But we know that IP addresses tend to be relatively static.  But they're also going way further, talking about unique tokens and referrer headers and, I mean, they're getting aggressive.  I mean, this is France saying ooh la la.



Mozilla says "No fair."  They recently published a 66-page sour grapes document complaining that they don't own any major platform, whereas Google, Apple, Meta, Amazon, and Microsoft each do.  And that each of those major players bundles their respective browsers with their operating systems and quite naturally sets them as the operating system default in the home screen or dock.  And that as a result, for most people, this placement is sufficient, and they will never see or pursue the extra steps necessary, as Mozilla says, to discover alternatives.  You know, one of my favorite observations, "the tyranny of the default."



So this paper is titled "Five Walled Gardens:  Why Browsers Are Essential to the Internet, and How Operating Systems Are Holding Them Back."  And they might have been titled the document "Why Firefox is losing market share, and it's no fair."  Now, I know that doesn't make me seem very sympathetic.  I actually am.  I love Firefox.  You and I, Leo, talk about it all the time.



LEO:  Using it right now, yeah.



STEVE:  Yup.  I've been a Firefox user as my primary browser on every one of my machines for decades.  Firefox is the default registered URL handler on every one of my PCs.  If a link is clicked, Firefox receives it.  What I am aggrieved by is the constant annoyance of the other non-Firefox browsers which, seeing that they are not the chosen one, use every opportunity to suggest that my browsing experience could be greatly enhanced if I were using them to view that page.



So Mozilla's 66-page paper amounts to them making a truly compelling case - I mean, there's no question that this is going on, we know it is - a compelling case for exactly how screwed they are going forward.  They blame the OS vendors for putting their own self-interest first.  You know, welcome to America.



It's unclear to me what this is actually about.  Is this a prelude to another browser wars antitrust lawsuit?  I hope not.  But some of the language in the 66-page complaint, which is what it actually literally is, is a complaint  does appear to be paving the ground for something.  And they sort of made an offhand reference to wouldn't it be all better if we could come to an agreement sort of thing.  So Google is currently funding Mozilla to the tune of $450 million per year in return for Firefox defaulting to Google as its search engine.  So there's the tyranny of the default for you again, this time working in Firefox's favor.



On December 27th of 2011, so 11 years ago, Wired Magazine published "Why Google Continues to Fund Firefox."  And their subhead was:  "Google has its own web browser, so why is the company renewing its revenue deal with Mozilla?  The answer is simple," they write.  "Google makes money by putting eyeballs in front of ads, and almost a quarter of the web's eyeballs use Firefox."  Now, I was sad to read that 11 years ago because that's decidedly no longer the case.  The 2022 market share for the top four browsers is Google Chrome obviously in first place at 77.03%; Safari in second place at 8.87%.  Mozilla Firefox



holding the third place at 7.69%.  And to me surprising, Microsoft Edge in fourth place at only 5.83%.



I think it's clear that Safari's edge is thanks to the gazillion iPhones and iPads since the macOS, while it's there, it would not be making nearly as huge a dent.  But bless its little digital heart, Firefox is hanging in there at number three, still nicely and somewhat amazingly edging out Edge by nearly two percentage points.  But it's unclear what Firefox's future is.  They laid off, what was it, 25% of their workforce a year ago.  And their deal with Google, I think, is up for something in 2023 is when this - I think it was a three-year deal for Firefox and Mozilla.  So people have said, oh, it behooves Google to keep Firefox alive because it keeps them from seeming like a monopolistic entity for antitrust purposes.  Who knows?



But anyway, I just thought I'd put a note about the 66-page boohoo note from Mozilla.  It's like, yeah, sorry, you don't have an operating system platform of your own.  And Leo, you and I know how bad a monoculture is.  The idea that everybody is using the same singular Chromium engine is bad because it means those mistakes are universal when they are found.



LEO:  Exactly, yeah.



STEVE:  Yeah. 



LEO:  Plus, well, I just want competition.  I want a variety.



STEVE:  Yup.  It's good.



LEO:  Safari's doing all right.  But WebKit is in its way like Chromium, kind of a dominant engine.  I need Mozilla to succeed.  We may have to just start raising money for them or something, if Google pulls out.



STEVE:  Yeah.  So back in November of 2020, Google announced what they called Manifest V3 for Chromium and Chrome.  And we talked about it at the time.  As I get into this, some of our listeners will go, oh, yeah, I remember.  The concern back then was the deleterious effect that it would have on adblockers, that is, this V3 Manifest, which comes as no surprise to Google critics.  So as you may recall when we were talking about this before, Google is changing the way Chrome's extensions function.  Rather than allowing individual extensions to receive, examine and either drop, modify, or forward each of the browser's outgoing requests, as has always been allowed until now, under Manifest V3 there's a new API called "declarativeNetRequest."  And it operates sort of the way its name suggests if you're into APIs, that is, it's declarative rather than - what's the reverse of declarative?  I'm blanking on the word.



LEO:  Imperative?



STEVE:  Yeah, imperative.  Okay.  So this "declarativeNetRequest" allows extensions to modify and block network requests in what Google calls a privacy-preserving and performant way.



LEO:  Implicit.



STEVE:  Well, it actually is.  They did say "performant."  So what this actually means is that Google remains in control.  What occurs under Manifest V3, which by the way is on its way rolling out, is rather than intercepting a request and modifying it procedurally - that was the word I was looking for instead of declarative, procedural - modifying it procedurally, the extension registers with Chrome, asking it to evaluate and modify requests, like matching requests, on its behalf.  The extension declares a set of rules, and we're not sure how many there may be.  But adblockers need a gazillion.  If you've ever seen the rule set on an adblocker, it just, you know, it makes your eyes water.



So the extension declares a set of rules, patterns to match requests, and actions to perform when matched.  The browser engine, Chromium, then modifies network requests as defined by these rules.  So you can see it's a completely different way of operating, and it's got the adblocker extensions a little nervous.  Google claims that:  "Using this declarative approach dramatically reduces the need for persistent host permissions."  And they're not wrong.  I mean, this is an elegant way of solving the problem.  But it definitely eliminates control from extensions that they have historically had.  But Chrome is also tightening down on and limiting the power of its extensions, and Google cynics are suggesting that it's a move to protect its advertising revenue.  Of course they are.



So it's for this reason that the Vivaldi Browser's lead developer took the time to post last Friday that come hell or high water - those are my words, not his - Vivaldi's adblocking would continue to be effective even in the face of Manifest 3. In his post on Friday, Julian wrote:  "The move to Manifest V3 makes it more difficult to run content blockers and privacy extensions in Chrome.  While some users may not notice a difference, users who use multiple extensions or add custom filter lists may run into artificial limitations set by Google."  He says:  "Perhaps wise to move away from Chrome?"



He says:  "As Vivaldi is built on the Chromium code, how we tackle the API change depends on how Google implements the restriction.  The assurance is, whatever restrictions Google adds, in the end we'll look into removing them."  He finished:  "Our mission will always be to ensure that you have the choice."



So Julian notes that the entire existing V2 API - I'm sorry, yeah, the existing V2 API continues to be present for Chrome's enterprise users.  So that means that it's only the consumer who is being hit with this restriction, and that all of the existing code remains accessible somewhere.  So it's going to be interesting to watch this one shake out.  While Firefox, as I've said, is my default URL handler, I do often use Chrome for ad hoc Internet research.  I edit this podcast.  The show notes in front of us are done in Google Docs every week.  And things have grown so horrendous on the 'Net that I could not live without an effective adblocker any longer.  If Chrome really does become an advertising browser and makes the ability to suppress the insanity that too many web pages have become, you know, they might drive a move back to Firefox.



LEO:  So I'm with you.  And I use Gorhill's UBlock Origin, just like you.  And by the way, it has a built-in cookie banner blocker, among other things.  It's one of the annoyances features.  But do you think there's a legitimate security reason for Google to insist on Manifest V3?



STEVE:  Yes.



LEO:  In other words, that web content API is potentially insecure.  It's potentially a problem; right?



STEVE:  Yes.  It's known as the webRequest API.  And, I mean, it literally is a "call each extension in turn and let them each look at it, modify it, drop it, or forward it."  So, I mean, these extensions as they are now are in the pipeline.  And so there is, this is why they use the word "performant."  



LEO:  Right.  



STEVE:  Because if an extension takes a long time to think about one of these queries that's been handed to it, the whole process slows down.  So what Google is doing is Google is trying to compromise here.  I mean, and it's a legitimate, you know, attempt at compromise.  They're saying we're going to build a screaming fast pattern-matching engine.  You put the matches in you want.  And it'll be a big regex machine.  You put the regular expressions in that you want matched, specify the changes you want made.  We'll do them for you.



So what that does is of course it completely eliminates this pipeline, this per-extension processing pipeline which both gives us, Google, the users, everybody, more security, and potentially substantially greater speed because Google is saying we're going to, you know, who knows what they're going to do?  They might at launch time, when all the extensions are in place and have registered their list of regex work, Google could compile it, like into some screaming fast blob that just, you know, queries go in, and the results immediately come out the other end.  So they can't do that now with the V2 architecture.  They need to move to this V3 model.  And once again it's going to be a tradeoff.  Extensions are going to lose some power.



LEO:  I wish we could find some sort of compromise, and I wish it didn't look so much like Google wanted to preserve their ad business.



STEVE:  I know.  And you know, Leo, that keeps coming up, the idea, unfortunately, that the entity offering a browser, which is the thing that displays ads, is the revenue for that entity, I mean, it creates, like it's a built-in conflict of interest.



LEO:  Yeah, yeah, of course.  Same thing when YouTube search results top the Google search results; you know?  You can go on and on.  Google self-deals all the time.



STEVE:  Yeah.  Okay.  So here's one that had never occurred to me before, while we're on the topic of Chrome.  A group known as Numen (N-U-M-E-N) Cyber Labs have published extensive write-ups on a pair of older and long-since-fixed Chrome vulnerabilities, CVE-2021-38003 and 2022-1364.  Both were Chrome zero-days patched in October 2021 and April of 2022, respectively.  And either one could be used at the time for remote code execution attacks against Chrome users.



What's interesting and chilling about Numen's observation is that they warn that even though these two security flaws have been patched in the main Chromium core and Chrome browser, the patch gap that exists in software that uses Chrome's WebKit engine as their built-in browser means that many mobile apps are still vulnerable to this, including, and they use this as an example, the most recent release of Skype, which is subject to a zero-day remote-code execution flaw because it uses the Chromium core and has not been patched, even though Chromium was, the most recent one in April and the previous one in October of last year.



I thought that was a fascinating observation, and one, as I said, we've never considered.  I often talk glowingly about how the Chromium guys jump on a report of a new zero-day and often push out an update only a day or two later.  But applications that incorporate Chrome's WebKit engine, Chromium, are taking a snapshot of the engine and may be far more lackadaisical about keeping that engine snapshot up to date.  After all, it's working.  Why bother with it?  Well, why indeed?  After all, the Chromium engine, as we know, is truly a work-in-progress moving target.  But that's anathema to projects that want to build from essentially static libraries.  I would be willing to bet that very few of them are pushing out new release builds of their application because one of their component dependencies, in this case Chromium, was updated.  And as we know, those Chromium updates are happening all the time.



So to me it seems unlikely in the extreme that apps are being that responsible.  So any and all of such applications - and again they showed on the screen Skype being taken over - might well be inheriting and existing with Chrome's historical vulnerabilities.  This again is another good reason for Google never to talk about them, no matter how old they are.  But unfortunately these Numen guys did a complete takedown of both of these.  So any app that is using an un-updated Chromium now who sees what Numen Cyber Labs has published, can start poking at any embedded browser engines to see if they're able to take the app over remotely.



So it's a chilling thing that we never really talked about.  But it's a consequence of, you know, a browser engine being so complex, being inherently a moving target, yet we get this, as they called it, the patch gap between when the library was taken and what version they're using and when it was built into their app.  And are they even bothering?  Do they even care?  Yikes.



Okay.  We all know that I'm not a big fan of Windows 11.  That's primarily because of the lies we were told about its hardware system requirements from the beginning, which never made a lick of engineering sense and which, sure enough, were eventually acknowledged to be untrue.  I remember Paul and Mary Jo saying, oh, yeah, yeah, yeah, that's not true.



LEO:  Although I'm going to add that last week we started talking about a new feature that's rolling out in 22H2 of Windows 11 that does perhaps explain 8th-generation Intel and TPM 2.0.



STEVE:  Okay.



LEO:  And it has to do with virtualization, and I can't remember the exact details.  But it perhaps then does make sense that they knew this was coming as a security update, and they wanted to make sure it was supported, and that if you were using Windows 11...



STEVE:  And they didn't want to drop that they...



LEO:  They didn't want to tell anybody yet.



STEVE:  Well, and they didn't want to allow a subsequent update to Windows 11 to suddenly say, oh, we're sorry.



LEO:  Yes.



STEVE:  You can't have the Windows 11 update because your chip is too old.



LEO:  Yeah.  Let me look at the notes from last week because you deserve the info, anyway, as best as I can interpret it.  There is a hint at why Microsoft chose 8th-gen as the dividing line a year ago, except what was that hint?  They didn't put it in the notes.  They just said we'll tell you about it.  But as I remember, it has something to do with virtualization.



STEVE:  Interesting.  Okay.



LEO:  Yeah.  So there may be kind of a reason for it; right.



STEVE:  So some new hardware level thing that the 8th-gen chips have that the previous ones don't.



LEO:  Precisely.



STEVE:  And Windows until now has not depended upon on that.



LEO:  Exactly.



STEVE:  Well, because it ran on all the chips.  



LEO:  Right, right.  And that may be why they say we won't promise to support it if you run it on older hardware.  They are allowing people to do that.  But you won't get the security, new security feature.



STEVE:  Right, right.  So it's still unclear on Windows 11 whether I will be eventually forced to move away from Windows 10, or whether Microsoft will eventually take no for an answer.  You know, I'm still happily - I'm sitting in front of Windows 7 right now.  Works great, and they leave me alone.  Anyway...



LEO:  It's a race between Episode 999 and when Windows 10 is no longer in support.  Let's just put it that way.



STEVE:  That's right.  So we'll see; you know?  Anyway, okay.  So in at least one instance, you know, it looks like they've done something useful.  Believe it or not, Microsoft will finally, at long last, be adding default brute force protection into Windows 11's notoriously insecure SMB file and printer sharing user authentication.  So it's called the SMB authentication rate limiter concept.



LEO:  Woohoo.



STEVE:  Who would have ever imagined you could do that with a computer?



LEO:  Geez.  Wow.



STEVE:  But it turns out, Leo, you need an 8th-generation Intel processor in order to do rate limiting.  You can't have it.  You could not have it on a - Windows 95 could not have done rate limiting.



LEO:  Never.  Never in a million years.



STEVE:  It's too advanced.



LEO:  Yes.



STEVE:  It is an advanced technology.  It was reverse-engineered from, where is that place where the UFOs are all seen?



LEO:  Yeah, Area 51, yeah.



STEVE:  Oh, it came out of Area 51.  They said, okay, we don't know, we're unable to crack these alien computers, and they won't let us keep guessing passwords.  They slow us down.  Huh.  Isn't it too bad we can't put that into Windows?  We'll have to wait till Intel's 8th-generation processors.  Anyway, we finally have it.  It's currently being tested by insider builds.  As its name suggests, this new advanced feature from the aliens will significantly rate-limit brute force attacks against a Windows 11 SMB service.  So anyone who either deliberately or inadvertently exposes their SMB services on port 445 to the public Internet, as so many people seem unable to keep from doing, they will receive a modicum of protection.



With the release of Windows 11 Insider Preview Build 25206 Dev Channel today, the SMB server service now incorporates a two-second default delay, that's what the aliens used, Leo, so they didn't want to change anything because that might have broken something, and it might be some magic there.  It uses a two-second default delay after each failed inbound NTLM authentication attempt.  This means that if an attacker previously sent, for example, 300 brute force attempts per second from a client for five minutes  thus 90,000 username and password guesses  now the same number of attempts would take 50 hours, rather than five minutes.  Somewhat sad to be celebrating such a simple measure that could have been implemented anytime in the past 20 years, but better late than never.



Also, as I mentioned, two U.S. senators, Rob Portman, who's an Ohio Republican, and Gary Peters, a Michigan Democrat, introduced a bill last Thursday in a bid to strengthen the security of open source software.  Together they co-sponsored the bipartisan, and I love this one, it's Securing Open Source Software Act.  And when I looked at it, I realized it was the SOS Software Act.  So Securing Open Source Software Act.  The goal is to help protect federal and critical infrastructure systems by strengthening the security of open source software.



And what do you think got their attention?  Yup, the legislation comes after a hearing convened by Portman and Peters on the Log4j incident at the beginning of the year, and it would direct our favorite agency, CISA, to help ensure that open source software is used safely and securely by the federal government, critical infrastructure, and others.  Now, how they actually do that remains to be seen.



The SOS Software Act directs CISA to develop a risk framework - because, you know, if you're going to be a bureaucrat, you've got to have a framework - a risk framework to evaluate how open source code is used by the federal government.  Apparently they don't know now.  CISA, oh, we're going to have a risk framework to evaluate how open source software code is used by the federal government.  CISA would evaluate how the same framework could be voluntarily used by critical infrastructure owners and operators.  This will identify ways to mitigate risks in systems that use open source software.  The legislation also requires CISA to hire professionals with experience - they're going to get some money - experience developing open source software to ensure that government and the community work hand-in-hand and prepare to address incidents like the Log4j vulnerability.  Yeah, let's prepare.



Additionally, the legislation requires the Office for Management and Budget to issue guidance to federal agencies - wow - on the secure usage of open source software and establishes a software security subcommittee on the CISA Cybersecurity Advisory Committee.  So the CISA Cybersecurity Advisory Committee will have a software security subcommittee that is used by the OMB or something.  So good luck.



LEO:  Yeah.



STEVE:  I have a healthy skepticism of bureaucracy and legislators.  It's unclear to me that they will ever get anything right.  But if the federal government wants to hire a bunch of open source software folks, who have been working up till now for free, to help in any way they can, then seems like it could be good.  It could help.



Recall that we talked a couple of weeks ago about the Albanian government's unexpectedly strong reaction to Iran's cyberattack on their infrastructure due to Iran being upset with Albania for providing sanctuary to a group of disaffected Iranians.  That was the MEK group.  Albania closed Iran's embassy and ejected Iran's ambassadors from the country.  We believed, without many facts to back it up, that Iran had been maintaining a presence inside of Albania's government networks for quite some time before the attack.  That meant that when Iran's rulers said "Let 'em have it!" Iran's cyberwarfare people simply had to flip a switch.



Well, now, last week, some new information has come to light.  The CISA and FBI said last Wednesday that hackers connected to Iran's military spent 14 months inside the networks of the Albanian government prior to launching the ransomware attack that caused widespread damage in July.  The FBI did not specify which Iranian hacking group was behind the incident, but explained that in their investigation they found the hackers exploited an Internet-facing Microsoft SharePoint through a well-known and long since repaired vulnerability CVE-2019-0604.



That CVE has been classified by cybersecurity experts as one of the most exploited bugs throughout 2020, having been abused by both nation-states and ransomware groups.  According to the alert, the hackers were able to maintain continuous access to the network for more than a year, frequently stealing emails throughout 2021.  By May of 2022, the actors began moving laterally and examining the network, performing wider credential theft across Albanian government networks.



This all preceded the July cyberattack that crippled the country's government.  The FBI confirmed reports from Reuters and researchers that the attacks were launched due to Albania's involvement with the group known as MEK.  Albania, as we talked about, when we talked about this a couple of weeks ago, has allowed about 3,000 members of the group to settle near Durres, the country's main port.  The agencies said that in July of 2022, the hackers "launched ransomware on the networks, leaving an anti-MEK message on desktops."



So we have a perfect example of, A, why Albania should have updated their instance of SharePoint shortly after patches for the vulnerability were made available; and, B, why having passive intrusion detection present, waiting and watching inside networks, can no longer be considered a luxury.  We know that, try as we might, real-world security is imperfect, and the bad guys only need to find a single imperfection.  And one of those bits of imperfection might take the form of a single well-meaning employee.  So it's most likely that the bad guys will eventually succeed if they are trying hard enough.  Therefore, any truly effective and secure solution must assume that a compromise will occur sooner or later.



That being the case, immediate detection of such an intrusion is every bit as critical as attempting to keep the bad guys out in the first place.  And the entire government of Albania learned a lesson of not having done either of those two things.  They didn't patch SharePoint when it was fixed, and they were completely unaware that they were hosting Iranian intruders in their networks for 14 months.  Talk about an advanced persistent threat.



Okay.  A piece of closing-the-loop feedback from a listener, and someone I know pretty well from the GRC newsgroups.  After hearing last week's discussion of the Uber attack, which was effective even in the presence of multifactor authentication, a well-known contributor to our newsgroups posted his thoughts into the Security Now! newsgroup, which is one of the many that we have at news.grc.com.  His handle in the groups is ferrix, F-E-R-R-I-X, and his real-world name is Greg.  I was aware last week that something didn't feel right about my take on the multifactor authentication attack.  Some people tweeted that it was likely an "MFA fatigue" attack.  And I think that they and Greg are correct.



So here's how Greg, who by the way works in MFA (multifactor authentication) professionally, explained what happened with the Uber contractor.  He wrote:  "When reading about the Uber hack, Steve assumed some details about the MFA that led his discussion slightly astray.  I work in providing MFA services for my day job, so I know a bit more pedantic detail than the average bear.



"If the MFA in question was a time-based one-time-password (OTP) as Steve said, then what he said about brute forcing codes would also have been correct.  An attacker would have in theory brute force logon attempts with codes 000000, 000001, 000002, et cetera, until matching the correct six digits.  It would have been an extremely loud attack since there's a pretty limited time to log in with the current code before it moves out of the window.  But that's not what happened here."



He wrote:  "Uber is using 'push notification' MFA, like what Okta and Duo do.  The user or attacker tries to authenticate to some resource X.  The MFA provider pushes a question to its app on the user's phone, 'Do you want to log into X?' with an Approve button.  The simple theory here is the attacker doesn't have the user's phone, there's no real way to attack the secure channel between the MFA provider and its app, and if the attacker repeatedly tries to log in, the user's phone would blow up with loads of spurious push requests to approve, which is very noticeable.



"The security model breaks either when users are naive, or attackers are clever in particular ways.  A naive user might approve an attacker's push by rote, even without thinking about it.  Or they might see the repeated attack push requests as, 'Well, it looks like something important is trying to run on X, I better approve it,' and thus be tricked.



"A clever attacker would schedule their attacks slowly, and at opportune moments where the real user might be plausibly trying to log into the resource, such as the beginning of the workday or after lunch.  There's a normal," he said, "'background radiation'" - using my term deliberately, he put it in quotes - "of false positive push requests that these complex systems generate, as various things try to sign into other things.  So it's very reasonable that a user might not realize that they're under attack, and they might tap Accept.  This is," he says, "(as far as I can tell), what happened to the Uber external staffer."



He says:  "Now let's talk about the mitigation Uber has turned on to improve their security posture, often called 'Verified Push.'  The resource logon page now shows a short challenge number or word.  The smartphone app now says 'Logging into X? Click the matching challenge,' then shows four or five multiple-choice buttons.  Now it's not possible for the user to blindly approve anymore.  They must select the button that matches the challenge, which they only know if they are actually looking at the X login page.  Else, the attacker would also have to communicate the correct challenge to the user in some out-of-band way, which is a more difficult attack model."



He finishes, saying:  "Orthogonally, please note that the above discussion does not contemplate man-in-the-middle attacks between a real user trying to log in and the resource X they're trying to access.  In that attack, the attacker can await the session to be validated, then steal the session to do their bidding.  To mitigate that threat, the system would need to use a phishing-resistant auth solution such as a properly implemented FIDO2 or, notionally, SQRL."  So Greg, thank you for the clarification.  And I think he's probably exactly right.



LEO:  Yeah, that makes sense, yeah.



STEVE:  Yeah.



LEO:  We actually heard that with earlier Lapsus$ attacks, that they were using this authentication fatigue, yeah.



STEVE:  Yes.  Known as MFA, multifactor authentication, fatigue, where the user just finally says, okay, fine, and lets the bad guy in.



Okay.  Briefly, I'm now on Book 8 of The Silver Ships, and all I can say is that anyone who enjoys science fiction stories has many wonderful, original, and different stories waiting for them.  Each book places our characters, whom we get to know quite well, in different, wonderful, and interesting situations.  As is evident from the fact that I'm already halfway through Book 8, I'm having quite a difficult time putting them down.



And despite the fact that I've admittedly fallen head over heels for this fabulous 24-novel science fiction book series, work on SpinRite is really coming along.  I've been fixing every cosmetic thing that I can find for a while now, and I have one last known cosmetic thing to fix.  It involves the mapping of SpinRite's now huge 16MB data transfer blocks to one of SpinRite's screens, its so-called Graphic Status Display, which is a grid that represents the aerial storage of the media being tested.  Originally, a single transfer block might have occupied more than one character cell in the Graphic Status Display.  I remember that floppy disks would do that.  So that used to be the case.



I removed the logic from managing that cross-cell mapping because it simplified and sped up SpinRite's inner loop.  And nothing matters more than speed.  And since drives were so huge these days, there's no way that the number of data transfer blocks on the drive would not be way more than the number of cells in the Graphic Status Display.  So I removed the logic for that, and sped things up.  But that also meant that for the first time, on very small drives, since SpinRite's new data transfer blocks are so large, and I'll be allocating a minimum of one transfer block per text cell on the Graphic Status Display, not all of the GSD screen might be used on very small drives.  I've run across this because I've got a 100MB virtual drive in VirtualBox where I do some of the testing.  And it went, whoops, you know, visually.  Anyway, so I'm fixing the case where that might happen.  And I expect I'll finish that work tonight.



At that point, I won't know that SpinRite is not finished.  But neither will I know that it is.  So the last thing I will do before I turn the GRC newsgroups gang loose on SpinRite's first fully functional alpha release, will be to simulate various forms of actual data corruption, then carefully watch it always do the right thing, putting individually recovered sectors back into the right place.  If anything there doesn't work, I'll fix it.  Then SpinRite will be ready for the group's final external testing.  And Leo, I'm ready for some final external water.



LEO:  Ah.  This can be arranged.  We have people who will bring you dihydrous oxide in a special container designed to retain all the CO2 goodness.  All right.  Now it's time to talk about the subject of the day, Mr. Gibson.



STEVE:  So I brought us all some bad news.



LEO:  Oh, no.



STEVE:  A couple weeks ago with the rise of Phishing-as-a-Service.  But, you know, forewarned is forearmed.  Right?  I've got some more bad news for us this week.



LEO:  Oh, no.



STEVE:  One of the definitions of the word "politics" is "the debate or conflict among individuals or parties having or hoping to achieve power."  It is in that sense of "politics" that I titled today's podcast "DarkNet Politics."



Last week's leak of today's preeminent LockBit 3.0 ransomware led to some very interesting discussion and conjecture by the industry's ransomware-watching security researchers.  After the fall of Conti, which we covered, remember all that crazy Costa Rica government nonsense?



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  LockBit 3.0 has risen to become the number one ransomware group in the ransomware industry.  And I hate using that term, but there it is, "industry."  And they've been making something of a splash in the underground.  They recently offered, get a load of this, $1,000 to anyone who would permanently tattoo their group's logo on their body.



LEO:  Oh, no.



STEVE:  They had a number of takers.



LEO:  Oh, no.



STEVE:  And I saw a photo of one.  Until they terminated the offer.



LEO:  Yeah?



STEVE:  Well, and Leo, we know where you have a tattoo of a logo.



LEO:  Yes, of the logo of a company near and dear, yes.



STEVE:  So today, the group's operations are so extensive that LockBit's victim count some weeks has been greater than all the other ransomware families combined.  Ever since Conti's leaks, which marked the beginning of Conti's end, and the curious wind-down involving the Costa Rican government that we covered, LockBit has taken over the ransomware throne.  Although business - and again, if you can call it that, I hate doing so.



Although business has been booming for the LockBit 3.0 group, things have recently shaken up a bit by a little known threat actor who claims that his group was able to compromise LockBit's servers to obtain and leak the builder and keygen modules  essentially all of the heart of the group's code.  It seems that within this odd underworld it's not possible to get too big or someone, a rival or an insider, will take you down.  Since this is somewhat reminiscent of the leaks which occurred and triggered Conti's downfall, it raises the question whether this may be the beginning of the end also for Lockbit, as well.  We'll see.



Okay.  So a threat actor going by the name Ali Quashji, which was a Twitter account with no reputation which was apparently created just to host this leak declaration announcement, claims to have hacked several of LockBit's servers and was able to obtain the LockBit 3.0 builder and the keys generator.  Researchers at Cyberint grabbed and analyzed the leaked code and declared it to be real and complete.  They said:  "Looking at the published files, we could find the builder and key generator modules.  The first of them build several executables that perform the encryption and loading phases of LockBit's ransomware attack flow, along with ransom note creation."



Well, The Record, which is a publication of Recorded Future, often does a great job when things like this happen of pulling things together and polling security researchers.  In this instance I thought that some of what they reported was really quite interesting.  So in what follows, I've merged some of The Record's reporting with my own interpretation and commentary.



The leak of the LockBit 3.0 ransomware encryptor was announced on Wednesday by security researcher - now, we would pronounce his name Export, his handle Export, but it's numeral 3, xp, numeral 0, rt.  So, you know, in LEET speak.  Anyway, 3xp0rt announced this.  Several experts and researchers confirmed to The Record that the builder works and allows anyone to create their own ransomware.  There's the phrase of the week:  "allows anyone to create their own ransomware."  In a message shared by 3xp0rt, someone allegedly connected to LockBit addressed the issue, attributing the leak to a disgruntled affiliate and dismissing the idea that what was stolen could be used by others to replicate what the ransomware group does.  Of course, you know, he would hope that's true.



So this LockBit representative was quoted:  "An affiliate program is not a locker.  It is a software package, and most importantly an impeccable reputation" - oh, give me a break - "that no one" - what is this guy smoking? - "that no one can tarnish."



LEO:  Oh, my, yes.



STEVE:  "No matter what software leaks occur.  Few people will agree to pay randomly to a pen tester without a reputation, hoping for a successful decryption and deletion of stolen data."  Now, as I said, I don't know what this LockBit representative has been smoking, but no one ever wants to pay anything to any criminal who has breached their network.



LEO:  Only the most trustworthy criminals.



STEVE:  That's right.  Only the criminals with a great reputation.  Wait, what?  And the reputations of underground criminals has little bearing on whether they get paid.  They get paid if there's no alternative, period.



LEO:  Yeah, yeah.



STEVE:  Okay.  But The Record's reporting noted that several cybersecurity experts expressed significant concern about that very prospect.  Emsisoft's threat analyst Brett Callow compared the situation to last year's leak of the builder for the Babuk Locker ransomware.  Brett said:  "As was the case when Babuk's builder leaked, we may well see other threat actors use LockBit's, which would obviously complicate attribution."  Adding to what Brett said, Huntress Senior Security Researcher John Hammond said less skilled adversaries gravitated to the Babuk ransomware tool because it was simple to customize and use.  Unfortunately, it wasn't the same quality as this one.



And Recorded Future's own ransom expert Allan Liska said his team has identified more than 150 "new" ransomware groups just this year.  Most of them are using stolen Conti or REvil code.  Allan said:  "At this time last year, Recorded Future was collecting from about 45 active DLSes."  That's short for Dedicated Leak Sites.  "Today, that's more than 100."  He said there is a real proliferation of ransomware groups, most using leaked/stolen code from other ransomware groups.  This is the same reason why the emergence of PHAAS  Phishing-as-a-Service  as I was talking about - is so disturbing.  The emergence of turnkey services allows those who are not skilled enough to assemble the required infrastructure to no longer need to.  It's been done for them in return for a piece of their action.



Dick O'Brien, the principal intelligence analyst for Symantec's Threat Hunter Team, said it's a "near certainty" that we will see other attackers reuse LockBit's source code.  According to O'Brien, LockBit's success is partly due to the fact that it has a very effective malware "payload."  Dick said that "Other ransomware operators could replace their payloads with rebranded variants of Lockbit, and you could see some aspirant groups use this to launch their own ransomware operations."  Excuse me for a second.



LEO:  I've run out of ads, so...



STEVE:  Yeah.  I think we can ask our editor to remove that.



LEO:  These guys are so grandiose, it's so amazing.



STEVE:  I know.  I know.  No one is going to use our stolen stuff because they're not us.  And only we can bless this with our reputation.  It's like, again, what?



LEO:  Yeah, they're awful.



STEVE:  Anyway, researchers have linked more than 1,029 attacks to LockBit since the group began its operation in 2019.  The group was considered a marginal player until just last year when it launched LockBit 2.0, a new version of its initial Ransomware-as-a-Service platform.  The group revamped again, launching LockBit 3.0 this past summer and quickly supplanted Conti as the most prolific criminal organization.  The gang had at least 68 victims just last month, 68 victims in August, so more than two a day on average, including a crippling attack on a hospital about an hour southeast of Paris that disrupted its medical imaging, patient admissions, and other services. You know, as we've seen.



The cybersecurity firm Dragos attributes about one-third of ransomware attacks targeting industrial systems in the second quarter of this year to LockBit, and Huntress Labs' John Hammond explained that the latest edition of LockBit had new features and functionality to encrypt files faster than ever before.  He said the leak of the builder software commoditizes the ability to configure, customize, and ultimately generate the executables to both encrypt and decrypt files.  I have, just for side interest, a slide showing the relative distribution by number of attacks of ransomware.  And LockBit is out in first place, with Conti in second, and then there's like a drop by two thirds for the rest of them that are also-rans that we've talked about from time to time. 



In his discussion with The Record, indicating a screenshot of the leaked configuration file, Hammond said:  "Anyone with this utility can start a full-fledged ransomware operation.  That is so customizable,"  he said.  "Note how the ransom note can be completely changed."



One small upside of the leak may be that security experts now have it, too.  So they're able to analyze and explore this builder software and potentially garner new threat intelligence that could thwart ransomware operations.  At a minimum, the leak gives cybersecurity experts greater insight into the inner-workings of LockBit, with the message from LockBit indicating that they have contracted developers and that they suffer, as well, from insider threats.



LEO:  Aww.



STEVE:  Oh, I know.  Poor babies.  Poor babies.  Recorded Future's Allan Liska said the leak could be a sign of disgruntled factions within the LockBit group.  He said:  "The large RaaS groups" - Ransomware-as-a-Service groups - "are notorious for paying their developers, IABs" - those are the initial access brokers, remember, who find the way in and sell their access - "and other support staff very poorly."  So it's not necessarily a surprise when someone retaliates.



John told The Record that after the Conti Leaks were made freely available, the Conti ransomware builder "gained mass adoption from other threat actor groups wanting to quickly and easily spin up their own ransomware operations.  Money is the real motive.  And when a tool like this is made available," he said, "it enables anyone to run the racket."



One thing to note is that, though it is customizable, the encryptor still changes the victim wallpaper to say "Lockbit Black," and that cannot be easily changed.  More skilled operators may attempt to change that.  Or lower-tier and less capable groups may prefer to have the legitimacy of looking like a LockBit attack.  The bottom line on all this, driven by the promise of easy money, what was once a somewhat blessedly high-level and high-end form of devastating attack is rapidly moving down into and becoming a commodity available to far less capable criminals to use.



And again, in retrospect, this was inevitable.  The lack of true bulletproof enterprise cybersecurity, which enables an environment of porous security, with the emergence of cryptocurrency, which solved the extortion payment problem, together has made massively profitable cyber extortion feasible like never before.  And now the last tools required to make the perpetration of these cybercrimes trivial are falling into the hands of the script kiddies.  God help us.



LEO:  Yeah.  Well, you know, you can hope that they eat each other alive, you know, that they just, you know, just keep fighting and infighting and all that stuff.  But good lord.



STEVE:  Yeah.  And we did see that the sanctions against Russia are what killed Conti because they aligned themselves powerfully with the Russian government and with Russia.



LEO:  Right.



STEVE:  And no one was able or willing to pay them because they were Russian, and they were now sanctioned. 



LEO:  Yeah.  What a story.  Golly.  Golly.  It's an interesting world we live in, and this show shows us in many ways how more and more interesting it gets with all these...



STEVE:  And Leo, what's so sad is think of all the resources being expended to fight this.



LEO:  Oh, I know.  Oh, I know.  Yeah.  Absolutely.



STEVE:  I mean, it's guaranteed employment for anybody who's interested in cybersecurity, that's for sure.



LEO:  Absolutely, yeah.  All right.  We do this every Tuesday, and if you're not completely dejected, I hope you'll come back again and do it again with us.  Tuesdays, 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC.  Right after MacBreak Weekly.  You can watch us do it at live.twit.tv.



Or after the fact on a podcast, Steve's got copies.  Actually, he has two unique copies at his website, GRC.com.  He's got the 16Kb audio for the bandwidth-impaired, and he has the transcriptions written by Elaine Farris, so you can read as you listen or use them to search or just read them standalone, get all the content that way.  He also has 64Kb audio.  We do, too, at TWiT.tv/sn, or on YouTube there's a dedicated channel.  Or you can subscribe in your favorite podcast player.



Let's see.  I guess that means it's time to adjourn this session.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#891

DATE:		October 4, 2022

TITLE:		Poisoning Akamai

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-891.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine a puzzlingly insecure implementation by Microsoft in Teams' design and at their complete rewrite of Microsoft Defender SmartScreen.  Roskomnadzor strikes again, and Exchange Server is again under serious attack with a new zero-day.  Cloudflare introduces Turnstile, their free CAPTCHA improvement.  Google publishes a fabulously engaging six-video YouTube series under the banner "Hacking Google."  We'll then spend some time sharing and replying to listener feedback before we examine a breathtaking flaw that was discovered in Akamai's global CDN caching, and what became of it.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Another flaw in Teams.  You might want to know about this before you let somebody use your computer.  We'll talk about that Exchange Server zero-day.  They're at it again.  A brand new way to do CAPTCHAs, thanks to Cloudflare.  I think we'll like this one a little bit better.  And then an Akamai flaw that they didn't want to pay security researchers to fix.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 891, recorded Tuesday, October 4th, 2022:  Poisoning Akamai.



It's time for Security Now!.  I know you've been waiting all week long.  Mr. Gibson is here.  He is prepared.  He is ready to talk about the world of security.  Steve Gibson from @SGgrc fame.



STEVE GIBSON:  Is that you looking at the screen, so I can, like, see through the magic mirror?



LEO:  I look back at you.  It's an affectation that performers often use.  I'm actually not even looking at you at all.	



STEVE:  Our audio listeners are like, what are those two clowns talking about?



LEO:  In order to look at you, I'd have to turn my back on our audience.  And I don't want to do that.



STEVE:  Oh, that's not good.



LEO:  So what I do is I look across to the left as if...



STEVE:  Didn't you study drama at Yale or something?



LEO:  Yeah, that's where I learned this.  Eye line, yeah.  So it kind of looks like I'm looking at you, and in fact I'm not.  But you're to the - it's complicated.



STEVE:  I'm in a computer, so that works.



LEO:  What's up, Steve?



STEVE:  So we've got the first podcast for October, the fourth quarter of 2022, titled "Poisoning Akamai."



LEO:  Oh, boy.



STEVE:  And, oh, it's really fun.  Not only what two youngsters figured out, and the fact that this all happened without many people knowing about it earlier this year, but what the consequences of this could have been, and also what happened after they went to Akamai to explain.  So really interesting.  But we're going to first examine a puzzlingly insecure implementation by Microsoft in Teams design, and also look at Microsoft's complete rewrite of Microsoft Defender SmartScreen.  And we'll talk about that.  Also Roskomnadzor strikes again.



LEO:  Ooh.



STEVE:  Yes.  It's October, so that's good.



LEO:  Spooky season.



STEVE:  Yes.  And Exchange Server is again under serious attack with a new zero-day which Microsoft knows about, hasn't fixed yet, and their suggestion mitigations have been worked around.  So we're hoping for an actual fix soon because it's a nasty one.



Cloudflare has introduced Turnstile, which is their free and much improved CAPTCHA which they're offering, even to non-Cloudflare users.  We've got to talk about that.  Also Google just yesterday published a fabulously engaging six-video YouTube series under the banner "Hacking Google."  So I have that to talk about.  And that's the tweet that you saw that I sent out.  I was so fascinated by that I forgot to tell everybody about the podcast.  Anyway, I'll get around to that when you tell us about our sponsor here in a minute.  Then we're going to spend some time sharing and replying to some listener feedback before we examine, as I said, a breathtaking flaw that was discovered in Akamai's global content delivery network caching system, and what became of it.  And of course we've got a Picture of the Week that's...



LEO:  That's bizarre.  Weird.  It's wacky.  Steve, I didn't have to go to Zip Recruiter to find you.  I had to have a ZIP drive to find you; right?



STEVE:  That's exactly right.



LEO:  Click of death, way back in 1999 or something like that.



STEVE:  Oh, yeah.



LEO:  We didn't know about Pictures of the Week back then.  We should have probably had those on The Screen Savers.  These are good.  I like this feature.



STEVE:  They were fun.



LEO:  Yeah.



STEVE:  So this one is not really anything to do with security, but it's just so fun.  So it falls under the broad banner, one of my favorite banners, "What could possibly go wrong?"



LEO:  What could possibly go wrong?  I like to do it with you.



STEVE:  Okay.  We'll do it from now on.  So this posting says:  "I have a 5,000-gallon aboveground pool in my basement.  It feels nice down there, but the water is freezing.  I have a tiny ass pump on it right now that kind of flows water, but I'm wanting to heat the whole pool to a reasonable temperature."  And then we see a picture of this monstrosity.  So we're looking at a basement with apparently a door open that is where the stair is leading down into the basement end, and this huge pool that looks like it's about to burst.  But I think that's probably the way they're supposed to look because you're going to have an awful lot of water pressure pushing the bottom skirt of the pool outwards.  And it's being, like, kept from expanding endlessly by a collar at the top.  But boy, you know, this is 5,000 gallons of water that only has one - it's like it's trying to leave the pool as hard as it can.



LEO:  I have to go to WolframAlpha for this one.  What is the water pressure, 5,000 gallons of water stored in - what would you say the area of that?  Hundred square feet?



STEVE:  Maybe 50 square feet?



LEO:  Fifty, okay.



STEVE:  A hundred square feet, yeah, maybe.



LEO:  We'll be generous, yeah.



STEVE:  Anyway, so he says, he gives us the picture, marking it "for reference."  And he says:  "I want something cheap that won't melt the pool as it is rubber."  He says, and then I love this:  "I know I'm not the only person out there with a pool in their basement."  And I'm thinking, maybe you are.



LEO:  I'm thinking that pool for sure.



STEVE:  Oh.  I don't know.  Anyway, just a great Picture of the Week.



LEO: According to WolframAlpha, 5,000 gallons of water stored in a 100-square-foot whatever, generates 5.066 x 10^10 square feet, foot square gallon pascals.  Don't know what any of that means.  It's bad.



STEVE:  How about millimeters of mercury?  That would be a pressure measurement that might be...



LEO:  Yeah, kilogram meters to the fourth per second squared is another unit.



STEVE:  Ah, no.  No.



LEO:  I'll have to convert that.  I'll get back to you.



STEVE:  Yeah.  Elon Musk uses those measurements for his SpaceX program.



LEO:  Yeah, yeah.  I'm not a rocket scientist.



STEVE:  Anyway, just another fun one to share.



LEO:  That's hysterical.  I hope he found something to heat that pool without melting the rubber.  



STEVE:  Yeah.



LEO:  We were thinking maybe a bitcoin mining device down there may be helpful.  Yeah?



STEVE:  Do they have submersible bitcoin miners? 



LEO:  No.  Oh, I know what you need.  You need a sous vide circulator in there.  You could sous vide yourself.



STEVE:  Maybe you could just heat the environment, and of course the water would eventually warm up to the ambient temperature.



LEO:  Well, the reason it's cold is it's sitting on bare dirt.



STEVE:  Yeah.



LEO:  That's going to get cold.  Anyway...



STEVE:  Yeah.  Looks creepy.



LEO:  The creepiest would be just seeing him get in it.  That's what I...



STEVE:  I hope the whole family knows how to swim because they'd be needing that.



LEO:  Oh, my god.  Yes.



STEVE:  Okay.  So three weeks ago the security firm Vectra published a report which closely examined the way Microsoft Teams manages its users' application authentication.  Their report is long, and we don't need to get into the nitty-gritty to understand what's going on.  So I'm just going to share two small pieces from their long report.



In their overview they explain:  "In August 2022, the Vectra Protect team identified a post-exploitation opportunity allowing malicious actors with sufficient local or remote file system access to steal valid user credentials from Microsoft Teams due to their plaintext storage on disk."  In other words, Microsoft Teams, after you have authenticated yourself, statically stores the authentication information, the tokens, in the local file system, there for everyone to access.  So they said:  "This plaintext credential management was determined to impact all commercial and GCC Desktop Teams clients for Windows, Mac, and Linux."  So common to all desktop platforms, the big three.



They said:  "While credential harvesting from memory" - you know, RAM - "is a common post-exploitation step, we believe that lowering the bar necessary to harvest creds down to just simple read access to the file system expands opportunities for an adversary, simplifies their task, and is particularly interesting when stolen credentials offer an opportunity to retain user access unencumbered by otherwise pesky" - one of my favorite words - "Multi-Factor Authentication speed bumps.  With these tokens, attackers can assume the token holder's identity for any actions possible through the Microsoft Teams client, including using that token for accessing Microsoft Graph API functions from an attacker's system.



"Additionally, these tokens are equally valid with MFA-enabled accounts, creating an allowance to bypass MFA checks during ongoing use.  Microsoft is aware of this issue, but indicated it did not meet their bar for immediate servicing.  Microsoft stores these credentials to create a seamless single sign-on experience within the desktop application.  However, the implementation of these security choices lowers the bar.



"Anyone who installs and uses the Microsoft Teams client in this state is storing the credentials needed to perform any action possible through the Teams UI, even when Teams is shut down.  When these tokens are stolen, it enables attackers to modify SharePoint files, Outlook mail and calendars, and Teams chat files.  Attackers can tamper with legitimate communications within an organization by selectively destroying, exfiltrating, or engaging in targeted phishing attacks.



"The thing that truly frightens us," they said, "is the proliferation of post-MFA user tokens across an environment.  It enables subsequent attacks that do not require additional special permissions or advanced malware to get away with major internal damage.  With enough compromised machines, attackers can orchestrate communications within an organization.  Assuming full control of critical seats - like a company's Head of Engineering, CEO, or CFO - attackers can convince users to perform tasks damaging to the organization."  They say:  "How do you practice phish testing for this?"



Okay.  So this is one of those "head buried deeply in the sand" issues which we've increasingly been encountering from Microsoft.  The kindest way, I think, to interpret this in Microsoft's favor is to suggest that Microsoft has now structured itself so that it's deliberately cut off from the outside.  Someone who has no authority or power is running interference and responds to any offering made at the foot of the Ivory Tower by incanting the phrase:  "We are aware of this, and it is not a security concern."  And that's as far as any inquiry goes.  If history is to repeat itself, especially now that this problem is well known, there will eventually be some egregious abuse of what is obviously a totally unnecessary and easily exploitable security weakness in Teams.  At that point, a wholly unnecessary emergency will ensue, and Teams will have this behavior-by-design changed.



It's totally true that having persistent and static access to a previous successful authentication creates a standing vulnerability.  Perhaps that's been done so that other components such as Skype and Outlook are able to share in this authentication, as indeed they are.  But in an alternative design, for example, they could share a common authentication service which then relies upon encrypted authentication tokens which would at least tie the tokens to the local machine's authentication service, perhaps its TPM, and would make them much more tricky to abuse.  Instead, Microsoft has chosen for whatever reason to simply store them in a well-known location in every local machine's file system, where they're accessible, not only to all Microsoft components, but also to anyone else who might wish to abuse this implied trust.



LEO:  You remember, I mean, we talked about the same thing with Google and passwords in Chrome stored in the clear.  Google's response initially was, well, if somebody has physical access to your system, you're out of luck anyway.



STEVE:  All bets are off.



LEO:  And maybe that's Microsoft's response; although remember Google did change that eventually.  Right?



STEVE:  Right.



LEO:  Yeah.



STEVE:  So, okay.  But Vectra also had - I was going to share two things.  Vectra also had another interesting piece of background to share.  They gave this section the clever title "Electron - a Security Negative."  It was clever, of course, because we've all agreed that electrons carry a negative charge.  Vectra is suggesting that the Electron development platform carries "negative security."  So here's what they explained about Microsoft Teams' use of the Electron application platform.



They said:  "Microsoft Teams is an Electron-based app.  Electron works by creating a web application that runs through a customized browser.  This is very convenient and makes development quick and easy.  However, running a web browser within the context of an application requires traditional browser data like cookies, session strings, and logs.  This is where the root of this issue lies, as Electron does not support standard browser controls like encryption, and system-protected file locations that are not supported by Electron out of the box, but must be managed effectively to remain secure.



"Therefore, by default, the way Electron works incentivizes creating overly transparent applications.  Since Electron obfuscates the complexities of creating the application, it is safe to assume that some developers may be unaware of the ramifications of their design decisions, and it is common to hear application security researchers bemoan the use of this framework due to critical security oversights."



So phrased another way, we might say that Microsoft Teams' choice to use the easy-to-use Electron development model, which employs JavaScript, HTML, and CSS, encourages rapid and easy application development by less experienced developers.  From what Vectra said, it also sounds as though electron's browser-centric development environment encounters significant resistance when trying to do things like storing encrypted data into the file system.  If that's true, then we have another case of Microsoft placing its short-term needs in front of long-term security and quality.



So, you know, let's use Electron.  We'll get platform-agnostic operation, and everything will be great.  Except oops, for whatever reason they just decided, I think exactly as you suggested, Leo, they've said something to the equivalent of, well, if someone's going to access your local machine, you're in trouble anyway.  So not our problem.  Yeah.  Until it is.  And I don't think we're going to have long to wait for this one.



In a posting last Thursday the 29th, titled "More reliable web defense," Microsoft explained that they had scrapped and entirely rewritten their Edge browser's built-in SmartScreen library.  They said:  "Starting in Microsoft Edge 103" - which, by the way, is three versions ago - "users can navigate the Internet with more reliable web defense thanks to the updated Microsoft Defender SmartScreen library that ships with Microsoft Edge on Windows.  The updated SmartScreen library was completely rewritten to improve reliability, performance, and cross-platform portability.  These benefits are the foundation leading up to the security improvements that will increase our ability to protect users from emerging threats."



And at the end they noted that "For enterprise customers who experience compatibility issues and need to revert to the legacy Microsoft Defender SmartScreen, we added a temporary policy called 'NewSmartScreenLibraryEnabled.'"  They finished:  "This policy will become obsolete in Microsoft Edge 108." 



Okay.  So it's unclear why anyone would have compatibility issues unless something they were doing was tripping SmartScreen false-positive responses.  But since we're currently at release 106, and the option to revert will only remain available in the next release 107, any enterprise having trouble with the rewrite should address those problems quickly because, unless Microsoft is convinced to delay the removal of that temporary policy, and we've seen that happen before, so that could happen again if enterprises say, wait, wait, wait, we're not ready yet.  Again, all enterprises that have needed to disable the update should look into fixing that fast.



And of course I'm a huge proponent of wholesale rewrites of anything.  As we know, we haven't yet figured out how to evolve software gracefully.  Part of the problem is that the various challenges software might face once released into the field are often not fully appreciated by those who initially design and write it.  So the process of watching an initial release interact with the world teaches its designers a lot.  The first reaction is to patch over any shortfalls in the original design.  But once those patches' patches have acquired patches, it's often the case that the best solution is to quit patching the patches and take everything that is now understood about the problem and start over.  So bravo to Microsoft for deciding to do exactly that, to start over.  We'll never know what precipitated that decision, but Edge's users will likely be the winners.



Okay.  I never pass up the opportunity to mention Roskomnadzor because it's just too fun.



LEO:  I love how you do it, too.  You, like, really give it the emphasis, yeah.



STEVE:  Oh.  And, you know, I don't speak Russian, but part of this makes me wish I did because it sounds like some of it is fun.  Anyway, an opportunity presented itself when Roskomnadzor added the popular music streaming platform SoundCloud to Russia's nationwide Internet block list.  And Leo, I have a link here, in the story here, at the bottom of page 3 of the show notes.  It's worth clicking that link just to glaze over.  As I said, I've got the URL to Roskomnadzor's block list page.  And thank goodness for the web's Western heritage, since at least the URL uses the Latin alphabet.  I'm unable to make heads nor tails of anything on that block list page following the URL.



LEO:  At least the numbers are Roman, and the CAPTCHA.



STEVE:  You think?  My CAPTCHA, the one I got was - I don't think I could have entered it properly.



LEO:  Oh, okay, well, they need to put some bicycles in there or something.  All right.



STEVE:  That's right.  Give me some stop signs and...



LEO:  Now, what do I do?  You know, this is where Google translate would be handy.  All right.  I'll figure it out.



STEVE:  Anyway, I just wanted to see that Russian is a weird language.  So, and I don't know why Roskomnadzor is, you know, I guess maybe that's the English translation.



LEO:  They anglicized it.



STEVE:  Yes, thank you.



LEO:  Romanized it.



STEVE:  So the presumption is that the blockage was the result of SoundCloud's hosting of podcasts which were discussing Russia's invasion of Ukraine.  Of course, you can't have any of that in a repressive totalitarian regime, so no more SoundCloud.  Add yourself to the lineup.



LEO:  Will Translate to the rescue.



STEVE:  Ah.  Universal service, it says.



LEO:  Yeah.  So now I can - now that I've unlocked it - have I  unlocked it?  Let's unlock it.



STEVE:  I wonder why there's a caption...



LEO:  I can specify a domain.



STEVE:  Why do you think there's a CAPTCHA on their block list?



LEO:  Oh, they don't want you to use robots; right?



STEVE:  You wouldn't, but yeah.



LEO:  So I can obtain the measures taken to restrict access to sites.  So what was the site that they inadvertently...



STEVE:  SoundCloud.com.  I assume it's dot com.



LEO:  SoundCloud.com, yes, it is.  Forbidden.  You are forbidden.  Roskomnadzor says no.  No, no, no.  You may not ever look at this secret.  Well, it's going to...



STEVE:  What'll happen if you put Noodles.com in?  Is that forbidden, too?



LEO:  I think it's more - I don't know what it has to do with.



STEVE:  Maybe you click the button, and it just says Forbidden.  Oh, we've got to do another CAPTCHA?



LEO:  Yeah, I think for every one.  Noodles.com?



STEVE:  Yeah.



LEO:  Okay.  Now I have to fill in CAPTCHA.  537863.  Very secret.  Okay.  Forbidden.



STEVE:  Ah, well.  There you go, Leo.



LEO:  Noodles.com is forbidden.  Not for you.



STEVE:  You cannot go.  You cannot have any noodles in Russia.



LEO:  No, is forbidden.



STEVE:  No.  Roskomnadzor forbidden.  Okay.  So Exchange Server once again under attack.  A Vietnamese group named GTSC discovered an active in-the-wild Exchange Server zero-day.  And let me just preface all of this by saying this is very bad.  I mean, this is - okay.  Here's what they said:  "At the beginning of August 2022, while doing security monitoring and incident response services, GTSC SOC" - that's their Security Operations Center - "team discovered that a critical infrastructure was being attacked, specifically to their Microsoft Exchange application.  During the investigation, GTSC Blue Team experts, determining that the attack utilized" - and this is a translation from Vietnamese, so bear with, but it's pretty good - "determining that the attack utilized an unpublished Exchange Server vulnerability, i.e., a zero-day vulnerability, immediately came up with a temporary containment plan.



"At the same time, Red Team experts started researching and debugging Exchange decompiled code to find the vulnerability and exploit code.  Thanks to experience finding the previous one-day Exchange exploit, the Red Team has a great understanding of Exchange's code flows and processing mechanisms.  Therefore research time was reduced, and the vulnerability was uncovered quickly.  The vulnerability turns out to be so critical that it allows the attacker to do a remote code execution on the compromised system.  GTSC submitted the vulnerability to the Zero Day Initiative (ZDI) right away to work with Microsoft so that a patch could be prepared as soon as possible. ZDI verified and acknowledged 2 bugs, whose CVSS scores are 8.8 and 6.3 respectively."



Again, the beginning of August 2022.  We're now two months downstream, right, August, September, we're in October.  This has not actually been fixed yet.  Since this was reported, GTSC said that they've encountered other customers who they're also charged with monitoring, whose infrastructures they're keeping an eye on, also experiencing a similar trouble.  And after careful testing, they confirmed that those other systems were being attacked using the same zero-day vulnerability.  To help the Exchange Server community temporarily stop the attack until an official patch is available from Microsoft, they published their coverage of their findings while responsibly excluding the information needed to recreate the attack.



The exploits caused Exchange Server to download a malicious DLL whose code is then injected into the always present and busy svchost.exe process in Windows.  Once that's done, the DLL is started, and it phones home to the machine at the IP address 137.184.67.33.  I imagine that changes from instance to instance, you know, DLL instance to DLL.  A simple and effective RC4 cipher is used with a key chosen at runtime to then encrypt the communications back to the command-and-control server.



The GTSC folks also explained.  They said:  "While providing SOC service to a customer, GTSC Blue Team detected exploit requests in IIS logs with the same format as the ProxyShell vulnerability."  And in fact this is now - I just saw this.  I wanted to check just before the podcast to make sure that I hadn't missed any updates, like Microsoft had just patched or just pushed an update.  But there's still nothing from Microsoft.  This is being called informally ProxyNotShell vulnerability, instead of the ProxyShell Vulnerability, because it is definitely a variation on that theme.



In the show notes, for what it's worth, I share an example of the URL that is in there.  They said:  "Checking other logs, we saw that the attacker can execute commands on the attack system.  The version number of these Exchange servers showed that they were already running with the latest update, so an exploitation using the actual ProxyShell vulnerability was impossible.  Thus the Blue Team analysts confirmed that this was a new zero-day remote code execution vulnerability under active exploitation."



So I've got links in the show notes.  There are indications of compromise published and available, if anyone wants to make sure their Exchange Server instance hasn't already been made a victim.  And there are mitigation steps that can be taken.  Last Thursday Microsoft publicly acknowledged the trouble with their own posting titled "Customer Guidance for Reported Zero-Day Vulnerabilities in Microsoft Exchange Server."  I've got a link in the show notes.



The trouble is, the mitigation which was first proposed by the GTSC people appears to be what Microsoft has copied and is echoing.  It recommends basically adding a pattern-matching "Rewrite" rule to the IIS web server which hosts Exchange.  But in an update from GTSC just yesterday, October 3rd, they noted, and they said:  "After receiving information from Jang" - whose Twitter handle is @testanull - "we noticed that the regex used in the Rewrite rule could be bypassed."  And they then link to a YouTube demo.



So I read that to mean that Microsoft's official proposed mitigation can be bypassed.  And in fact there is an article just posted from Bleeping Computer confirming that Microsoft's mitigation of this zero-day remote code execution vulnerability for Exchange Server can be bypassed.  So things are really not good.  Let's hope that Microsoft gets a permanent fix for this problem published soon, and that all Exchange Server users jump on getting their systems updated.  I don't know of anything else that we can do in the meantime.



So last Wednesday our friends at Cloudflare posted the formal announcement of the availability of some of the work they've been focused upon this past year.  This has been some time in coming.  They've been working on it, and it's now available.  Their posting was "Announcing Turnstile, a user-friendly, privacy-preserving alternative to CAPTCHA."



So our long-time listeners know that through the years we first introduced the abbreviation CAPTCHA, standing for Completely Automated Public Turing test to tell Computers and Humans Apart.



LEO:  I think it was Carnegie Mellon that did the first CAPTCHA.  Right?  And it was kind of clever.  And now Google does it, and it's so unclever.  I'm so tired of telling Google that's a bike; that's not a bike. 



STEVE:  I know.



LEO:  Clearly we're training their Waymo Division on how to drive.



STEVE:  And I'm often sure that it's correct, and Google says no.



LEO:  No.  It's terrible.



STEVE:  Here are some boats or whatever.



LEO:  Horrible.



STEVE:  Yeah.  So...



LEO:  And as you've said before, by the way, ineffective.



STEVE:  Yes.



LEO:  If you're really a bad guy, you know how to defeat these CAPTCHAs easily.



STEVE:  Well, also, I'm sitting here as a normal user doing web-based research with an IP that hasn't changed in several years.



LEO:  Yeah, they know who you are.



STEVE:  And Google is supposed to know, yes, exactly.  And so the idea is that it's supposed to not bother you unless it's not sure about whether or not.  And it's like, what are you doing?  Am I your free image analyzer?



LEO:  Yes.  That's exactly.  It's so obvious.



STEVE:  Yeah.



LEO:  Sorry.



STEVE:  So thank you.  So we've followed the evolution and use of CAPTCHAs.  So in keeping with our CAPTCHA covering history, here's what Cloudflare has done.  And it looks like to be 100% good news.  They said:  "Today we're announcing the open beta of Turnstile, an invisible alternative to CAPTCHA.  Anyone anywhere on the Internet who wants to replace CAPTCHA on their site will be able to call a simple API, without having to be a Cloudflare customer or sending traffic through the Cloudflare global network.  Sign up here for free."  And the URL, it's easy, www.cloudflare.com/lp/turnstile.  That's it.  You fill out a form in order to create an identity with them, and you make three modifications, which I'll explain in a second.



They said:  "There is no point in rehashing the fact that CAPTCHA provides a terrible user experience."



LEO:  Yes.



STEVE:  "It's been discussed in detail before on this blog, and countless times elsewhere.  The creator of the CAPTCHA has even publicly lamented that he 'unwittingly created a system that was frittering away, in 10-second increments, millions of hours of a most precious resource:  human brain cycles.'"



LEO:  Yes.  Yes.



STEVE:  He said:  "We hate it, you hate it, everyone hates it.  Today we're giving everyone a better option."  And of course this comes from our friends at Cloudflare.



LEO:  Yeah.



STEVE:  They said:  "Turnstile is our smart CAPTCHA alternative.  It automatically chooses from a rotating suite of non-intrusive browser challenges based on telemetry and client behavior exhibited during a session."  They said:  "We talked in an earlier post about how we've used our Managed Challenge system to reduce our use of CAPTCHA by 91%.  Now anyone can take advantage of this same technology to stop using CAPTCHA on their own site."



They then go on to explain that it's not only CAPTCHA's miserable user experience that is the problem.  They said:  "While having to solve a CAPTCHA is a frustrating user experience, there is also a potential hidden tradeoff a website must make when using CAPTCHA.  If you are a small site using CAPTCHA today, you essentially have one option, an 800-pound gorilla with 98% of the CAPTCHA market share.  This tool is free to use, but in fact it has a privacy cost.  You have to give your data to an ad sales company.



"According to security researchers, one of the signals that Google uses to decide if you are malicious is whether you have a Google cookie in your browser.  If you have this cookie, Google will give you a higher score.  Google says they don't use this information for ad targeting; but at the end of the day, Google is an ad sales company.  Meanwhile, at Cloudflare, we make money when customers choose us to protect their websites and make their services run better.  It's a simple, direct relationship that perfectly aligns our incentives.



"In June we announced an effort with Apple to use Private Access Tokens.  Visitors using operating systems that support these tokens, including the upcoming versions of macOS or iOS, can now prove they're human without completing a CAPTCHA or giving up personal data.  By collaborating with third parties like device manufacturers, who already have the data that would help us validate a device, we are able to abstract portions of the validation process, and confirm data without actually collecting, touching, or storing that data ourselves.  Rather than interrogating a device directly, we ask the device vendor to do it for us.



"Private Access Tokens are built directly into Turnstile.  While Turnstile has to look at some session data like headers, user agent, and browser characteristics to validate users without challenging them, Private Access Tokens allow us to minimize data collection by asking Apple to validate the device for us.  In addition, Turnstile never looks for cookies, like a login  cookie, or uses cookies to collect or store information of any kind.  Cloudflare has a long track record of investing in user privacy, which we will continue with Turnstile."



They then explain a bit more about what's under the hood, saying:  "To improve the Internet for everyone, we decided to open up the technology that powers our Managed Challenge to everyone in beta as a standalone product called Turnstile.  Rather than try to unilaterally deprecate and replace CAPTCHA with a single alternative, we built a platform to test many alternatives and rotate new challenges in and out as they become more or less effective."



LEO:  Oh.



STEVE:  In other words, what they're really also saying, yes, is that when the bot farms...



LEO:  Figure it out.



STEVE:  ...start working it out, they're just going to change the rules, and no users will have to have any effect.



LEO:  What are the odds we're going to end up having to identify bicycles after a while?  Oh, lord.



STEVE:  Let's hope not.



LEO:  I hope not.



STEVE:  At least we'll know that if that has to happen, it had to happen. 



LEO:  Right.



STEVE:  It's not something that, you know...



LEO:  Convenient for Google, yeah.



STEVE:  Yes.  Again, I have no explanation for the fact that I'm having to click on parking meters in order to tell Google that...



LEO:  It used to, every once in a while, would say, oh, yeah, yeah, you just say I'm not a robot, and they say of course you're not, we know that.  But that's gone away now; right?



STEVE:  Apparently.  I haven't seen that for a while.



LEO:  Unh-unh.



STEVE:  So they say:  "First we run a series of small non-interactive JavaScript challenges gathering more signals about the visitor browser environment.  Those challenges include proof-of-work, proof-of-space, probing for web APIs, and various other challenges for detecting browser quirks and human behavior.  As a result, we can fine-tune the difficulty of the challenge to the specific request.



"Turnstile also includes machine learning models that detect common features of end visitors who were able to pass a challenge before.  The computational hardness of those initial challenges may vary by visitor, but is targeted to run fast.  You can take advantage of Turnstile and stop bothering your visitors with a CAPTCHA even without being on the Cloudflare network.  While we make it as easy as possible to use our network, we don't want this to be a barrier to improving privacy and user experience.



"To switch from a CAPTCHA service, all you need to do is" - and they have three things.  "One, create a Cloudflare account, navigate to the 'Turnstile' tab on the navigation bar, and get a site key and secret key.  Two, copy our JavaScript from the dashboard and paste over your old CAPTCHA JavaScript.  Three, update the server-side integration by replacing the old site verify URL with ours."  They said:  "There's more detail on the process below, including options you can configure, but that's really it.  We're excited about the simplicity of making a change."



LEO:  Yeah, make it easy.  But remember the people who are using that crappy Google CAPTCHA are doing it for one reason.  They're lazy as hell.  And I bet you, you watch.



STEVE:  No big effect?



LEO:  Yeah.  Because that's why they're using this crappy one.  They don't care.



STEVE:  Well, those of us who care will...



LEO:  Do you have CAPTCHAs on your site?



STEVE:  No.



LEO:  You see?  My point exactly.  Neither do I.



STEVE:  No.  No.  So anyway, oh, I did create a shortcut for this for our listeners.  Anybody who wants to go to that signup page, it's the Shortcut of the Week, so it's grc.sc/891.  That'll get you directly to sign.



LEO:  I also wonder that if adblockers - certainly NoScript would kill it.  I wonder if it's seen as intrusive by some adblockers because they are running some stuff in the background to see who you are; right?



STEVE:  Right.  Although I would imagine Google is doing that, too; right?



LEO:  Oh, Google's worse.  I think Cloudflare even points out that if you're not logged into Google, if you don't have a Google cookie on your page, they assume you're up to no good.



STEVE:  You can't be human.



LEO:  You can't be a nice person, yeah.



STEVE:  You're not logged into Google.



LEO:  Yeah.



STEVE:  That's right.  Okay.  So my last piece is really cool.  This is what I posted to my Twitter feed that so distracted me that I forgot to post about today's podcast.  Google has put together a marvelously engaging series of six 15- to 19-minute videos under the banner "Hacking Google."  It's a world-class production such as you might expect from a company with Google's resources, which of course we're all providing to them.  And yes, of course, these are ultimately promotional.  But that doesn't dissuade me from recommending them without reservation because they are gorgeous.  And they will be, I think, of tremendous interest to this podcast's listeners.  I've already, since this morning, received a bunch of feedback, and I've got more loves and retweets on that tweet than I normally get on my weekly announcements of the podcast show notes.



So I first - so this is kind of funny.  I stumbled onto the second one in the series, not realizing that it was number two, since it was numbered 001.  Of course...



LEO:  Oh, I love it.



STEVE:  I know.



LEO:  Zero-based numbering.  God bless you, Google.



STEVE:  Yes, they started numbering from zero, as I've always wished we had...



LEO:  Yes.  We'd have one more episode.



STEVE:  ...the foresight to do.  That does allow us to squeak out one last one when 999 wraps over, my three digits back to zero.  In any event, I've only had the time so far before today's podcast to watch that second one all the way through.  But based upon the one I watched, I mean, look at those graphics, Leo.



LEO:  Yeah, it's nice, yeah.



STEVE:  It's got stunning graphics.



LEO:  It's network news production quality, I mean, just very high quality.  Better than network news, really.



STEVE:  It is really better than anything that I've seen.



LEO:  Documentary production quality, yeah.



STEVE:  Yeah.



LEO:  Well, they've got money.



STEVE:  Yes, exactly, they've got the money.  There are six videos:  Operation Aurora; Threat Analysis Group, that's the one I saw which talks about the origin and status of Google's TAG team, and we're always talking about them on this podcast because they're doing such great work.  The third one is Detection and Response; the fourth one, Red Team; the fifth one, Bug Hunters; and then the last one is Project Zero.  Again, I've only had a chance to watch the Tag Team one.  And it was really cool.  So anyway, I commend our listeners to that.  The videos are collected...



LEO:  There's a playlist on YouTube; yeah? 



STEVE:  Yes.



LEO:  Yeah.



STEVE:  They are collected into a YouTube playlist for easy access.  I have a link to it in today's show notes, which will take you directly there.  Also, and it's weird, too, because when I tried to create my own shortcut to it, the redirect wouldn't go to the playlist.



LEO:  No, yeah.



STEVE:  It insisted on starting at the first video.  But if you google the phrase "hacking google playlist," then a few links down from the top you'll find the actual playlist page, which I do have a link to in the show notes.  Or, if you want to use a GRC shortcut, grc.sc/hackinggoogle.  That will bounce you to the first one, and then they all link successively.



LEO:  First one, which is Episode 0.



STEVE:  Zero, yes.



LEO:  Actually, Google should do what we do.  We start all our shows now with Episode 0, but that's the trailer.  So, see, and if Google had done that, they have a trailer.  But if they just made their trailer Episode 0, you would not have been fooled.



STEVE:  Yes, that's a very good point.



LEO:  There's a programming language that I really like, but there's one thing I hate about it.  It's called Julia, and it uses one indexed arrays.  And it's like - but they just say, this is how people count.  



STEVE:  How dumb is that?



LEO:  My point exactly.



STEVE:  I mean, we're having to do this ridiculous math every single time.



LEO:  Just to subtract one.  Or add one.  Yeah.  Well, their point is that normal people have to do the ridiculous math to do zero-based arrays.



STEVE:  Oh, my god.  Well, and one of the most common problems are the off-by-one examples.  



LEO:  Exactly.  



STEVE:  I mean, I spend more time thinking, okay, do I mean greater than, or greater than equals, or equal to?



LEO:  Yeah.  I also do, you know, I always cover my rear by doing greater than or equal to, just in case.



STEVE:  Yeah, and it's funny, too, I do the same thing.  When I am ending a loop for a counter, I don't say end it when this is equal to something.  It's just superstitious when I say when it's equal or greater.  Because, you know, why not?



LEO:  Even though it's demonstrably the case it will have that number.



STEVE:  Absolutely.  And if it doesn't, then you've got bigger problems.



LEO:  But I do exactly the same thing.  It's never equal.  It's always greater.  Equal seems too precise.



STEVE:  You're trusting the computer.  I get it.  It's like, well, it's hard to justify, but it just, it seems better.



LEO:  They also have infix arithmetic functions.  And while I know everybody knows infix, it just, you know, once you start using...



STEVE:  Who is this Julia thing aimed at?



LEO:  Data scientists.  It's actually a really nice language.  Very, very nice.  But I do have to question that choice.



STEVE:  So they've dumbed it down in order to...



LEO:  They dumbed it down.  Just like Python is a little dumbed down, too, because they want - but even Python...



STEVE:  In order to make it more domain-specific, yeah.



LEO:  Or just more accessible.  But I think just get used to zero-indexed arrays, and you're done with it.



STEVE:  Wow.  Yeah.



LEO:  That was like Pascal would start strings with the string length, and then one is the first letter.  That's dumb, too.



STEVE:  Yeah.



LEO:  Null-terminated strings, it's the only way to go.



STEVE:  That is a win.  And that's the beauty - all of my code is null-terminated.



LEO:  Yeah.  Because you're coding also in assembly, and it just - and that's where this all comes from.



STEVE:  Yes, and in fact that's the problem with a one-based index.



LEO:  Right, because it's incorrect.



STEVE:  I'm also thinking in terms of the offset from the pointer.



LEO:  Right, right, because your pointer points to the first thing in the array, and it's zero in.



STEVE:  Yeah, they're really going against God's will on this.



LEO:  I think so.



STEVE:  I don't - yeah.



LEO:  God or John von Neumann, one or the other, is going to be very unhappy.  Sometimes they seem the same.



STEVE:  So we had a listener, James, who said:  "Hi, Steve.  As a 2019 Honda Accord owner, I've followed your detailed explanation of the key fob hacking saga with a vested interest.  I don't leave anything of value in the car specifically because you made it clear anyone can get in, and even if they can't steal the car without the fob, they can take anything at any time.  Unfortunately, last week I was their next victim.  Fortunately for me, they didn't get what they were after, as I don't keep my lug nut lock keys in the car anymore since the rims are quite worthy of theft.  They did rifle through EVERYTHING," he has in all caps, "and got a bag from the trunk that I liked.  But I didn't even notice the $20 in the glove box under the empty lug nut lock key bag.



"Although the car and fob were confused," he says, "my fob would unlock but not lock the doors, I was able to get the car to the dealer."  So he's saying the attack confused the fob/car relationship, as we would expect it to.  He says, "I was able to get the car to the dealer.  And by the time I got there, the fob had somehow synced so it was functioning normally.  I explained the experience and my full knowledge that this is a Honda-specific technical flaw that I know the dealer cannot fix, but could they at least wipe and reset my system so that the currently stolen code wouldn't work anymore for this set of thieves?  I was told by the service department manager, Bill, that the reset costs $220, and I would have to pay for it. I declined."  So I thought that was an interesting bit of feedback from the field regarding an attack that we covered in some length, enough for James to know exactly what was going on.



John said:  "Hi, Steve.  I've noticed that there's been a lot of discussion around phishing protection on some of the most recent episodes of Security Now! and how SQRL, FIDO, et cetera will address or resolve it.  One thing you haven't mentioned recently, though, is that just using a password manager such as LastPass or Bitwarden, et cetera, will also provide a degree of protection as the autofill will fail as a fake URL will not match the one linked to the stored credentials.  Long time Security Now! listener, and thanks from Brisbane.  John."



And I put that in here just because I wanted to amplify it.  He's absolutely right.  I had forgotten to mention in our recent discussion of this that it is one of the benefits of a password manager, which takes what's in the URL absolutely literally.  And so if you're expecting an auto logon, and you don't get it, it's protected you from a phishing scheme that was using a lookalike URL or something similarly confusing.



Eric Seidel said:  "Hi, Steve.  I've been listening to Episode 889 and wanted to mention I've also been using Security Now! since I got my CISSP for CPEs to maintain the CERT."  There's a bunch of acronyms for you.  He says:  "I've never had any issue either when submitting them with ISC.  Thanks again for a great resource for that."  And I just wanted to say that as a consequence of my talking about that in 889, many of our listeners said, yeah, that's what I do, too.  So it's clear that that works for people.



Manuel said, or asked:  "I have a question that I'm hoping you might answer.  I recently installed backup software (EaseUS Todo Backup) and later realized that it's Chinese software.  Now I'm wondering if I just compromised my computer and my whole home network?"



LEO:  Oh, lord.



STEVE:  "Do you have any advice on this" - I know, Leo.  Hold on.  "Do you have any advice on this way of thinking?  Should I reinstall my Windows?"



LEO:  Oh, my god.



STEVE:  "Look for a way to reinstall the BIOS/UEFI, buy a new phone, a new TV?"



LEO:  Yeah, throw it all out.



STEVE:  "A new router, a new everything?  Where should I draw the line?"



LEO:  Oh, he's joking.  Thank god.



STEVE:  He said he just started listening to the podcast.  So, but this is, I mean, it's worth discussing because, you know, Kaspersky is now being looked at very unfavorably because they of course are a Russian cybersecurity firm.  We know Kaspersky well.  There's, from everything I've seen, zero evidence to suspect them of any bad behavior.  But it's just creepy that they're Russian.  And we know that companies in Russia don't have necessarily full autonomy over their own actions.



LEO:  Right.  Yeah.  And there's, you know, that's an easy thing to do because there are lots of other choices for antiviruses that aren't from Russia.



STEVE:  Right.



LEO:  I guess you could say the same for Todo Backup.  I recommend EaseUS all the time on the radio show because they offer good quality products, in many cases for free.  Todo has a free version that does a good job.  Maybe that's why it's free.



STEVE:  And it happens that my favorite remote access software, I've talked about it often, Remote Utilities, it's also from a software publisher in Russia. 



LEO:  Yeah.



STEVE:  And it's like, well, I guess being forewarned is useful.  But, I mean, and it is tough where we've got like this saber rattling going on increasingly between the U.S. and China, with tensions escalating.  At some point, you know, and apparently, like, teams on both sides poking around aggressively inside of each other's networks.



LEO:  I got an email from somebody today saying I will never buy an iPhone because they're made in China.  I'm only buying Samsung because it's made in South Korea.  And I guess, you know, I can't disagree with that.  There is zero evidence that just because something's made in China that it is somehow dangerous.  I guess maybe software it would be a little bit easier.  But if they were exfiltrating stuff from your backup, I think even if you didn't notice, somebody would notice that.



STEVE:  Well, and we've got motherboards all being made now in China.



LEO:  Right.  Everything. 



STEVE:  Components.  And yes.



LEO:  And there you don't really have that much of a choice.  I mean, good luck finding one anything, a TV, a router, a phone that's not made in China.



STEVE:  And we've also talked about the difficulty of even like visually inspecting a motherboard. 



LEO:  Right.



STEVE:  Look at all those little bitty chips.



LEO:  Right.  What do they do?



STEVE:  No one knows what - yes, exactly.  No, it's, I mean, it's a concern. 



LEO:  I mean, you can be too paranoid.  I also don't want people to be xenophobic.  You can't - don't conflate the Chinese Community Party, which is, yeah, absolutely awful, with the people of China.



STEVE:  And xenophobia was exactly the word that I've had on the tip of my tongue through this because it's just, you know, it's not the right way to be.



Cristian Sanchez said:  "Hi, Steve.  Wondering if you have any thoughts on this Washington Post article's assertion that public WiFi is safe."  And he linked to it.  He said:  "I admit I clicked on it expecting to laugh at thoughtless misinformation, but the discussion in the comments turned me around.  Is the near ubiquity of HTTPS enough to declare public WiFi safe?  What about man-in-the-middle hijacking DNS?  Long-time listener, big fan of the show.  Keep up the good work.  Sincerely, Cristian."



And I read the article, and it was written by somebody who really knows what they're talking about.  And they had the position that I do, which is, yes.  There has been such a transformation since we were first talking about Firesheep back then, where open access WiFi was a catastrophe, because the Firesheep, remember, was a Firefox plugin where you were able to - oh, in fact I'm talking on my own lines.  I've got some piece of the Washington Post here that I wanted to share.



So the Washington Post wrote, and I've edited a bit to bring it up to the level of our listeners, they said:  "You probably don't need to worry about public WiFi anymore.  Here's what a creep in a coffee shop could actually learn about you."  That was their headline.  "From uncovered webcams to reused passwords, it's tough to keep track of how much risk our everyday digital activities actually pose.  For example, take WiFi networks in airports and coffee shops.  They're part of life for anyone who travels or works remotely.  They also have a reputation as cybersecurity risks.  Do they still deserve it?



"To see what potential hackers could see on a shared network, we invited professionals from cybersecurity company Avast to 'compromise' my home network, all with my consent.  We logged onto the same network at the same time, just like we would at a coffee shop, to see how much data a bad actor with a few free tools could learn about an unassuming WiFi user.  What we found, or didn't find, might be a relief for the coffee shop crowd.  After a few minutes clicking around my finance, work, streaming, and social media accounts, Avast's team could see the sites I'd visited, though not what I'd done there; the time of day; and the specific device I used, in this case a MacBook Pro.  It's not nothing, but it wouldn't do hackers much good if they were looking to rip me off."



He says:  "Chester Wisniewski, a principal research scientist at security company Sophos, said that it's also relatively reckless for hackers to sit around messing with public networks.  Quoting him:  'That type of data isn't only low yield, it's high risk.  If I can phish your password from my chair in Moldova and have zero risk of going to jail, why would I get on an airplane to go to your local Starbucks?'"



LEO:  Yeah, that's a good point, yeah.



STEVE:  So our author said:  "In the Internet's earlier days, the vast majority of web traffic was unencrypted, meaning anyone savvy enough to eavesdrop on a network could see everything you type at a website."  And that's where I interjected "Our longtime listeners will all remember Firesheep."



He said:  "By 2017, the balance had shifted with more than half of all web traffic using the encrypted HTTPS protocol, according to data pulled from Mozilla.  Today, few legitimate sites remain unencrypted, with more than 90% of websites loaded in the United States obscured from prying eyes, according to Mozilla's data.  This means even if someone used a public network to spy on you, what they'd discover probably wouldn't be very valuable."



Anyway, the article finishes:  "Focus your energies on cybersecurity chores within your control, such as setting strong passwords, saying yes to software updates, and learning the signs of a scam.  And don't sweat the public WiFi too hard.  If a site, link, or app seems sketchy, steer clear."  Which of course is always great advice.



So anyway, great question from a listener, and thank you for the pointer to the Washington Post article.  I agree.  You know, somebody really concerned could use DNS over TLS, right, so that even the places they're going, even their DNS queries would not be visible in the clear because DNS by default is still not yet encrypted.  I don't see that changing.  That would be difficult to change.  But we could certainly, as individuals, cause our DNS to go through a secure TLS tunnel, and then nothing about what we're doing would be accessible.



LEO:  Somebody I'd love your opinion on a thing called the WiFi Pineapple.  Are you aware of this?



STEVE:  Yeah.



LEO:  Yeah, they call it a pen testing device, although you could easily use it in a coffee shop to attack WiFi users.



STEVE:  Yeah, I think the power there would be if somebody were using a laptop where the laptop had inbound security problems, like Windows file and printer sharing was set up to be online.



LEO:  Sure.  Then you'd...



STEVE:  So there...



LEO:  But one of the things you can do with this is mimic a preferred network and then have that device log into you.



STEVE:  That would be bad because most Windows users have their Windows firewall set up to be transparent on their local network.  So if someone knew who you were and could masquerade as your local network, then your machine would log into it automatically, and it would have access inbound through your firewall in a way that the...



LEO:  Now there's somebody inside your computer; right.



STEVE:  Right.  Now there's somebody on your network.



LEO:  On your network.



STEVE:  Yeah.



LEO:  One of the ways that you might use this, in this area anyway, probably yours as well, Comcast is the dominant cable provider.  They put in these Xfinity routers, as you know, that open access points, public access points.  And I would bet a majority of people who are on the road in areas like ours make sure that their phone and their laptop, if it sees an Xfinity network, will join it.  So the first thing I would do if I had a WiFi Pineapple is spoof an Xfinity.com network.  Just to see.



STEVE:  Yup, and you just immediately get connection.



LEO:  Yeah.  And you could, if you wanted to, pass the Internet through it, but you'd still be on their network.  So I feel like if you were really a determined attacker, there are still things you can do.  Yes?



STEVE:  Yeah.  Yeah.  And so the only solution to that would be to be choosing the proper VPN.  Remember that you need a VPN that VPNs everything your computer does, not just, for example, your web traffic.  You want it to completely encapsulate your network.  That would protect you from everything we've just been talking about because it would be linking through all of your local infrastructure connection back to a VPN server somewhere, either back to your home or to a good reputable commercial provider.



LEO:  You used the phrase, and I think it's very apt for this, is what's your threat model?  So if you're working for the NSA, your threat model is very different than Leo going down to the Starbucks.  And so you have to know what your threat model is and act appropriately.  And so most people, you know, this Post article's probably accurate.  Don't have to do anything.



STEVE:  I think that's true.  Sean Nelson said:  "Hey, Steve.  I've been looking for a product like this for years, and I finally found it.  I think it might interest you, too.  It's an SD card with encryption built in."



LEO:  Ah.



STEVE:  "This allows you to take pictures or videos without risking that the contents will be viewed or confiscated by anyone else.  From what I can tell, it works by storing a master key set by the user."  And I'm going to explain in detail how it works in a second.  But he said:  "Then, each time the card powers on, it creates and stores a new symmetric session key, which gets processed through the master key for safekeeping.  Any new files get encrypted using that new symmetric master key.  When the device is powered off, the key is removed from volatile memory, and the only persistent copy of the key is encrypted with the master key.  If you take a picture and power off the camera, next time it powers on, no pictures are visible."



He says:  "You can take new pictures; but they, too, are unreadable after you turn off the camera.  However, when you attach the SD card to a computer and supply it with the master key, it is able to download the pictures, along with each associated session key, and unlock everything on the camera.  I've been looking for this for my dash cam for a long time.  Given the state of the world, many others might be looking for something similar."  So I think this is very cool and clever.  I've got the link to it at the bottom of this article.  It's another example of the principle I often observe, which is that the generic and now well-proven tools we already have can be combined in an endless number of ways.



From what Sean described, here's how I would design this device, and it's likely what its maker, SwissBit, has done.  During setup on a PC, a public key pair would be created, and the SD card would be provided with the public key, and that's all.  Then, whenever the SD card is powered up, an internal high-entropy generator would synthesize a transient 256-bit symmetric key in RAM.  That key would be immediately encrypted under the card's configured public key, and only the encrypted key would be written to non-volatile store and retained.  Then, during the card's use, all data flowing in and out of the SD card being read and written would pass through that AES 256-bit cipher, which would be transparent to the device it's plugged into.  It would look just like a regular SD card, which was initially blank.



When the camera and/or SD card is powered down, the RAM-resident 256-bit symmetric key is forgotten and lost.  Since it was encrypted under the user's public key, the only way for it to ever be decrypted would be with the use of the matching private key, which is deliberately unknown to the camera and the SD card.  So it's a very slick solution for using an SD card to continuously capture photos and videos while never allowing the contents of the card to be exposed.  Anyway, very cool, Sean, thank you for sharing it with our listeners.  I wouldn't be surprised if we have some people interested.  He did mention that it's pricey.  I think it was $159 for 32GB.



LEO:  Oh, yeah, wow.



STEVE:  If memory serves.



LEO:  That's like 10 times more than the normal price.



STEVE:  Yeah.  So...



LEO:  Plus don't ever have a problem with that card because there's nothing on it but gobbledy-gook.



STEVE:  That's true.



LEO:  You can't recover that data.



STEVE:  And don't lose your matching private key, either.



LEO:  Yeah, right, right.



STEVE:  Yeah.  Jeff said:  "Hey, Steve.  I noticed on Threema's blog that they just joined Proton, Brave, the Tor Project, and a couple of other Internet services to launch the Privacy Pledge initiative."  And so that's at privacy-pledge.com.  I won't go into any detail.  I just kind of wanted to give them a heads-up.  They've got a bunch of, I guess, okay, five privacy-forward principles, this group, which is Brave; David Carroll; Mailfence; Mojeek; Neeva; Open-Xchange; OpenMedia; Proton, you know, the encrypted mail people; Threema; the Tor Project; Tutanota; and You.com.  And they're good ideas.  The Internet above all should be built to serve people.  This means it honors fundamental human rights, is accessible to everyone, and enables the free flow of information.  Businesses should operate in such a way that the needs of users are always the priority.  That's the first principle.



Second one, organizations should only collect the data necessary for them to sustain their service and prevent abuse.  They should receive people's consent to collect such data. People should likewise be able to easily find a clear explanation of what data will be collected, what will be done with it, where it will be stored, how long it will be stored for, and what they can do to have it deleted.



Third principle, people's data should be securely encrypted in transit and at rest wherever possible to prevent mass surveillance and reduce the damage of hacks and data leaks.



Fourth, online organizations should be transparent about their identity and software.  They should clearly state who makes up their leadership team, where they are headquartered, and what legal jurisdiction they fall under.  Their software should be open source wherever practical and open to audits by the security community.



And finally, web services should be interoperable insofar as interoperability does not require unnecessary data collection or undermine secure encryption.  This prevents the creation of walled gardens and creates an open, competitive space that fosters innovation.



So they say:  "This is the Internet that we deserve.  This is the Internet we are fighting for.  It is within our reach.  We simply need to be bold enough to seize it."  And maybe smoke something.  I mean, we all wish that was the Internet we have.



LEO:  It's not, I mean, honestly, anybody should be able to sign this because there's plenty of ways out of this, like you should only click the data necessary for you to sustain your service.  Okay?



STEVE:  Yeah.



LEO:  It should be encrypted at rest whenever possible.  Okay.



STEVE:  Yeah, I know.  And so it's sort of a happy...



LEO:  It's pretty namby-pamby, frankly.



STEVE:  Yeah, reminds me of the Haight-Ashbury...



LEO:  Oh, I get it now, yeah, yeah, yeah, yeah, yeah.  I mean, I'm glad that they said this.  But honestly, Google could sign this without any hesitation.



STEVE:  Yeah.  I don't think they would be admitted.



LEO:  You're not invited.  Sorry, Google.



STEVE:  You go to the form, and you put your name in, and they say "We'll get back to you."



LEO:  They send you this image.  Bye.



STEVE:  Hope that's a parachute, not a backpack.



LEO:  Yeah.  Uh-huh, yeah.  That's an odd image to put next to your request to join the privacy pledge.  I'm not sure - jump off the cliff with us.  See what's in that backpack.



STEVE:  Yeah, don't look down.  Donn Edwards, my last share here.  He said:  "Hi, Steve.  Surely the EU nonsense would go away if EU sites changed the analytics.google.com URL to analytics.google.eu."



LEO:  Oh.



STEVE:  Yeah, I know, that's what I thought, too.



LEO:  That's clever.



STEVE:  He said:  "Then Google could use their EU servers to do the analytics in keeping with EU rules and laws.  Keep up the good work.  Donn."  Now, I didn't know when I read this whether analytics.google.eu worked.  So I gave it a try.  It returned a DNS lookup failure.  So that doesn't appear to be an immediate option for those in the EU.  What I assume Donn meant was that, if Google wanted to respond to this concern, this might be a clean means for them to do so.  With nation after nation ruling against the use of Google Analytics as it stands today as being unlawful, something likely needs to change.  So I thought that was a very cool suggestion:  analytics.google.eu.  I mean, it's almost, you know, you're, like, supporting the EU when you make that change.



LEO:  Yeah.  They'd have to actually create a server at that address first.



STEVE:  That would be - I think they could do that in about, you know...



LEO:  Two seconds.



STEVE:  About two seconds.



LEO:  How long does it take to provision that, yeah.



STEVE:  Somebody presses a button somewhere, and now there's Google Analytics at .eu. 



LEO:  It's someone telling that that does not exist.



STEVE:  I know.  I was surprised.



LEO:  Analytics requests are redirected to the U.S. probably in every case, I would bet.



STEVE:  That's where they're most useful, Leo.



LEO:  Yeah, that's where we really want the information.



STEVE:  That's right.



LEO:  Let us take a time out because you are going to talk about this Akamai thing, this poisoning.



STEVE:  Ooh, and it's neat.  So last Thursday we got a glimpse into a world-shaking flaw that very few people knew about until now.  A 23-year-old Italian security researching enthusiast by the name of Jacopo Tediosi and his friend Francesco Mariani stumbled upon a flaw in Akamai's CDN that could have ruined many days for half the world's websites that rely upon Akamai.



A content distribution network is, at its heart, a massive global distributed cache.  It keeps track of a website's content, pretty much everything, and holds the most recent current version of a website's content in a cache which is local to that remote site's visitor.  In that way, since the cache is close, the site's performance remains snappy even though its web server might be on the other side of the planet and on a not particularly fast connection.



So first of all, imagining that this can be done at scale safely is nuts.  But that hasn't - it's like, no.  Don't try to do that.  Bad idea.  But that hasn't stopped the world's very successful CDNs from doing it anyway.  What's absolutely obvious is that this only works if the massively distributed web asset cache maintains its coherence so that it always accurately reflects the correct contents of a remote website.  If it were possible in some way to deliberately alter a CDN's distributed cache to change any of those locally cached remote web assets, like for example a site's JavaScript, nothing less than havoc would reign, since such an attack would be tantamount to having the ability to directly alter a site's served content.



Since this podcast is titled "Poisoning Akamai," you already know that this was somehow done.  But the reason this podcast is titled "Poisoning Akamai" is because the story told by 23-year-old Jacopo of their discovery, and the discovery's aftermath, is absolutely worth sharing.  



So posting to Medium.com, Jacopo opens his story by explaining:  "In March 2022, my friend Francesco Mariani and I were teaming up on a private bug bounty program organized by WhiteJar to search for bugs on a website that was using Akamai CDN.  The Akamai WAF" - that's the Web Application Firewall - "rules were bothering us while experimenting with the most common attack types, so we quickly got bored and started trying more esoteric payloads and mixing them.  Finally, we ended up finding a vulnerability that really made us exclaim, 'Wow, we broke half the web.'"



But let's start from the beginning.  He says:  "At one point we were intrigued by an unusual DNS Failure response, received by sending twice an HTTP/1.1 GET request to the host being tested with the Connection: Content-Length header and containing another GET request to www.example.com as its body."



Okay.  Now, at this point I'll interject that I remember that we did an extensive podcast on exactly this subject, but I cannot recall what the specific topic was.  It involved chaining HTTP requests or problems with HTTP chaining or proxying.  What these guys are talking about is having an HTTP-style GET query, not a POST query, where the GET query contains in its body, which is normally empty for GET queries, instead another HTTP GET query following all of its regular query headers.  That's not the normal format for GET queries, where the specification of the object being queried for is in the GET URL path.  But again, I know we covered this years ago.



In this case, their example shows a GET query containing the query headers "Connection: Content-Length," which is not normal for a GET query, and "Content-Length: 53," also not normal.  Those are what you see in POST queries because that specifies the after-the-query POST contents will be there, and their length.  So upon seeing this odd DNS Failure response, Jacopo wrote:  "Weird behaviors like this can often be overlooked while testing so many things, but luckily this time we decided to dig deeper."  He said:  "I have to admit it took me a while to figure out what was going on, and I also had to reread Nathan Davison's excellent article on 'hop-by-hop' headers that I had studied in the past."  And so it is, for what it's worth, indeed an excellent article which I would recommend to anyone who's interested in digging more deeply into this topic, though it's not necessary for understanding what's going on here.  I've got a link to the article in the show notes.



He said:  "As explained in RFC" - this is Jacopo.  "As explained in RFC 2068 Section 13.5.1, there are some special headers named 'hop-by-hop' which are removed from proxies before forwarding requests to the next proxy or the destination."  And of course that's what Akamai is, is it's a network of proxies which are caching proxies.  He said:  "The addition of the Connection header allows including more hop-by-hop headers in addition to the default ones.



"Specifying the Content-Length header as hop-by-hop, it happened that Akamai's first proxy removed it, turning the request body into a second request.  Akamai's second proxy then resolved the two requests separately.  Since the first proxy received two responses, but only one was expected, a desynchronization occurred, and the second response was queued and subsequently sent in response to requests from other clients/users, causing an HTTP Smuggling Vulnerability."  That's a formally named thing.



He said:  "Understanding this in detail requires a certain degree of knowledge about network architecture, web protocols, and other fancy stuff, so I try to explain it more easily with a chart."  And I have the chart in the show notes, and Leo, you had it on the screen there.  So that sort of gives you a sense for what it is.  What it very clearly shows is what happens.  It all boils down to an Akamai caching server query parsing error, which results in a single query, rather than being kept as a chain, being split into two and forwarded to two separate destination client web servers.  Then the two separate queries are answered, each by their own server, and returned to the cache for caching, which is what Akamai does.  But the cache was only waiting for the reply to a single original query that would have normally been chained.  Instead, it was split.  So it places the unexpected reply into a queue, and that's the behavior that we talked about a few years ago where it then is available for handling serialized HTTP queries.  And that reply will be returned to someone else.



So Jacopo says:  "However," he wrote, "I could not immediately understand why the DNS error was showing up and why www.example.com was not being resolved.  The answer was actually quite simple, but my co-worker's intuition was crucial:  Akamai's proxy that routes requests appeared to resolve DNS only internally within Akamai's network."  Okay.  So that's sort of a subtle point.  He was making the point that it was a fluke that the DNS failure occurred.  It was the DNS failure that caused them to look more deeply into why the DNS failure, even though what was actually going on, it doesn't really matter that Akamai's only resolving local DNS queries.  It's just that it was that fact that led - it was the first little piece of the breadcrumb trail that they then followed that led them to a serious bug in Akamai's caching architecture.



He said:  "We were using a VPN to verify that the desynchronization was an 'open' one, meaning that it affected the responses given to IP addresses other than the ones we were attacking from."  So in other words, they used a VPN to shift their IP and then generate a query from a different IP to see if it was open, meaning that multiple users at different IPs would also be affected.  And they confirmed it was.



He said:  "Also believing it possible that the bug concerned all Akamai customers around the world, we changed our target from [the original server they were testing at, which they redacted from their notes] to more popular sites.  To our amazement, we noticed that it worked on them all, and that sometimes 'smuggled' responses were being server-side cached from Akamai Edge Nodes for the entire geographic area close to the IP sending the malicious request.  This allowed us to semi-permanently, depending on cache times, create new arbitrary contents within almost any domain served by Akamai, resulting," they wrote, "in a HUGE [all caps] impact.



"As a proof of concept we created for the whole Italian area the newly cached page demo.paypal.com/jacopotediosi_hackerone.js, containing the content of www.sky.com/robots.txt, another Akamai customer, because we didn't own a host on the Akamai network to use for publishing our arbitrary contents."



Okay.  So just to make this clear so far, they arranged to place a brand new page into Akamai's web cache named "/jacopotediosi_hackerone.js" associated with the root of demo.paypal.com.  Thus anyone else in Italy who was also being served by Akamai's cache for that locale which had been poisoned would be able to receive that cached page by asking for it from demo.paypal.com's web server.  So upon receiving that query, the local Akamai cache would see that it had a copy of that page in its local cache and would return it quite quickly to whomever asked.



He said:  "Once we understood the seriousness of the situation, we decided to report it ethically and responsibly, first of all to Akamai.  Unfortunately, we quickly realized that Akamai doesn't have a bug bounty program, hall of fame, swag giveaways, or anything."  And they clipped out a piece of the email that they received on 3/25/2022 at 20:29 from Akamai written to Jacopo Tediosi.  And the email from Akamai says:  "Hi, Jacopo.  It's quite clear what's happening here (thanks again for the very detailed report).  We had no issues confirming your report.  We're working on mitigations and making changes to our HTTP parsing and processing logic.  Unfortunately, Akamai does not have a bug bounty program at this time, and we are not able to provide a direct monetary award.  But regarding some other means of recognition of your contributions, I'll get back to you soon.  Thanks."



LEO:  Send him some stickers.  Stickers are always good.



STEVE:  Un-effing-believable.



LEO:  Thanks for letting us know.  Okay.



STEVE:  So Jacopo says:  "We are white hats, but we're still not willing to work for free because this vulnerability was very critical.  And our skills are rare, complex, and sought after, and we think they deserve to be valued.  So while Akamai was patching following our report, we chose to race against the time by asking for bounties from single Akamai customers."  Which is so clever.



He says:  "While this may sound strange, from our point of view on technologies, those who use a framework, plugin, CDN, whatever, assume both their benefits and their risks.  Thanks to our work, Akamai and all their customers have been made aware of a security issue" - boy, and how - "and have been able to fix it.  So it's just fair that they pay for our service because without us the vulnerability would still be there.



"We used bbscope to extract links for all the public programs on the most popular bug bounty platforms.  Next, we wrote a short bash script to filter from the list only the domains whose DNS pointed to Akamai."  Okay.  So in other words, they very cleverly reported this bug as it related to specific major Akamai customers whose websites were, indeed, still in serious danger until Akamai pushed out a fix.  And what were their results?



Jacopo wrote:  "First, WhiteJar immediately gave us 5,000 euros for their private program."  They said:  "On Bugcrowd, they were not competent enough to understand the vulnerability and closed both our reports for Tesla.com, which was vulnerable, as 'duplicated,'" they said, "of a ticket clearly not related to ours, and for LastPass.com as 'not applicable' because they were unable to reproduce.  Intigriti, regarding the Brussels Airlines program which we showed was vulnerable, told us that 'Brussels Airlines is already aware of any request smuggle vulnerabilities in their web assets.'"  And Jacopo said, "Yeah, any, LOL."  He says:  "And closed our ticket as 'duplicated.'



"On HackerOne, some programs refused our tickets and closed as 'not applicable.'"  He said:  "Starbucks replied the vulnerability, in their opinion, wasn't a major security issue.  PlayStation staff failed to reproduce, even after we created a new page for them under the www.playstation.com domain.  Marriott informed us that cache issues were temporarily out-of-scope for awards."  He said:  "However, many other programs paid us.  We received $25,200 from PayPal, $14,875 from Airbnb, $4,000 from Hyatt Hotels, $750 from Valve (Steam), $450 from Zomato, and $100 from Goldman Sachs."



LEO:  A hundred dollars?



STEVE:  I know.



LEO:  Just a hundred dollars?



STEVE:  Just a little stingy there, Goldman.  "In particular," they said, "Airbnb handled the situation outstandingly, applying custom rules on Akamai's Web Application Firewall in less than 24 hours to block requests containing 'Connection: Content-Length' even before Akamai's official fix.  PayPal was also a curious case because they confirmed our report and issued a bug bounty long after Akamai's fix.  So we don't know if they ever saw the vulnerability working, or if they just trusted our proof-of-concept video.  Akamai fixed it by applying some rules that prevent specifying the 'Content-Length' keyword with the 'Connection' header value, but we're not sure that there are no other bypasses or other unexpected similar ways to split the requests.  Unfortunately, Microsoft and Apple acknowledged our reports after Akamai had already deployed a fix, but they thanked us anyway via private emails."



So as we already know, I think from all of this I'm most disappointed in Akamai.  Their business is doing something that is so inherently dangerous, where a mistake as we've just seen could have truly horrific consequences, and they have no formal or informal bug bounty established.  The idea that a company of this size could just say "Gee, thanks, guys" and not write them a sizeable check on the spot is unconscionable.  The way these guys arranged to monetize their discovery was quite ingenious.



LEO:  Clever, isn't it?  That's so clever.



STEVE:  Yeah.  But that also means that they knew, because they're clever, they knew full well that for an exploit with this much power and virtually universal application, I mean, Microsoft and Apple sites could be compromised.  They could have asked the likes of Zerodium for a million dollars, and they would have received it.



LEO:  Sold it to the bad guys, in other words.



STEVE:  Yes.  They would have received it.  Anyway, I thought it was a very interesting story about something that happened earlier this year.  And because the guys were white hat hackers, few people ever learned of it.



LEO:  Yeah.  Akamai fixed it.  Still hasn't given them anything.  But they made more than $50,000 by going to the victims, basically.



STEVE:  The victims, yes, showing the victims that they had problems.



LEO:  This works because there was a mitigation the victims could employ.



STEVE:  Right.  The individual customers of Akamai have the so-called Web Application Firewall.



LEO:  Right.



STEVE:  And so on a user-by-user basis, they were giving them a patch for their firewall.



LEO:  Had that not existed, then it'd be really a problem.  They wouldn't have been able to get any money out of anybody.



STEVE:  None.  None.



LEO:  Except just put out a Medium post as they did and say these guys are cheap.



STEVE:  Boy.



LEO:  Thank you, Steve Gibson.  Shining light into the dark corners of the Internet.  It's a pretty powerful beacon, too.  And thank you for doing it.



STEVE:  You never know what's going to crawl out.



LEO:  Yeah.  Every Tuesday, right here.  We do it about 1:30 or 2:00 p.m. Pacific.  That's, let's see, 4:30 to 5:00 Eastern.  That's about 20:30 UTC.  If you want to tune in and watch us live, that's how you get kind of the first pressing, the extra virgin Security Now!.  The livestream is at live.twit.tv.  You can chat with us at irc.twit.tv.



You can also go to Steve's site and get a copy:  GRC.com.  He's got two unique versions of the show.  He's got of course the 64Kb audio, but he's got 16Kb audio for the bandwidth-impaired and those great transcripts by Elaine Farris, which lets you read along as you listen or search for topics and that kind of thing.  All of that at GRC.com.  While you're there, pick up a copy of Steve's true bread and butter, SpinRite, the world's best mass storage maintenance and recovery utility, 6.0 the current version, 6.1 coming any day now.  And you'll get a free copy if you buy it today, buy 6.0 today.  Steve also has lots...



STEVE:  And you'll get very early access to the beta because it's going to - because of the fact that it's a DOS app, I'll have that available to 6.0 owners a long time before it's all packaged in its window boot generating form.



LEO:  Oh, okay, cool.



STEVE:  So that's the other thing is, I mean, I'll be working as quickly as I can, but there's just going to be some delay.



LEO:  Good to know, okay.



STEVE:  So there'll be a really early access.



LEO:  Another good reason to get it right now:  GRC.com.  Lots of free stuff there, fun stuff.  You can leave Steve feedback there:  GRC.com/feedback.  You can also leave it on his Twitter account.  He's @SGgrc.  His DMs are open.  @SG, Steve Gibson; GRC, Gibson Research Corporation.  That's another good place to leave feedback for Mr. Gibson.  We have 64Kb audio and high-quality 720p video available at our site, TWiT.tv/sn.  You could download it there.  You could subscribe in your favorite podcast player.  There's even a YouTube channel dedicated to Security Now!.  Plenty of ways to get it.  The only thing I ask, get it every week.  You don't want to miss an episode.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#892

DATE:		October 11, 2022

TITLE:		Source Port Randomization

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-892.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a massive customer information leak from a surprising source.  Meta notes where their users are being harvested.  And in an industry first, Uber's CSO has been convicted.  We have more, much more, cryptocurrency industry turmoil.  A new appointee in the U.K. wants to drop their use of the GDPR.  The NSA is looking for next summer interns, IBM learns that incident responders are feeling quite stressed out, and Microsoft continues to fumble their Exchange Server response.  I have news of SpinRite and of my discovery of a lovely little Single Board Computer.  And after sharing some listener feedback, we're going to look at a recent mistake made in the Linux kernel that allowed its users to be tracking online.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Boy, he has a lot to talk about.  A new job offer from the NSA.  You won't believe the meme they chose. 	We'll also talk about Uber's CSO.  He's been convicted of a heinous crime.  You won't believe it when you hear it.  And then he's going to talk about his discovery of a lovely little single-board computer I know many of you are going to want to buy.  Plus a look at source port randomization.  It's a big show, and it's all ahead, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 892, recorded Tuesday, October 11th, 2022:  Source Port Randomization.



It's time for Security Now! with this guy right here, Steve Gibson, the star of our show.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  I've silenced my phone so we won't be interrupted by...



LEO:  No yabba-dabba-dos?



STEVE:  No yabba-dabba-dos.  I'm looking forward to the day when I have to shut those off because they're so annoying.



LEO:  Every five seconds, another SpinRite.



STEVE:  Not there yet, but I'm working in that direction.  I have some news on that front that I'll share later.  So we're Episode 892 for October 11th.  And I titled this one "Source Port Randomization," which is a subject we've spoken of often.  But it came up again as a consequence of a mistake that the authors of the Linux kernel recently fell into.  So I think that's going to be interesting.  We have, I know that we have a large Linux following among our listeners, the techie most of ours.  So we're going to look at first a massive customer information leak which arose from a surprising source.  Also Meta notes that they did some analysis to discover where their users' credentials are being most harvested.  And in a weird industry first, Uber's ex-CSO has been convicted of some interesting misbehavior.



LEO:  Ooh, yeah.  Oh, no kidding.



STEVE:  Yeah.



LEO:  We talked about that when it happened, I think.  When he got caught, anyway.



STEVE:  Yes, yes.  And that was two years ago that the allegations were made, and the indictment happened last week.  So we have more, much more - I mean, the outcome of the trial, rather, he was found guilty.  We'll talk about that.  We've got more, much more, cryptocurrency industry turmoil, which just...



LEO:  Nonstop.



STEVE:  Oh, my god, Leo.  But, I mean, and just it's like it's creative turmoil.  It's like, what?  We also have a new appointee in the U.K. a month ago who has decided that she wants to drop the U.K.'s use of the GDPR.  Oh, also the NSA is looking for next summer interns.  I'll provide information for our listeners who might be interested in signing up.  IBM has learned that incident responders are feeling quite stressed out.  And Microsoft continues to fumble their Exchange Server response to the most recent Exchange Server problems that we started talking about last week.  As I mentioned, I've got news of SpinRite.  And I'm going to share my discovery of a lovely little single-board computer, basically Steve's Dream SBC.  And then after sharing some listener feedback, as I said, we're going to look at a recent mistake made in the Linux kernel that allowed its users to be tracked online.



LEO:  Oh.  I thought you were going to talk about the recent mistake in the Linux kernel that would fry some users' monitors.  Did you see that?



STEVE:  No.



LEO:  Yeah, they were warning people not to download the new kernel.



STEVE:  Wow.



LEO:  I think they've fixed it.  But, yeah, because something, I mean, that's amazing that you could do something in software that would destroy hardware.



STEVE:  Back in the CRT days it was possible to mess up their H and V sync in a way that would actually damage the circuitry.



LEO:  Right.



STEVE:  But yeah, in an LCD world, that's really interesting.



LEO:  I'll look into it.  I'll find out what it is, and I'll let you know.  Steve Gibson has the Picture of the Week.



STEVE:  This just kind of cracked me up.  For those who are not seeing the show notes or the video, what we have is a Tesla, clearly recognizable by everyone, that apparently doesn't have much faith in its ability to find the next charging station.  And so I titled this "DIY Hybrid?"  Strapped to the back of it in what looks like sort of a permanent installation, it's got a gas-powered electric generator and a bunch of gas cans.  So I guess if the battery runs low, this thing being sort of a DIY hybrid, you'd just cruise off to the side of the road, gas up the generator, plug your car into itself, into this little caboose that it's got, and charge her back up.  And then you're ready to go again.  So anyway, not specifically anything about security, but I just thought this was kind of humorous.  So, yeah, interesting.



The first piece of our show was a tweet that came from our last week's topic source.  Remember Jacopo Tediosi.  He was one of the two Italian researchers who discovered the serious Akamai vulnerability.  Anyway, he knew about the podcast.  And he said:  "Thanks @SGgrc for talking about my Akamai vulnerability on the Security Now! podcast."  And he gave a link to it at TWiT.tv.



LEO:  I saw that tweet.  It was so cool.



STEVE:  Yeah, it was very cool.  And he said:  "The analysis and explanations you made were very accurate!"  So anyway, Jacopo, thank you for following up.



LEO:  He didn't call you out on mispronouncing his name, though.



STEVE:  And actually I replied to him politely, thanked him for his tweet, and said I hope I didn't mangle the pronunciation of your name too severely.  So, right.



LEO:  He's probably used to it.



STEVE:  Oh, goodness.  So it turns out that there's a non-security breach way, like a means, for a user of a cryptocurrency exchange to have their name, their account balance, and all of their transactions exposed to the public.  And that's if the currency exchange files for bankruptcy.  Whoops.



Something known as the Celsius Network cryptocurrency platform deliberately exposed the names and complete transaction histories of hundreds of thousands of its customers.  Okay, now, timeout.  As an aside, "hundreds of thousands of its customers"?  Leo, what most mystifies me is how these random also-ran startups acquire hundreds of thousands of customers.  What are people thinking?  Who are these people?  And, you know, it's just a mystery.



Anyway, the company filed a - get this - 14,532-page document, because of course lots of transactions for all of its hundreds of thousands of customers, thus requiring a 14,532-page document as part of its bankruptcy proceedings the week before last that contained the names and recent transactions of every user on its platform.  The judge in this case, the bankruptcy judge, allowed the company to redact the document, but only their customers' physical and email addresses were allowed to be removed because the rest of the information was required in their disclosure during their regular bankruptcy procedures and proceedings.



So the document, for anyone who's interested, is available via PACER and other legal document portals.  So not so private if the cryptocurrency platform that you're using goes belly-up and chooses this means of shutting themselves down.  Just something to keep in mind.



A posting last Friday by two security-focused employees of Meta, you know, Facebook's parent, disclosed the results of a recent search through the Apple and Google app stores.  They explained that they had identified more than 400 malicious Android and iOS apps targeting Facebook's users which were being used to steal specifically, I mean, the reason these apps were created was to steal their Facebook login credentials.  They reported their findings to Apple and Google and have asked the users they identified to change their passwords since their credentials have almost certainly been compromised.



Now, I thought that the nature of the come-ons to entice the downloads of these apps was interesting.  So they were, first of all, majority were photo editors, including those that claim to allow you to "turn yourself into a cartoon."  Those apparently are very popular among Facebook users.  Also we had VPNs claiming to boost browsing speed or grant access to blocked content or websites.  In other words, like solving a problem that people have.  Oh, this will make your browsing twice as fast, whatever.  Probably not true, but it got them to download.  Also phone utilities such as flashlight apps that claim to brighten your phone's flashlight.  Yes, get more light out of your flashlight if you download this phone utility.



Then we had mobile games which were falsely promising higher quality 3D graphics.  Health and lifestyle apps, you know, horoscopes and fitness trackers.  Business or ad management apps claiming to provide hidden or unauthorized features not found in official apps which are being offered by the tech platforms.  So interestingly, by far the majority at nearly half, 42.6%, of those 400-plus apps were all photo editors.  Very popular.  Next, dropping down to 15.4%, were the business utilities, then the phone utilities at 14.1%, and games at 11.7%, and the others making up the result.



So, you know, standard advice applies.  First of all, try hard, I mean, really try to avoid downloading just every tasty-looking goody that you see.  We've said this before, and I think it's still true.  There is a very small probability, you know, given the vast number of good apps that are out there, the probability is diminishingly small that something next, something more that you download will be malicious.  But the probability is not zero.  So if you can, and you care about not having malware running in your device, don't do it.



And don't be too quick to click the download link.  Do as much research about the app and its reputation as possible.  And I would suggest you do that off the platform.  That is, go elsewhere.  Don't rely on that in-place reputation because the other thing these guys are known to do is to load themselves up with faked five-star ratings and thumbs up and things.  So look elsewhere for other sources of reputation.  So, you know, again, be careful.  Be cautious.  But just know, as we've noted before, that some percentage of these things in the app stores, much as Apple and Google both are trying diligently to keep these things clean and scrubbed, they exist for a while on this app platform.



Okay.  Uber's former CSO, their Chief Security Officer, by the name of Joe Sullivan, was found guilty at trial due to his actions following a 2016 data breach at Uber.  And I'm wording this carefully because there was actually some misreporting about this in the press.  It's like, the implication was, for reporters who were not being careful, that Joe was responsible for the breach.  Not at all the case.  Right?  I mean, he's a C-Suite guy.  Those guys don't get their hands dirty.  Anyway, we'll get to that in a second.



Reading from a statement made on August 20th, 2020 - so as I said, two years ago when these charges were filed in the Northern District of California - they put out a statement about the fact that this was being done.  They said:  "The complaint describes how Sullivan played a pivotal role in responding to Federal Trade Commission (FTC) inquiries about Uber's cybersecurity.  Uber had been hacked in September 2014" - okay, so that's a different - that's two years before, a different instance - "had been hacked in September 2014, and the FTC was gathering information about that 2014 breach.  The FTC demanded responses to written questions and required Uber to designate an officer to provide testimony under oath on a variety of topics.



"Sullivan assisted in the preparation of Uber's responses to the written questions and was designated to provide sworn testimony on a variety of issues.  On November 14th, 2016" - so near the end of 2016 - "approximately 10 days after providing his testimony to the FTC, Sullivan received an email from a hacker informing him that Uber had been breached again.  Sullivan's team was able to confirm the breach within 24 hours of his receipt of the email.



"Rather than report the 2016 breach, Sullivan allegedly took deliberate steps to prevent" - and this is allegedly because it's two years ago.  Now it's been found to be true at trial - "allegedly took deliberate steps to prevent knowledge of the breach from reaching the FTC."  So he tried to bury this.  "For example," they said, "Sullivan sought to pay the hackers off by funneling the payoff through a bug bounty program.  Uber paid the hackers $100,000 in Bitcoin in December of 2016, despite the fact that the hackers refused to provide their true names.



"In addition, Sullivan sought to have the hackers sign non-disclosure agreements.  The agreements contained a false representation that the hackers did not take or store any data, when in fact they had.  When an Uber employee asked Sullivan about this false promise, which was in the nondisclosure, Sullivan insisted that the language stay in the nondisclosure agreements.  Moreover, after Uber personnel were able to identify two of the individuals responsible for the breach, Sullivan arranged for the hackers to sign fresh copies of the non-disclosure agreements..."



LEO:  I just love that piece.  Don't tell anybody.



STEVE:  I know.  Yeah, don't tell anybody what happened.  And oh, by the way, now that we know who you are, we want you to actually execute the nondisclosure agreements which would be binding under your true names.



LEO:  And they gave them a bug bounty.



STEVE:  Yes, yes, yeah.  We're just going to say that you found a problem, not that you actually attacked us using it.



LEO:  Appalling.



STEVE:  Yes.  So "The new agreements retained the false condition that no data had been obtained.  Uber's new management ultimately discovered the truth and disclosed the breach publicly, and to the FTC nearly a year later, in November of 2017.  Since that time, Uber has responded to additional government inquiries."  So all of that was proper.



"The criminal complaint against Sullivan alleges Sullivan deceived Uber's new management team about the 2016 breach.  Specifically, Sullivan failed to provide the new management team with critical details about the breach.  In August of 2017, Uber named a new Chief Executive Officer, a new CEO.  In September of 2017, Sullivan briefed Uber's new CEO about the 2016 incident by email.  Sullivan asked his team to prepare a summary of the incident.  But after he received their draft summary, he edited it.  His edits removed details about the data that the hackers had taken and falsely stated that payment had been made only after the hackers had been identified."  That wasn't the case.



The two hackers identified by Uber were prosecuted in the Northern District of California.  Both pleaded guilty on October 30th, 2019, to computer fraud conspiracy charges, and now await sentencing.  The criminal complaint makes clear that "both hackers chose to target and successfully hack other technology companies and their users' data after Sullivan failed to bring the Uber breach to the attention of law enforcement."  In other words, by not dealing with law enforcement forthrightly, the hackers, who had been identified, continued to roam free to hack and damage other companies as a direct consequence of Sullivan's actions of covering all this up.



So at trial Sullivan was found guilty of lying to authorities and obstruction of justice.  Those were the charges, lying to authorities and obstruction of justice.  Nothing to do directly with Uber being hacked.  It's like, you know, we know that kind of thing happens.  The trial, however, was a landmark case, being the first time a Chief Security Officer faced criminal charges, indirectly at least, relating to a security breach.  Though it was only, you know, obviously, indirectly about the breach itself.  Joe's big mistake was his attempt to cover-up and mislead investigators that ultimately landed him, as we know now, in some very hot water.  Interestingly, Joe was once a prosecutor in the same office that had charged him.



LEO:  I didn't know that.



STEVE:  Yeah.



LEO:  He should have known better.



STEVE:  Well, I was thinking that maybe he thought he knew how to finesse the system.  Right, like having once worked there, he figured, hey, I know how to get around this.



LEO:  Oh, gee.



STEVE:  So he now faces up to eight years in prison and up to half a million dollars in fines, which will be determined at his upcoming sentencing hearing.  What has never been made clear in the reporting that I've seen is why he did this.  He was a C-level executive for a major corporation, Uber. Guys at that level aren't pulling wires and getting their hands dirty.  They attend meetings, and golf.  So it was almost certainly not directly Joe's fault that somewhere in a back room two attackers somehow crawled into Uber's network.  What I wonder is whether he had a big hunk of Uber stock that he worried would collapse in value if the news of this got out.  If so, perhaps he believed that he could cover the whole thing up from the top to protect Uber's market value.  In any event, I imagine that he regrets that decision now.



LEO:  Can't spend that stock in prison.  No, you can't.



STEVE:  No.  So more cryptocurrency chaos.  I believe that this podcast's listeners would be well served for me to periodically note the ongoing chaos that exists within the cryptocurrency world.  It's not my position to advise anyone of anything.  But being armed with realistic viewpoints can only be valuable.



To that end, the news is that the multi-cryptocurrency exchange platform Binance was hacked.  Binance has paused its Binance Smart Chain, BSC as they call it, blockchain bridge after a threat actor used an exploit there to generate and steal 2 million Binance coins.  The abbreviation for that currency is BNB.  They are currently worth around $560 million.  Now, the thieves were unable to make off with all $560 million because Binance reacted quickly to what they discovered.  But the bad guys still absconded with 20% of the $560 million in illegitimately created funds.  So $112 million worth of the Binance coins.  So not bad for a day's work.



LEO:  We were talking about this on Sunday on TWiT.  And apparently the bridge software, which is what allows you to move crypto from one place to another...



STEVE:  Chain, right.



LEO:  ...is a very common source of hacks.  And this is the fourth or fifth massive hack of a crypto bridge of some kind in the last couple of years.  It's a huge vulnerability.  And of course that's where the money is.  It's like you can tap into the oil pipeline and just say, yeah, give me some of that.



STEVE:  Yeah.



LEO:  You know, it's - wow.



STEVE:  Okay.  And so while we're on the subject of bad ideas, I'll also note that the Zcash blockchain has been subjected to a spam attack.  Yes, spam isn't just for email anymore.  This was done by creating bloated but inexpensive "shielded transactions" on the Zcash blockchain.  And as a consequence of this attack, which has been underway since June, the size of the Zcash blockchain has more than tripled to over 100GB.  As the Zcash blockchain has grown huge, purely as a result of bogus transactions...



LEO:  They have to store every one of these tiny transactions.



STEVE:  Yes.  The cryptocurrency experts now expect Zcash node servers, which must retain a full local copy of the entire blockchain, to start failing due to memory shortage.



So, okay.  All of this points - the only way that you can regard this is to an extremely immature technology coupled with a gold rush attitude.  Recall that in the actual California Gold Rush, between 1848 and 1855, with very few exceptions, the only people who made money were those who were selling the gold digging, panning, and sluicing supplies to the hopeful miners.  It wasn't those who were panning for gold.  It was the people who sold them their pans.



LEO:  No, in fact I saw just the other day at auction a pair of Levis found in an old coal mine from the 1880s sold for $76,000.  So they're still making money.



STEVE:  Still making money.  And, you know, you can't get graphics cards anymore, right, because all of the mining rigs have sucked up all the GPUs everywhere.



LEO:  Well, that's the problem is now you can because there's no more money to be made because of proof of work, proof of stake. 



STEVE:  Right.



LEO:  So now you can get a lot of highly used GPUs.  They're flooding the market.



STEVE:  I think this would be a good time to take a break, and then we're going to talk about the U.K.'s plans to drop out of the GDPR.



LEO:  And actually, you know, in conjunction with NFTs, I've been saying this for a while, the companies that make money in NFTs are the people minting them.  They're collecting the...



STEVE:  Well, look at Kevin.



LEO:  Yeah.  They're collecting the gas fees.  Well, Kevin made $50 million, his proof collective.  And then he raised another 50 million because it's such a good...



STEVE:  Leo, where?  Where is this money?  Who are these people?  Is there something [crosstalk] something somewhere.



LEO:  I hope, I sincerely hope that for the most part they're bitcoin bros.  They're crypto millionaires and billionaires who are reinvesting.



STEVE:  Like the Winkeldingies or whoever they were down in...



LEO:  The Winkeldingies.  And I pray it's them and not some poor working stiff who says, yeah, my stocks aren't doing so well.  Maybe I'll get into this crypto thing.  But unfortunately, you know, because Robinhood and all these other easy trading apps sell crypto now, I suspect that a lot of this is coming from people who can't afford the losses.  And it's very sad, you know.  Of course my stock market portfolio has tumbled even more.  So maybe I should buy some bitcoin.  A little doge.  Who knows?



STEVE:  You know, I figured I was probably an old relic.  So I just looked up the definition.  It says "An object surviving from an earlier time, especially one of historical or sentimental interest."



LEO:  Bingo.



STEVE:  And I think that works.



LEO:  Bingo.



STEVE:  I'm the old relic.



LEO:  I'm getting ready for - I love the Advent of Code coding competition in December.  I'm getting ready for it.  I'm trying to do it in Lisp.  And I'm sitting there...



STEVE:  Wait, I thought you were going to do a new language.



LEO:  I was looking at Julia, but you convinced me anything that starts, indexes arrays at one...



STEVE:  Oh, yeah.



LEO:  No, unh-unh.



STEVE:  I know.



LEO:  I really love Lisp.  So I'm on Day 7 of the first year, 2015, and I need to do bitwise operations.  And I'm thinking, if I were Steve, this would be easy.  He lives in the pits.  But now I've got integers, and I've got to figure out - and actually it's not a problem except when I get to the twos complement representation.  I'm getting numbers going negative, and I've got to figure out a way to just ignore that.



STEVE:  There are.



LEO:  You, you wouldn't have to think about it.  Yeah, of course there are.



STEVE:  There are an amazing number of really cool bitwise hacks.



LEO:  I know.  In fact, I have a book that's almost entirely bitwise hacks.  I was going to ask you this because I want to give it to you.  Have you ever read "Hacker's Delight"?



STEVE:  No.



LEO:  Okay.  Don't buy it.  I am going to send it to you.



STEVE:  Cool.  Cool.



LEO:  It is a classic, and it is almost entirely, like, weird bitwise hacks.  



STEVE:  And like sort of edge cases that turned out to be useful.



LEO:  Yeah, that's the whole idea is once you know this, the idiom, you'll use it all the time.  And you of all people because you work in assembler, this is going to be like, oh, yeah.  You probably know 90% of them.  But it's a good book.  I'm going to send it to you.



STEVE:  That's fun.  Cool.  Thank you.



LEO:  You're welcome.



STEVE:  Okay.  So I'm not sure whether this is good or bad, though I'm leaning heavily toward bad, for a reason I'll explain.  Last Monday, Michelle Donelan, the U.K. Secretary of State for Digital, Culture, Media and Sport - that's literally her title, the U.K. Secretary of State for Digital, Culture, Media and Sport.  Luckily, Leo, for your sake they didn't say "cyber."



LEO:  Sport's bad enough.



STEVE:  Sport's bad enough.  I agree.  That one, I said wait a minute, did I...



LEO:  We don't have in this country, we don't have a Minister of Sport.  But a surprisingly large number of countries do.  And I watched, reason I know is I watch the Formula I races.



STEVE:  Don't they understand it's plural?  It's supposed to be sports.



LEO:  And they call it "maths."  Maths and sport.



STEVE:  It's not like I'm playing "card."  I'm playing "cards."



LEO:  But it's maths, plural.  So go figure.  What is it...



STEVE:  Oh, that is true.



LEO:  ...Winston Churchill said is America and England, two countries separated by the same language.



STEVE:  So she was appointed to her position of Digital, Culture, Media and Sport about a month ago.  On this last Monday she announced plans for the U.K. to drop the EU's GDPR in favor of designing their own...



LEO:  What?



STEVE:  I know, their own new data protection system.  And this is the point where I groan.  Michelle was speaking at the Conservative Party Conference in Birmingham where she said that the U.K. government will look to pass new legislation inspired by data protection laws used in Israel, Japan, South Korea, Canada, and New Zealand.



Now, on the one hand, that sounds maybe better than the GDPR.  But the concern is that we only have the one single global Internet.  And that was the whole point of the Internet in the first place.  That's what makes it so useful and amazing.  But now governments are getting into the act of deciding how the Internet should uniquely treat each of their own precious citizens, even if that differs from how the Internet treats everyone else.  Governments want to have borders, but the Internet was designed to ignore them.  And so basically we have a clash of fundamental principles here.  And it's going to be a mess, Leo.  And so my concern about the U.K. leaving the GDPR is, okay, now we're going to have the UKBR or UKGDP or who knows what.  But wow, you know, one more mess to deal with.



Okay.  This seems cool.  Rob Joyce is the Director of Cybersecurity at - his Twitter handle is @NSAgov.  He recently tweeted that the NSA is looking, now looking, for next summer interns.  He wrote:  "It's never too early to make summer plans.  @NSACyber 2023 Summer Internships are open.  CompSci" - he's got a number for it - "CompSci 1191813.  Cybersecurity 1191816.  And Engineering, 1191817."  He says, "Apply at intelligencecareers.gov/NSA.  Use the numbers above.  Find your passion.  Hurry, applications close on Halloween."  So...



LEO:  Appropriate.



STEVE:  We're in, yeah, we're in the Halloween month.  So through the rest of this month, applications are open to apply to the NSA for a summer internship.



LEO:  You don't think this is a joke?



STEVE:  No, I checked.  I mean, the picture is weird.  Maybe this is how somebody who is not at all cool tries to look like they're cool, I think.



LEO:  It's a little creepy.  It's a little weird.



STEVE:  It is right from the Twitter feed.  If you go to Twitter and put in, what's his handle again, it's @nsacyber.  No, sorry, @nsagov. 



LEO:  @NSA_CSDirector is the Twitter handle.  But you could search for Rob Joyce probably.



STEVE:  Oh, right.  Right, right, right.  And actually I did that, and it came right up.  So it's like that's actually what he tweeted.



LEO:  Wow.  It reminds me, you nailed it, it's somebody who is not cool thinking, what would the kids do?



STEVE:  Exactly. 



LEO:  What's the meme I could post?  And then, you know, it does remind me a little bit of "Goodwill Hunting" and the very famous scene where they ask Matt Damon to apply, I can't remember if it's - I think it is the NSA.  If you haven't watched that, that would be a good response.



STEVE:  Oh, you kidding?  It's one of my favorite movies.  It's a great movie.



LEO:  Yeah.  Where he goes - he does a great monologue about, "Well, I'll tell you why I am not going to work for the NSA."  It's really good.  I'm sure, I tell you what, if you read the thread, the responses for this tweet, it will be in it.  I almost am certain.  Anyway, that's great.



STEVE:  Okay.  So IBM did a survey.  We've talked a lot about the job opportunities available across the security industry.  There are many.  They are plentiful.  And they show no sign of diminishing.  I think the stuff we talk about, the needs of the industries we describe are only growing greater.  But they can be demanding, and it can interfere with other life priorities.  IBM recently conducted a survey of 1,100 professional cyber incident responders.  Here are the seven takeaways from the survey.



First, Cybersecurity Incident Responders said that the sense of duty to help and protect others and the businesses was by far the most influential factor attracting them to the profession.  Continuous opportunity to learn and being rooted in problem solving followed as the most influential factors.  So people who want to help.  That's cool.



At the same time, number two, "sense of responsibility toward their team/client" and "managing stakeholder expectations" were ranked as the most stressful aspects of responding to cyber incidents.  Around half of the 1,100 selected these among their top three stressors.  So they were stressed by the sense of responsibility toward their team or client, and managing stakeholder expectations, you know, like solving the problem for their bosses.



Third takeaway, according to 48% of responders, the average incident response engagement is two to four weeks.  And nearly 30% say an incident response engagement lasts more than four weeks on average.  The overwhelming majority states that it's not uncommon to be assigned to respond to two or more incidents that overlap.



Fourth takeaway is that the first three days of responding to an attack are seen as the most stressful.  Additionally, more than a third say they are working more than 12 hours a day during the most stressful period of the engagement.



The fifth takeaway is that 81%, so four out of five, a little over four out of five of Cybersecurity Incident Responders think the rise of ransomware - no surprise - has exacerbated the stress/psychological demands required during a cybersecurity incident response.  Right?  Because their enterprise is frozen and is under threat.



Sixth takeaway, two thirds, 67% of Cybersecurity Incident Responders said they experience stress and anxiety in their daily lives as a result of responding to an incident.  And finally, nearly 65%, again, two thirds of Cybersecurity Incident Responders have sought mental health assistance as a result of responding to cybersecurity incidents.



LEO:  Wow.  Holy cow.  Wow.



STEVE:  Yeah.  To that end, the majority of respondents, 84% did also say that they do have access to adequate mental health support resources.  So they're able to get help.  But, I mean, they're under tremendous pressure, probably not chronically, but acutely, when something happens.  So I think this suggests two things.  First, cybersecurity incident response may not be for everyone.  You know, it's probably, you know, think about your personality type.  Do you like adrenalin?  Are your adrenals in good shape and providing you with what you need?  I think seriously that should be a consideration.



LEO:  It must be high pressure.  I mean, very intense when it happens.  Probably a lot of time sitting around, and then all of a sudden boom.



STEVE:  Yes.  Yes.  You know, imagine something gets into your network.  And it's doing bad stuff.  And it's up to you, I mean, like it's not - it's you, not anybody else.  And like alarms are going off, and people's computers are crashing, and it's like, yeah, that's not when you want to slip the blood pressure cuff on your arm to see how you're doing.



The other thought I had was that, although you'll certainly want to be a salaried employee, if there's any way to work in bonuses to your contract, for when the job does disrupt your life, that should be a consideration, too.  Being the only one left at work, working all night, while everyone else is home laughing and sleeping, is much easier if you know that your special contribution is being valued with some additional compensation.  And if you have a significant other in your life, it can make it easier to explain to them, honey, I'm sorry, but I can't come home.  So anyway, I thought that was interesting.  It is, you know, we've only talked about all the opportunities so far.  And this survey of 1,100 people says, yeah, you know, there's lots of opportunity, but it's not a cakewalk all the time if something nasty crawls into your enterprise that you're responsible for.



LEO:  It's probably a lot like being a first responder, a fire fighter, an EMT, a police officer.  It's, you know, when things go haywire, they go.  And you have to be there.  And you probably need a certain kind of constitution; right?



STEVE:  I think that's the case.  I think that's the case.



LEO:  Yeah, the right stuff.  Just cool under pressure.



STEVE:  Right.  And I'm sure there are people who, like, who thrive on the idea of that much need being piled on them when their shoulders are able to handle the burden.



LEO:  Yeah, we have a picture of that person here.  The NSA Director's tweet, yeah.



STEVE:  Yeah.  Okay.  So speaking of being stressed out, something's going on over at Microsoft, and it's not good.  The topic is the status of Microsoft's mitigations for that pair of zero-day Exchange Server vulnerabilities we discussed last week.  Those were the new pair discovered while being used in the wild, exploited in the wild, in the networks of clients of that Vietnamese cybersecurity firm GTSC.



First, in updating myself for today's podcast, I checked to see whether patches for these two new bad problems were available.  That would be the optimal answer; right?  Get it fixed.  But after at least a week and a half, and it turns out a lot more than that, as we'll see in a second, the answer to that is no.  No emergency patch for Exchange Server so far.



Then, since there was news last week that the initial mitigations proposed by Microsoft had immediately been bypassed, as I noted last week, I wasn't quite sure of some of the language, but Bleeping Computer said - they confirmed, yup, that original mitigation proposed by Microsoft has been bypassed.  I went to see what Microsoft had done since then, and they really appear to be chasing their tail.  They updated their guidance for scripts for IIS mitigation on October 4th, 5th, 6th, 7th, and 8th.  Each time they're correcting typos or making small tweaks to the script, apparently trying to get it right.  It's like, one of the comments is "remove the space that was unnecessary."



LEO:  That's the least we can do.  It's so bad.



STEVE:  Oh, god.  So nothing about this response feels like the A team has been brought in.  And then we learn that Microsoft has been aware of this problem for much longer than was previously known.  They were, in air quotes, "investigating it" after becoming aware of it back in August.  In their posting titled "Analyzing attacks using the Exchange vulnerabilities CVE-2022-41040 and 41082," Microsoft wrote:  "MSTIC observed activity related to a single activity group in August 2022 that achieved initial access and compromised Exchange servers by chaining 2022-41040 and 2022-41082 in a small number of targeted attacks."  This is in August.



"These attacks," they wrote, "installed the Chopper web shell to facilitate hands-on-keyboard access, which the attackers used to perform Active Directory reconnaissance and data exfiltration."  Oh?  So apparently nothing to worry about?  It's like, what?  In August.  They said:  "Microsoft observed these attacks in fewer than 10 organizations globally.  MSTIC assesses with medium confidence that the single activity group is likely to be a state-sponsored organization."



Then they said:  "Microsoft researchers were investigating these attacks to determine if there was a new exploitation vector in Exchange involved" - okay, make that yes - "when," they said, "the Zero Day Initiative (ZDI) disclosed CVE-2022-41040 and 2022-41082 to Microsoft Security Response Center (MSRC) in September of 2022."



So, gee, look at that.  Exchange Server is being attacked.  Hmm.  What's for lunch?  Unbelievable.  They were investigating.  While attacks were underway, 10 organizations they had identified, and now we're, what, we don't know when in August, so somewhere between 1.5 and 2.5 months downstream of this, of them seeing that Exchange Server is being exploited by a remote execution exploit which is taking companies over and allowing bad guys to perform reconnaissance on enterprises' Active Directory servers.  And Microsoft is updating their advice after this became public, every day removing extraneous spaces from their scripts.  Not impressive.



In SpinRite news, I have finished all of the redesign, and SpinRite is working as far as I know.  But that knowledge doesn't yet go very far.  So now I start the final work of inducing known data errors and watching SpinRite perform its sector-by-sector data recovery.  That's what I'll be working on tonight and subsequently until I've demonstrated to myself that SpinRite is in fact ready.



LEO:  That's interesting.  How do you make data errors happen?



STEVE:  It turns out in older drives, and I have a 2TB, I have three actually 2TB Seagates and a bunch of older Maxtors.  You had the ability there, there was a command called READ LONG where you told the drive don't bother with error correction, just give me the raw data.  It's called a "long read" because it's the data plus the error correction code which is tagged on at the end.  And that facility is one of the ways that SpinRite is able to perform data recovery, even on sectors that the drive says are not good.  Well, even though they're not good, there's still something there.  SpinRite is able to say, give me what you've got, and let me worry about it.  And that's where this Dynastat, the Dynamic Statistics comes in.



LEO:  So do you go to a known bad sector and then ask to do a long read?



STEVE:  No.  Because there's the complement command, WRITE LONG.



LEO:  Ah.  So you can screw it up.



STEVE:  Where I'm able to - yes.  Exactly.  I'm able to induce varying length bit errors and cause the drive to do what it's going to do when it encounters it for the first time.  And so that allows me to deliberately poison sectors in various ways and then, as if that hadn't happened, have SpinRite come along and watch what it does in order to recover the data.  So it's very cool.  Unfortunately, it's been - those things have been removed from newer drives.  But I've got plenty of older drives where it's still feasible.  And even a 2TB drive.  So I've got lots of ability to do that.



LEO:  That's really interesting.  I didn't know that.



STEVE:  Yeah, it's really neat.  Okay.  So once I know that it is doing what I want it to do, I'll release that to GRC's newsgroup gang, and we'll find, I'm sure, the various things that I've missed.  Cosmetic things, logging things, who knows what.  I'll get those fixed, and we'll move SpinRite from Alpha to Beta.  When we're there, anyone who owns SpinRite will be able to download the DOS executable that I've been developing, and which everyone's been testing.  Since I won't yet have it packaged as a turnkey Windows app, you'll need to use GRC's InitDisk or ReadSpeed or arrange to boot your own DOS however you want to.  And then you'll be able to run that DOS executable, and it would be the real SpinRite 6.1, just as it's finally going to be.



Something else happened last week that was interesting.  Although we won't get to high-speed native USB support for SpinRite until somewhere in v7, I'm thinking 7.1, only because I don't want to delay 7.0, which will be the first SpinRite ever to be bootable over UEFI.  I want to get that out as quickly as possible.  And there's no reason to hold it because 7.1 will be free for everybody anyway.  So I designed SpinRite 6.1, today's forthcoming SpinRite, to work with any size USB drive through the motherboard's BIOS as it always has, but now with no size limitations, if the motherboard supports it, and many do.



But it occurred to me that I had never explicitly asked any of our testers to try attaching a huge drive, larger than 2.2TB, which is the largest drive that's addressable with 32 bits.  You've got to have more than 32 bits to go beyond 2.2TB.  That's why the old-style master boot record, the MBR, that only has 32-bit size fields in its definition, which is why you can't use an MBR on a drive greater than 2.2TB.  Anyway, I had never asked them to try to attach a huge external drive to a USB port to confirm whether SpinRite sees the large drive and can indeed work with it.  That is, today, the SpinRite 6.1 we're going to be getting.  And it turns out we learned that it does, and it can.



In the show notes, for any prospective SpinRite 6.1 owners, I've got some screen shots which our testers, which two of our testers provided.  And I'm looking at them.  And thank you, Leo, for them being onscreen now.  Highlighted at the bottom of a list of different drives on this one person's system, we see a 4.0TB drive, which will be interfaced through the BIOS.  You notice the first five drives are AHCI, so that's SpinRite's direct hardware access to the AHCI chip itself, which it now knows how to talk to.  So, but then the lower four drives have been connected to USB ports through the BIOS.  So a 4TB drive.  And SpinRite through the BIOS will be able to scan that drive in 30.1 hours.



Then in the next slide he's run the benchmark, the full benchmark on that drive, which allows it to perform a finer grained performance analysis.  And so SpinRite's estimation was revised to 29.45 hours.  And we can see the various speeds at which SpinRite can talk to that 4TB drive.  So while 30 hours is not fast, it used to be 30 months for a drive this size.



LEO:  That's a lot faster.  Wow.



STEVE:  We're doing way, way better, way better than we used to.  And in fact, on the next screen, on the next page is somebody else who provided a snapshot.  They show a 3TB drive that SpinRite estimates it will scan in 10.1 hours.  So that's way faster.  So the lesson there is it is a function of the BIOS.  Both BIOSes allow SpinRite to see a drive larger than 2.2TB, in one case 4TB and the other 3TB.  But this second one can do a 3TB drive in 10 hours, which is a lot faster than a 4TB drive in 30 hours.  So your speed will vary.  And in fact we do see on that second, that second slide, he had a 1TB drive attached through AHCI.



So it was a SATA drive where we see it doing 1TB in 30 minutes.  And that exactly corresponds to my estimate.  Remember that I expected that we would - my newer estimation was that SpinRite would probably be able to do 2TB per hour.  And that's what we're seeing consistently, 2TB per hour or, in this case, here it is showing 1TB in half an hour.  So SpinRite 6.1 will finally be easy and practical to use again on today's very large drives.  And even before we get to 7, SpinRite 7 and 7.1, where we've got hardware support for USB, that's where we'll be able to run USB-3 at the same speed as SATA because it is as fast.  At that point external drives will be able to run as fast as internal drives.  Anyway, I'm having a ball.



LEO:  Yeah, that's a huge improvement.  And it was a disadvantage using SpinRite on a giant drive was it would take forever.



STEVE:  Oh, yeah, it wasn't practical.



LEO:  But a day, a day is not, I mean, a day and a half is totally doable, totally.



STEVE:  Yeah, and you can certainly like run it over the weekend, if you had to, yeah.



LEO:  Yeah, exactly, yeah.



STEVE:  Yeah.  Yeah, we're getting there.



LEO:  Yay.



STEVE:  Okay.  Now, I want to take a moment to talk about a beautiful little affordable $120 plus shipping Single-Board Computer that I am starting to use as of Sunday, last Sunday, which I'll be using for SpinRite's development going forward.  It's called ZimaBoard, Z-I-M-A-B-O-A-R-D.  And in many ways it's the perfect little platform for SpinRite.  I'll get to that in a second.  To get SpinRite to the point where it is today, which is its ability to talk directly to any and all PC hardware owned by every single one of our hundreds of SpinRite development testers, and I should note we currently have 367 registered testers in GRC's GitLab instance.  So that's the population of people who have been testing SpinRite so far.



I have been gladly purchasing enumerable old motherboards and drives from eBay.  This has been going on for the last year.  When I've been unable to duplicate some obscure problem that any of our hundreds of testers were experiencing out in the field, buying what they had was often the only way to get to the bottom of some really bizarre behavior.  So that's what I would do.  But that's all now behind us, at least until SpinRite starts being used by its entire owner base.  I do fully expect that I will encounter some new mysteries, and I will deal with those as they come along.  But that's, you know, that's the nature of bypassing the BIOS.  Now that we're talking to the hardware, there's obscure hardware out there.  But, boy, I've seen a lot of it.  I think it's clear that we've reached the 99.999% point.



So it's time for the next stage.  What I wanted going forward was a completely silent testing platform.  And this little ZimaBoard looks perfect for that.  No more incessant whirring fan noise while I'm trying to focus.  The ZimaBoard is fanless, with a custom heat sink fin design and just the right number of ports and expandability.  It started out on Kickstarter, where it was 4,905% overfunded.  In other words, more than...



LEO:  There's a market.



STEVE:  More than 49, yes, more than 49 times the number of project backers that they were hoping for.  People went nuts over it.  And it's now a going commercial concern.  Through the years the recurring question that we've been asked over and over is what GRC would recommend as a perfect PC platform for running SpinRite on a drive.



LEO:  Yeah.



STEVE:  In lieu of dedicating someone's main machine to that task.



LEO:  A desktop, yeah.  And I've found a few desktops, the ones I own, which won't work at all with SpinRite.



STEVE:  Right.



LEO:  So the UEFI, I guess.



STEVE:  Yes, and it won't be until 7 that we're able to run there.



LEO:  So this is an answer to a question I've been meaning to ask you.  This is great.  If you were going to do, if you want to run SpinRite and, you know, you do it enough, it's worth spending 119 bucks to get a little machine to do it.



STEVE:  Yes.  And many people also have inventories of drives, like drives that they've taken out of service.  And so this allows you to run SpinRite at absolute full-on speed without tying up any of your other resources, as you said, for $119.



LEO:  Do you put Windows on it, or you just run FreeDOS on it?



STEVE:  So, I will get there.



LEO:  Okay.



STEVE:  So years ago, when I was writing the TechTalk column for InfoWorld magazine, I stumbled upon a wonderful motherboard, the ultimate keyboard, an RLL controller, and MFM drives that worked perfectly under RLL encoding.  So I conceived of something I called "Steve's Dream Machine."  It was a hit with my column's readers.  A PC supplier, Northgate Computer Systems, took up the idea of purchasing and bundling all the components and offering them as "Steve's Dream Machine."  What I think I've found here with this ZimaBoard is Steve's Dream SBC - Single-Board Computer.



It is 100% Intel chipset with the exception of its dual gig network adapter, which is a Realtek 8168 chip.  Now, it turns out that's perfect for my development needs since I have DOS network drivers for that chip.  It has a pair of 6Gb SATA 3 connectors with a cable to provide power for one drive.  But for $4 you can get a dual power cable.  It has a pair of USB 3.0 ports.  So SpinRite will be able to run drives attached to either SATA or USB 3.  And it has a single PCIe x 4 connector for the expansion of anything else.  That could be a PCIe to IDE adapter, if SpinRite needed to repair any older IDE drives; or an NVMe adapter, if SpinRite needed to be run on NVMe drives once they are supported, and they will be under v7.  It has built-in video through a mini DisplayPort which can do 4K video at 60Hz.



And critically, the ZimaBoard offers both UEFI and traditional BIOS support.  It has a very comfortable Award BIOS with all of the bells and whistles, you know, drive boot order and so forth, everything that old DOS hands are hoping to see, so that SpinRite will be able to boot FreeDOS and run without trouble.  It could boot from an attached USB thumb drive, and I've done that, if you wanted to leave the Debian-derived CasaOS Linux that's shipped with the board in place; or FreeDOS and SpinRite could be installed onto the board's built-in 16GB eMMC drive.  That's what I'll be doing.  Either way, I'll be able to use the same platform for SpinRite's future development under UEFI.  So it's perfect for both now and for what's next.



There are three ZimaBoard models which vary in speed and size, but the smallest of the three is what I purchased because it's enough for doing stuff with DOS.  I have two of them now, one for each of my locations.  As I mentioned, the smallest of the three contains a 16GB eMMC drive which is preloaded with a Debian Linux variant which they call CasaOS.  The board is broadly compatible, able to run any Intel OS, Linux, Windows, pfSense, OpenWRT, NAS software, and anything else.  And they sort of have it targeted at your own cloud or multi-drive NASes and so forth.



If you go to ZimaBoard.com, if you click on the "Order Now" button on the home page, and then again on the page that comes up, you'll get to the place where you set the quantity and the model number you want.  If you scroll down that third page to the bottom, you'll find a "Buy One Get One Free" offer that explains, well, it's not another free ZimaBoard, but it's a free power adapter.  They say:  "Buy ZimaBoard and get a free 12V/3A Power Adapter."



LEO:  Oh, that's what - you need that; right?  Yeah.



STEVE:  Which you need that anyway, and that saves you 12 or $15 or something.  So that's what I would recommend.  There's a 10% off discount coupon available, but you probably can't use both.  As I mentioned, the ZimaBoard comes with cabling to supply power to a single SATA drive.  But there's an optional dual SATA power cabling for $4, actually it's $3.90, that you may want if you intend to power two SATA drives from the SBC.  And that's also what I'm doing.



So anyway, I now have a terrific answer to the often-asked question, "What does GRC recommend for running SpinRite standalone."  I don't think you can do better than that.  I mean, I've been using it.  It's just beautiful.  You'll need a mini display port cable to a display port monitor.  And then the way they have it, the way they suggest you set it up is you plug it into your router, and then you use a browser to talk to it.  So I guess it boots up with a, you know, it boots up this Debian Linux variant with a web server running and waiting to be connected, and then brings up some sort of a UI.  I didn't do any of that.  I just blew it off and used fdisk to zero the partition and made a bootable DOS because that's what I'll be using.  But there is much more for anybody who's interested.  So it's just, you know, it's a beautiful solution for SpinRite and other things that I thought our listeners would find interesting.



LEO:  I want to hear all about address, random address access.



STEVE:  Yes.  Got some feedback to share first.



LEO:  Oh, okay.



STEVE:  Yeah.  One of our listeners, ZendoDeb, said something that I thought was brilliant.  He said:  "@SGgrc re CAPTCHA discussion from Security Now 891."  That was last week.  He said:  "I've wondered if using Firefox makes it worse, since Firefox is now stove-piping cookies, especially third-party cookies.  So when you show up at a new site, Google can't find a cookie."



LEO:  Ohhh.



STEVE:  That is brilliant.  Brilliant.



LEO:  That's what's going on.



STEVE:  Exactly.  Exactly.  There's no question.  That's why we Firefox users are saying, hey, why am I having to click on chickens constantly, or fuzzy bears or whatever it is.  Yeah, it is a consequence of the fact that Google is highly ranking the presence of their own cookie as one of the signals that they're using.  And when you go to a site that you haven't been to before, there's no Google cookie there, thanks to the per-site stove-piping that Firefox is now doing.  So very, very clever observation.  Thank you, Zendo.



RobinR said:  "Hi, Steve.  With all these buffer overflow and use-after-free issues, I've seen talk of getting development to switch to Rust.  My question to you is what kind of concerns or defensive techniques do you do when developing in assembly?  Is it the fact that you are so low level you are forced to be aware of everything, and thus don't fall into the same traps?  Additionally, would you change anything with a piece of software that you knew would be always on and be available on the Internet?"



And I thought about this for a while.  So first of all, I do have a piece of software which is always on and available on the Internet.  And that's GRC's server.  It is laced with a lot of my project code.  ShieldsUP! itself.  Probably the most complex, asynchronous thing I've written which is always online is the DNS spoofability test.  That thing has all kinds of asynchronous queries off to individual servers as it discovers them, lots of things happening dynamically.  I have the same problems that anybody writing in C would have, which is to do that I create a linked list of tasks, and each of the objects that are pointed to in the linked list is a structure which I allocate in RAM which contains the details of where that task is and what's going on.



Those have dynamically created lists of outstanding queries and their responses.  I don't know how many there will be, so that's a list.  So it's an extremely dynamic construction.  And it's been running for many, many years, and it's never had a bug or crashed.  So I think the advantage I have is I'm first of all one developer, so I don't have a problem explaining anything to myself.  And while there's a lot going on, it's still not nearly as complicated as what has happened to today's browsers, which are just like, I don't know if there's any one person whose single mind is able to encompass the entire thing.  And the same is certainly true for operating systems.  So I am at a low level.  I'm essentially at the level that C operates because all the things I just described is exactly how I would code something were I coding in C.  There's not that big a difference.



LEO:  So you do, in assembler, you're doing effectively your own malloc.  You've allocating memory.  You have to remember...



STEVE:  And I'm doing reference.  And I'm doing my own reference counting.  Yeah.



LEO:  You do your own garbage collection, in other words.



STEVE:  Yup.



LEO:  You don't need - and you don't, you know, the off by one problem probably is a little bit less of a problem for you because you're so intimately connected with what's going on.  I think some of the problems that come from high-level languages is programs are so insulated from what's going on that they can make - it's easy for them to make a mistake.



STEVE:  Well, and we talked, for example, about Microsoft's decision to use Electron as their platform for implementing Teams.  The problem is that that's JavaScript, HTML, and CSS.  You don't have to be a power coder in order for something to look like it's working in JavaScript.  And so exactly as you say, Leo, I think that does tend to admit less capable or less rigorous programmers.  The lower level the language, the more careful you need to be, or it's very obvious that, you know...



LEO:  Yeah, you see what's going on, yeah.



STEVE:  ...when something's going to - yes. 



LEO:  And I think also the reason they're talking about Rust is Rust is - it is garbage collecting, but it's very tight, there's very tight constrained, you know, it's a static type system, and it really tries very hard to keep you from making mistakes. Every time we see languages like that, like Ada, I think programmers appreciate it, but also don't like to use it.



STEVE:  Yeah, exactly.  I mean, yeah.  They're like nanny languages.



LEO:  Yeah.



STEVE:  It's like, okay, well, yeah, you know, that's a language.  But, boy, I don't want to code in that.  That's, you know, no fun.



LEO:  Rust is impressive.  And I guess what it's replacing, which is mostly C and to some degree C++, is bad enough.  So I guess people who use Rust like it.  And I played with it a little bit.  It's very impressive.  But there's a lot of boilerplate, a lot of extra code.  It's like Java a little bit in that respect.  Somebody like you, and to some degree me, I don't want to spend a lot of time typing in all that crap.  I just want to...



STEVE:  And I think what's going to happen is we'll get to the point where coders will not be given a choice.  That is, what we see happening is that we're getting to the point where we've got all the processing power we need.  It used to be that we didn't have enough RAM, and we didn't have enough speed, to support the...



LEO:  Overhead.



STEVE:  Okay, yeah, right, exactly, to support the overhead of sophisticated languages that do a lot to protect the way they're operating.  Today we do.  And I think at some point there will be a browser that bites the bullet and says, we're coding everything in Rust because we're done with use-after-free errors, period.  And we don't care if you don't like it.



LEO:  Remember, Rust was written by Mozilla.  I mean, it comes from Mozilla.  There's a reason.



STEVE:  Right.



LEO:  It is very much for that.  And by the way, once you compile it, one of the reasons people like Rust is it's a systems-level language.  It can be as fast as C and C++.  So once you compile it, it's very efficient.



STEVE:  And think of the upside, Leo, if you get paid by the line.



LEO:  I'm thrilled that Rust is now in the Linux kernel.  That is a good thing for everybody who uses Linux.



STEVE:  Yes.  I completely agree.  I think that's neat.



LEO:  And the issue was really libraries and support, and a lot of that's being handled now.  So that's good.



STEVE:  So I've got one for you here from Ben Hutton.  He says:  "Steve, we often hear breaches could have been avoided through the implementation of a systematic software patching and update strategy.  For enterprises, there are many solutions.  While performing tech support for a relative today, I found IObit Updater," he says, "IObit being a name I had previously trusted for the better part of a decade, was showing adverts for commercial products in the same" - I know - "in the same space as notifications for software updates.  Finding this unacceptable, I looked for an alternative solution.  I found one and expected to pay, but the consumer/home edition was free, and it seems like there are no limitations to speak of.



"Is there a solution you would suggest for Windows users for installing updates, free or otherwise?  The solution I found looked suspicious, but had attained 'leader' in Gartner's magic quadrant for patch management, Summer 2022.  The solution I found is 'Patch My PC.'"  He says:  "Only tried it today, so not an endorsement, but seems to do the job."



And so, Leo, my particular approach is just to rely on individual apps to tell me when they need to be updated, and then I update them.  You know, like Notepad++ is, my god, would the guy just leave it alone, please?  Because it keeps wanting to update itself.  But normally apps today take care of that.  And it looks like what Ben's talking about is some sort of an overwatcher who rifles through your system, looks at all the apps you've got installed, checks their versions, checks to see whether that's the latest, and then like gets involved in like telling you that you need updates.  I just kind of thought maybe from your Tech Guy stuff on the weekends that there's...



LEO:  Not heard of this one.  The good news is Microsoft finally is acknowledging the need for this and has a package manager, believe it or not, called Winget.  Which, you know how package managers on Linux will manage updates for everything on your system, including system updates.  That's the idea of Winget.  It's new, relatively, so I'm not sure how complete it is.



STEVE:  So it would be things through the Microsoft Store, probably.



LEO:  Actually, that's an interesting question.  The store does its own updates automatically.  I think Winget goes beyond that.  You do get it from the Microsoft Store.  I'll have to ask Paul about that.  But I think my sense is Winget is a full, or intended to be a full package manager for Windows.



STEVE:  So how would it know about, I mean, like in the case of our Unixes and Linuxes, we have a repository.



LEO:  Because you install through the package manager; right?  So the package manager, as you install stuff, makes a database of installed stuff.  And then when you do an app get update or whatever, it will look at that database, see what's been installed, check for new versions.  It does that in the repositories, exactly.  So you would need some sort of Microsoft-maintained database of application versions.  And then you could download them.  So, yeah, I mean, I think that's probably why it doesn't yet do everything.  But it does create these manifests.  It has sources, source repositories.  So I'm hopeful.  But I'll have to take a look.  I haven't looked at Patch My PC.  I think it's an unfortunate name.



STEVE:  Yeah.



LEO:  But, you know, doesn't mean it doesn't work well.  It looks like it's for Microsoft's own endpoint manager.  So it sounds like - and most of these guys, I'm looking at the engineers, are Microsoft MVPs and so forth.  So it looks similar to Winget, to be honest with you, 710 supported products.  I mean, when you're looking at Debian, and you're looking at apps, there's more than 10,000 packages that [crosstalk] knows about.



STEVE:  Yes, yes.



LEO:  I mean, this is a remarkable ecosystem on the Linux side.  I'd love to see Windows get to that point.



STEVE:  Yeah.  Okay.  JT Rehill.  He said:  "A quick question.  You or Leo mentioned in a side comment a couple episodes back that uBlock Origin can block those damn GDPR cookie pop-ups."



LEO:  God damn 'em.  I hate 'em.



STEVE:  Oh, I hate them.  He says:  "I've tried clicking on the 'block all pop-ups' button," he says, "I use Chrome by the way, but that doesn't do it.  Can you please tell me how you do this, or if there is another alternative that you know of."



LEO:  I thought I showed this from the show, but maybe I showed it afterwards.  You want to go to uBlock Origin's filter lists and then go down and expand Annoyances.  So there's a whole - and it's hidden.  There's a whole bunch of filters underneath annoyances.  I checked uBlock filters annoyances and Fanboy's annoyances.  And Fanboy's incorporates the EasyList cookie list.  And while it's not 100%, it's 90%, at least, of all those cookie pop-ups.



STEVE:  Oh, nice.  I didn't know that either.  So I'm glad we asked.



LEO:  It's a nice thing.  As you know, out of the box uBlock Origin does everything you'd want it to do.  But it can do a whole lot more.  If you go into the filter lists, they support a massive number of filter lists.  I don't think you need to add them all, but that's a couple you might want to add.



STEVE:  Good old Gorhill.



LEO:  Amazing.  You know, I can imagine - I see him with a long beard.



STEVE:  Uh-huh, exactly.



LEO:  Living in a cabin somewhere up in the Pacific Northwest and [crosstalk].



STEVE:  Get out of my cave.



LEO:  He's probably nothing like that.  But that's what I think of, yeah.



STEVE:  He's like a Dvorak curmudgeon, I think, yeah.  So Joel Clermont, he said:  "Just listened to SN-890 about Google Analytics in the EU and thought you might be interested to learn about Fathom Analytics."  It's usefathom.com.  "They are designed from the ground up around privacy and designed their infrastructure to comply with GDPR, including an option to have your data never leave the EU."



LEO:  I love this.  I want to use this.



STEVE:  Yup.  He says:  "I have switched all my sites to it over a year ago and love it."  And then I have a link to his blog, which is really good, and I recommend it to our listeners, titled "Why I Switched to Fathom Analytics."  It turns out it does more, no, wait, it does a better job with less of the random cruft that Analytics - he said that Analytics had all kinds of crap that he didn't need.



LEO:  Right.



STEVE:  But what this one does, it does better than Google Analytics was doing in his opinion.  So we have something that will give our sites analytics and be privacy respecting of our users.



LEO:  I will try to convince our team to use it.  We still use GA, I'm sorry to say.



STEVE:  Yup, just have them take a look at it.



LEO:  Yeah, yeah.



STEVE:  So Blaine Trimmell said:  "You talked about the safety of public WiFi.  But that article only talked about browser traffic.  So if you are only using a web browser, then yes, most likely safe.  But what if you're using apps that communicate unencrypted for their work?  And apps on mobile devices might be making non-TLS requests.  So I would say still not safe without a VPN."  He says:  "Have to remember someone in China could hack the WiFi router in San Francisco and capture the traffic.  You do not need to travel and be local."



So anyway, I thought that was worth noting.  He's absolutely right.  I was thinking entirely of everything being done through the browser with the fact that the world has switched to HTTPS.  But it certainly is the case that you could have an app, I mean, I hope you wouldn't, but you could have an app that just says, you know, nobody's probably looking, and does its thing, whatever it might be, in the clear.  So anyway, Blaine, thank you.  That is certainly a good point.



Bob Karon said:  "Hi, Steve.  In ref to SN-891," last week.  "As an IT consultant, I never use public WiFi.  Not so much from fear of hacking from someone else on the same WiFi, but from the provider of the WiFi itself.  An IT person who runs it could set up a proxy or man in the middle much easier and scrape all data through it.  I always tell my clients, turn the hotspot on your phone on, and use that for your laptop if needed.  I feel there's much less chance of Verizon trying to steal my traffic than some local coffee shop IT guy or even a big airport.  Unlimited data is very common now on cell plans anyway.  Thanks for the great show for all these years.  Bob."  So anyway, I just wanted to share that idea.  I often use my iPhone's hotspot when I'm somewhere that I don't have WiFi, and I want to have access.



And what I promise will be the last CPE comment, but I liked it because there was a little more information, David Lemire said:  "I'll trouble @SGgrc with one more CISSP CPE comment.  When I was way behind on CPEs for my first year of certification, I found a blog post on the ISC website that specifically listed your podcast among a number that could count for free CPEs."  He said:  "Really saved my behind."  So I just wanted to mention that it's not that they're, like, allowing it.  They're formally endorsing Security Now! as a source of ongoing education.



LEO:  Good.  That's great.  I didn't know that.  That's great. 



STEVE:  Yeah, really cool.



LEO:  I just wanted to add that, thanks to the Discord, the Winget repository is actually a GitHub repo, and you can submit your software just as a pull request in the repository and say, hey, I'd like you to manage my updates.



STEVE:  Ah, nice.



LEO:  So a really - I think this ultimately could be really good if the community gets behind it.  It's in the Microsoft GitHub repo, Winget packages.



STEVE:  And at this point, that's not built into Windows?



LEO:  No.  I don't think so.  You have to get it - you can get it from the Windows Store.  I would just love to see it just become the default way to install software [crosstalk].



STEVE:  Yeah, well, yeah, because...



LEO:  ...Windows Store; you know?



STEVE:  Yeah, and I was going to say that, if at some point application developers could depend upon that being present, then we could eliminate all of the individual check-for-update stuff.



LEO:  Yeah.  So annoying, yeah.  This is so much better.  Thank you, [Name], for that.  I appreciate it.  This is why we love our Club TWiT members.  Appreciate it, [Name].



STEVE:  Okay.  So an unintended side effect in Linux.  As we know, Internet Protocol addresses endpoints by IP address.  And at an IP address, we have a 16-bit port number which identifies specific services operated at that IP address.  So an end-to-end connection will have an IP address and port on one end, like the  source IP and source port, and an IP address and port on the other end, the destination IP and port.  At the receiving end where a client is connecting to a service like web, email, or whatever, the port, as we know, is typically well known:  443, 25, 110, whatever.



And on the client's connection-initiating end, it has long been the case that when a client asks its operating system for a new outbound connection, the OS's TCP/IP network stack simply moves linearly upward, starting above the reserved service port range at port 1025, sometimes 1024, and incrementing numbers until some upper limit, perhaps all the way up to 65,535 is wrapped, before wrapping around.



So traditionally the way all TCP/IP network stacks worked, when client applications asked to initiate a new outbound connection, the stack would simply initiate the next free port in line, and you often see sequential numbered ports in blocks like if you do a netstat command on your system.  They're not just scattered randomly.  They're in a linear list.



Okay.  But 11 years ago, back in 2011, having the OS allocating client connection ports, which is to say the source ports, linearly was seen as a potential problem since it made the next ports to be used guessable by an adversary.  And that guessability might allow adversaries to hijack connections, by just like assuming what they were going to be, assuming what the client's IP and source port would be.  That's the only way you designate an endpoint.  So if a bad guy injected traffic, sent traffic toward the destination, there's no way to differentiate it from the traffic coming from the legitimate source.  Sequence numbers comes into play there also for TCP connections, but we've already talked about all that in the past.  So we know that this is all possible since it was precisely the lack of source port randomization that alarmed Dan Kaminsky about the spoofability of DNS servers Internet-wide.  Attackers could blindly spoof replies by guessing the linearly allocated source ports of outstanding DNS queries.



So in response to this perceived threat, RFC 6056 was published by the IETF, titled "Recommendations for Transport-Protocol Port Randomization."  And its abstract, the abstract of the RFC reads:  "During the last few years, awareness has been raised about a number of 'blind' attacks that can be performed against the Transmission Control Protocol (TCP) and similar protocols.  The consequences of these attacks range from throughput reduction to broken connections or data corruption.  These attacks rely on the attacker's ability to guess or know the five-tuple (Protocol, Source Address, Destination Address, Source Port, Destination Port) that identifies the transport protocol instance to be attacked.



"This document describes a number of simple and efficient methods for the selection of the client port number, such that the possibility of an attacker guessing the exact value is reduced.  While this is not a replacement for cryptographic methods for protecting the transport-protocol instance, the aforementioned port selection algorithms provide improved security with very little effort and without any key management overhead.  The algorithms described in this document" - there are five of them - "are local policies that may be incrementally deployed and do not violate the specifications of any of the transport protocols that may benefit from them, such as TCP, UDP, UDP-lite, Stream Control Transmission Protocol, Datagram Congestion Control Protocol, and RTP," they say, "provided that the RTP application explicitly signals the RTP and RTCP port numbers."  And so that's what they said.  So the idea was, you know the RFCs, Leo, they're nothing if not thorough.



LEO:  A great thing to read if you're getting a little sleep-deprived, yes, absolutely.



STEVE:  Hell, that's how I learned all this stuff in the early days was literally sat down, okay, RFC 1.



LEO:  Read the RFCs.  Oh, god.



STEVE:  So the idea was, since the source port chosen by the OS doesn't matter at all, there is no reason not to be a lot more clever when choosing the next one.  RFC 6056 presents five different algorithms for doing just that, and it states that the so-called Double-Hash Port Selection algorithm offers the best trade-off.  Consequently, it was recently adopted, with minor modifications, in the Linux kernel, starting with kernel version 5.12-rc1.



And this prompted a trio of industrious researchers at the Hebrew University of Jerusalem to take a look at Linux's result.  What they found was not good.  Their paper titled "Device Tracking via Linux's New TCP Source Port Selection Algorithm" will be presented during the 32nd USENIX Security Symposium, which is upcoming, but I have the paper now.  They explain in their abstract, which is worth sharing here because we'll see what happened.



They said:  "We describe a tracking technique for Linux devices, exploiting a new TCP source port generation mechanism recently introduced to the Linux kernel.  This mechanism is based on an algorithm, standardized in RFC 6056, for boosting security by better randomizing port selection.  Our technique detects collisions in a hash function used in the said algorithm, based on sampling TCP source ports generated in an attacker-prescribed manner.  These hash collisions depend solely on a per-device key, and thus the set of collisions forms a device ID that allows tracking devices across browsers, browser privacy modes, containers, and IPv4/IPv6 networks, including some VPNs."



They said:  "It can distinguish among devices with identical hardware and software, and lasts until the device restarts.  We implemented this technique and then tested it using tracking servers in two different locations and with Linux devices on various networks.  We also tested it on an Android device that we patched to introduce the new port selection algorithm."  And by the way, Android was going to adopt it, but changed its mind when this happened.  "The tracking technique works in real-life conditions, and we report detailed findings about it, including its dwell time, scalability, and success rate in different network types."  And finally:  "We worked with the Linux kernel team to mitigate the exploit, resulting in a security patch introduced in May of 2022 to the Linux kernel, and we provide recommendations for better securing the port selection algorithm in the paper."



So the principle that I wanted to highlight and that we keep seeing playing out over and over is that things that once seemed to be "secure enough," mostly because we weren't trying as hard as possible, are no longer considered to be so.  The mess with modern processor microarchitectures, Spectre and Meltdown and the rest, is a perfect example.  For quite some time we were all happily living with the way our processors worked, and with all of the performance those optimizations delivered.  But that all ended overnight when some very clever academic researchers started looking much more closely.



Another example is DRAM.  Same story there.  Everything seemed fine until researchers began wondering whether too many bits may have been squeezed into too small a space, and whether that might create some adjacent row interference.  And sure enough, we know what the consequence of that was.  Similarly, the issue of IP source port assignment was happily ignored.  Then Dan Kaminsky realized that it could be a disaster for DNS.  So operating systems moved to change to ephemeral key-based pseudorandom assignment.  And then these clever researchers said "ah, not so fast," and discovered that there's a unique per-machine pattern that can be used for tracking.



LEO:  Wow.



STEVE:  I wonder what will be next.  Stay tuned to this podcast to find out.



LEO:  Never underestimate the ingenuity and perseverance of a hacker.  That's just the rule there.  Amazing.



STEVE:  Yeah.  I mean, that is the case.  And so all these things, we lived with them for years, sometimes decades.  And then someone said, I don't know about that.



LEO:  Not so fast.



STEVE:  Not so fast.



LEO:  I love it.  I love it.  Mr. Gibson, you're a gem.  A jewel.  And if I could say it in assembler code, I would.  But I'm sending you a book.



STEVE:  And an old relic.



LEO:  An old relic.  I'm sending you two books.  I told you about one.  I'm sending you another I just thought of.



STEVE:  Ah.



LEO:  That's entirely in x86 assembler.  I hope you enjoy it.  No prose.  No prose.  I don't know if you've ever seen this book.  Its title is "xchg rax,rax."



STEVE:  No.



LEO:  And I think that's all you need to know.  The author is xorpd.  And I just thought I'd send it to you because it's kind of silly.



STEVE:  Cool.



LEO:  Yeah, yeah.  You know, everybody should have an assembly language written book on their shelf.  Just in assembly.  I bet you, I'm actually really curious if you can look at it, and you go, oh, yeah, I know what that does.  Oh, yeah, yeah, yeah.  Oh, that's cute.  Oh, what a laugh.  I bet you'll laugh reading this.



STEVE:  Sounds great.



LEO:  Yeah, no kidding.



STEVE:  Exactly my kind of puzzle.



LEO:  You should show this to Lorrie.  Anyway, Steve is the best.  We are so glad we have him every Tuesday right here, talking about security and technology in the most lucid way possible.  He even makes RFCs seem entertaining.  You'll find us here at 1:30 p.m. Pacific, right after MacBreak Weekly, 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC.  If you follow Steve on Twitter, @SGgrc, two reasons to do that.  The show notes go there right before the show so that you can download them and read them along.  He also has them on his website.  But you can also message him there.  His DMs are open.  If you've got thoughts or comments, that's where a lot of the feedback on the show comes from.



GRC.com's the website to go to, Gibson Research Corporation.  Not only for SpinRite.  And as you can see, this is probably a good time to get SpinRite.  If you buy 6.0 right now, you'll get 6.1, which is imminent.  Perhaps even participate in the testing.  You could be the one that says, "Steve, I found a flaw."  You know, get a gold star on your forehead.  Help him out here.  GRC.com.



While you're there, of course you can get a copy of the show.  Steve has two unique versions, a 16Kb audio version which sounds a little bit like a Thomas Edison cylinder, but has the one benefit, it's small, for the bandwidth-impaired.  We also have transcripts written from that 16Kb Edison cylinder by Elaine Farris.  She gives us beautiful transcripts that you can read along as you listen, or use them to search for parts of the show.  All of that's at GRC.com, along with a 64Kb audio version, full quality audio version.  GRC.com.  Plus check out  all the other stuff he does.  If you want to try his DNS caching utility and think about all the stuff going on behind the scenes in his server - was the server written in assembler?  No.  That's in C.



STEVE:  So IIS, it's Microsoft's IIS.



LEO:  Yes.



STEVE:  But it has a really nifty add-on facility called IISAPI, which is I-I-S-A-P-I.  And so it's a huge IISAPI - I've written a huge IISAPI extension which is the ShieldsUP! and the certificate testing and all of the stuff that GRC's site does, and all of the ecommerce I wrote in assembler also.



LEO:  Wow.  So did you write glue code in C or C++ and then everything else can be in assembler, or do the whole thing?



STEVE:  No.  Just assembler.  Yeah, it turns out that the calling convention for the API is - all you have to do is set up the stack and just jump to a call.



LEO:  Nice.



STEVE:  So it all works directly, yup.



LEO:  It's a nice, you know, it's a nice feeling when you - it's almost like you're looking into the machine and seeing it work.  And you get it.  You understand it.  It's pretty cool.



STEVE:  I just like it, yeah.  I just, I think that's why coders code is that they, you know, at any level you get a sense of satisfaction. 



LEO:  Sure.  But that's why - I think that's why assembly language coders pursue this what seems a seemingly arcane art because you are writing in the computer's native tongue.



STEVE:  Yeah.



LEO:  You're exchanging rax with rax.  You're doing it at the very base level of it, which is cool.



STEVE:  Although it's also why I doubt I'll ever use, I'll ever code ARM in assembler because it just doesn't seem friendly.  It's not, I mean, RISC, Reduced Instruction Set Computer, as opposed to CISC, Complex Instruction Set Computer.  I like CISC.



LEO:  Well, you've learned all the instruction codes; right?



STEVE:  Yeah.



LEO:  You've got them up here.  So, yeah, and probably you can do in one instruction what RISC requires five for.  I would guess that's what it is; right?



STEVE:  Yes, well, for example, I'm able to add two locations in memory with a single instruction; whereas RISC you have to load it in register, load the other one in register, add the two together, and then store the result back out.



LEO:  Yeah, that's powerful, yeah, yeah.  I get it, yeah.



STEVE:  So [crosstalk] architecture.



LEO:  GRC.com.  We have copies of the show at our website.  In fact, if you go to TWiT.tv/sn, you'll see every show ever recorded, all 892 of them, one after the other there.  You can also go to YouTube.  There's a Security Now! YouTube feed that has all the shows that we've done in video, anyway, there, which is not all of them.



And probably the best way to do this, if you don't, you know, if you want to get all the old shows, you know, the feed only has the most recent 10 shows.  If you want to get all the old shows, you've got to go to the website.  But if you just want to get the new show when it comes out, subscribe in your favorite podcast client, set it to auto download, and you're just going to get it, and that way you can listen, you know, whenever you say, oh, I've got a minute or two, let me listen to some Security Now!.  Which I think a lot of people do.



But, you know, if you want to listen on Tuesday, listen live, that's fine, too.  Live.twit.tv's the live stream; irc.twit.tv to discuss it.  Or, of course, in our Discord.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#893

DATE:		October 18, 2022

TITLE:		Password Change Automation

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-893.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine several more serious Microsoft security failures which have just come to light, and a new useful Windows security feature that was just added.  The new Passkeys logon technology received its own website to monitor its progress, and Cloudflare logs another record-breaking DDoS attack.  Signal drops its legacy support for SMS/MMS on Android, Fortinet attempts to keep a new bad authentication bypass quiet, the White House proposes work on an IoT cybersecurity seal of approval, and the U.S. Treasury department levies a hefty fine against a cryptocurrency exchange for not caring who they send money to.  I have some updates on SpinRite, my just-discovered ZimaBoard, and two pieces of listener feedback.  Then we're going to finish by examining a new standardized means of accessing websites' password change pages.  And we also have our first-ever Security Now! Video of the Week.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Man, Microsoft has really blown it this time.  Take a look at why you should never use the Office encryption technology, and why you maybe shouldn't trust their device driver security.  Oh, boy.  We'll also talk about Cloudflare and the largest DDoS attack ever, the White House's proposal on an IoT cybersecurity seal of approval, and then we'll about proposals for a better way to change all your passwords.  It's that and a whole lot more, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 893, recorded Tuesday, October 18th, 2022:  Password Change Automation.



It's time for Security Now!.  Oh, boy.  I look forward to this all week long because I just kind of, you know, every time I see a story about security, ransomware, viruses, I say, "I wonder what Steve will say?"  And now we know.  Steve Gibson is here of GRC.com.  Hello, sir.



STEVE GIBSON:  Leo, great to be with you for Episode 893.	



LEO:  Wow.



STEVE:  Yup.  We're getting the hang of this maybe.  So this week's topic is Password Change Automation, which is an overstatement of what we're actually getting.  But as I explain here, before we wrap up today's podcast, it does represent a baby step in the right direction, and a potentially exciting new standard.  So we're going to talk about that.  But first we're going to look at several more serious Microsoft security failures which have just come to light, and a new, useful Windows security feature that was just added with Patch Tuesday's, last Tuesday's updates.



We also have a new Passkeys, well, I'm sorry.  The new Passkeys logon technology has received its own website which we can use to monitor Passkeys' progress moving forward.  Cloudflare logs another record-breaking DDoS attack, just insane levels of traffic, which if you didn't have one of the main DDoS mitigators in front of your server, just even pouring ice on it wouldn't help.  It would just melt down.  We've got the news of Signal, one of the premier end-to-end encryption messaging platforms, dropping their legacy support, I didn't even know they still had it, of SMS and MMS on the Android platform.



LEO:  Yeah.  You don't know because you use an iPhone.



STEVE:  Right.



LEO:  But on the Android it's a big deal because then I can use it as my sole messenger.



STEVE:  Right, right.



LEO:  Which I won't be able to going forward.



STEVE:  Nope.  Also the company Fortinet has sort of attempted to keep a new bad authentication bypass quiet.  It was a zero-day that was affecting their customers.  We'll talk about how that turned out for them.  Also the White House proposes work on an IoT cybersecurity seal of approval, and the U.S. Treasury department has - yeah, god help us - has levied a heavy fine against a cryptocurrency exchange for not caring who they sent money to.  I also have some updates on my work on SpinRite and on my just-discovered, and I'm more in love with it than ever, ZimaBoard that we talked about last week.  I'll share two pieces of listeners' feedback.  Then we're going to finish by examining, as I mentioned, this new standardized means of accessing websites' password change pages.  Oh, and we have the first-ever Security Now! Video of the Week.  Not Picture of the Week because this one needs to be animated and seen to be believed.  It is so good.



LEO:  Can't wait.



STEVE:  In fact, I made it our Shortcut of the Week, but not given a number.  It's grc.sc/gate for any of our listeners who want to jump ahead.  Anyway, lots of fun today.



LEO:  Can't wait.  It's going to be a great day for Security Now! fans.  And we're going to get to that in a moment.  But first - what are you laughing about?  It's true.  It's true.



STEVE:  Okay.  So, yes.  That does have sound.  And since this is a Twitter video, it defaults to being muted.  But the sound is important.  For those who are not watching this, we see somebody who is opening a gate.



LEO:  And it opens just fine; right?



STEVE:  Yeah, well, so there's a whole bunch of technology, it's important to recognize.  To the left of this is a proximity card reader, a computerized gate mechanism.  It's got a solenoid-actuated release.



LEO:  And it's got a stalled video.  I wish I could play it.



STEVE:  Solenoid-actuated release that allows the latch to be electronically opened only if this individual's proximity card permits him to go through the gate.  So all this technology is in place.  And we should explain also that the gate is sort of a wire grid.  So it's not a solid gate.  You're able to see through it easily.  And it's just sort of got a wire screen with, like, gaps of about two inches around it.



Anyway, so this individual is demonstrating to us that, sure enough, the gate is locked.  He rattles it, and it won't open.  Puts his security card up to the reader.  All sorts of technology happens.  The ID is read from the card.  It's shot off across the planet somewhere to verify that this individual has the credentials required to allow the door to open.  Affirmative confirmation returns.  The computer processes the message, activates the power on the solenoid, pulls the latch back, and, oh, the gate opens, sure enough.



Now, to demonstrate the problem here, he then closes the gate, attempts to open it again.  It will not open.  Rattles.  But then he notices, wait a minute.  On the other side of the gate, through this wire, there's a handle.  I wonder what happens if I push that handle down.  Whoops.  The gate opens.



LEO:  Let's try one more time without the sound.  I don't know if it's the sound that's screwing it up.  But all right.  Because it stops here every time with the sound on.  Ah, there we go.  And bingo, boingo, bongo.



STEVE:  Yes.  Now, this is one of those cell phone videos that's very narrow and very high.  So we cannot see to the sides.  We're hoping that this gate is not out in the middle of a field by itself.



LEO:  It looks like it's a pretty secure facility, given all the technology on that thing.



STEVE:  Yeah.  We have of course seen gates that are out in the middle with well-trodden paths circumventing the gate, making you wonder why anyone even bothered.  Anyway - won't open?



LEO:  Freezes every time at that point.  I don't know why.



STEVE:  And then he reaches through the gate, pushes down the handle latch, which opens it.  So clearly anyone on the other side was meant to be able to come in, or maybe that's probably to leave this protected facility.  But you can't enter.



LEO:  You can check out, but you just can't check in.  No, wait a minute.



STEVE:  It is just like, yeah, anyway, for anybody who's been listening and wondering what we're talking about, I made this, I gave this a GRC shortcut.  So it's grc.sc/gate, G-A-T-E, grc.sc/gate.  It's definitely worth checking out.



LEO:  And by the way, this guy, I would follow him on Twitter because he has a lot of similar silly things in there.



STEVE:  Oh.



LEO:  Mistakes and so forth in security.  So I like his - I might start following his tweet.  Here's a picture of a vehicle where there's a cutout in the back, exposing some wires, that says:  "When you need to access the CAN bus, but a vendor has installed next-gen AI-driven security controls to thwart attacks."  Well, easy enough, you just cut a hole in the fiberglass.  



STEVE:  Oh.



LEO:  Oh, lord.



STEVE:  Yeah, it's very similar.  That's great.



LEO:  Yup.



STEVE:  Yeah.  So his Twitter name is @HamzahBatha.  Again, @HamzahBatha.  And as you said, a good feed.



LEO:  Yup, yup.



STEVE:  Okay.  So I was torn between titling this piece "Won't Fix" or "Secure Enough."  I settled on "Won't Fix," but we'll get to what is secure enough.  So "won't fix" is what Microsoft told the guys, actually guy, who noted and reported his or their - he is with a company, a well-known company - their discovery.  They said, essentially:  "Thanks very much, but we're going to leave it as is."  So last Friday, the pattern of Microsoft deliberately choosing not to fix a latent, well understood, and potentially serious security vulnerability repeated itself once again, which is redundant.



Many years ago, when this podcast was first laying out the fundamentals of encryption, we talked about cipher modes.  Any practical encryption system starts with an underlying symmetric cipher.  The one that the industry has settled upon currently is the Rijndael Cipher, which was chosen to be the AES standard.  Therefore we also call it the AES Cipher.  It's a 128-bit-wide block cipher, meaning that it takes a block of 128 bits at a time, which is 16 bytes, and under the influence of a key, which is typically kept secret, and the key is often 256 bits, it takes those 128 input bits and arranges to map every possible input combination of those 128 bits into a different output combination of 128 bits.  That's the encryption.



So as long as the key remains the same, every time the same 128 bits is presented, the same different 128 bits is produced.  And of course that's required for the cipher to be useful.  Obviously, it must be deterministic and not generate random outputs.  Same data in, same data out.  But this determinism also poses a problem which we've talked about several times in the past.  If the same plaintext input block always produces the same ciphertext output block, then someone examining the encrypted ciphertext, the output, who sees identical output blocks appearing, instantly knows that the input blocks were also identical.  They may not know what they were; but, given sufficient time and statistical analysis, significant information can be leaked.  And if any of the input text is known, like standard query headers, packet protocol headers or data, boilerplate, or any other overhead which is encrypted as part of this, then someone examining output blocks can see what those known input blocks encrypt to.



So this simple and straightforward yet ineffective mode and method of encryption is known as Electronic Codebook or ECB.  And no one uses it for encryption specifically because it is clearly and obviously insecure.  And by now you can probably guess where we're going with this.  Someone did use it.  Anyway, the most famous and clear example of the failure of Electronic Codebook mode to effectively protect the secrecy of data is the classic demonstration of the image of the Linux Penguin.  I've got it in the show notes.  It's also on Wikipedia's page under "Block Cipher Mode of Operation."



And in the show notes we see three images.  On the left is the input, like the so-called plaintext image of the Linux Penguin.  The middle is that penguin encrypted using ECB, Electronic Code Book mode, which is to say simply taking the encryption block, encrypting it into a different block.



LEO:  How could they even call that encryption?



STEVE:  Exactly.  I know.



LEO:  I mean, that's not encryption.



STEVE:  I know.



LEO:  Now, this really is a good example of why ECB is so bad.



STEVE:  Yes, isn't it?  It's just perfect.  And what Leo's referring to, for those who aren't seeing this, is you can tell, you can still see that it's the Linux Penguin.  I mean, it's not pretty anymore, the colors are lost, but it's like it's not obscured.  It's there.  Because every time the same block of bits was ciphered, it came out to be the same different block.  And in an image context, the image survives.  It's not great, but it's there.  And compare that to the third frame here, which is just noise.  All other modes except ECB result in noise.  I mean, like no picture at all, just static.  And that's of course what you want from your encryption algorithm.



So what we actually want, as I said, is shown in that rightmost of the three panes, where the result is pure noise without any vestigial remnant of the original image.



LEO:  There's no evidence that there was any information; right?  That's the key.



STEVE:  Correct.  It is mathematically indistinguishable from entropy, from pure noise



LEO:  So there's no information there.  Yeah, yeah.



STEVE:  Yes, yes.  And as I said, any cipher mode other than the simplistic ECB results in something like the third image, just noise.  Actual encryption which does not leak information about the unencrypted plaintext is what we want.  Now, I'm not going to delve into great detail about the other encryption modes since we have carefully and fully covered all this before.  And the Wikipedia link that I've got in the show notes about this will refresh anyone's memory if they're curious.  But the crucial weakness of simple Electronic Codebook encipherment is that each encrypted block stands alone.



The good news is this is easily resolved.  Every one of the other popular encryption modes solves this simply by chaining.  The most famous of these modes is, and it's as good as any, is CBC, which stands for Cipher Block Chaining.  CBC simply XORs the result of the previous encrypted block with the plaintext to be encrypted by the next block.  That's all it takes.  By chaining the encrypted result into the next encryption, blocks no longer stand alone.  Each block is affected by all previous blocks.



So what happened with Microsoft last Friday?  The company now known as WithSecure, which was formally F-Secure Business, published their distressing summary of events under the title "Flaw in Microsoft Office 365 Message Encryption could expose email contents to attackers."  They explained:  "Adversaries can exploit the flaw, for which there is no patch available, to obtain information that could lead to a full or partial information disclosure."



So this is from Helsinki, Finland, which is where these guys are located.  They said:  "Today, WithSecure (formerly known as F-Secure Business) published a security advisory warning organizations of a security flaw in Microsoft Office 365 Message Encryption."  So this is Office Message Encryption, or OME for short.  So they said:  "OME, which is used by organizations to send encrypted emails internally and externally, utilizes the" - wait for it - "Electronic Codebook implementation."  And calling it an implementation, even that is kind of a stretch.



They say:  "...a mode of operation known to leak certain structural information about messages.  Attackers able to obtain OME messages could use the leaked information to partially or fully infer the contents of the messages by analyzing the location and frequency of repeated patterns in individual messages."  It's the repeated patterns, for example, that allows us to see the Linux Penguin even after it's been "encrypted."  And they said:  "...and then matching these patterns to ones found in other OME emails and files."



Harry Sintonen, WithSecure's security researcher, who discovered the issue, said:  "Attackers who are able to get their hands on multiple messages can use the leaked ECB info to figure out the encrypted contents.  More emails make this process easier and more accurate, so it's something attackers can perform after getting their hands on email archives stolen during a data breach" - notably encrypted email archives encrypted under this ridiculous Electronic Codebook encryption - "or by breaking into someone's email account, email server, or gaining access to backups."



So according to the advisory, the analysis can be done offline, meaning an attacker could compromise backlogs or archives of previous messages.  Unfortunately, organizations have no way to prevent an attacker that comes into possession of affected emails from compromising its contents using the method outlined in Sintonen's advisory.  The advisory also highlights that no knowledge of the encryption keys is needed to conduct the analysis.  Yes, because it's not actually encryption.  And that the use of BYOK, Bring Your Own Key scheme, does not remedy the problem, meaning it doesn't matter if you privately key this.  This is not actually good encryption.



Sintonen shared his research with Microsoft in January of 2022.  While Microsoft acknowledged the problem and paid Sintonen via their vulnerability reward program, they opted not to fix the issue.  While organizations can mitigate the problem simply by not using the feature, I guess and not assuming that they have encryption, it does not address the risks of adversaries gaining access to existing emails which were previously encrypted with OME.  



Sintonen said:  "Any organization with personnel that used OME to encrypt emails are basically stuck with this problem.  For some, such as those that have confidentiality requirements put into contracts or local regulations, this could create some issues.  And then of course there's questions about the impact this data could have in the event it's actually stolen, which makes it a significant concern for organizations."



Because there's no fix from Microsoft or a more secure mode of operation available to email admins or users, WithSecure recommends avoiding the use of OME as a means of ensuring the confidentiality of emails.  And I'll note that the trouble is not just theoretical.  WithSecure's technical write-up placed an image in an Office 365 message, encrypted and sent it using Microsoft's OME (Office Message Encryption), and this was the result.  I have another picture in the show notes where you can, as clearly as you could with the Linux Penguin, see the word "Fail" written quite clearly in the image because yes, that's what this is, an encryption fail.



So as we've seen over and over, Microsoft's industry dominance is so complete that the details of what they do no longer matter.  This won't cause them to lose a single Office 365 customer.  And they know it.  So why bother fixing it?  Just call it "encrypted" and figure that it's encrypted enough.  And this begs the question of how this could have ever happened in the first place.  I mean, okay, so, yeah, we're confronted with this, like, horrible design, I mean, I'm reticent to call it a design, you know, I mean, it's like...



LEO:  Why does anybody use ECB encryption?  I mean, is there some useful reason?  Or is it just old?



STEVE:  No, it's not even old.  I mean, nobody ever - the thing that you need with CBC is you need an initialization vector.  Remember that I said you take the output of block cipher N and XOR it with the plaintext of block cipher N+1 before encryption.  But that then begs the question, okay, how do you start?  Because the first encryption won't have a previous result to use.  So that's where you need a 128-bit initialization vector.  It doesn't need to be secret.  It does need to change with every time you use the encryption.  But that's simple.  Just increment it.  Again, it needs to be unique, but it doesn't need to be secret.  So you would have to include that with the message, or in a header for the email.  But that's trivial, too.



So, I mean, I can't - so, okay.  Arguably I know how encryption works.  Clearly, whoever designed this doesn't.  I mean, Leo, I mean, it's like, whoever, truly, whoever did this doesn't know how to encrypt things.  So they just, like, took some symmetric cipher, and I don't even know if it's AES, maybe, hopefully, but they took a symmetric cipher and just said, oh, feed blocks in, send blocks out.  Which, as we've seen, isn't good.  And galling is that when Microsoft was told this at the beginning of the year, they said yeah, you know, it's encrypted enough.  We're going to leave it the way it is.



LEO:  It's not encrypted.



STEVE:  No.



LEO:  And it's a lie to say it's encrypted.



STEVE:  I know.



LEO:  It's baffling to me.  And why would you even choose this in the first place, let alone not fix it?



STEVE:  The only thing I can suggest is that this was given to somebody who was utterly incompetent to be given this job.  And they said, okay, yeah, done, it's encrypted.



LEO:  Might as well just XOR it.  You know?  Why does ECB exist?  Why is it there?



STEVE:  Because it can.  I mean, because, like, just somebody...



LEO:  Did somebody once think it was a good way to do it?



STEVE:  So I think probably it's because once upon a time we had the Caesar cipher, where you took the alphabet, and you skewed it by some number of characters; right?



LEO:  Yeah.  That's basically what this is; right?



STEVE:  Yes, exactly.  That is Electronic Codebook.  The idea is you have something.  You look up in a codebook what it maps to, and that's what you write down.  And then at the receiving end they have the reverse codebook, where you look up what you got, and you saw what it was mapped to in the first codebook.  And so it's sort of there just because it completes the picture.  But nobody should use it.  I mean, for this reason.  And how to do it right is like in the next paragraph.  It's like the guy stopped reading Wikipedia after three paragraphs.  Just read the fourth paragraph, where it says don't do this.  It's like, how does this get into Microsoft Office 365?  And then not only does it get in, but they've said, eh, we're going to leave it the way it is.  Good enough.



LEO:  Unbelievably incompetent.



STEVE:  Which really does make you worry about what's happening at Microsoft. 



LEO:  Well, I mean, they could even say, oh, whoops, let's fix it.  They're not even going to fix it.



STEVE:  No.



LEO:  Unbelievable.  Well, the bad guys must love this.



STEVE:  Oh, yes.  



LEO:  Jiminy Christmas.



STEVE:  Okay.  But we're not quite done yet with Microsoft for the week.  Ars Technica's coverage of this next piece bore the headline "How a Microsoft blunder opened millions of PCs to potent malware attacks," with the subhead "Microsoft said Windows automatically blocked dangerous drivers.  It didn't."



LEO:  This one's also boneheaded.  



STEVE:  Yup.  I know, Leo.  And, well, we'll see.  They even got belligerent toward those who were trying to say, uh, it doesn't work.  Anyway, before we get into the details of what was discovered, recall that this is something also that we've covered in the past.  The issue is that kernel drivers, by their nature and position in the system, run with the highest available privileges, in the Windows kernel, where they can do anything and everything.  You know, rootkits are an example; right?  They're able to filter the API in order to make files disappear from the file system.  So they must be absolutely trusted.



But they also contain complex code which requires a level of attention to detail that can sometimes be lacking.  So otherwise perfectly benign drivers can have identified exploits.  Such drivers will originate from valid reputable sources and bear their reputable publisher's digital signatures.  So they're entirely valid.  And signatures never expire.  If a signature is valid at the time that it was used for signing, it remains valid.  But what if a problem is identified later in an otherwise valid driver, a problem that's maliciously exploitable to gain privilege elevation?  Once this becomes public knowledge, bad guys can bring and install one of these valid drivers into a system that otherwise has no need for them.  It's like their own private backdoor.  This exploit technique is known as BYOVD, Bring Your Own Vulnerable Driver.  And it's a real concern since kernel drivers are no less subject to bugs than anything else.



The only solution is to prevent known vulnerable drivers from being accepted and loaded into Windows.  And that requires that all such known vulnerable drivers be blacklisted.  Of course, this critical protection strategy is only as effective as the list of known vulnerable drivers is kept current.  And so begins our story.  As Ars Technica's Dan Goodin writes, he said:  "For almost two years" - and later he says three - "Microsoft officials botched a key Windows defense, an unexplained lapse that left customers open to a malware infection technique that has been especially effective in recent months.



"Microsoft officials have steadfastly asserted that Windows Update will automatically add new software drivers to a blocklist designed to thwart a well-known trick in the malware infection playbook.  The malware technique known as BYOVD, short for Bring Your Own Vulnerable Driver, makes it easy for an attacker with administrative control to bypass Windows kernel protections.  Rather than writing an exploit from scratch, the attacker simply installs any one of dozens of third-party drivers with known vulnerabilities. Then the attacker exploits those vulnerabilities to gain instant access to some of the most fortified regions of Windows," Dan writes.  "It turns out, however, that Windows was not properly downloading and applying updates to the driver blocklist, leaving users vulnerable to new BYOVD attacks."  Imagine that.  How could that possibly happen?  Raise your hand if you're surprised.



Dan fleshes this out with some nice background, writing:  "BYOVD has been a fact of life for at least a decade."  Probably back when we talked about it.  "Malware dubbed 'Slingshot' employed BYOVD since at least 2012, and other early entrants into the BYOVD scene included LoJax, InvisiMole, and RobbinHood.  Over the past couple of years we've seen a rash of new BYOVD attacks.  One such attack late last year was carried out by the North Korean government-backed Lazarus group.  It used a decommissioned Dell driver with a high-severity vulnerability to target an employee of an aerospace company in the Netherlands and a political journalist in Belgium.



"In a separate BYOVD attack a few months ago, cybercriminals installed the BlackByte ransomware by installing and then exploiting a buggy driver for Micro-Star's MSI AfterBurner 4.6.2.15658, a widely used graphics card overclocking utility.  In July, a ransomware threat group installed the driver mhyprot2.sys, a deprecated anti-cheat driver used by the wildly popular game Genshin Impact, during targeted attacks that went on to exploit a code execution vulnerability in the driver to burrow further into Windows.  A month earlier, criminals spreading the AvosLocker ransomware likewise abused the vulnerable Avast anti-rootkit driver aswarpot.sys to bypass virus scanning."  And note, all of these were known.  All should have been blocked.  None were.  "Entire blog posts," he writes, "have been devoted to enumerating the growing instances of BYOVD attacks, with posts from security firms Eclypsium and ESET among the most notable."



Okay.  So in other words, these are very real threats and attacks which could be, and should be, thwarted by Windows, but are not being.  Eclypsium's blog post is cleverly titled "Screwed Drivers - Signed, Sealed, Delivered," and ESET's is titled "Signed kernel drivers - Unguarded gateway to Windows' core."  And Eclypsium enumerates the publishers of such drivers, which include ASRock, ASUSTek, ATI/AMD, Biostar, EVGA, Getac, GIGABYTE, Huawei, Insyde, Intel, MSI, NVIDIA, Phoenix Technologies, Realtek Semi, Supermicro, and Toshiba.  All good companies, and all capable of making mistakes.



Dan writes:  "Microsoft is acutely aware of the BYOVD threat and has been working on defenses to stop these attacks, mainly by creating mechanisms to stop Windows from loading signed-but-vulnerable drivers.  The most common mechanism for driver blocking uses a combination of what's called memory integrity and HVCI, short for Hypervisor-Protected Code Integrity.  A separate mechanism for preventing bad drivers from being written to disk is known as ASR, or Attack Surface Reduction.  Unfortunately," writes Dan, "neither approach seems to have worked as well as intended."



As we'll see, that statement of Dan's is actually being quite kind.  He continues:  "Microsoft has touted these protections since at least March 2020" - so more than like 2.5 years - "when the company published a post promoting 'Secured Core' PCs, which have HVCI enabled out of the box.  Microsoft presented Secured Core PCs, and HVCI in general, as a panacea for in-the-wild BYOVD attacks, stemming from either buggy drivers or 'wormhole' drivers, which are those which were created and vulnerable by design."



Microsoft said:  "In our research we identified over 50 vendors that have published many such wormhole drivers.  We actively work with these vendors and determine an action plan to remediate these drivers.  In order to help further customers identify these drivers and take necessary measures, we built an automated way in which we can block vulnerable drivers and that is updated through Windows Update.  Customers can also manage their own blocklist as outlined in the sections below."  The post went on to say, writes Dan, "that Microsoft threat research teams continuously monitor the threat ecosystem and update the drivers that are in the Microsoft-supplied blocklist.  This blocklist is pushed down to devices via Windows update."



A few months later, Microsoft Senior VP of Enterprise and OS Security David Weston tweeted that by turning on these protections, Microsoft users were safe from an ongoing BYOVD attack that had recently made the rounds.  Weston wrote:  "Security vendors are going to tell you that you need to buy their stuff, but Windows has everything you need to block it."  Multiple Microsoft posts have made the same claim about automatic updating ever since.  One from last December said that signed drivers reported to be vulnerable are blocked by default through Microsoft's automated Windows Update mechanism when Windows 10 has HVCI enabled.



But Dan writes:  "As I was reporting on the North Korean attacks mentioned above, I wanted to make sure this heavily promoted driver-blocking feature was working as advertised on my Windows 10 machine.  Yes, I had memory integrity turned on in Windows Security" - it's Windows Security, Device security, Core isolation - "but I saw no evidence that a list of banned drivers was periodically updated.



"So I reached out to Microsoft and asked if someone would provide me with background about how the protection worked.  The response from Microsoft is 'nothing to share.'  I then turned to Peter Kalnai, a researcher at security firm ESET, who has had plenty to share about BYOVD attacks.  I asked Peter for help testing this driver blocklist feature.  Very quickly, we found it lacking.  When Kalnai enabled HVCI on a Windows 10 Enterprise system in his lab, for instance, the machine loaded the vulnerable Dell driver that had recently been exploited by Lazarus.



"Around the same time, researchers, including Will Dormann, a senior vulnerability analyst at security firm ANALYGENCE, had been tweeting for weeks that various drivers known to be actively used in BYOVD attacks were not being blocked the way Microsoft had advertised.  One Dormann observation was that, even with HVCI turned on, his lab machines loaded a vulnerable driver known as WinRing0 just fine.



"Upon further investigation, Dormann discovered that this vulnerable WinRing0 driver wasn't present in the Windows-recommended driver block rules.  In the same thread, he went on to show that, despite Microsoft's claims that ASR is capable of blocking vulnerable drivers from being written to disk, he could find no evidence that this feature worked at all.  Microsoft has yet to address this criticism or to provide Dormann with guidance.  Dormann went on to discover that the driver blocklist for HVCI-enabled Windows 10 machines hadn't been updated since 2019, and the initial blocklist for Server 2019 only included two drivers.



"As scrutiny of this situation increased, a Microsoft project manager finally admitted that something had gone wrong with the update process for the driver blocklist.  Gee, imagine that.  The manager tweeted that Microsoft was 'fixing the issues with our servicing process which has prevented devices from receiving updates to the policy.'  What the program manager was saying boiled down to this:  If you thought HVCI was protecting you from recent BYOVD attacks, you were probably wrong.  Windows 10 hadn't updated the list in almost three years.



"Coinciding with the project manager's tweet, Microsoft released a tool that allowed Windows 10 users to deploy the blocklist updates themselves which had been held back for three years.  But this is a one-time update process.  It's not yet clear if Microsoft can or will push automatic updates to the driver blocklist through Windows Update."  I do have a link in the show notes at this point to the posting that Microsoft made that it will allow end users or hopefully enterprises to update their blocklists.



Dan goes on:  "While Microsoft's response to my questions about driver blocklist updating was indifference, company employees have been actively dismissive to admins and researchers who began asking their own questions about the topic.  Recently, for instance, when Dormann pointed out a demonstrably false claim in an August tweet from Weston, that ASR ensured that updated driver blocking happened automatically, Weston did not" - he's a senior VP; right? - "did not apologize or even admit the problems.  Rather than confirming an update lapse that spanned more than two years, Weston bristled, saying only that updates 'are in the servicing pipeline,' and that Microsoft has already 'provided a tool to do it right now.'



"The closest Microsoft has come to an admission of failure is a comment from a company representative saying:  'The vulnerable driver list is regularly updated; however, we received feedback that there has been a gap in synchronization across OS versions.  We have corrected this, and it will be serviced in upcoming and future Windows updates.  The documentation page will be updated as new updates are released.'  The representative didn't say how long the gap lasted or what upcoming and future Windows updates would fully fix this problem."



So, wow.  Dan's coverage of this mess continues with some powershell scripting that allows individuals to take matters into their own hands.  I've placed a link to his entire Ars Technica article in the show notes.  But everyone gets the idea by now.  We have another example, not only of gross incompetence, but also of denial and belligerence when employees are presented with embarrassing truths.  Obviously Microsoft hasn't bothered to test themselves any of this for the last three years.  They said, oh, look what we've got now.  But apparently it never worked.



The security industry has been blogging about this and posting about this and waving their arms in the air trying to get Microsoft's attention.  Perhaps now that a high-profile report has been pulled together, thanks to Dan's reporting, the pressure will have escalated to get this fixed.  You know, each week I listen to Paul and Mary Jo on Windows Weekly.  They are every bit as puzzled by the decisions that Microsoft is making.  I sincerely hope that this is a pendulum swing, and that we are at the nadir now, and that things are going to begin swinging back in the right direction.



I'm still not yet ready to give up on Windows.  It's still the best user experience in the world.  And I know that the enterprise world has no other choice.  It's Windows.  Unfortunately, Microsoft knows that, too.  Let's hope they can and will start getting their act together.  I always say that anyone can make a mistake.  But this feels like bad policy from on high.  What's good policy for us is taking a break right now, Leo.



LEO:  Yes.  And we will undoubtedly talk about this tomorrow on Windows Weekly with Paul and Mary Jo.  I'd be curious what their take is on both of these stories, frankly.



STEVE:  And I think, Leo, I'll take over while you take a sip of something.



LEO:  So it's my turn.  Thank you, Steve.  All yours.



STEVE:  So what else happened this week?  Microsoft - wait for it - has finally added an RSS feed for Windows updates.  Yes, after endless years of pleading from its customers, Microsoft has finally made available an RSS feed for its security updates portal.  I've got the feed link in the show notes.



LEO:  That's kind of amazing that they've never had this.



STEVE:  I know.  And like there's been so much pressure on them, it's like, just can we have it as an RSS?  And it's like, no, we're going to do it this way.  Anyway, they finally said okay.  Now, I think Linux is kind of getting the better of them, Leo.  They sort of seem to be, you know...



LEO:  I'm not saying anything.



STEVE:  So I've got a link in the show notes.  I've got a link to their blog posting announcing it for anybody who's interested.  You can find that there.



Passkeys, the industry's first agreed-upon replacement for passwords, now has its own useful promotional website.  That's at Passkeys.dev, P-A-S-S-K-E-Y-S dot D-E-V.  The home page has that original Google/Microsoft Passkeys demo video that we saw back at Passkeys' launch or announcement.  But the most interesting content for most of us who have been following along is probably a list of, though it's still quite short, of websites where the Passkeys logon experience can be explored.  So under Docs, Tools & Libraries, Test & Demo Sites, we find WebAuthn.io and Passkeys.io.  That's pretty much it for the moment.  



LEO:  That's kind of disappointing.



STEVE:  I know, yeah.



LEO:  Okay.



STEVE:  Yubico has got a demo site, and WebAuthn.me appears to be available for testing.  I don't know whether this list is exhaustive.  I would expect that Apple, Google, and Microsoft at least...



LEO:  It's now in iOS 16, and it's going to be in Android.  I think...



STEVE:  Yup.



LEO:  I feel like probably this list is not exhaustive because there's probably new guys coming on every five seconds, you know.



STEVE:  Let's hope.



LEO:  Let's hope.



STEVE:  Although remember it does take some work at the server end.  And so again, we'll see how this goes.  So hopefully Apple, Google, and Microsoft at least would be supporting their own Passkeys standard soon.  There is a device support page which contains a nice grid of which versions of what support what level of Passkeys.  So anyway, that'll be something to keep our eyes on.



As I mentioned, Cloudflare's quarterly DDoS threat report for the just-ended third quarter of 2022 noted that it had mitigated a large-scale DDoS attack that reached an astonishing 2.5 tbps, okay; or restated, 2,500 gbps.  2,500 gbps.  The attack was launched by a Mirai botnet variant and aimed at the Wynncraft Minecraft service.  I was curious, so I went over to the Wynncraft site, Wynncraft.com.  Never been there before.  It's kind of cool-looking to see what a state-of-the-art Minecraft service looks like.



As I mentioned at the top of the show, Signal said that it will be dropping its longstanding fallback support for sending and receiving SMS and MMS messages in its Android app in order to improve its privacy and security.  As it was, SMS and MMS were only supported under Android, and they were a remnant from the earliest days of Signal when it was known as - remember this? -  "TextSecure."



LEO:  Oh, yeah.



STEVE:  Yeah, TextSecure.  That's what Signal became.  Or that's what - wait.  Signal became of that.  Or something.



Oh.  There was some good news for Windows users.  We previously talked about Windows Remote Desktop Protocol (RDP) finally receiving some relief in the form of failed authentication attempt rate limiting.  With last Tuesday's October updates, Windows 10 and 11, and for what it's worth Windows 7 and Server 2008 R2 if they're on the extended service plans, will have received new Group Policy features which enable the implementation of similar lockout policies for local administrative account logins.  That is, you know, somebody banging on your keyboard while you're away at lunch.



Since I'm sure that this will be of interest to our listeners, I'll share some details.  Microsoft's posting explained.  They said:  "In an effort to prevent further brute force attacks/attempts, we are implementing account lockouts for Administrator accounts.  Beginning with October 11th, 2022" - which was last week, last Tuesday - "or later Windows cumulative updates, a local policy will be available to enable local administrator account lockouts.  This policy can be found under Local Computer Policy\Computer Configuration\Windows Settings\Security Settings\Account Policies\Account Lockout Policies."  That's, you know, under GP Edit.



"For existing machines, setting this value to Enabled on existing machines using a local or domain GPO will enable the ability to lock out Administrator accounts.  Such environments should also consider setting the other three policies" - well, yeah, you kind of have to, you'll see in a second - "the other three policies under Account Lockout Policies.  Our baseline recommendation is to set them to 10/10/10.  This means an account would be locked out after 10 failed attempts within 10 minutes, and the lockout would last for 10 minutes, after which the account would be unlocked automatically.



"For new machines on Windows 11, version 22H2" - which is just now coming out, as we know - "or any new machines that include the October 11, 2022 Windows cumulative updates before the initial setup, these settings will be set by default at system setup."  So this becomes the new default for Windows local account login.  "This occurs when the SAM database is first instantiated on a new machine."  That's the security accounts database.  So if a new machine was set up and then had the October updates installed later, it will not be secure by default and will require the policy settings above.  If you do not want these policies to apply to your new computer, you can set the local policy above or create a group policy to apply the Disabled setting for "Allow Administrator account lockout."



And finally, they said:  "Additionally, we're now enforcing password complexity on new machines if a local administrator account is used.  The password must have at least three of the four basic character types - lowercase, uppercase, numbers, and symbols.  This will help further protect those accounts from being compromised because of a brute force attack.  However, if you want to use a less complex password, you can still set the appropriate password policies in Local Computer Policy\Computer Configuration\Windows Settings\Security Settings\Account Policies\Password Policy."



So it's also noteworthy that other reporting has indicated that a similar feature to block SMB, that is, Windows Printer and File Sharing-based brute-force attacks, is in the works.  So it only took, what, 30 years, and Microsoft is beginning to bring their systems and their network protocols up to a standard that will help protect their users.  Yay.



Other than that, in the past week there were more DeFi and cryptocurrency bridge exploits and currency rip-offs.  No surprise.  As I mentioned, a nasty Fortinet high-end enterprise security appliance zero-day was found.  That was confirmed last week and has been under active use in attacks.  It's now been, unfortunately, fully elucidated in an unauthorized and arguably premature public disclosure.  It's expected that the attacks will soon escalate as a consequence of this.



When Fortinet first learned of the vulnerability privately several weeks ago, they quietly updated their code and sent private messages to their customers urging them to update immediately because of the seriousness of the authentication bypass that had been discovered.  That broke from industry standard protocol of acknowledging an event when it's learned.  Fortinet in this case chose not to go public at the time.  Well, it's certainly public now, and then some.



A week ago, last Tuesday, the White House put out a press release saying that it's working on a cybersecurity label that would be applied to smart IoT devices, similar to the Underwriters Labs UL seal of approval, to help inform Americans which devices "meet the highest cybersecurity standards to protect against hacking and other cyber vulnerabilities."  Okay.  The administration said it plans to meet with vendors, industry groups, and government agencies - that's hopeful - later this month to discuss how this labeling scheme would be managed.  The White House said the new cybersecurity labels will first be mandated for "the most common and most at-risk technologies," which they feel are routers and home cameras, "to deliver the most impact, most quickly."



So it'll be interesting to see how this develops.  What will be the requirements imposed upon devices to receive these cybersecurity approval labels?  Are they going to be worthless labels, or worthwhile?  And how will their application be enforced?  We'll find out.



And finally, the U.S. Treasury's Financial Crimes Enforcement Network (FinCEN) fined the cryptocurrency platform Bittrex $29.2 million for failing to detect and block payments to sanctioned entities and also failing to detect payments to dark web markets and ransomware groups, you know, other financial crimes.  FinCEN said Bittrex made over 116,000 transactions valued at over $260 million to sanctioned entities and connected to criminal activity over the past few years.  Apparently, as few as two minimally trained employees were tasked with monitoring more than 20,000 individual transactions per day.  In other words, Bittrex wasn't taking its monitoring obligations very seriously.  And now they'll have to pay nearly $30 million in fines.  Ouch.



I got a little bit of additional good news on the SpinRite front.  Two more of SpinRite's 378 currently registered development testers weighed in since last week's podcast with some interesting performance numbers that I wanted to share.  The first screen shows a 6.0 TB drive, attached to his machine's SATA port #4, having a full-drive read-scan time of only 7.64 hours.  So that's attached to an, you know, and it's AHCI, not USB, AHCI attached to AHCI port 4.  So 6TB, able to be scanned by SpinRite in 7.54 hours.  So that's not quite a terabyte per hour, it's actually 1.25 hours per terabyte, but it's gratifyingly close.  As we can see, SpinRite really, 6.1 really will be able to scream.



The other recent report shows a similar level of performance through USB.  This guy has 11 drives attached to his machine.  He's a little drive heavy.  Six of them are on USB ports.  One of them was a boot drive.  The one that's highlighted at the bottom is an external 4TB drive that SpinRite 6.1 will be able to completely read-scan in just 5.14 hours.  We've never seen that sort of performance until now.  As I mentioned before, it makes SpinRite practical once again on huge drives.



Until we get to SpinRite 7.1, which will add native hardware USB drivers, SpinRite's maximum USB performance will utterly depend upon the machine's BIOS because that's what we're still going through.  But as we see here, it can, given the right BIOS, be very fast.  Like no reduction in speed over AHCI at all.  I'm sure that was a USB3.0.



And in reviewing the past week's worth of Twitter communications, I encountered a SpinRite success story that I thought would be fun to share.  Ryan Becker tweeted from @rb14060.  He said:  "Steve, wanted to share a SpinRite success story.  A friend of mine called me saying his four-year-old laptop was running incredibly slow, to the point of him not being able to do any work.  Upon my arrival I found that Task Manager was reporting 100% disk utilization with nothing running in the background.  Immediately suspecting a failing drive, I recommended he purchase an SSD and have me clone the drive over.  However, both Clonezilla and Macrium Reflect gave data read errors partway through the drive and aborted the clone.  He's an insurance agent with hundreds of client files and, of course, no backup.



"So I dug up my copy of SpinRite and loaded it to a USB stick, only to have the laptop fail to recognize that the stick was bootable.  Luckily, InitDisk with its FreeDOS option made the laptop recognize the stick, and I was able to copy the SpinRite EXE to the drive.  I set SpinRite off in Level 2, and it took the better part of the day to run."  That's of course SpinRite 6.0 he's got.  "After it completed, I attempted to clone the drive again in Macrium, and this time total success.  He is now happily chugging away with an SSD, and all of his data is intact.  Thanks for a wonderful product.  Looking forward to 6.1 and beyond.  Please feel free to share this on the podcast if you wish."



So first of all, Ryan, thanks for sharing your success.  It's really going to be fun to be fielding a bunch more of those sorts of stories once 6.1 is in everyone's hands.  And note that the first failure of SpinRite 0 to boot is the reason I took the time before doing anything else to create InitDisk.  SpinRite 6.1 will still be able to create a bootable diskette, but only because all of that code is already written, and it's actually kind of cool.  But it's clear that the future is bootable USB thumb drives.  So 6.1 will of course incorporate InitDisk's quite robust USB boot setup technology.  Of course that's why I wrote it in the first place was to be incorporated into all future gizmos that I've got, SpinRite 6 and 7 and beyond and other stuff.



Two pieces of Closing the Loop feedback.  Guillermo Garcia said:  "Hi.  Listener since Episode 001."  He said:  "On SN-892" - so that was last week - "you and Leo discuss the benefits of using uBlock to filter cookie pop-ups.  As I am very much aware of tracking, I take the time to configure each one to deny all, if possible.  I wonder on which state will the cookies be set when the pop-up is blocked.  Maybe they remain on?  Any thoughts?"



Guillermo and everyone, it's interesting.  One of the things that I learned caught me by surprise, which was when I was doing that cookie forensics work many years ago, closely looking at the way cookies were being handled by browsers.  One of the things the cookie forensics system does is it notes the timestamp on the test cookies which performing those forensics tests set.



What we discovered was that if you, depending upon the browser you were using, if you had cookies enabled before, but then you disabled them, some browsers would stop sending the cookies they already had, whereas some browsers apparently stopped receiving new cookies, but did not block existing cookies.  Which I sort of thought was the wrong behavior.  If you said "Turn off all cookies," well, I don't want anything to be sent anymore.  Some browsers stopped receiving them.  So it was a little bit of a glitch which I wondered about at the time.  So anyway, that's what would happen if you turned cookies off is, you know, it's worth turning them off and deleting them all from your browser to make sure it's not one of those that keeps sending them.



And Timo Grun, he said:  "Regarding SpinRite, great to hear about all the exciting developments.  Kind of frustrating to know that it can still only be used on old computers in my attic."  Okay.  So we currently have 378 development testers registered in our GitLab instance.  So many people have machines that will run SpinRite on machines that still only offer a BIOS.  But I am 100% sympathetic to the need for SpinRite to boot over UEFI.  The BIOS is the past, and UEFI is today and tomorrow.  And that's the reason for my changing plans and deferring native support for USB and NVMe until after SpinRite is able to boot and run on any UEFI-only machine without any BIOS and without DOS.  So I will be getting there, Timo and everybody else, as quickly as I can.



Leo, I received your books.  Thank you.



LEO:  Oh, yay. 



STEVE:  Very, very cool.



LEO:  You can throw them out.  You can do anything you want with them.  I just thought they looked like something you might be interested in.



STEVE:  No, no, no, no.  "Hackers Delight" looks like it will be really fun to read through over time.  And the book titled "xchg rax,rax," that is really fun.  So for people who don't know, it is the plainest, most simple book you could ever have.  All it is, you open it, and it is three or four, maybe up to like nine, but sometimes just two, just a few lines of 64-bit Intel assembly code.  No explanations.  No introduction.  That's all they are.  And even the page numbers are in hex.  I mean, this guy is a hacker's hacker.  And I found some quotes from him that I thought would be fun to share.  He said, "Initially I wanted to publish the book through a publisher.  But no publisher wanted to go with the" - and he has in quotes "minimalistic," which is to say the least - "design."  He said:  "One publisher said he would publish the book if I added explanations for every code snippet."



LEO:  No.  No.



STEVE:  And he said:  "But I was not willing to do so."  He said:  "Another friend recommended adding QR codes to the pages, linking to explanations, but I wasn't willing to do this either."



LEO:  Good for him.



STEVE:  He said:  "I remember trying to decipher each of those code snippets myself."  So maybe he collected them.  He says:  "I felt that I could not let my readers down by handing them an easy solution."



LEO:  Good.



STEVE:  So then somebody reviewing the book wrote:  "Much of the joy of the book comes from discovering the nuances of these tiny programs.  Many rely on assembly-specific tricks that do not really translate 'up' into the virtual machines defined by higher level programming languages."



And then he said something interesting:  "Those who have trouble deciphering can view xorpd's" - that's the pseudonym of the author of this - "can view xorpd's video series titled 'Assembly Language Adventures,' which teaches assembly programming beginning with the very basics, reaching a level of expertise through 29 hours of instruction.  Xorpd created the book as a companion piece while working on the video, pulling his favorite assembly snippets together."  So I've got a link in the show notes, xorpd.net/pages/x86_adventures.html, which will take you there.



And there he says:  "What is Assembly Language?  A computer only knows" - this is him writing.  "A computer only knows how to execute a small set of commands, or instructions.  Those are really simple commands, such as adding or subtracting numbers, comparing numbers and so on.  Assembly language is the language of those commands.  Using assembly language you can create computer programs that instruct a computer to do things in the most basic level possible."



He says then:  "Why learn x86 assembly language?"  And he has four bullet points.  "You are the kind of person who really likes to know how things work.  In this course you're going to get solid understanding on how computer programs work from the inside.  Two, become a better programmer.  Knowing how things work down there will help you make better decisions, even as a high-level programmer."



LEO:  I agree.  I agree.  Yup.



STEVE:  Yup.  "If you were always wondering what is the 'stack,' or what are those pointers everyone talks about, you came to the right place.  Three, write faster code.  When you really want to get the most of your processor, writing in raw assembly is needed.  We're not going to talk about optimizations in this course.  However, you will get a solid foundation so that you can continue exploring on your own.  And finally, fourth, you want to become a reverse engineer or a security researcher, read the code of viruses, or look for software vulnerabilities.  As most of the time the original source code will not be available to you, solid understanding of x86 assembly language is mandatory."



So Leo, again, thank you for the two books.  They will absolutely find me spending some time with them.  They look great.



LEO:  Good.



STEVE:  And lastly I wanted to share a little bit of additional ZimaBoard goodness.  I know that a lot of our listeners loved learning about ZimaBoard last week because I got a lot of tweets and feedback.  I've spent all of the last week deep into working on SpinRite, and that work has been exclusively with the $120 ZimaBoard single-board x86 computer that I talked about last week.  I love it.  One problem I had was that when I'm doing things at the machine level in the debugger, I might manually move the program counter somewhere out of sequence to force some piece of code to run.  Or often sometimes it's like to make something run, like to rerun a subroutine where the result wasn't what I expected.  So I want to go back to it now, and this time go into it, whereas last time I stepped over it.



So at the machine level in a debugger, this is easy to do and super useful.  But it can leave the machine in an unstable state if I attempt to then proceed or exit.  The bottom line is that the machine is often hanging hard so that the keyboard's famous three-fingered salute, CTRL-ALT-DEL, does nothing.  And unlike most, but not all, desktop machines, because it's meant to be used as an embedded appliance, the ZimaBoard doesn't have a hardware reset button.  So whenever the board was hanging, I had been forced to pull the power, wait a second, and plug it back in.  I hate doing that.  And since my current test drive is a 2TB Seagate spinner, it's being forced to spin up and down, which I also hate.



But when I had disassembled the ZimaBoard earlier, upon receiving it, you know, of course I took it apart.  That's a required part of the process of getting to know it and falling in love with it.  I noticed 14 printed circuit pads arranged in a 2x7 grid at the card's edge, just in front of the external PCIe connector.  And the case had a cutout there which allowed those 14 pins to be accessed externally.  So I was hoping that among those pins might be a signal ground and a hardware reset line to which I could attach a hardware reset button.



So over the weekend, this past weekend, I decided to seek the help of ZimaBoard's creators.  I found that they maintain a very active community on Discord, so I jumped online and posted my question in the evening, I think it was like Saturday evening, to their support channel.  The next morning, when I awoke and checked, not only was there an answer, but my prayers were answered.  It turns out that the 14-pin pads are a complete PC front panel extension.  There's a power button, a reset button, and connections for three activity LEDs.  I've got a picture of that in the show notes for anyone who's interested, and a link to the page that that came from over at docs.zimaboard.com.



So anyway, that's the last thing I needed.  When I've been testing SpinRite on all these various motherboards, I've not been mounting them each in a case because there's just no need to.  But I have needed a power switch and a reset button.  And I found this cool little - it's like a little round hockey puck that's got a big power switch and then a little reset button.  I actually use them in reverse.  The big easy-to-press button is reset because I'm doing it so often.  And the little button powers the machine on and off.  And so, and it's just got a - I got it for like, it was a little over $6 from Amazon, that gives me a little portable, you know, an external power switch and reset button, which is really handy.  So I've got another one of those that I'll be - actually two that I'll be hooking up to each of my ZimaBoards at my two locations.  And now my life is complete.



LEO:  Let's talk about changing your passwords automatically.



STEVE:  Yeah.  We're going to begin with a brief refresher about so-called "well-known" website assets.  The most famous of these is the venerable "robots.txt" file.  When automated spiders or bots began exploring the web, and as websites began evolving beyond a collection of static web pages, it started becoming possible for bots to get stuck in infinite loops at a site, like following a link that led to another page, that had a link that led back to the first one, and so forth.  Or they might begin rapidly requesting all of a site's dynamically generated web pages to place an undue burden on the site's web server.



So a convention was created.  When a bot would enter a site, it would check for a specific file named "robots.txt" residing in the root directory of the site, so "/robots.txt."  If present, that file would provide bots with a series of hints, essentially some guidance metadata, about where they could and could not safely venture.  For example, my GRC.com website has a robots.txt file.  It contains, first of all, it has a statement, User-agent: *, meaning this applies to all who come.  Then I have a Disallow: /x.  That is an alias for GRC's scripts directory, where it makes no sense for any search engine to wander.  In some cases I wanted friendlier URLs without the /x, so I translate those on the fly.  But bots would see something else.



So I'm also, for example, asking bots to stay away from any URL beginning with /ppp, since those are GRC's Perfect Paper Passwords pages, where it doesn't make any sense for a bot to go.  And I also ask them to stay away from the cookies forensics page because, again, same reason, it's an automated test that doesn't make any sense for a search engine to dip into.  And TWiT.tv has a robots.txt file which looks like this.  I have it in the show notes.  It's got a big TWiT.tv spelled out in block ASCII characters.  Then also it has a User-agent: *.  Leo's page has a Crawl-delay: 10, which asks spiders, bots, and search engines to only pull one page every 10 seconds, and also offers a sitemap to them which allows them to discover things that they want search engines to see that they might otherwise not find for themselves.



LEO:  Yeah, that's supposedly good practice.  I don't know.



STEVE:  Yes, yeah, exactly.



LEO:  If you say it's good, I'll keep doing it.



STEVE:  You know, you and I have comparatively simple sites.  Amazon.com's robots.txt file is 152 lines of mostly "Disallow" URLs.  Facebook's weighs in at a hefty 610 lines.



LEO:  Oh ho ho.



STEVE:  Yeah.



LEO:  We only have three.



STEVE:  Yeah, exactly.  You have three.



LEO:  We do have a cool TWiT.tv logo in our robots.txt.



STEVE:  Yeah, yeah, yeah.  Facebook, 610 lines.  And it's sort of entertaining to browse through.  It begins with an off-putting block of text which reads:  "Notice:  Collection of data on Facebook through automated means is prohibited" - this sounds like Mark - "unless you have express written permission from Facebook and may only be conducted for the limited purpose contained in said permission."  And then there's a - it says "See" and then he's got a URL to site_scraping_tos_terms.



LEO:  Holy cow.



STEVE:  I know.  Then it goes on to list, by bot, where they are permitted to venture and where not.  We have the Applebot, the Baiduspider, Bingbot - you've got to love just saying "Bingbot" - the Discordbot, something known as facebookexternalhit, also the Googlebot, the Googlebot-Image, ia_archiver, LinkedInBot, MSNbot, Naverbot, Pinterestbot.  And then we have, I kid you not, the "Screaming Frog SEO Spider."  We've got the SeznamBot, something called Slurp, Teoma, the TelegramBot, the TwitterBot, Yandex, and Yeti.  And those were all "Disallow" URLs in a long list.  Then the file goes through the entire list again giving them specific "Allow" URLs.



So anyway, obviously there are many bots roaming the Internet these days.  Of course, there's no practical way for any website to refuse to serve pages to any agent that wishes to request them.  In other words, there's no enforcement mechanism.  The whole robots.txt facility is simply advisory.  But when it was recognized that beyond a single "robots.txt" file there were a great many more sorts of metadata that websites might wish to publish, not to users but to automated visiting bots, scanners, other tools, who knows what, it became clear that a more mature mechanism was needed.  And the first order of business was to avoid cluttering up the site's root directory with a growing number of random metadata files.  So the World Wide Web Consortium, the W3C, standardized upon the placement of everything else into a specially designated subdirectory off of the root.  That directory is named .well-known, or /.well-known/.



Wikipedia explains it this way.  They say:  "A well-known URI is a Uniform Resource Identifier for URL path prefixes that start with /.well-known/.  They're implemented in web servers so that requests to the servers for well-known services or information are available at URLs consistent with well-known locations across servers.  Well-known URIs are Uniform Resource Identifiers defined by the IETF in RFC 8615."  So that's a relatively recent one.  They say:  "They are URL path prefixes with the start of .well-known.  This implementation is in response to the common expectation for web-based protocols to require certain services or information be available at URLs consistent across servers, regardless of the way URL paths are organized on a particular host."  In other words, let's eliminate website-to-website variations for this one particular purpose.



So Wikipedia says:  "The URIs implemented in web servers so that requests to the servers for well-known services or information are available at URLs consistent with well-known locations across servers.  The IETF," they write, "has defined a simple way for web servers to hold metadata that any user agent, for example, a web browser, can request.  The metadata is useful for various tasks, including directing a web user to use a mobile app instead of the website or indicating the different ways that the site can be secured.  The well-known locations are used by web servers to share metadata with user agents.  Sometimes these are files, and sometimes these are requests for information from the web server software itself."  Meaning another URL.  "The way to declare the different metadata requests that can be provided is standardized by the IETF so that other developers know how to find and use this information."



Okay.  So Wikipedia lists 46 different items, which are currently defined under the .well-known, including the one that we'll be talking about in a minute.  So Wikipedia is keeping itself current because this was just released.  Most of the 46 well-known item names are obscure, but a few are interesting.  There's "keybase.txt," which Wikipedia says is "Used by the Keybase project to identify a proof that one or more people whose public keys may be retrieved using the Keybase service have administrative control over the origin server from which it is retrieved."  In other words, it's an authentication mechanism located at .well-known/keybase.txt.



The one we originally talked about when we first introduced the concept of the .well-known subdirectory was "security.txt."  Wikipedia reminds us that:  "Security.txt is a proposed standard for websites' security information that is meant to allow security researchers to easily report security vulnerabilities.  The standard describes a text file called 'security.txt' in the .well-known location, similar in syntax to robots.txt, but intended to be machine- and human-readable, for those wishing to contact a website's owner about security issues.  Security.txt files have been adopted by Google, GitHub, LinkedIn, and Facebook."  And doubtless countless others.



So as we know, security researchers have been frustrated in the past by the difficulty in finding the person who should receive problem reports.  They'll send an urgent email to the Contact Us info, you know, the only thing they can find, and either never receive any reply, or receive a canned "Thanks for contacting us.  Your query will be examined, and the proper person will get back to you shortly," if that ever happens.  So the idea behind "security.txt" located in the .well-known directory, is to allow a site's technical support staff, likely not upper management, but the guys who are actually down there pulling wires, to prearrange a means for being directly informed of things that they want to know that someone might discover.



Now, two engineers at Apple, Ricky Mondello and Theresa O'Connor, realized that an addition to the existing .well-known facility could be employed to help users, and perhaps their password managers and other tools, to know where to go to change their passwords for any supporting site.  It's a small thing, but some of the best ideas are.



As we know, the problem is that there is no commonality among websites for logging in and out, and managing one's identity.  It's a completely ad hoc invention over and over again for each website.  We're beginning to see some coalescence of UI features, you know, the idea of account management being located in the upper right corner of website pages.  That's becoming increasingly common.  But what is definitely lacking is any generic direct access mechanism which would allow a user, or some automation, to get to specific aspects of a site's account management.  In every case, it's currently necessary to click on a series of links, looking at the result of each click, make a best guess as to what to click on next, as we navigate toward our desired account management feature.



The idea that occurred to Ricky and Theresa was to add an object to the .well-known collection named "change-password."  Whenever that resource was requested, the reply would be a URL which the requester should then follow in order to be immediately presented with a site's password-change page.  Once this has caught on, you can imagine that password managers and web browsers would add a "change site password" feature to their own user interfaces.  When the user visits a site, just as browsers currently request the favicon to show the site's small icon identity, they would passively query for the presence of the site's "change-password" resource in the .well-known subdirectory.



If the query returns a "404 Not Found," then the browser's or password manager's "change site password" option would be disabled and grayed-out.  But if the query returns a change URL password, the client's UI feature would be enabled.  And if its user should click on "change site password" feature, they would be immediately jumped to the proper page at that site, having short-circuited any and all intermediate stages to get there, creating complete unification.



It's true that some password managers have taken it upon themselves already to offer somewhat similar features across a limited and specific collection of sites.  But this has been accomplished through brute force automation of the user-facing user interface for a specific site, which is prone to failure if or when a site upgrades or changes its users' experience.  What this new "change-password" standard accomplishes is to provide a means for cutting out all intervening guesswork and intermediate stages to provide a URL which will take its visitor directly to that page.



And I mentioned that it's a standard because it is.  The W3C has taken this up and has published the first draft of this new addition to the .well-known website metadata.  Its specification page is titled "A Well-Known URL for Changing Passwords."  Which is somewhat unfortunate since some of the tech press has apparently only read the title and assumed that this was more than it is.  As we've seen, this doesn't actually change anything.  It simply redirects its visitor to the website's password change page, in the process transparently bypassing all of the intervening steps.  So it's a "baby step."



We could wish that the URL returned an XML-format SOAP-style API endpoint which would entirely automate the process of authenticating the user with their current username and password, accept the replacement password, and confirm that this update has been made at the server side.  But that's not what we're getting.  But we're getting the first small baby steps in that direction.  It has the benefit that it should be quite easy to implement, and any common frameworks should be able to easily support it so that it can become widespread quickly.  Again, a useful baby step.



LEO:  Well, we're making progress, anyway.



STEVE:  Yup.



LEO:  You wouldn't expect a unified way of doing this.  I guess maybe there could be, but...



STEVE:  Oh, yeah.  I mean, it wouldn't be difficult for someone to specify that at all.



LEO:  Yeah.  Someone to specify it, but then somebody else to adopt it in the million pages.



STEVE:  Yeah.  And it really would be cool, when you think about it.



LEO:  Oh, it would be, be so huge. 



STEVE:  Well, if Bitwarden, even at this level, if Bitwarden or LastPass, for example, had in their dropdown "Change site password," that jumped you for any site immediately to their password change page, that would be very cool.



LEO:  Oh, be great, yeah.



STEVE:  So this gives us at least that much.



LEO:  Good.  Yeah.  Anything that could automate this process would be very valuable.  Would Passkeys help this?  Could there be an automated system for that?  I guess not because they're no credential to change; right?  You don't...



STEVE:  Right, Passkeys actually, thank god, it obsoletes usernames and passwords completely.



LEO:  Right.



STEVE:  It replaces them both.



LEO:  But you must allow, I know you do in SQRL, allow a way for somebody to change their proof; right?



STEVE:  Oh, sure, yeah, yeah, yeah.  You can still have a username and password, and you are able to change your SQRL identity.  Actually the way you do it is you give them both your old and your new.  So it validates you with the old and then switches over to the new.



LEO:  But there is that issue, and I don't know what's going to happen with Passkeys if I lost my phone, you know, there are issues.  I mean, this has - there are times you have to change this stuff; right?  Even with Passkeys and SQRL.  



STEVE:  Yup.  And that's what was so nice about SQRL was if you lost your phone, it didn't matter because your entire identity was a single QR code.



LEO:  Right, right.



STEVE:  And you just put that in a drawer somewhere.



LEO:  Steve Gibson.  See, that's - I think that's the obvious and smart way to do it, but okay.  Fine.  Steve is the best.  GRC.com's the place to go.  You can get a copy of the show there as well as our site, GRC.com.  The GRC version actually there's two unique versions.  There's a 16Kb audio version, which is, what is that, one seventh, one sixth the size.  One sixth the size, I guess.  And the advantage of that is you can download it in a limited bandwidth situation.  Doesn't sound great, but it's there.  That's the version that Elaine Farris uses because she's, I don't know what, she's got a satellite Internet, I guess, and she's in the middle of horse country.  She does the transcriptions, downloads it and types it all out.  And that makes it easy to read.  It's great for search.  It's great to read along as you listen.



Of course Steve has the full quality audio, the 64Kb audio, as well.  GRC.com.  While you're there, pick up SpinRite, the world's best mass storage and maintenance recovery utility.  I was using it the other day, and I saw, oh my god, the copyright 2004.  But if you write it right, you don't need to fix it all the time.  However, I'm hoping it will be a 2022 copyright, if not, a 2023 copyright for 6.1.  That's due imminently.  Probably - maybe early 2023.  Let's not rush it.



STEVE:  Yeah.  Yeah, it's going to be in full - it'll be running within a week or two, and then in alpha test.



LEO:  Right.  We've got to make sure it's perfect before we release it.  But if you buy now, you'll get a copy of that when that comes out, any day now.  We have a copy of the show.  We have actually two copies.  We have the audio that Steve has, but we also have a video, if for some reason you want to watch.  And we do, you know, there are some visual aids.  There's stuff to see.  There's pictures.  That's at TWiT.tv/sn. 



There's also, of course, a YouTube channel dedicated to it.  That's only video, but a great way to share it with somebody else because YouTube will let you share a little clip, which is nice.  And then probably the best way to get it, if you want to listen every week, and I'm sure you do, is subscribe to it in your favorite podcast player.  And that way you'll get it automatically the minute we put it out.



We do Security Now! on Tuesdays at about, it was a little late today, sometime between 1:30 and 2:00 p.m. Pacific, 4:30 and 5:00 p.m. Eastern, 20:30 UTC.  You can watch us do it live.  That's why I mention the times because we actually do stream it as we do it.  And that's why the time varies, because shows before go long or whatever.  But you can watch the stream at live.twit.tv.  You can also chat with us.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#894

DATE:		October 25, 2022

TITLE:		Data Breach Responsibility

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-894.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we note the release of an updated Firefox browser and Google's welcome and interesting announcement of a super-secure-by-design open source operating system project.  We look at the latest cryptocurrency craziness and at a new Windows zero-day which bypasses downloaded executable file security checks.  And speaking of zero-days, Apple just patched their iPhone and iPad OSes against their ninth zero-day of the year.  We then take a look at the forces driving the evolutionary demise of previously rampant banking malware and at today's critical VMware update.  Then, after sharing and addressing some interesting listener feedback, we'll take a look at new Australian legislation aimed at punishing data breaches and consider the ethics of Australia's proposed new heavy fines.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Google has created the perfect operating system.  Is it possible to be absolutely secure?  We'll talk about Apple zero-days, Windows zero-days, cryptocurrency craziness, and then who's responsible for that data breach?  Is it you, or the technology you're using?  All that coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 894, recorded Tuesday, October 25th, 2022:  Data Breach Responsibility.



It's time for Security Now!.  Yay!  All week long we wait for this show where Steve Gibson is going to explain it all to us.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again for the final episode of September.



LEO:  Of October.  October.



STEVE:  October.  I don't know what month it is.  October, yes.



LEO:  It's the spooky season.  Do you wear a Halloween costume?



STEVE:  You know, I think, and I don't have any memory of this, I think I must have been really scared by, like...



LEO:  You don't like it.



STEVE:  I don't like it.  When I was like three or four or something, I just must have just had the crap scared out of me.



LEO:  Because I have this ketchup hat.  And I was going to volunteer - I also have a mustard hat.  You could wear that, and we'd...



STEVE:  We'd be the condiment twins. 



LEO:  Condiment twins.



STEVE:  That's really my goal.



LEO:  Forget that.



STEVE:  Anyway, no, I've never been a fan of Halloween.  So, but I am a fan of thinking about our industry, which brought the title "Data Breach Responsibility" to this podcast.  I encountered, and we'll get to it by the end, the news of a bracing legislation being proposed this week in Australia to really up the ante on the penalties for data breaches following a string of very embarrassing high-profile problems in Australia in the past month.  And it was a great opportunity to sort of step back a little bit and think about whose responsibility our data breaches are.  And what's neat is that the last several years of this podcast has given us a perfect foundation for having this discussion.



So anyway, this is Security Now! Episode 894 for the 25th of, yes, it is October.  We're going to talk before that about some news of a new Firefox browser release, Google's welcome and interesting announcement of a secure-by-design new open source operating system project, which will probably never amount to anything, but it's exactly the kind of thing we need to be, like, taking seriously.  So I'm glad they're doing it.  We're also going to look at the latest cryptocurrency craziness and at a new Windows zero-day which manages to bypass downloaded executable file security checks, unfortunately.  Actually, what's unfortunate is that Microsoft's not so sure it's real because they couldn't reproduce it.  Oh, my god.



And speaking of zero-days, Apple just patched their iPhone and iPad OSes against the ninth zero-day of the year, which we'll touch on.  We then take a look at the forces driving the evolutionary demise of what was previously rampant banking  malware and at today's critical, today as in it happened this morning, critical VMware update.  Then after sharing and addressing some interesting listener feedback, as I said, we're going to wrap up by taking a look at this, well, at the downstream effects of really upping the ante on data breach penalties and why that may not be the best thing to do.  And we do have a great Picture of the Week which needs some explanation.



LEO:  Yeah, because I'm not laughing, I'm just looking.  Cool.  A great show ahead, as always.



STEVE:  I think so.



LEO:  All right.  I'm ready for you to explain this post in the ground.



STEVE:  So, okay.  For those who are not seeing this, we have - in the background is a sidewalk, and then sort of a bricked region which abuts a street-side curb.  This was supposed to be a parking meter.



LEO:  Ah.



STEVE:  And the meter is missing.



LEO:  Yeah.



STEVE:  And what's so fun about this, and this is, you know, at the height of geekdom, is that the lower end of this parking meter's mounting pole which the meter would be sitting on...



LEO:  Now I get it.



STEVE:  Is number 404.



LEO:  404, meter not found.



STEVE:  Meter not found.  So Joel Rownak...



LEO:  That's geeky as heck.



STEVE:  ...tweeted me - that is so great.  Joel Rownak tweeted this picture.  He said:  "Steve, I thought you'd get a kick out of this.  There's supposed to be a parking meter there.  I chose this parking spot because the meter was missing, a real money-saving opportunity."



LEO:  Yes.



STEVE:  "I noted that the meter number was 404.  I don't need to tell you what that means in HTTP."  So anyway, Joel, thank you for a great listener feedback Picture of the Week.  That's a goodie.



LEO:  That's a unique one.  You're probably the first person to get this one.  That's good.  I like it.



STEVE:  I think so.  And, you know, probably the last because who's going to...



LEO:  You've got to be pretty geeky.  I stared at it.  I was trying to figure out what - I don't get it.  And then, yeah, now that you've explained it, of course, yeah.



STEVE:  It's good.  Okay.  So Firefox 106 is out.  Last Tuesday, a week ago, Firefox 106 became publicly available with a collection of new features and security improvements.  Feature-wise, and there are a couple of cool things here, it's now possible to edit PDFs, including writing text, drawing, and adding signatures.  And setting Firefox as the default browser also makes it the default PDF application for Windows.  So I thought that was interesting, you know, to be able to actually modify PDFs in a browser.  I guess that's good if you need to do that.



Also in Firefox private windows can now be pinned to the Windows taskbar in Windows 10 and 11 for simpler access.  And they have been redesigned to increase their feeling of privacy.  Now, that's what Mozilla said.  I looked at it and, okay, you know, I'm not sure what the change was.  They're very dark and very sparse-looking, so I guess that's more private-feeling.



Also, swipe to navigate with two fingers on a touchpad, you can swipe right or left to perform a history back or forward motion.  I actually like that on my iPad.  I did that once in front of Lorrie, she said, "Wait, what did you just do?  How did you do that?"  I go, "Oh, well, you know, it's a geek thing."  Anyway, it now works for Linux users on Wayland.  And Wayland, for those unfamiliar with the term, will be the successor to the original venerable X Windows system, which is how the standard GUI in Unix land which has survived today is being succeeded by something called Wayland.



Also for this version of Firefox on macOS 10.15 and higher, text recognition has been added to images, which allows you to extract text from selected images, like a meme or a screenshot or whatever, and the extracted text is copied to the clipboard in order to share, store, or search without needing to manually retype what you're seeing in the picture, which is kind of a cool thing.



Firefox's WebRTC capabilities also received a major upgrade with their move, or Firefox's move, from their use of libwebrtc went from version 86 to 103, which brought a bunch of improvements:  better screen sharing for Windows and Linux Wayland users, lower CPU usage and increased frame rates during WebRTC screen capture on the Mac, RTP performance and reliability improvements, richer statistics, and cross-browser and service compatibility improvements.  So just a bunch of nice upgrades to the browser which is all of our favorites, many of our favorites.  So it continues to  move forward.  There are also a bunch of security improvements, nothing breathtaking.  I looked through them, it's like, okay, I'll just say that there are a bunch of them instead of enumerating them.



Okay.  So driven by events and evidence I've - and I'll be doing this at the end of the podcast also - often bemoaned the increasingly sad state of affairs which describes the insecurity of today's operating systems and software.  I've said that someday, somehow, this has to change.  So it was with some interest that I stumbled upon an announcement of just exactly this sort of change last week which I wanted to share.



I'm not suggesting, as I said at the top, for a second that this particular effort by three talented software engineers at Google will ever amount to anything more than a learning experience for them and hopefully for others.  But even if it never moves beyond GitHub, where it is, this is the sort of shape that an eventual effort will take.  And I'm quite clear about one thing.  No closed commercial OS is ever going to get us there.  All the popular operating systems we're familiar with - Windows, Linux, Unix, macOS - were designed with security in mind, but in a fundamentally ad hoc fashion.  That's just the way operating systems have been designed sort of generically until now.  They were all designed to have security features, but none of them were provably secure-by-design operating systems.



And, you know, they're all old; right?  I mean, that's just the way things were.  Security wasn't the issue that it is beginning to finally become today.  So it's instructive, I think, to listen to the language these three developers use to describe their effort.  It's a different way of thinking, and it's the language of the future.



So they said:  "As we find ourselves increasingly surrounded by smart devices that collect and process information from their environment, it's more important now than ever that we have a simple solution to build verifiably secure systems for embedded hardware.  If the devices around us cannot be mathematically proven to keep data secure, then the personally identifiable data they collect, such as images of people and recordings of their voices, could be accessible to malicious software.



"Unfortunately, system security is often treated as a software feature that can be added to existing systems or solved with an extra piece of hardware.  This generally is not good enough.  Our team in Google Research has set out to solve this problem by building a provably secure platform that's optimized for embedded devices that run machine-learning applications.  This is an ongoing project with plenty left to do, but we're excited to share some early details and invite others to collaborate on the platform so that we can all build intelligent ambient" - I love their use of this word, this term "ambient" - "intelligent ambient systems that have security built-in by default."  And actually they meant designed-in by default.



They finish, saying:  "To begin collaborating with others, we've open sourced several components of our secure operating system, called KataOS, on GitHub, as well as partnered with Antmicro on their Renode simulator and related frameworks.  As the foundation for this new operating system, we chose seL4 as the microkernel because it puts security front and center.  It is mathematically proven secure, with guaranteed confidentiality, integrity, and availability."



Okay.  So I'll just note that seL4 is an open source, verifiably secure microkernel with a heritage going back to the mid-'90s. So this is exactly the right sort of foundation for a more functional OS to then be built upon.  They also use an seL4 framework known as CAmkES, I guess you'd pronounce it, C-A-M-K-E-S, to provide static, what they said is statically defined and analyzable system components.  The CAmkES stands for Component Architecture for micro kernel-based Embedded Systems.  It's a software development and runtime framework for quickly and reliably building microkernel-based, what they said was multiserver operating systems.



It follows a component-based software engineering approach to software design, resulting in a system that is modeled as a set of interacting software components.  They said these software components have explicit interaction interfaces and a system design that explicitly details the connections between the components.  So all of that kind of feeling, this is the language of where we have to be headed.  It's utterly different from the essentially ad hoc design of all of today's operating systems that we're familiar with, where random coders grafted on, and are still grafting on today, new pieces of stuff.  And, you know, after doing so, away it went.  Ship it.  And in the case of Windows, we'll find the bugs later.



Anyway, continuing with the Google engineers' description of their work and offering, they said:  "KataOS provides a verifiably-secure platform that protects the user's privacy because it is logically impossible for applications to breach the kernel's hardware security protections, and the system components are verifiably secure.  KataOS is also implemented almost entirely in Rust, which provides a strong starting point for software security, since it eliminates entire classes of bugs, such as off-by-one errors and buffer overflows.



"The current GitHub release includes most of the KataOS core pieces, including the frameworks we use for Rust such as the seL4-sys crate" - whatever that is - "which provides seL4 syscall APIs" - okay, now I know what it is - "an alternate root server written in Rust," they said, "needed for dynamic system-wide memory management, and the kernel modifications to seL4 that can reclaim the memory used by the root server.  And we've collaborated with Antmicro to enable GDB debugging and simulation for our target hardware with Renode.



"Internally, KataOS also is able to dynamically load and run third-party applications built outside of that CAmkES framework.  At the moment," they said, "the code on GitHub does not include the required components to run these applications, but we hope to publish these features in the near future.



"To prove-out a secure ambient system in its entirety, we're also building a reference implementation for KataOS called Sparrow" - okay, so Sparrow will be the reference implementation for an instance of KataOS, they said - "which combines KataOS with a secured hardware platform.  So in addition to the logically secure operating system kernel, Sparrow includes a logically secure root of trust built with OpenTitan on a RISC-V architecture.  However, for our initial release, we're targeting a more standard 64-bit ARM platform running in simulation with QEMU."



And finally they said:  "Our goal is to open source all of Sparrow, including all hardware and software designs.  For now, we're just getting started with an early release of KataOS on GitHub. So this is just the beginning, and we hope you will join us in building a future where intelligent ambient machine-learning systems are always trustworthy."



So I have a link in the show notes to the project, actually to their announcement on opensource.googleblog.com.  But I wanted to share this because, again, this is the sort of effort we're needing to begin to gain experience with the shape of an entirely new class of software which won't be fragile by design.  We finally, I believe, have sufficiently inexpensive processing power and memory so that design compromises won't be driven and arguably needed in order to make this sort of next-generation system viable.



We've spoken before about how Windows' original designers, among many other wise decisions which have since been breached, placed Windows GDI, its graphical device interface module, outside of the Windows kernel, up in userland, where its faults would not give attackers access to the kernel.  But in their quest for more performance, Microsoft chose to discard security concerns and move GDI into the kernel.  As we know, the result has been an apparently endless stream, because they're still coming, of serious vulnerabilities which were created because this extremely complex and error-prone code library, GDI, was given full kernel privileges for the sake of performance.  So obvious and clear tradeoff.



Now, it's true that the ivory tower design of an operating system can be compromised.  We've seen it.  But we can hope that now, given the power, the performance, the memory that we've got, that a system which has been built for the express purpose of exploring and possibly of creating truly secure code will now never need to make such a mistake.  So again.



LEO:  Do you think that, though, something designed from scratch like this, I mean, obviously it's better to start to be secure by design.



STEVE:  It's the only way to start.



LEO:  But if you have something that, I mean, okay, I'm not going to include Windows in this.  But an existing operating system that has had years to be patched and fixed and updated, and then can be hardened, I almost think I'd prefer - I'll give you an example.  There's a secure Linux called Qubes, you're probably familiar with that.



STEVE:  Yeah, of course, right.



LEO:  And it's everything sandboxed.  It is the Linux kernel, but it's designed to be fully secure.  I feel like I almost would trust that better because people have been banging on this open source code for two decades, three decades now.



STEVE:  So I completely agree with you.  If you wanted something usable, yeah.



LEO:  Well, that's for sure.



STEVE:  That's the problem is that this is all ivory tower, pie-in-the-sky, here's how we make something that is absolutely unbreakable.  And then someone says, okay, what kind of word processor does it have?



LEO:  Right.  And even then, by the way, Qubes is not very usable, either.  But then, by the way, no one's perfect.  Even these academics, they could...



STEVE:  Well, they don't have - you can't do anything with it.



LEO:  Yeah, but they may have accidentally left in something, too.



STEVE:  No, now, that's the point I want to make.  When they say "provably secure," Leo, they mean that it is testably, mathematically - this is a different way of programming.  It's like how I'm upset when someone says all software has errors.  No.  There's no reason that, I mean, software can be perfect.  If the hardware is correct, the hardware doesn't have bugs, then software is just math driving the hardware.  It can be perfect.



LEO:  Is Rowhammer a software error or a hardware error?  That's hardware; right?



STEVE:  Hardware.  Absolutely.



LEO:  So these wouldn't necessarily be immune to Rowhammer.



STEVE:  Correct.  Correct.  So it needs a hardware platform which is secure.  And now, you know, you raise a good point.  And so is Spectre and Meltdown.  Spectre and Meltdown were edge cases where there was a way to bypass some optimizations by leveraging this.  But there's a whole class of software problems, all the buffer overflows, all the off-by-ones, all of the use-after-free problems, you know, this whole big batch of things can be fixed.



And so, again, I'm not suggesting for a second that this will be the solution or that in any way this is practical.  But we're not going to get to where we need to be if we keep doing things the way we have been because we have been doing them.  And we've even been talking about security.  Yet we're not, you know, things are not getting any better.  And I agree with you.  As everyone knows, I just wish Microsoft would stop changing Windows.  Leave it alone and fix it.  Because the more they add things, everything they do is going to - it brings new bugs into the system.  So yeah, the idea of...



LEO:  Yeah, it's hard to fix a moving target, absolutely.



STEVE:  Oh, you can't.  It's impossible.  All the evidence suggests it.  Anyway, I just - I wanted to sort of introduce this because again, this has been kind of quietly percolating in the background.  But the problem has always been that when you start to do something with it, the architecture becomes a stumbling block.  Just like it did with Windows.  Windows had a client-server architecture.  And that's what this is.  When they talk about a root server, they're talking about the microkernel is a server with clean transactions to clients which are running with no privileges.  And there's nothing that the client can do that can make the microkernel make a mistake, as long as the microkernel is written correctly, and there are ways to mathematically demonstrate there are no bugs.  I mean none.  There is not a bug.



And again, because it is math, it is entirely possible for software not to have any flaws.  And so the only way we're going to get where we could be is if we fix our hardware and we fix our software.  And this is what the fixed software looks like.  It's better to have software you don't ever change and you only fix bugs.  I completely agree with you.  And you can actually use that for something.  You can't use this stuff for anything because it's brand new.  And it's not emulating any existing system.



LEO:  It probably doesn't have any I/O either, because I/O is - very difficult to prove I/O correct.



STEVE:  Oh, Leo, you don't want I/O.  No, you can't trust any input.



LEO:  I/O is nonfunctional, yeah.  The side effects are the opposite.



STEVE:  No, you can't.  Who knows what someone's going to send it.  No.



LEO:  Yeah, right.



STEVE:  No, you just have to put it in a box and go, ooh, isn't that lovely.



LEO:  So I would submit that's a toy.  You know.  I mean...



STEVE:  But that's where we're going to start.



LEO:  Yeah.



STEVE:  Of course it's a toy.  It is a toy.



LEO:  When software's provably correct, it would have to be functional; right?  You would have to be able to know that there is no side effects, that what goes in comes out every time the same thing.



STEVE:  That's what it means.  That's what it means.  I mean, again...



LEO:  That's where I/O gets you in trouble.  



STEVE:  This is the notion that we're discussing this to convey, is that it is not the case that it is impossible to produce a perfect operating system.  It is possible to produce a perfect operating system.  Now, at the moment it's not usable.  But it is perfect.  I mean, it is - you can mathematically prove its perfection.  So there are mechanisms in place now in the ivory tower for doing that.  And Google is saying, okay, let's take that and see what can be done.  So again, we're not going to get there unless we do this.  And yes, I know this sounds a little bit like SQRL, where seven years ago I said, hey, I know how to solve this problem.



LEO:  The difference is, you're not going to do this; right?  This is not something you're planning on...



STEVE:  No.



LEO:  Okay.



STEVE:  No, no, no.



LEO:  You've got other fish to fry.



STEVE:  Believe me, I am deep in the I/O land, Leo.  I am seriously in I/O.



LEO:  Yeah, you have to - you are I/O.  Your whole thing is I/O.



STEVE:  That's right.  That's right.  Okay.  So, okay.  Get a load of - you're going to love this one, Leo - This Week in Cryptocurrency Craziness.  The story revolves around an organization known as Mango Markets.  And in researching this I learned that .markets is now a top-level domain.  So Mango Markets is mango.markets.  And let me tell you, this is a crazy place.  Its self-description is "Decentralized, cross-margin trading up to 20x leverage with lightning speed and near-zero fees."  Okay, I don't know what any of that means.  But I do know that it's not good.  The top of its home page brags "Long and short everything.  Lightning fast.  Near-zero fees.  Permissionless."  Now, I strongly suspect that they didn't mean to be quite as "permissionless" as things turned out to be because a serial DeFi (Decentralized Finance) DeFi abusing hacker by the name of Avraham Eisenberg managed to take $14 million in cryptocurrency...



LEO:  What a surprise.



STEVE:  No, no, that's the beginning - which he had retained from a previous DeFi hack and compound it into $114 million from Mango Markets.  Which you might be inclined to say would now be Mangle Markets. 



LEO:  1400 million?



STEVE:  114.  114 million.



LEO:  Oh, 114 million, okay.



STEVE:  Mango Markets, yeah.  Oh, yeah.  But wait, this is not - we're not done with it.  This gets weirder.  Mango Markets' initial tweets immediately following the incident were, and this is @mangomarkets, they said:  "We are currently investigating an incident where a hacker was able to drain funds from Mango via an oracle price manipulation.  We are taking steps to have third parties freeze funds in flight.  We will be disabling deposits on the front end as a precaution and will keep you updated as the situation evolves."



LEO:  This is, by the way, exactly why you want to put your money in a bank.



STEVE:  Exactly.



LEO:  No price oracles in a bank.



STEVE:  Exactly.  "If you have any information, please contact blockworks@protonmail.com to discuss a bounty for the return of funds."  In other words, they're saying, whoever you are, ouch.  How would you like a bounty?  And give us back our money.



LEO:  Because what he did, he says, was completely legal and allowed by the algorithm.



STEVE:  Yes.  A short summary of what then transpired raised so many questions in my mind that I dug a bit deeper.  Here's what the short summary I first encountered read:  "Mango Markets exploiter comes forward.  In a series of tweets over the weekend, an individual named Avraham Eisenberg took credit for the attack on Mango Markets following which he walked away with $114 million worth of cryptocurrency.  Eisenberg came clean after he was publicly identified as the attacker last week and after the Mango Markets community voted to allow him to keep $47 million of the exploited funds if he returned $67 million back to the platform, so it and all the other projects that depended on it could avoid insolvency."  Leo.  Wow.  What has gone wrong with the world?



Avraham was doxed, and his identity became public knowledge.  So he tweeted the following.  Get a load of this.  This is from @avi_eisen.  He said:  "I was involved with a team that operated a highly profitable trading strategy last week."



LEO:  I should say so.



STEVE:  Oh, my god.  "I believe all of our actions were legal open market actions, using the protocol as designed."



LEO:  It's kind of like short-selling; right?



STEVE:  Yes, exactly.  Yes.  He said:  "Even if the development team did not fully anticipate all the consequences of setting parameters the way they are.  Unfortunately, the exchange this took place on, Mango Markets, became insolvent as a result."



LEO:  Whoops.



STEVE:  Yup, whoops.  "With the insurance fund being insufficient to cover all liquidations.  This led to other users being unable to access their funds.  To remedy the situation, I helped negotiate a settlement agreement" - wow - "with the insurance fund."



LEO:  He felt bad.



STEVE:  "With the goal of making all users whole as soon as possible, as well as recapitalizing the exchange.  This is similar to how auto-deleveraging works on exchanges such as Binance and BitMEX, clawing back some profits from profitable traders in order to ensure" - just get me out of this, Leo.



LEO:  What?



STEVE:  Thank god...



LEO:  Oh, you made too much money.  We need some of that back.



STEVE:  Yeah, we're clawing it back "to ensure all users' funds are protected."  Wow.  "As a result of this agreement," he finally tweeted, "once the Mango team finishes processing, all users will be able to access their deposits in full with no loss of funds."



LEO:  Unbelievable.



STEVE:  And he keeps $47 million with everyone's blessings.  Okay.  This wasn't a security breach or a hack in the sense of finding and exploiting a flaw in some code.  This guy, I have to say, I've read his blogs, really knows his way around crypto-driven decentralized finance.  Mango's first tweet mentions "an oracle price manipulation."  So we have some sort of half-cocked DeFi - you know, decentralized finance - system that was prone to manipulation.



And this is the point I've been making here recently:  Nothing about this entire wacky DeFi NFT whatever the hell it is world seems mature.  That first note I encountered mentioned that this negotiated agreement was made by a vote of the stakeholders.  I found, Leo, that voting page.  I have the link in the show notes.  It's that dao.mango.markets page.  And the so-called discussion is somewhat humorous.  Many of the lesser stakeholders cannot imagine sanctioning this manipulation of the system.  If you scroll down, so that's the agreement there about what the guy's going to be returning.  All of those amounts of different cryptocurrencies are being put back.



So, and what was interesting is that there's  like people giving thumbs up and thumbs down which are their votes on this.  And the voting strength is not per member, but it's by the size of one's wallet.  So not surprisingly, those with the most money to lose were in favor of anything that would make them whole again, as opposed to those who didn't have that much in the game and were just furious at the idea that anyone was talking about, I mean, they were thinking this guy is a criminal.  But he wasn't.  All he did was play by their rules and took them to the cleaners to the tune of $114 million.



So anyway, this Avraham guy has a substack where he has talked about how all this works and explained some of the schemes he's come up with for leveraging fluctuations in the DeFi world.  Instead of DeFi, it's "deepfivalue" is his substack, deepfivalue.substack.com.  He currently has, I think, four long blog postings.  In July of last year his blog was "Deep Dives into Cryptonomics."  August 7th of last year was "Anatomy of a Yield Farm."  And then in January this year, "How Our Team Makes Millions in Crypto Risk-Free."  So I think the term, Leo, is "jumped the shark" for what has happened in crypto land.  And I'm not spending any time understanding this craziness because I think, you know...



LEO:  Well, the thing is, I mean, it's pretty clear.  They're making up the rules as they go along.  And there are flaws in it.  And somebody takes advantage of it; and, oops, you have no recourse.



STEVE:  Yup.



LEO:  I love it that there were, what is it, 400 million votes in favor of a refund?  Quite a few.  Quite a few.  That's why really if you want to put your money somewhere safe, put it in a bank.  Put it in a bank.  They've got rules.  They're regulated.



STEVE:  And it's got to be that people think that they're going to somehow make a lot of money?



LEO:  Well, that's, I mean, this is what the real basis of all of this is, is greed.



STEVE:  Uh-huh.



LEO:  It's just greed.  And, you know, that's what a con man says.  I can't rip you off unless you're greedy.



STEVE:  Yup.



LEO:  You can't rip off an honest person.



STEVE:  Yup.  Okay.  Let's take a break, Leo.



LEO:  All right.



STEVE:  Talk about a non-greedy sponsor.



LEO:  Our sponsors are never greedy.  They want to give you things.  They're wonderful.  Thank you for the opening.



STEVE:  So yesterday, Bleeping Computer disclosed the news of a new zero-day flaw being exploited in the wild on Windows machines.  The flaw enables executable files downloaded over the Internet by attackers to run without producing any pop-up warnings.  In this case, the flaw that was found, JavaScript files were being downloaded and used to install the Magniber, M-A-G-N-I-B-E-R, Magniber ransomware.  And it turns out that the flaw allows any executable, as I said, to bypass Windows detection warnings.



Now, we've talked about the Mark-of-the-Web.  It's implemented.  Remember the Mark-of-the-Web for files that come into your computer over the Internet.  It's implemented using a very cool and underused feature of Microsoft's NTFS file system which is known as Alternate Data Streams.  Alternate data streams have been a feature of NTFS from the start, though they've never received much attention.  A file's alternate data stream is its name followed by a colon and then the name of the stream.  So it can be literally accessed by creating a file with the filename that already exists, then a colon, and then the name of a stream.  It's like sort of a branch off of the root file.  It's just that simple.  It's another dimension of file metadata, aside from date created, last modified, read-only, compressible and so forth.



And in fact the Windows file directory listing command "dir,"  you know, just standard dir, can be given the /R command line switch which will cause it to add to its listing any alternate data streams that may exist.  So, for example, if you were to go into your Windows Download folder and then open a command line and do dir /R, since almost by default the files coming in have been downloaded over the Internet, you'll see a whole bunch of stuff you've never seen before if you add the /R.  You'll see the alternate data streams that have tagged all of those.  And those alternate data streams are named Zone.Identifier.  So when a file has the alternate data stream Zone.Identifier, this identifies the file as having originated from the scary Internet Zone and causes several things to happen.



One thing that happens is that Windows SmartScreen system is invoked when anyone or anything attempts to run the file.  SmartScreen raises an eyebrow and looks to see what it can determine about the file.  It examines it.  Is this commonly downloaded using some cloud-based comparison system?  Examines the signature, is this thing signed?  Presumably that matters.  And so on.  



So in addition, and this is where we were talking about the Mark-of-the-Web earlier, Microsoft Office uses this MOTW flag to determine if the file should be opened in Protected View by any Office apps, which would then cause macros to be disabled as an extra measure of protection.  And that's good.



Okay.  So what has come to light is that there's a zero-day flaw that's currently under active use.  It is being used to install ransomware, bypassing Windows executable file protections.  Which has the effect of conveniently, well, conveniently for the bad guys, bypassing all of the Mark-of-the-Web flagging.  HP's threat intelligence team reported that attackers are infecting devices with Magniber ransomware using JavaScript files, as I mentioned before.  And here's the kicker.  The JavaScript files seen distributed by the Magniber threat actors are digitally signed, but the signature is broken.



Will Dormann, whom we've referred to often, he's very active, a senior vulnerability analyst at ANALYGENCE, discovered that the attackers signed these files with a deliberately malformed key in the signature.  When any executable file is signed in this manner, even though it's been downloaded from the Internet and received a proper MOTW flag, Windows does not display any security warning.  The SmartScreen check never occurs, and the file is automatically executed.



Will further tested the use of this malformed signature technique and was able to create his own proof-of-concept files that similarly bypass the Mark-of-the-Web and SmartScreen warnings.  Note that when an unsigned file is downloaded and opened in Windows 10, unsigned, a Mark-of-the-Web security warning is properly displayed.  And when a properly signed file is downloaded and opened in Windows 10, a Mark-of-the-Web security warning is properly displayed.  It's only when a malformed signed file is downloaded and opened that Windows 10 remains silent and just runs the file.



So this appears to be a bug that was introduced into Windows 10 since Will observed that a fully patched Windows 8.1 machine displays the Mark-of-the-Web security warning as expected, even when it encounters a maliciously broken signed file.  And something changed with Windows 11, since the bug does still have an effect when executing files from an archive, but not when running them directly.



It appears that the bug originated from the new feature in Windows 10.  It's under "Check apps and files" SmartScreen feature, which is Windows Security > App & Browser Control > Reputation-based protection settings, which first appeared in Windows 10.  Disabling the "Check apps and files" causes Windows to revert to its older behavior, where Mark-of-the-Web prompts are unrelated to Authenticode signatures.  And then you do get the Mark-of-the-Web caution.  On the other hand, then you don't get the SmartScreen filter checking.  So it appears to be the signature checking that's resulting in a verification bypass.  And again, HP discovered this being used in the wild to sneak ransomware into Windows machines.



And finally, I would say believe it or not, but routine listeners to this podcast will believe it, Will Dormann shared the proof of concept with Microsoft, who said they were unable to reproduce the Mark-of-the-Web security warning bypass, even though both he and Lawrence Abrams at Bleeping Computer had easily done so.  So we'll see how this one goes.  As far as I know, there's no CVE assigned to this yet.  Reported by HP and Microsoft says, oh, well.



LEO:  That's hysterical.  Was it just some flunky?  Maybe some intern answering the phones that day?  "I don't know, I don't see it.  I just don't see it."  It's bizarre.



STEVE:  Like I said, Leo.  I know.



LEO:  He published a proof of concept; right?



STEVE:  Yup.  Yup.  Showed them how to do it.  And they say, yeah, it's not happening for us.



LEO:  That's worse than, oh, it's intended.  It's intended.



STEVE:  I know.



LEO:  That's like I don't, well, I don't see it.



STEVE:  Yeah, yeah.  Okay.  So for the record, I'll note that yesterday, as part of a bundle of security updates, Apple patched and fixed its ninth iPhone zero-day vulnerability of the year.  And Apple said the same thing they always say, which is that "It may have been actively exploited," although no one, even Apple, thinks that it wasn't.  And of course that's why it's a zero-day.



All we know is that CVE-2022-42827 is an out-of-bounds write flaw reported to Apple by an anonymous researcher and caused by software writing data outside the boundaries of the current memory buffer.  So your classic memory overrun.  This can result in data corruption; application crashes; or, in the hands of an expert hacker, apparently code execution.  And Apple did say that if successfully exploited in attacks, this zero-day could have been used - oh, yes, they're using the past tense - could have been used, because now it's been patched, by potential attackers to execute arbitrary code with kernel privileges.



The complete list of impacted devices includes the iPhone 8 and later, so it's been around for a while; iPad Pro, all models; iPad Air third generation and later; iPad fifth generation and later; and iPad mini fifth generation and later.  Apple addressed the zero-day vulnerability in iOS 16.1 and iPadOS 16 with improved bounds checking.  So again, nine of those.  So where are we?  Nine of those by the end of month 10.  So they're doing pretty well, less than one a month.



Okay.  The evolutionary demise of banking malware.  While assembling today's podcast I ran across some interesting commentary which described the various factors that have worked to reduce the prevalence, actually to near zero, of once dominant banking malware.  I've edited it for our audience because a lot of it was stuff that we really well know.  But  here's the gist of what happened, as described.



Researchers from the security firm Mandiant have reported this week that Ursnif, I'll pronounce it "UR sniff," which is also known as Gozi, which we've talked about, or Gozi/IFSB, one of the oldest and last few remaining banking trojan operations that were still active this year, has completely ditched its banking fraud-related features and now appears to operate as a basic backdoor trojan, the type of barebones malware typically used now in Access-as-a-Service schemes that rent access to compromised devices.  And of course they're renting them typically to ransomware perpetrators.  In other words, just creating and opening doors for others to use is what this once preeminent banking trojan has become.  According to Mandiant, the change took place this summer when Ursnif developers started distributing a new Ursnif, which its version was tracked under the codename LDR4.



Mandiant cites several reasons for Ursnif's new radical redesign.  At least two leaks of its earlier codebase had occurred.  Multiple branches of its authentic codebase had been slowly diverging and were making it harder to support their features across different botnets - oh, boohoo - but also it was an ancient codebase that had finally reached the end of the road when IE was formally removed from Windows.



Mandiant said:  "In June 2022, with Internet Explorer finally being fully removed from Microsoft Windows, the RM3 variant was officially seen as a 'dead' malware from a technical point of view, as RM3 was reliant on IE for some of its crucial network communication."  And there we see the mixed blessing of IE being allowed to stay around for so long because so many enterprises were utterly dependent upon it for their internal software, which would not run without it, as we've discussed many times.



So it's a surprise that Ursnif lasted as long as it did, operating on a banking trojan model alone.  It had become obvious by the mid-2010s that the banking trojans, the banking malware scene was dying, at least on the desktop.  Banks, finally growing tired of a decade of thefts from customer accounts, at last rolled out multifactor authentication and transaction verification systems.  Though they were not foolproof, these systems did their job and made it significantly more difficult and time-consuming for banking malware operators to steal money from individually compromised accounts.



Back in 2016, Emotet and TrickBot had converted their codebases from banking trojans to generic modular backdoors, and those two were some of the first to do so.  And even though they kept their banking modules around, the Dridex and Qbot malware also followed suit later.  In all instances, the primary driving force behind this shift in malware economics was the rise of ransomware and enterprise network big-game hunting.  As it became clear to ransomware operators that they could extort obscene amounts of money from companies and government networks, they started to look for ways into those networks.  And the existing old-time bot networks that had been converted were their way in.  This led to the birth and rise of a market for so-called IABs, we've discussed initial access brokers, where smaller threat actors would exploit corporate networking and server gear, plant backdoors, then sell access to these systems to ransomware gangs and their affiliates.



Evil Corp was the first major botnet operator to realize they could use their existing banking trojan to drop ransomware inside the thousands of corporate networks they had at their disposal through the Dridex botnet, and even launched internal teams to write and deploy their own internal forms of ransomware.  And recall how often I noted that it was never safe to assume that an infected router would only be used as a proxy to bounce traffic, or as a DDoS agent to flood.  Sooner or later, I said, the operators who had established a foothold on these routers were going to turn around and take a look inside the network that they had infected to see what might be valuable there.



Because the Dridex botnet operated on a closed model where its operator was only providing limited access to their botnet to only a handful of very carefully vetted operators, the "service-oriented" Emotet, and later TrickBot, which were open to anyone who wanted to sign up and who had been vetted, cornered the market in Malware-as-a-Service, working with ransomware gangs.  And after law enforcement finally cracked down on Emotet and TrickBot, two others, IcedID and Qbot, stepped in as ready replacements after years of having slowly been growing their own botnets in the shadows of Emotet and TrickBot.



The world of underground malware is not difficult to understand. It's simply about the minimum amount of work that can be performed to obtain the largest profit.  Banking and carding is now difficult, thanks to banks, and ransomware is easy, thanks to all the bazillion reasons we talk about every week.  There's no good reason to run a banking botnet these days, especially one as old and complicated as Ursnif had become.  It's far easier to create and manage a simple botnet, spam bored corporate employees until they infect themselves, and then sell access to ransomware or cryptomining gangs for a cut of the profits.



What this does mean, though, the takeaway for us is that the urgency to remove anything that might manage to crawl into your network has never been greater.  Stats from the latest intrusion reports indicate that ransomware can be deployed within 30 to 60 minutes of an initial intrusion.  This strongly suggests that the deployment of responsive intrusion detection should be front of mind for all security teams going forward.  There's just - there's no substitute for watching.



One last bit of news.  Today, this morning, VMware released an update to repair a critical, low complexity, CVSS 9.8 remote code execution vulnerability in VMware Cloud Foundation, which is their hybrid cloud platform for running enterprise apps in private and public environments.  This 9.8 out of 10 flaw, identified as CVE-2021-39144, was published August a year ago in 2021.  The flaw exists in the open source XStream library which is used by Cloud Foundation.  And I can't excuse VMware for not having updated this a year ago because this was a well-known flaw in a stream library they're using.  And a year has passed.  The update happened this morning.



It carries that 9.8 rating because it can be exploited remotely by unauthenticated evildoers in low-complexity attacks that don't require user interaction.  In other words, it's easy to do, and it's just a matter of finding a server on the public 'Net and doing it.  That's as bad as it can get.  And due to the severity of the issue, which has existed, as I said, for more than a year, VMware has also released security patches for other of their end-of-life products which remain in use, which were also using this XStream library for some time.  VMware's release advisory said:  "Due to an unauthenticated endpoint that leverages XStream for input serialization in VMware Cloud Foundation" - which is NSX-V - "a malicious actor can get remote code execution in the context of 'root' on the appliance."



Okay.  So the trouble is a deserialization flaw, and we've discussed many times that data deserialization, which is inherently a data interpreter, is often a source of extremely subtle, yet highly exploitable bugs.  The National Vulnerability Database, that is, the guys that maintain the CVS system, says:  "XStream is a simple library to serialize objects to XML and back again.  In affected versions, this vulnerability may allow a remote attacker who has sufficient rights to execute commands on the host only by manipulating the processed input stream."  Leo, I/O, as you were just saying.



LEO:  Yup.



STEVE:  And speaking of flaws from last year, earlier this month VMware informed customers who updated to vCenter Server 8.0, the latest version, that they'll have to keep waiting for a patch to access a high-severity privilege escalation vulnerability disclosed nearly a year ago, last November 2021.  So I don't know what's going on with VMware, but these are serious vulnerabilities, and they should be fixing them more quickly.



Okay.  I have some Closing the Loop tidbits to share.  Nicolas Ross, he said:  "@SGgrc In SN-884 you mentioned a piece of software you use to make graphs of SNMP (that's what they're called) counters for your pfSense router.  What was that software?"



Okay.  So first of all, Nicolas was reminding me that it's SNMP, Simple Network Management Protocol, that was the term I was blanking on during the podcast a few weeks ago when I was talking about monitoring the flow in and out of my pfSense-based router.  The fabulous freeware utility that I love for Windows, also for Mac, which I've talked about in the past, is called NetWorx, N-E-T-W-O-R-X.  It's by a company called SoftPerfect, and they're at SoftPerfect.com.  The version I have is 5.5.5, which is the last of the freeware releases.  After a long history of this thing being freeware, when they went to v6.0.1, which is the release after 5.5.5, it became trial ware for $30, and then you buy it for 25 bucks.



In my opinion, it's certainly worth $25.  But 5.5.5, which is what I'm using and will continue to use because it's perfect, is available on the web.  I just found it on FileHippo.  And I have a link in the show notes.  The download, just because you always want to do this when you're downloading from a download site, not the origin, the download is digitally signed by a DigiCert certificate issued to SoftPerfect in 2016, signed with both SHA-1 and SHA-256 because they wanted it to work on both old and new versions of Windows.  So double-check that, and you can trust it.  And I did.  I downloaded a copy just last night of 5.5.5 from SoftPerfect and verified the certificate because I knew I'd be talking about it today.



The reason I love this so much is that although it has many features that may be of interest, three things about it stand out for me.  First, as I mentioned at the time, it will monitor any interface's SNMP counters.  So I have it watching the WAN interface on my pfSense router, which by the way it found for me.  I didn't have to dig around.  You're able to say don't look at my local interfaces, I want to monitor something else.  And it enumerated them for me, and I selected from a list, and it just worked.  So they really, they got that right.



Second, it offers, and this is the key, a logarithmic scale for bandwidth monitoring and display, which is so much better than a linear scale, and it's exactly what you want.  And I have a picture of a snapshot I made last night where you can see why a logarithmic scale is so good, because that way you can see fluctuations in activity when not much is happening on your network while still seeing maximum-rate connection-saturating transfers without going off scale.  I have mine sized, as you can see in the screenshot below, so that the vertical scale snaps to decade powers-of-10 scaling with the max set to 1GB.  Since I can pull down 300Mb over my Cox cable modem, that moves the graph into the very last upper scale region without going off scale.  Yet when things aren't going along very much, like on this graph I'm also seeing something around just a few Kbits down toward the beginning of the graph, yet back toward the back it's up just shy of 100Mb.



Anyway, love this thing.  What the red and green lines on the show notes show is red is always incoming for me.  That's something is downloading data into my network, and I want to know what that is, or like that it's happening, at least.  And the green is things I'm sending outwards.  So anyway, just very cool.  I have it, it auto-starts when I boot any of my Windows desktops.  It sits running on a monitor.  It behaves itself.  You're able to set it up so there's no extraneous Windows nonsense.  Anyway, just a perfect piece of software.  Again, NetWorx from SoftPerfect, N-E-T-W-O-R-X.  And get the free one, or get the paid one, whatever you want.



Ando David Roots, he sent me a little dialogue.  He said:  "She:  It's 3:00 a.m.  Why are you up?"  And then "Me:  @SGgrc recommended a book."  "She:  But you finished the Ryk Brown ones."  "Me:  It's a new series, different author."  And then "She:  A symbol" that didn't resolve to anything, but probably a fuming face.  Anyway, I'm now on Book 14 of the Silver Ships series.  And boy, has it been an interesting ride so far.  One of the most interesting topics raised across the series, and it's probably safe to say, though I'm not finished yet, that it's one of the series' primary explorations, is the truly, and I mean this in all sincerity because I don't think we're necessarily a long way away from this, the truly intriguing question of the rights of sentient digital entities.  Near the start of the first book we meet Julien, the sentient intelligence that runs a derelict starship.  The series' primary human character, Alex, forms a deep bond with Julien, and the books explore many aspects of digital intelligence and the rights of sentient machines.  Really interesting.  Anyway, I'm loving the series, for what it's worth.



Zacc said:  "@SGgrc Can we get a link to the hockey puck-shaped power and reset button you mentioned on the show?"  He was talking about the thing that I got from Amazon, which I love, and I have like seven of them, I think now, attached to  motherboards and now attached to both of my ZimaBoards, which is just - it's like a little floating hockey puck with four feet where there's a big easy-to-press button in the top, and a little harder to press reset button.  And I've got them swapped since I'm most often resetting my hardware.  So the big easy-to-press button is the reset button, and the little one is the power button.  And it's also got two lights in it, one for power and one for hard drive activity.  So anyway, love it.  It's less than seven bucks, and the link is in the show notes.  



LEO:  Attached to the motherboard?



STEVE:  Yeah, exactly.  Yeah, exactly.  So you could, normally those little connectors will run out to the front panel of a case.



LEO:  Right.  I recognize them, yes. 



STEVE:  Yup.  And so this allows you to instead extend it out to, like for example, if your computer case is under your desk, or behind your desk, where getting to it is not easy, this basically remotes it.  And it's a long cord.  I only need it to be, you know, a foot.  But I could get six feet or something. 



LEO:  It's a four-foot coil.



STEVE:  Yeah.



LEO:  Four feet long.



STEVE:  Oh, okay.  So Bart said:  "Hi, Steve.  I can't remember if it was the last SN episode or the one before, but I remember you mentioning how much faster SpinRite 6.1 can surface test today's larger drives.  Was just wondering, is that your own tests or the feedback from the test community?  I tried looking for a SpinRite alpha/beta to experiment with on my own drives and PCs, but nothing that would go past the drive detection phase, which by the way is working flawlessly with my particular setups.  Have I missed something, or is there no such release yet?  Will we be able to get past the dragons?"  I actually forgot that he said that.



On all of the SpinRite test releases, if you try to - if you select a menu option after selecting a drive, you then do what SpinRite does to actually start running the drive.  Up comes a red cautionary screen with the title "Beyond this Point Be Dragons."  So that's what he was talking about.  Or here lie dragons or something like that.



But anyway, the numbers I've been sharing are those from the testing community.  And it's great to hear that the drive detection phase worked well for Bart, since that's where the past year and a half have gone.  Getting to that stage requires all of the new aspects of SpinRite to be working to such a degree that actually running SpinRite is almost anticlimactic.  That said, none of the testing releases have gone any further, since that's the next step I'm currently working on, which is also happily also the last step.  And actually it's really going very well.  In those testing releases, SpinRite is already estimating the total time required to run on a drive by extrapolating from the speed tests it performs during startup.  And it does show you those in a scan time column in the various user interfaces, Bart, in case you missed that.



_SKIN_ said:  "Love your show.  Question, though.  Why on earth is SpinRite 6.1 going to be free?  Not even an upgrade fee?  You're leaving money on the table.  We want to support you!"  These all had exclamation points.  "I think I'm going to buy it again just because of how hard you worked on it."



Okay.  So the short version is a promise is a promise, even if it's a promise I made nine years ago.  Actually more than, yeah, I think it was nine years ago in 2013.  And while that's enough, more than that, I am sure that many people have purchased SpinRite 6.0 based upon the promise that they'd be getting 6.1 at no charge.  And while this has indeed turned out to be a much bigger challenge than I originally expected, I'm happy to do it because along the way I discovered some very cool new tricks that I didn't suspect in the beginning, which leaves me to believe that a future SpinRite will have a few tricks up its sleeve that are unsuspected.  So there will be a SpinRite v7, 7.1, 7.2 and beyond.  So I'm happy to have 6.1 available for free.  I basically want to addict everyone to what SpinRite has become and then we'll go from there.



Will Morhler said:  "Last week's SN, the discussion of ECB mode of encryption brought back a question I've always had:  Isn't any chaining mode of encryption brittle?  If any part of the encrypted text is lost, isn't it the case that all subsequent blocks cannot be decrypted?"



You're absolutely right, Will.  But as we might say, that's more of a feature than a bug.  One of the things we often want to do is detect any tampering of any kind, deliberate or inadvertent, with a file's contents.  Huge damage could be done by, for example, adding or removing a zero from financial data.  Since the integrity of the data is often just as important as its secrecy, we're often wishing to both encrypt and authenticate.  And in the past we've spent some enjoyable time considering whether it's better to encrypt first, then apply authentication to the encrypted result, or to authenticate a file before encrypting it.



And hint:  If you cannot do both at once, and some cipher systems can and do, then you'll want to encrypt first and then apply authentication to the result.  The reason is that upon reversing the process at the receiving end, or the reading end, you never want to decrypt anything that may have been altered. It's always best to verify that the file has not first been tampered with; and, if so, drop it immediately if it has been altered.  And getting back to the point, as a result of that, having brittle encryption that will always cause the file to essentially explode downstream of the point where anything was changed could be considered a good thing.  It won't authenticate, certainly.  And, boy, it will not decrypt.



And lastly, Ken Dudley asked:  "Hello.  What transcription software do you use for the podcast?"  It is, Ken, a fabulous system known as Elaine Farris, who is located at edigitaltranscription.com.  She's not as cheap as software; but, boy, she is a lot better.



And one piece of miscellany.  For the first time ever I used Edge to assemble this document yesterday and today.  The reason was that Chrome, which I have always been using on my main Windows 7 system, has become laggy for some reason, and it's really become a problem using Google Docs, even just scrolling.  I thought that it might be some extension that I'm running, but I'm only running LastPass and uBlock Origin.  Although I did note that uBlock Origin believes it's blocked more than 5,000 things in Google Docs.  Wow.



LEO:  Yeah.  Wow.



STEVE:  So maybe I ought to actually try disabling uBlock Origin, come to think of it.



LEO:  It's busy.  It's very busy.



STEVE:  Yeah.  So I've been putting up with this problem for weeks, but yesterday the lag was really interfering with assembling this document.  So of course I first switched to running Google Docs under Firefox.  It was super snappy and ran like a charm, but the Verdana font I like to use, I didn't even bother trying any other fonts because I didn't want to go away from Verdana, had an annoying kerning problem.  The letters were not spaced uniformly at all.  It was conspicuous.  And it turns out I'm not the only person to have reported this.  Many others online have posted similar like kerning problems in Google Docs and Firefox looking for solutions.  Nothing they tried worked.  Nothing I tried worked.  So for a while I tried to ignore it, but the wrongness of it bothered me.  Finally I thought, I'll use the other Chromium-based browser that I have, that being Edge.  Well, Edge worked beautifully, and I wrote and produced this entire show notes document using it.



So here's the thing that caused me to bring all this up:  Under Edge and Windows 10, where I was working last evening, but not under Windows 7, the Windows ALT-TAB key sequence switches among recently opened browser tabs in addition to top-level desktop apps.  As it happens, for me that's a huge win, since during the preparation of these notes I'm often jumping back and forth between browser tabs.  Having seen this behavior under Windows 10, I'm now missing it today under Windows 7, where all I'm getting is top-level application windows, as I always have.



As all old Windows hands know, ALT-TAB is the Windows keyboard shortcut for changing among the desktop's foreground apps.  And I'm often using it to quickly jump back and forth among running apps, typically cutting and pasting to move selected content around.  But I've never seen it jump between different tabs on the same browser.  It must be because Edge actually is running each browser tab in its own process space for inter-tab isolation.  So in a sense the browser tabs really are individual browsers.  But this was unexpected, and I just wanted to make a note of it.



LEO:  I'll just point out you can change that in system multitasking.



STEVE:  No kidding.



LEO:  You can say show Microsoft Edge tabs with snapping or pressing ALT-TAB.  You can do ALT-TAB's five most recent, three most recent, or don't show tabs.



STEVE:  Nice.



LEO:  Now, this is Windows 11.  I'm not sure what the interface is on Windows 10 or 7.



STEVE:  I'll bet it's the same.  I'll bet it's the same, yeah.



LEO:  But I bet it's the same.  And yeah, because they're separate processes.  By the way, Chrome they're separate processes, too.  That's the advantage of Chromium.



STEVE:  Yeah, yeah.  Well, and Firefox has done that.  I mean, I don't know if you - have you looked at Task Manager?  But, boy, if you've got a bunch of tabs open, half of it is your browser processes.



LEO:  They kind of have to do that, I would guess; right?



STEVE:  Yeah.  I think they do, unfortunately.  Be nice if they could like nest them and show them in Task Manager.  But anyway, they would still be there.  By the way, I also wanted to note that Google just announced...



LEO:  Look at all my Firefox tabs.



STEVE:  There you go.  Yup.  And they are nested.



LEO:  Yeah.



STEVE:  It's a very nice-looking...



LEO:  Yeah, again, this might be a Windows 11 thing.  I don't know.



STEVE:  Ah.



LEO:  And let's see Edge.  Yeah, I only have a bunch of processes under Edge.  But I think, I don't know, but I would guess these are all Firefox tabs.



STEVE:  Yeah, yeah.  Very cool.



LEO:  Yeah.



STEVE:  So I wanted to note that Google just announced that Chrome's support for Windows 7 and 8.1 will be ending early next year.  I think it was in February.  That's one of the dumbest things I've heard recently.  Windows 7 still commands 10% of Windows desktops, one of mine being among them, and Windows 8.1 has an additional 2.7%.  So that's 12.7%, or more than one out of every eight desktops still running 7 or 8.1, most of them 7 by a huge margin.  And the dumbest thing about this is that it must be arbitrary and deliberate on Google's part.  There's absolutely no reason why Chrome should care whether it's running on top of 7, 8.1, 10, or 11.  If Chrome cannot ignore the version number of the Windows operating system it's running on, then they're doing it wrong.



Presumably, Google's decision to abandon Chrome on pre-Windows 10 machines synchronizes with the end of Microsoft's Extended Security Update, their ESU programs, for Windows 7 and 8.1, which is occurring on January 10th next year.  I suppose that at some point I'll need to give up my use of Windows 7 on my primary working system, but I'd rather just keep working.  If I have to switch Windows versions, I'm going to suffer a bunch of down time, you know, getting everything set up and reinstalled and going again, which is a constant annoyance for Windows.  But it's our life.



So over the weekend a piece of surprising sort of over-the-top news from Australia got me thinking about the nature of data breach culpability and legislative response.  Everyone who's been sharing this podcast for a few years will have heard and learned enough to have a well-formed and well-informed opinion about culpability.  But not so our legislators.  So first, here's the news from Australia.



On the heels of a month of high-profile and embarrassing attacks in Australia - there was Optus, Telstra, Medibank, Woolworths, and EnergyAustralia - three days ago on Saturday, the office of Australia's Attorney General posted an aggressive intention regarding legislation that is slated for discussion and adoption this week.  One question that comes to mind is whether what happens in Australia remains in Australia, or will this aggressive legislation just be the first and prove to be the harbinger of similar actions by other governments.



Anyway, so here's what the Attorney General's office published on Saturday.  "The Albanese Government will next week introduce legislation to significantly increase penalties for repeated or serious privacy breaches.  When Australians are asked to hand over their personal data, they have a right to expect it will be protected.  Unfortunately, significant privacy breaches in recent weeks have shown existing safeguards are inadequate.  It's not enough for a penalty for a major data breach to be seen as the cost of doing business.  We need better laws to regulate how companies manage the huge amount of data they collect, and bigger penalties to incentivize better behavior.



"The Privacy Legislation Amendment (Enforcement and Other Measures) Bill 2022" - which is happening this week - "will increase maximum penalties that can be applied under the Privacy Act of 1988 for serious or repeated privacy breaches from the current 2.22 million penalty to whichever is the greater of $50 million; or three times the value of any benefit obtained through the misuse of information; or 30% of a company's adjusted turnover in the relevant period."  Whichever is greater.



They said:  "The Bill will also provide the Australian Information Commissioner with greater powers to resolve privacy breaches; strengthen the Notifiable Data Breaches scheme to ensure the Australian Information Commissioner has comprehensive knowledge and understanding of information compromised in a breach to assess the risk of harm to individuals; and equip the Australian Information Commissioner and the Australian Communications and Media Authority with greater information sharing powers.



"The Bill is in addition to a comprehensive review of the Privacy Act by the Attorney General's Department that will be completed this year, with recommendations expected for further reform."  And finally:  "I look forward to support from across the Parliament for this Bill, which is an essential part of the Government's agenda to ensure Australia's privacy framework is able to respond to new challenges in the digital era.  The Albanese Government is committed to protecting Australians' personal information and to further strengthen privacy laws."



Wow.  Now, I can see both sides of this issue.  For me, it's not as cut and dried as:  "Companies who hold consumer data due to valid business needs, and who subsequently fail to keep such data private, are so clearly to blame that they should be penalized with a crippling monetary fine."  I made that up, but that's what I mean.  In the real world we have this notion of "asking for it."  Like, well, yeah, it's too bad that his car broke down when it did, but he hadn't taken it in for service in over 10 years, so he was kind of asking for it.  Right?



It seems to me that the underlying cause of a data breach absolutely does matter.  Culpability need not be a mystery, and it ought to be a factor.  After all, it certainly is a factor in all other aspects of the metering out of what we call "justice."  But that means that each issue needs to be taken on a case-by-case basis, just as it is elsewhere in the law.  In the case of data breaches, the legislature might not want to do that, and may not know how to do that.  But it's bad legislation if culpability isn't factored into the outcome.



If a company is shown to have been running a five-year-old and many times obsoleted instance of, for example, VMware in the cloud, that was riddled with known and long-since-patched vulnerabilities, where one of them was inviting the bad guys into their network to root around and exfiltrate data, then, yes, any reasonable person would find their IT department management grossly negligent, incompetent, and culpable.



And on the subject of "asking for it," which relates to breach responsibility, I would add that when such a breach occurs, thus drawing everyone's attention to the fact and nature of the breach, the leaked information, which would then be public, should be examined to see whether any personally damaging information was being retained in excess of the minimal needs of the business's purpose because doing so should be a big no-no.



But if a company was running only the latest currently patched state-of-the-art updated software, active intrusion detection and network monitoring, holding periodic employee cyber-awareness conduct training, conducting simulated intrusion drills, and randomly sending their own employees test bait phishing emails, and yet despite all of that, due to an entirely unknown zero-day vulnerability in the latest Microsoft enterprise software - or VMware, which we just talked about - someone crawled into their network and exfiltrated a database containing the bare minimum of personally identifiable customer information, how can it possibly be fair to levy a crippling fine of no less than 50 million, and perhaps much more, onto such an enterprise?



The world as a whole is culpable for time and time again having chosen features over security; for having chosen to make our computer systems so overly complicated that they have become unknowable to anyone using them.  We're all mostly just hoping that things are going to turn out all right.



This happened, of course, because it's not possible to sell actual security as a feature.  Security is the absence of problems.  You cannot sell an absence upfront when problems take time to be found and revealed.  Way back before the release of Windows XP, when Steve Ballmer was jumping up and down onstage bellowing at the top of his lungs that Windows XP was the most secure operating system that had ever been created, it wasn't possible to prove or to know then just how wrong time would prove that statement to be.  I know Steve Ballmer, and I like Steve Ballmer.  He's a big huggable teddy bear.  But as Bill Gates would say, he's not a "technical guy."  So I imagine that Steve sincerely believed what he was selling.  But it didn't matter how much he believed.  It wasn't true.



So we've built ourselves an industry where software is intellectual property that isn't sold.  It's licensed.  And where the agreement to license said software invariably incorporates a statement explaining that the licensor is making no representations of any kind whatsoever as to the operation or fitness of purpose of the software, and where the licensee agrees to hold the licensor harmless for any and all consequences of any failure on the part of the software to function as they might hope.  Often going so far, in the case of overly zealous attorneys, as to say that the software's publisher has no idea whatsoever whether the software works, what it does, or what it may or may not do.  Sign here.



In such a world, while it is definitely possible for negligence to be the proximate cause of horrendously damaging data breaches, it is also entirely possible for a well-meaning and perfectly well-behaving company to be the blameless victim of the software industry we have collectively built.  Until and unless things change, I dearly hope that all future legislation keeps this in mind.  And I hope that any organization who did everything right, yet was breached anyway, is able to find a good law firm which is able to make this case for them.



LEO:  Bravo.  But what would your best guess be as to how many of the breaches are the company doing something stupid like leaving their S3 bucket unlocked?



STEVE:  Almost all.



LEO:  Yeah.  So this...



STEVE:  Almost all.



LEO:  So, yeah, I admit, security's hard, and there's companies screwing up.  But a lot of these breaches are the fault of the company getting breached; right?



STEVE:  It is really, really, really difficult to be perfect.



LEO:  Yeah, yeah.



STEVE:  And unfortunately, that's where the bar is.  You've got to be perfect. 



LEO:  Yeah, yeah.  Well, welcome to computers.  Right?  I mean, this is how it's always been.



STEVE:  You know, I used to accept requests to be an expert witness in jury trials, or even in court trials where I was just talking to a judge.  And in fact it was - the last one I did was in New York, explaining to a judge why an NEC competitor MultiSync Monitor - and MultiSync was NEC's trademark.  But I was trying to explain to the judge, who had a green oxygen tank sitting next to him...



LEO:  Oh, boy.



STEVE:  ...why I think it was Paradise was the other - was the competing monitor firm on whose behalf I was testifying because they were right that NEC was wrong to sue them over their claim that it would be the last monitor you needed because it adapted to the PS2's differing sync, and you did not need a new monitor.  Anyway, they ended up the outcome was wrong.  And I decided I'm never going to testify again because it's too frustrating.  But if someone comes to me, and they've done everything right, and they're being sued by their government, put me on the stand.



LEO:  There you go.



STEVE:  I will read this column, and we'll go from there.



LEO:  There you go.  I will defend your right to not be sued for that breach.  Steve Gibson, he's right here.  You can find him right here every Tuesday, if you need him to testify on your behalf.  You may regret that.  He comes here every Tuesday around 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  We're still in daylight savings time.  We will not be in two weeks.  So just make a note of that.  I know people in the EU have already gone to - ended summertime.  But UTC hasn't.  We haven't moved ourselves yet.  UTC is still the same, 18:30 UTC.  Or no, 20:30 UTC.  But it will in a couple weeks because UTC never moves, but we do.  And because, you know, we have to follow the sun, as it were.



The reason I mention this is so you can watch us live at live.twit.tv.  There's live audio, and there's also live video.  You can take your pick.  Chat with us live if you're watching live at irc.twit.tv.  On-demand versions of the shows also available from Steve for free.  They have the ads baked in at GRC.com.  GRC.com.  And he has other things that you might want including those great transcripts from Elaine Farris.  He's got  16Kb audio as well as 64Kb audio.  He also has SpinRite, the world's best mass storage maintenance and recovery utility.  You can get the version 6.0 right now and guaranteed upgrade to 6.1 the minute it's available.  GRC.com.



He's on the Twitter, @SGgrc.  You can leave messages for him in his DMs.  They're wide open, @SGgrc.  You can also get a copy of the show at our website, TWiT.tv/sn.  That's the free ad-supported version.  YouTube also has an even more ad-supported version.  And there's also of course the best way probably to get it, subscribe in your favorite podcast player.  Our feeds only do the last 10 shows because it's a feed.  It's not intended to have every show, all 800, what is it, 800, what have you got, 894.



STEVE:  94.



LEO:  Yeah, holy cow.  But you can go to the website, get all the old shows there, all the way up to Episode 1.  And make sure you do because you want the complete set; right?  Yes.  Steve, enjoy your Silver Ships.  You're up to Number 15?



STEVE:  14.



LEO:  Oh, so you only have six more volumes.



STEVE:  Actually, it turns out that I found another six after the 24.  So it's an even 30.



LEO:  I thought Ryk Brown was prolific.  Wow.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#895

DATE:		November 1, 2022

TITLE:		After 20 Years in GCHQ

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-895.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we revisit the Windows driver blocklist, which has received a long-needed update, and Microsoft's own definition of a CVE.  We note that sometime today the OpenSSL project will be releasing an update for an ultra-critical flaw in OpenSSL v3, and we look at a remote code execution flaw in Windows TCP/IP stack.  We have a ubiquitous problem in the past 22 years of the widely used SQLite library, and a surprising percentage of malicious proofs of concept found in GitHub.  Passkeys gets another supporter, and the first part of a professional tutorial explaining how to exploit the Chrome browser is released.  After some listener feedback and a SpinRite update, we look at the goodbye posting of the U.K.'s head of cybersecurity after 20 years.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have an update on that Microsoft Windows driver blocklist flaw, I guess you'd call it.  For three years they haven't updated it.  Finally they have, and how you can do it manually any time you want.  SQLite, S-Q-L-I-T-E, has a big security flaw.  It's everywhere.  We've got to fix this one.  And then some thoughts after 20 years in Britain's GCHQ, what we've learned about security.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 895, recorded Tuesday, November 1st, 2022:  After 20 Years in GCHQ.



It's time for Security Now!, the show where we cover your safety, at home and abroad, mostly on the Internet, generally with computers.  Mr. Steve Gibson is the king of security.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  Good to see you.



STEVE:  Great to be with you for the first day of November.



LEO:  Can you believe it, November 2022.  Amazing.



STEVE:  Wow.  We're going to have a little rain tonight.  I heard you are in the rain now.



LEO:  Yeah.  It's actually cleared up.  We sent it down south, just for you.



STEVE:  Thank you.  Well, we could use all the moisture we can get down here.



LEO:  Yeah, no kidding.



STEVE:  So thank you very much.  Okay.  So there was an interesting array of news, but nothing really grabbed me until I was catching up on my Twitter feed, and somebody, probably one of our listeners over in the U.K., posted a link to sort of a memoir of - actually it was like the goodbye posting for or posted by the guy who's been running the U.K.'s cybersecurity for the last decade and has been at GCHQ for the last two decades, for 20 years.



And so he had a lot of interesting things to say.  It won't shock anybody because if you have been listening to this podcast, interestingly enough, these are pretty much the themes that keep echoing through the microphone here.  But still, interesting to get a perspective from somebody completely outside of this podcast.  So this Episode 895 is titled "After 20 Years in GCHQ."  And I went back and forth about whether it should be "at GCHQ" or "in GCHQ."  But I just thought, well, okay, we'll go with "in."  But we're going to revisit first the Windows driver blocklist which has received its long-needed after three years update.



LEO:  Oh, man.



STEVE:  I know.  And actually some weird official policy statement about this, like what?  Anyway, also, Microsoft has developed their own definition of a CVE, which seems determined not to have as many just by changing the definition.



LEO:  Of course.



STEVE:  We also will note that sometime today, actually it did already happen, the OpenSSL project will be releasing an update for what was originally believed to be an ultra-critical flaw in OpenSSL v3, not the earlier versions.  But now they sort of, I guess, mitigated the criticality due to the fact that it's not so easy to have it happen.  Anyway, so we're going to mention that.  Also we're going to look at a remote code execution flaw in Windows TCP/IP stack.  We've got a ubiquitous problem in the past 22 years of the widely used SQLite library and a surprising percentage of malicious proofs of concept found in GitHub.  Passkeys is to get another supporter.  And the first part of a professional tutorial explaining, get this, how to exploit the Chrome browser has been released.



LEO:  Finally, a tutorial.  Always needed that.  That's good.



STEVE:  That's what we need.  But the best news, it is not on YouTube.  I thought, oh, my god, am I going to have to watch another YouTube video?  No.  This is just you could scroll, thank goodness.



LEO:  We must be old-timers, because I have the same exact reaction.  It's like, please.



STEVE:  Yeah.  Every time I'm looking for something, it's oh, you can find out about this by watching this YouTube video.  It's like, no, no.  Just tell me.  Could you just please just print it?



LEO:  There's an interesting incentive at play that I just found out about because YouTube won't put an ad in unless you're a certain length.  So they always have to get to a certain point.  And that's why they pad these things.



STEVE:  In other words, yeah, in other words it's not - it doesn't generate money for the person posting it until it's long enough to have YouTube do an insert.



LEO:  Precisely.  So then they pad it.  And for some reason they decided, I guess, well, we won't, maybe because we want to keep people watching, we don't give you anything we're going to talk about until about 10 minutes in.  So all this prologue, it's like, please, I don't, I mean...



STEVE:  Yeah.  Anyway...



LEO:  Anyway.  We're old.  We're old.



STEVE:  The good news is the tutorial is written, and you can scroll.



LEO:  Yay.



STEVE:  Okay.  Then after a bunch of listener feedback and a quick SpinRite update, we're going to look at the goodbye posting of the U.K.'s head of cybersecurity after his 20 years.



LEO:  Interesting.



STEVE:  So I think a neat podcast for our listeners.  And of course we've got the Picture of the Week, which is kind of fun because you wonder why they put a high security gate on this entrance or exit or whatever the hell it is.



LEO:  I really - the  best part of this is the subtitle.



STEVE:  Yes.



LEO:  So I'll leave that for you.  Now, the Picture of the Week.



STEVE:  Okay.  So there's a staircase, sort of like, I don't know, like the back of a building or something coming down, where clearly the intent was to prevent people who weren't somehow authorized to gain access to the staircase from the outside.  I don't know if there's like a panic bar that you can push to release in order to get out, but you certainly can't get in.  And so the staircase has this protection gate which is closed, very tall, so it would be very difficult for you to, like, you'd have to have two people like to give you a leg up in order to scale this and get over it.  So, okay.  Now, the problem is that, if you just go around the corner from this gate, there's the stairs.



LEO:  Just climb right over the railing.



STEVE:  And there is a low railing, less than half the height of the big gate which would be difficult to scale.  And the railing has been conveniently created to sort of have like a ladder.  So you could even use it to help you climb over it and get you to the stair.



LEO:  Oh, we don't want to lock you out.



STEVE:  So, right.  Now, what was fun about this that you commented on was that somebody who prepared this picture, they, in a big green rectangle, circled the gate, which is what you're supposed to focus on, and labeled it "In Scope."  And then, because this picture shows both sides of this problem, there's a red box that circles this simple means of bypassing the gate which is labeled "Out of Scope."



LEO:  Not our problem.



STEVE:  Yes.  And so then I gave this Picture of the Week the title "Out of Scope" = "Somebody Else's Problem."  Meaning, yeah, we didn't quite achieve our goal here.  But the fact that you can bypass this gate,  well, that's...



LEO:  Not my problem.



STEVE:  Not our problem, that's right.  You said you wanted a gate there, boss, so we gave you a gate.  Yeah, okay.



So recall our coverage last week of Ars Technica's reporting, which was titled "How a Microsoft blunder opened millions of PCs to potent malware attacks"; and that, indeed, some follow-up digging revealed that many malware strains had been found to be actively leveraging known vulnerable drivers in what is now called the BYOVD, the Bring Your Own Vulnerable Driver attack.  And also recall that Microsoft's minions, even those in elevated positions of authority, were openly rude and hostile to the researchers who were just trying to help, to prompt them and to understand what was going on.  And Microsoft said, yeah, this is the way we want it.



Well, okay.  This may be where shining a bright public light can help, since Microsoft has gotten off their butts and, finally, after three years of neglect, fixed this arguably important issue.  Last Thursday, ZDNet carried the news with their headline "Next Windows 10/11 Patch Tuesday" - okay, that means a week from tomorrow, no, sorry, a week from today, next week, next Tuesday will be Patch Tuesday for November.  They said:  "Next Windows 10/11 Patch Tuesday fixes Microsoft's botched vulnerable driver blocklist."  And the subhead was "Microsoft addresses an issue preventing Windows 10's vulnerable driver blocklist from being updated with new vulnerable drivers."



Okay.  So here's how ZDNet summarized the situation.  They said:  "Microsoft has released a new non-security preview" - has released, you know, how like they do the non-security preview so the month before.  So "Microsoft has released a new non-security preview of November's Patch Tuesday update for Windows 10 and Windows 11 22H2.  It brings improvements to the taskbar, Microsoft Account, and Task Manager, as well as a fix for a serious Microsoft blunder that left a hole in the Windows 10 vulnerable driver blocklist.  The preview is a non-security update that's available for Windows 10 and Windows 11 22H2.  It contains all the changes in the upcoming November Patch Tuesday except security patches.



"However, this preview also includes Microsoft's answer to a serious security-related error that the company made with its Windows kernel vulnerable driver blocklist, an optional security hardening capability introduced in Windows 10, version 1809" - so like the old one - "that's on by default in Windows 11 22H2.  Microsoft has explained that the failed updates to the blocklist were due to it" - in this weird language - "to it only updating for 'full Windows OS releases,'" they said, "although it's not clear if this means previously installed Windows versus fresh installs, or just that older versions of Windows were stuck on a blocklist that couldn't be updated."  Like what does it mean, "full Windows OS releases"?



In a support page detailing "the vulnerable driver blocklist after the October 2022 preview release," Microsoft states:  "This October 2022 preview release addresses an issue that only updates the blocklist for full Windows OS releases."  Okay, so does that mean they're changing that?  Or not?  We don't know.  They said:  "When you install this release, the blocklist on older OS versions will be the same as the blocklist on Windows 11, version 21H2 and later."  Okay, so a catch-up.



"Microsoft had told Ars Technica," writes ZDNet, "that it was in fact regularly updating the vulnerable driver list, but that there was 'a gap in synchronization across OS versions.'"  Again, it's no longer possible, apparently, to get Microsoft to actually tell anyone what's going on.  You just have to poke it and experiment and figure it out for yourself.



So they said:  "So the October 2022 preview release is the promised fix, which should be released broadly in the November 2022 Patch Tuesday" - again, next week - "through Windows Update."  Microsoft's release notes for the October Windows 11 22H2 preview update states:  "It updates the Windows kernel vulnerable driver blocklist that is in the" - I'll explain what this means in a second, but they said:  "in the DriverSiPolicy.p7b file."  Again, I'll explain that.  "This update also ensures that the blocklist is the same across Windows 10 and Windows 11.  For more information, see KB5020779."



So Microsoft has a page where they talk about recommended driver block rules which provides a bit more clarity.  They say on that page, and I have a link in the show notes, they say - and this is what's weird; okay?  "The blocklist" - this is official Microsoft.com.  Actually it's learn.microsoft.com.  They said:  "The blocklist is updated with each new major release of Windows, typically one to two times per year" - but now I guess only one time a year because we're not doing two times a year major Windows updates - "including most recently with the Windows 11 2022 update released in September 2022.  The most current blocklist is now also available for Windows 10 20H2 and Windows 11 21H2 users as an optional update from Windows Update.  Microsoft will occasionally publish future updates through regular Windows servicing."  But apparently like only with each new major release of Windows.  Which nobody understands.



Then they continue, saying:  "Customers who always want the most up-to-date driver blocklist" - well, who would want that? - "can also use Windows Defender Application Control (WDAC) to apply the latest recommended driver blocklist contained in this article.  For your convenience, we've provided a download of the most up-to-date vulnerable driver blocklist along with instructions to apply it to your computer at the end of this article.  Otherwise, you can use the XML provided below to create your own custom WDAC policies."



Okay.  So they're saying here that the blocklist is updated with each new major release of Windows, typically one to two times per year.  So it's curious that, for some reason, the exploitation of known malicious Windows kernel drivers will apparently be allowed, deliberately and by policy, for as long as one year, or during the length of time of the gap between major Windows releases.  And this is current policy.  Given that BYOVD now has its own abbreviation and that the exploitation of such drivers has unfortunately become popular, it sure would seem that some policy rethinking of this, you know, this we're only going to update this list with new major releases of Windows should be reconsidered.



That page also said:  "Customers who always want the most up-to-date driver blocklist can also use Windows Defender Application Control to apply the latest recommended driver blocklist contained in this article."  So I have those instructions and a link to that page in the show notes.  But anyone wishing not to wait can easily update their system's driver blocklist.



So to give you a sense for this, it's download the WDAC policy refresh tool, and that's easy to find.  It's at aka.ms/refreshpolicy.  Download and extract the vulnerable driver blocklist binaries.  Now, that's easy to find, too.  That's at aka.ms/vulnerabledriverblocklist.  And that actually gives you a zip with four files.  They said:  "Select either the audit-only version or the enforced version and rename the file to SiPolicy.p7b.  Copy SiPolicy.p7b to" - and then your windows directory \system32\codeintegrity.



Then they finally said:  "Run the WDAC policy refresh tool you downloaded in Step 1 above to activate and refresh all WDAC policies on your computer."  So, okay.  Not difficult to do.  For some reason, they don't want to do that for us except every major Windows release or so.



LEO:  Is there some way to protect that file?  Like make it be, like...



STEVE:  Presumably. 



LEO:  Need root access to it or something?  Kind of just sitting there as a plaintext file or something.  I presume...



STEVE:  Now, so, yeah, presumably that WDAC policy refresh tool is not something that malware can use.  That seems to be the thing you need to take this, all those four zip files, they're 95KB files, all dated October 19th, so middle of last month.  And you get to choose between audit or enforce in either the server or the workstation flavor.  So there's four different ones.  And hopefully, as you suggest, Microsoft has prevented bad things from happening to these policies or turning them off.  But it's just strange to me that like why aren't known bad drivers, known malicious drivers, as important to fix every month as everything else?  I don't get it.



Oh, by the way, when you apply these new policies, you must reboot because they note on their page that just having the policies doesn't prevent things that are already running, like they're not going to get kicked out of Windows.  It's only when they're trying to get themselves loaded that the policy looks at it and goes, uh, not so fast, and denies it access.  So, and it turns out I can't even turn it on on my Windows 10 system because I've got a couple drivers that are not sanctioned.



LEO:  Oh, now I know why they didn't do this.  Not all drivers are sanctioned.



STEVE:  Yes.



LEO:  And they knew it would cause tech support calls.



STEVE:  Yes.



LEO:  There you go.



STEVE:  Like, hey, why can't I use this?  I want to turn this on.



LEO:  Are they weirdo drivers that you have?  I mean, what is -



STEVE:  Yeah, they're a couple weirdo drivers.  So, yeah, there's one that I actually hacked myself and signed.



LEO:  What could possibly go wrong?



STEVE:  It was something that Lorrie needed for one of her - in order to distribute software.  And it wasn't working, so I hacked it and fixed it.



LEO:  It's nice to have a handy husband around the house, I must say.



STEVE:  And then Windows wouldn't run, and it wouldn't load anymore, so I got that signed.  And then when it popped up in the list last night, I thought, oh.  Yeah, that could be a problem.



LEO:  Yeah, maybe you don't want this blocklist.



STEVE:  Anyway, for everybody else, when you try to turn this on, Windows will say, what about this?  And I think there were three things that were a little sketchy that I had in my computer.  So, yeah.



LEO:  But folks, he knows what he's doing.  Don't try this at home.



STEVE:  No.  And it turns out it's kind of tricky to get Microsoft to sign a driver.  You've got to tell them, I'm a good guy.  And they said, we don't know.  We heard your podcast.  Oh.



So last Tuesday the Singapore-based security firm STAR Labs disclosed a verified and patched vulnerability in Microsoft SharePoint Server 2019, which is current.  It was an exploitable post-authentication Server-Side Request Forgery, which is not good.  Now, what's bizarre is that while this was clearly a flaw that needed fixing, and fix it they did, Microsoft refused to assign a CVE identifier for STAR Labs' finding and their work.  Now, this didn't escape the notice of some well-known security insiders who took note and tweeted.



Kevin Beaumont, whom we often quote, who tweets as GossiTheDog, he quoted the write-up where they noted, they said, "they" meaning STAR Labs, said "The bug had been fixed, but it did not meet the criterion required to get a CVE."  And then he put like a weird emoji after that.  He's like, what?



And so Will Dormann, also often quoted on the podcast, he said, replying to Kevin's tweet, he said:  "A Microsoft 'CVE' is a 'security update with associated MSRC write-up.'"  And he said:  "Using this definition, as opposed to how the rest of the world uses CVE, is what allows them to say 'this isn't worth a CVE' and mean it."  Wow.



So now, just as we have Microsoft's own definition of zero-day, now we have disappearing Microsoft CVEs courtesy of ignoring the industry's established vulnerability monitoring system by simply not registering discovered problems which require repair and updating.  It's not a bug if we don't call it one.  And recall that we saw something like this before where Microsoft was claiming that if no user action was required, then no security event was warranted.  We'll just all pretend it never happened since we'd rather project that image to the world.



And on the subject of important things not being swept under the rug, we have, as I noted, what was believed to be an upcoming OpenSSL critical vulnerability update.  Okay.  So this was interesting because in order to facilitate the race which would be between releasing a patch to an important open source system and those who would invariably attempt to take advantage of the patch window before all systems were updated and patched, exactly one week ago today the OpenSSL project gave the industry one week of advance notice of a forthcoming, what they called then, highly critical patch to OpenSSL 3.0.  The patch release is today, November 1st.



Commenting on this back then, Andrew Ayer tweeted:  "This will be OpenSSL's first critical vulnerability since 2016.  Examples of critical vulnerabilities include 'significant disclosure of the contents of server memory, potentially revealing user details.  OpenSSL 3.0.7 update to fix Critical CVE out next Tuesday, does not affect versions before 3.0.'"



Okay.  So the vulnerabilities, and we know what they're numbered, they're CVE-2022-3786 and 3602, they affect version 3.0.x and do not impact OpenSSL 1.1.1 or LibreSSL.  That was all that was known as of actually earlier this morning when I checked.  Now we know that the patch is out.  We know that it is less than critical.  It's now considered high, apparently because of some mitigations to how difficult it is to fix.  I expect we'll be talking about it next week.  Everything should be known.  The source code diffs will be made.  We'll figure out what they changed.  It'll be reverse engineered.  Maybe it'll be the case, hopefully the bad guys won't be able to get much leverage from it.  We'll know more next week.  And Leo, let's find out more about our sponsors.



LEO:  Good time to do it, of course.



STEVE:  And then we'll talk about this remote code execution in the TCP/IP stack in Windows.



LEO:  Just a never-ending litany of problems and issues and security flaws.  That's why almost all your advertisers sell security products.  What a surprise.



STEVE:  Speaking of putting glue in the USB ports.



LEO:  Yes, yes.



STEVE:  A little tidbit that crossed while I was putting the podcast together, but I didn't lock it down in the show notes, was that someone plugged his USB rechargeable vape stick into his MAC.



LEO:  Yeah?



STEVE:  And up popped a screen asking that this thing wanted permission to access his computer.



LEO:  OMG.  What the what?



STEVE:  So we've talked about the USB condom, which these vape stick users should be using because this is not something that you want to have happen, obviously.  Yeah.



Okay.  So speaking of network vulnerabilities, last week there was a worry about the possible impact of a TCP/IP remote code execution vulnerability in Windows.  That's an eye-opener, since the assumption is that it's a problem in the core kernel code, where the TCP/IP stack lives.  And the worry was heightened by the fact that a proof-of-concept exploit was published to GitHub by researchers at Numen Cyber Labs who had reverse engineered the vulnerability through patch analysis.  Again, Windows patched it, and you can look at the difference between what was there before and what's there now and figure out what's going on.  But the fact that there were patches to analyze, as I said, meant that the problem had already been patched, as indeed it had been nearly two months previous, and this was in September's Patch Tuesday.



Now, after reading through their research, it became clear that the conditions required for this vulnerability to be abused in the field were unlikely to occur.  So it was mostly a theoretical vulnerability.  I was shaking my head when I saw that the trouble was caused by fragmented packet reassembly in IPv6's IPSec handling.  How many times - now, this would only apply to listeners of the podcast forever.  But how many times in the years of this podcast, when we were discussing the low-level technologies of IP protocols back in the early days, did we encounter exploitable bugs resulting from the attempt to reassemble fragmented IP packets?  It turns out it's just difficult to do that.  And I think in the stack that I wrote for ShieldsUP!, I think I just ignore fragmented packets when they come in fragmented because it should actually never happen any longer.  It's legacy need.  Anyway, it was a constant theme for us in the old days.



So in any event, nothing widespread will amount from this.  It's too specific, rare, and it's been patched for two months.  But this will be one of those growing number of vulnerabilities that major, long-term, nation-state players will add into their known-exploits database for possible selective deployment when they encounter a still-unpatched system which qualifies for this very specific vector of attack.  Though I have no firsthand knowledge or any evidence of this being done, there's just no chance that such databases do not exist in the world today.



If someone were to put me in charge of the United States process for this, or similar people in China or Russia in charge of their cyberwarfare effort, given that we know that vulnerabilities never really die, creating such a database would be the first thing you would do because when someone says to you "We need to get into this person's system," this specific system, you inventory that system.  You look at what you can determine about that system.  And then you query your database of known exploits that specifically target that system, and then you start going down the list in order to find your way in.  That's just the way it's going to be done.  Probably is being done right now.



Okay.  And speaking of proofs of concept published on GitHub, I wanted to warn any of our listeners who might enjoy grabbing and trying such proofs of concept for themselves that a just-published study of GitHub's hosted proofs of concept found that a surprisingly high percentage of all of them were deliberately malicious.



Three academic researchers at the Leiden Institute of Advanced Computer Science at the Leiden University in The Netherlands published their research titled "A study of malicious CVE proof-of-concept exploits in GitHub."  The abstract of their paper explains.  They said:  "Proof-of-concept exploits for known vulnerabilities are widely shared in the security community.  They help security analysts to learn from each other, and they facilitate security assessments and red teaming tasks."  Of course, we also know that they make it easier for bad guys to turn those proofs of concept into actually aggressive, malicious code.  And that's a problem.



"In recent years," they said, "PoCs have been widely distributed, for example, via dedicated websites and platforms, also via public code repositories like GitHub.  However, public code repositories do not provide any guarantees that any given PoC comes from a trustworthy source, or even that it simply does exactly what it's supposed to do.



"In this work we investigate PoCs shared on GitHub for known vulnerabilities discovered from 2017 through 2021.  We discovered that not all PoCs are trustworthy.  Some proofs of concept are fake, in other words, they do not actually offer PoC functionality; or even malicious, for example, they attempt to exfiltrate data from the system they are being run on, or they try to install malware on the system.



"To address this, we have proposed an approach to detect if a PoC is malicious.  Our approach relies on detecting the symptoms we've observed in the collected dataset, for example, calls to malicious IP addresses, encoded malicious code, or included Trojanized binaries.  With this approach, we discovered, get this, 4,893 malicious repositories out of 47,313."



LEO:  Wow.  That's like 10%.



STEVE:  Yes.



LEO:  No, no.  That's 1%.  But still, a large number.



STEVE:  No, no.  No, you were right first.  10.3%.



LEO:  Wow.



STEVE:  10.3%, more than one in 10 of the studied repositories have symptoms of malicious intent.  They said:  "This figure shows a worrying prevalence of dangerous malicious proofs of concept among the exploit code distributed on GitHub."



So, you know, we've previously noted here that code repositories such as NPM and PyPI have become laced with malicious fake libraries.  So I wanted to make sure that everyone knew that now, sadly, GitHub's published PoCs need to be treated with similar caution.  As has often been said, we can't have nice things.



LEO:  So these are repositories labeled as proof of concept of malicious code.



STEVE:  Yes.  Well, no.  Proofs of concept of vulnerabilities.  



LEO:  Vulnerabilities, I mean, right.



STEVE:  So, yeah.  So a vulnerability exists.  Someone said:  "Here's a proof of concept."



LEO:  It's an example of malicious code.  



STEVE:  An example of how to exploit the vulnerability.  So it's not itself vulnerable.  For example, you run it, and it pops up the calculator.exe.



LEO:  Right.  Says, see?  See what I did?



STEVE:  Uh-huh, even though you're not supposed to.



LEO:  Except many of them are malicious in and of themselves.



STEVE:  One in 10.



LEO:  I would like to know what they call the signs of malicious intent are.  I mean...



STEVE:  Well, contacting a known malicious IP or having embedded malicious code which has been obfuscated so as not...



LEO:  And it's unrelated to the exploit being demonstrated.



STEVE:  Yes.



LEO:  That's the key, to me.  Because obviously they use malicious code.  It's a demonstration of malicious code.  So it might have to contact that server, for instance.  But if it's unrelated or it's trying to hide something, then maybe.



STEVE:  Well, no.  Okay.  So it's not a demonstration of malicious code.  It's a proof of concept...



LEO:  Of a vulnerability.



STEVE:  ...demonstration of a vulnerability.  So it's like, you know, there's this vulnerability in SQL.  Here's some code to show you how to benignly, I mean, these are always meant to be, like to benignly show how to use the vulnerability, like to exploit it.  It doesn't actually do anything bad.  It just says, "Hi Mom."



LEO:  Right.  See, I was able to do this, right, yeah.



STEVE:  Yes, exactly.  And you should not have been able to do that.



LEO:  Right. 



STEVE:  Like, oh, look, Johnny Drop Tables.  Whoops.  The Johnny Table's gone now, so that's not good.  So anyway, so the point is just, you know, proceed with caution.  Don't assume that a white hat hacker posted a proof of concept for something you're all hot and bothered about finding out how to do yourself.  Treat it with care because more than one in 10 do not have your best interests at heart.  They're taking advantage of what was the presumed sort of trust among developers.



Okay.  And winning without contest the title for the best-named vulnerability write-up of the year, we have a potentially serious SQLite exploited flaw named "Stranger Strings."  Great name.  The concept, or rather the concern of this, behind this, is significant because this flaw was first introduced into SQLite v1.0.12, which was released on October 17 of the year 2000, more than 22 years ago.  And it was only just recently fixed in release 3.39.2, which was released this summer on July 21st, 2022.  In other words, this flaw has been present in SQLite for 22 years.  And the biggest problem - you might be thinking, oh, you know, SQLite, I don't use that.  The biggest problem is SQLite is by far the most popular embedded database which is used, quite literally, everywhere.



LEO:  Oh, yeah.  Absolutely, yeah.



STEVE:  To get a quick sanity check just now, as I was putting the show notes together, I opened a command prompt, switched to the root directory of my primary system drive, and I entered the command dir *sqlite*.* /s, "s" meaning check all subdirectories.  And the text console exploded with hits and scrolled off into oblivion.  One thing I immediately noticed was that if you have anything from Mozilla, you've got lots of SQLite.  Mozilla loves their SQLite for Firefox and Thunderbird, in my cases.



Okay.  Since this was too much information, I tightened the search to just the SQLite DLL.  So I did a dir command.  First of all, I cleared the screen because I wanted to get my console back.  I mean, it was just literally, it was like the thumb just scrolled off to nowhere.  So clear the screen.  Dir *sqlite.dll /s, and the result was far more useful and much more chilling.  The apps I have installed on my system which embed the SQLite database engine, some of which I use frequently, but many others which I haven't used after first installing them are novaPDF 9, Zend Studio, Acrobat 9, Amazon's Kindle Reader, AutoIt, NetBeans, Perl's CPAN library, Python, Microsoft Edge, Thunderbird, Firefox, FreeCAD, Calibre 2, Stream Catcher Pro, NetWorx - that little network app I talked about last week, even it - Ping Plotter, and PHP.  And not one of these is an explicit database app.  SQLite is the way the world's apps organize any data that they are being asked to retain, even if it's just user preference settings.



So here's what the "Stranger Strings" guys had to say.  Stranger Strings is what they called this.  They're from Trail of Bits.  And they said:  "Trail of Bits is publicly disclosing CVE-2022-35737, which affects applications that use the SQLite library API.  CVE-2022-35737 was introduced in SQLite version 1.0.12 (released on October 17th, 2000) and fixed in release 3.39.2 (released on July 21st, 2022)."  In other words, 22 years.  And that means every single copy of SQLite that everybody has, unless it's been fixed, and it probably hasn't, is vulnerable to this.



They said:  "It's exploitable on 64-bit systems, and exploitability depends on how the program is compiled.  Arbitrary code execution is confirmed when the library is compiled without stack canaries, but unconfirmed when stack canaries are present.  And denial-of-service is confirmed in all cases."  Now, just to remind everybody, a stack canary is something which the compiler can be asked to stick on the stack in order to protect from stack overrun.  The idea is that when you perform a return to using the contents of the stack to decide where to go, before the program does it, it verifies a cookie on the stack has not been overwritten before assuming that the stack has not been smashed.  So it is possible that the SQLite library has been compiled with those cookies on the stack, in which case the app will crash rather than execute malicious code.



They said:  "On vulnerable systems" - but I have no knowledge of whether or not stack cookies are typically compiled into binaries of SQLite.  If somebody does, shoot me a note.  "On vulnerable systems, 35737 is exploitable when large string inputs are passed to the SQLite implementations of the printf functions, and when the format string contains the %Q, %q, or %w format substitution types."  And we not too long ago were talking about printf problems.  Anyway:  "This is enough to cause the program to crash.  We also show that if the format string contains the ! special character to enable Unicode character scanning, then it is possible to achieve arbitrary code execution in the worst case, or to cause the program to hang and loop nearly indefinitely."



They said:  "SQLite is used in nearly everything" - echoing my comment - "from naval warships to smartphones to other programming languages.  The open-source database engine has a long history of being very secure.  Many CVEs that are initially pinned to SQLite actually don't impact it at all.  This blog post describes the vulnerability and our proof-of-concept exploits, which actually does impact certain versions of SQLite.  Although this bug may be difficult to reach in deployed applications, it is a prime example of a vulnerability that's made easier to exploit by what they called 'divergent representations' that result from applying compiler optimizations to undefined behavior.  In an upcoming blog post, we'll show how to find instances of the divergent representations bug in binaries and source code."



They said:  "A recent blog post presented a vulnerability in PHP that seemed like the perfect candidate for a variant analysis.  The blog's bug manifested when a 64-bit unsigned integer string length was implicitly converted into a 32-bit signed integer when passed as an argument to a function."  So there was an implicit type conversion flaw.  In fact, we talked about one of those not long ago.  They said:  "We formulated a variant analysis for this bug class, found a few bugs.  And while most of them were banal, one in particular stood out, a function used for properly escaping quote characters in the PHP PDO SQLite module.  And thus began our strange journey into SQLite string formatting."



Anyway, they go on and on.  Their posting, I have a link to it in the show notes, is extensive and interesting.



They did have a great piece of commentary toward the end about the testing of SQLite and how this high-severity flaw happened and remained hidden for 22 years.  They said SQLite - and this is actually comforting.  "SQLite is extensively tested with 100% branch test coverage."  Meaning every single branch in the code receives a test.  They said:  "We discovered this vulnerability despite these tests, which raises the question, how did the tests miss it?  SQLite maintains an internal memory limit of 1GB, so the vulnerability is not reachable in the SQLite program.  The problem is 'defined away' by the notion that SQLite does not support big strings necessary to trigger this vulnerability.



"However, the C APIs" - SQLite is 100% written in C and highly cross-platform portable.  They said:  "However, the C APIs provided by SQLite do not enforce that their inputs adhere to the memory limit, and applications are able to call the vulnerable functions directly.  The notion that large strings are unsupported by SQLite is not communicated with the API, so application developers cannot know how to enforce input size limits on these functions.



"When this code was first written, most processors had 32-bit registers and 4GB of addressable memory, so allocating 1GB strings as input was impractical.  Now that 64-bit processors are quite common, allocating such large strings is feasible, and the vulnerable conditions are reachable," where before historically they weren't, which I think is really interesting.  This essentially surfaced because we went to 64 bits, and now that conversion from 64-bit lengths to 32-bit lengths, which could never have been a problem before, suddenly could be.  They said:  "Unfortunately, this vulnerability is an example of one where extensive branch test coverage does not help because," they said, "because no new code paths are introduced.  100% branch coverage says that every line of code has been executed, but not how many times.  This vulnerability is the result of invalid data that causes code to execute billions of times more than it should.



"The thoroughness of SQLite's tests is remarkable.  The discovery of this vulnerability should not be taken as a knock on the robustness of these tests.  In fact, we wish more projects put as much emphasis on testing as SQLite does.  Nevertheless, this bug is evidence that even the best-tested software can have exploitable bugs."



So on July 14th of this year, 2022, they reported the vulnerability which they discovered to the CERT Coordination Center.  On the 15th, CERT/CC reported the vulnerability to SQLite's maintainers.  On the 18th, the SQLite maintainers confirmed the vulnerability and fixed it in the source code.  And on July 21st the SQLite maintainers released SQLite version 3.39.2 which included the fix.  The problem, of course, again, is that many, and probably most, of the individual applications which each brought along their own private copy of what is now a theoretically vulnerable SQLite engine have not subsequently been updated.  None of those things that I talked about, most of those have been updated since July.  So what do these guys say about this?  How do they appraise the real threat, if any, that this represents?



They wrote:  "Not every system or application that uses the SQLite printf functions is vulnerable.  For those that are, CVE-2022-35737 is a critical vulnerability that can allow attackers to crash or control programs.  The bug has been particularly interesting to analyze for a few reasons. For one, the inputs required to reach the bug condition are very large, which makes it difficult for traditional fuzzers to reach, and so techniques like static and manual analysis were required to find it.  You wouldn't find it just by throwing crap at the wall and seeing if something crashed.  For another," they said, "it's a bug that may not have seemed like an error at the time that it was written, dating back to 2000 in the SQLite source code, when systems were primarily 32-bit architectures."



So here again we have a bug that's very much like Log4j.  It's buried, unseen, inside random applications, many of which will never be updated since everything is working just fine.  Highly active and maintained apps like Firefox, Thunderbird, and Edge have all likely already updated their code.  But many others never will.  We can hope that no remotely accessible applications would be vulnerable to this.  And it seems unlikely that any would be.  But like so many other similar problems we've seen, this adds again to the growing list of latent known vulnerabilities which riddle today's software systems.



A quick note that PayPal said they're going to be starting to support Passkeys.  Last week they announced their support for Passkeys.  As a PayPal user, I'm super interested in having this experience, as I would imagine most of our listeners are.  Coverage of PayPal's announcement appeared to suggest that the support was already there, but PayPal's press release was not specific.  They said, dated October 24, 2022, PRNewswire:  "Today, PayPal announced it is adding Passkeys as a secure login method for PayPal accounts."



So I just logged into PayPal through Safari on my iPhone with iOS 16.1, and I was unable to see any option for Passkeys anywhere.  And, I mean, I really dug around.  So I'll ask any other PayPal-using listener who uses Twitter to shoot me a tweet if and when they actually see that PayPal's support for Passkeys has gone live.  A lot of the press coverage of this, and it got a lot of coverage, and many of the coverage said that it's there.  But if they actually saw it or found it, I was unable to.  So, but cool that that would be there.  I would love to have this experience, see how it works.



LEO:  On the sign-in screen, tap the account name field.  Type other options, passkey, from nearby device or similar.  I don't know, I'm just - this is from Apple, not from PayPal.  I'll try it while we're - you do have to, it looks like, on your phone, for a new account on the account signup screen, Internet account name.  When you see an option to save a passkey, so something has to pop up.  But let's see, on an existing account, which is you, sign in with your password, then go to the account management screen, where you should see that option.  I'll try it.  I'll try it, and I'll get back to you.



STEVE:  Cool, cool.  Oh, one very cool thing.  The last thing I need to share with everyone is potentially quite exciting for the right sort of listener.  CrowdStrike's Jack Halon has just released the first of his three-part extensive tutorial series titled "Chrome Browser Exploitation."  And as I mentioned and we had fun with at the top of the show, I was so relieved to see that it was not on YouTube.  Jack's tutorial leads its reader through the details of exactly how to poke at Chrome.



He tweeted:  "Today I'm finally releasing a new three-part browser exploitation series on Chrome.  This was written to help beginners break into the browser exploitation field.  Part 1 covers V8 internals such as objects, properties, and memory optimizations.  Enjoy."  So I've got a link to it in the show notes.  It's on github.io, jhalon.github.io/chrome-browser-exploitation-1.  So I think many of our listeners might find it interesting.



One, actually two pieces of miscellany.  So many people tweeted to me about the recent passing of Kathleen Booth at the age of 100.  She was born on July 9th, 1922 and lived until September 29th, just a couple days ago.  Anyway, I wanted to thank everyone for making sure I knew.



Kathleen was a very early pioneer of stored program digital computers.  She developed and built several machines through the course of her life.  And the reason for everyone's tweets is that she's credited with developing an early notation for her machines' instructions, which she referred to as "Contracted Notation."  It's considered to be the earliest predecessor of what today we now call "assembly language."  And of course everyone knows of my fondness for assembly language.



Kathleen remained very active with computers and computation throughout her life, and in 1993 she co-authored a book on neural networks titled "Using neural nets to identify marine mammals" with her son Dr. Ian Booth.  So quite a remarkable woman.  And thank you to everyone who wanted to make sure that I knew.



Okay.  Bit of Closing the Loop.  Humili Math tweeted, he said:  "The wonders of OS provability reminds me of Knuth."  And Donald was quoted:  "Beware of bugs in the above code; I have only proved it correct, not tried it."



LEO:  Of course, famously, he wrote all of his code in a pseudocode that doesn't actually run on anything.  I think it's called MIX.  I can try and remember the name of it.



STEVE:  Yes, MIX.  MIX was the pseudo machine that he wrote his code for.  And anyway, he's got a great sense of humor.  He's famous for that.



LEO:  I love it.



STEVE:  So "Beware of bugs in the above code; I have only proved it correct, not tried it."



Someone tweeted to me, their name is Kal, and he said:  "@SGgrc Glad you tried Edge for SN-894.  Did you get to try the built-in, no extension needed, vertical tabs?  They're the best implementation I've ever seen.  Also you can use uBlock Origin Lite in Firefox and Chromium browsers now.  It's compatible with Manifest v3."  And Kal, to answer your question, and everybody else's, yes, the first thing I did was turn on vertical tabs in Edge.  I used it again yesterday, and actually all morning.  So I like it a lot.



David Flint said:  "Listening to SN-894 and the discussion on the proposed Australian legislation.  Is a possible benefit of the strict liability for data breach not a possible reason for the breached entity to pressure Microsoft into taking responsibility for the flaws in their software?"  Oh, were it so.  He says:  "As you yourself have said, it is not impossible to produce perfect software.  It just takes time and effort.  That seismic change can only be a benefit."



Well, I certainly agree on that issue, David.  At this point, with their size and grip on the world's personal and mostly enterprise desktops, Microsoft is utterly untouchable.  Their market value, their stock evaluation, is the only thing that I can imagine swaying them, and really nothing puts them at risk.  They're too big to care.  The licensing agreements hold Microsoft harmless for any behavior of their software and systems.  And none, not one of the things they have chosen not to care about endanger their position.  We're occupying a world where now, for the enterprise, which they've clearly indicated is the only thing they care about, there is no practical alternative to Microsoft.  So there's nowhere for anyone to go.



Allan Wilkins said:  "Can a new form of cryptography solve the Internet's privacy problem?"  Okay, now, that's an interesting question.  And the answer is no because our current cryptography is not the problem.  Today's crypto, when it's done right, is utterly unbreakable.  It can and does provide perfect privacy.  The problem is that data that's been encrypted will eventually be decrypted.  After all, it's only useful after it's been decrypted.  And as soon as it's decrypted, privacy problems arise.  If the data remains encrypted, no problem.  But if you're never going to decrypt it, you might as well delete it, in which case again no problem, but also then no data.  So the problem is not with the encryption or with the crypto, it's what happens the rest of the time.



LEO:  You've got mail.



STEVE:  I turned that off.  Why is it talking to me?



LEO:  Was that Pebbles?



STEVE:  I guess I turned it back on.  It dates from the days of CompuServe.  Somebody on CompuServe who has that audio file.



LEO:  Oh, wow.  Oh, it was their kid, I remember that, yeah, yeah.



STEVE:  Yeah, a four year old.  Adam Jamal Craig, he said:  "There comes a time in every programmer's life where they try to optimize their layer 0 I/O keyboard workflow.  Have you ever tried alternative keyboards" - and then he lists Kinesis, split,  or other ergonomic keyboards - "or alt keyboard layouts (Dvorak, Colemak, et cetera)?"



And then my answer is I never have.  My first wife's nickname for me, the one I can repeat in public, was "creature of habit."



LEO:  Yes.



STEVE:  Which was quite appropriate.



LEO:  She knew you well, I think.



STEVE:  Creature of habit.



LEO:  Yes.



STEVE:  I appear to be extremely sensitive to any change whatsoever in my keyboards.  My perfect world would have a limitless supply of cream-colored Northgate OmniKey 102 super-rigid and clanky keyboards with a high actuation force.  The CTRL key must be to the left of the "A" key, with the function keys to the left of them in two vertical columns where they're easily accessible, not arranged along the top where they're virtually unreachable.



LEO:  You like the old IBM layout with the function keys on the left.



STEVE:  Yes.  I don't like it, Leo.  It is where they're supposed to be.



LEO:  I haven't seen a keyboard that way in a long time.  And that's why you've saved those Northgates.



STEVE:  I'm sitting in front of it right now.



LEO:  Wow, I forgot about that.



STEVE:  Then to the right is the inverted "T" navigation pad, and at the far right is the 10-key numeric pad with the Num Lock  permanently off.  By some grace of God I am sitting in front of that keyboard right now, and I have another at my other location.  Both are still working after 35 years, knock on wood.



LEO:  They're probably pretty clicky, too, aren't they.



STEVE:  Oh, my god.  I mean, I have to be - while you were talking, I was typing something very, like, as quietly as I could.  But you can't be.  So the beauty of this keyboard, which has been in front of me, think about it, for more than half of my life...



LEO:  Wow.



STEVE:  Because I'm 67, and I've had it for the most recent 35 years, is that my brain appears to be directly connected to it.  If I stop to think about typing, I stumble.  But if I don't think, I just do, then text and actions flow unbidden.  Now, yes, I've tried other keyboards.  The closest key switch...



LEO:  Is this it?



STEVE:  Nope, nope, because the function keys are along the top.



LEO:  Well, there's also on the left.  You get the best of both worlds.



STEVE:  That's interesting.



LEO:  This is the OmniKey Ultra, which is a newer...



STEVE:  Although, oop, there's a problem, the ESCAPE key is up there in the top.



LEO:  Oh, that's no good.  No, no, no.



STEVE:  You can't have that.  It's supposed to be where the back tick is, to the right of the numeric 1.  I know.  So the closest key switch I've found is the "Cherry MX Blue."



LEO:  Yes.



STEVE:  It has the highest actuation force with the most clicky snap action.  You know, I need both.  There are keyboards I could switch to if I really had to.  But I already have some keyboard repair kits standing by, waiting for the day that I'll need to bring one of these beasties that I have back to life.



LEO:  There are people who rejuvenate these and sell them.  And they sell them at great price, I might add, hundreds of dollars, yeah.



STEVE:  Yes, yeah.



LEO:  Did you have these originally?



STEVE:  Yes.



LEO:  These are your original.



STEVE:  I bought these from Art, what was his - Art something at Northgate Computer.



LEO:  Oh, at Northgate Computer.



STEVE:  Yes.



LEO:  So this is a function key.



STEVE:  There it is.



LEO:  Is this it?  No, that has function keys.



STEVE:  Yeah, you've got to go back to the 102.  Is that a 102?



LEO:  It says 102.



STEVE:  It's not the 102.



LEO:  It's a phony 102.



STEVE:  Yeah.  Close.



LEO:  Close, but no cigar.  See, I use the tilde and back tick in Lisp and Emacs.



STEVE:  Well, I do, too.  I've got one.  It's just not in the wrong place.



LEO:  Of course not.  And now you make me want these.  I want one of these.



STEVE:  Oh, they're so nice, Leo.  They're clanky.  And it's got a PS/2 connector so then you need a PS/2 to USB converter.



LEO:  Yeah, I have a Model M, an old IBM Model M that uses that, yeah.



STEVE:  Nice.



LEO:  But, no, not as nice as yours, though.  I now want function keys on the left.



STEVE:  That's where they're supposed to be.



LEO:  Kids who are listening are, what?  What are you talking about?



STEVE:  WordPerfect, remember WordPerfect?



LEO:  Yes.



STEVE:  It used the crap out of those.  You had CTRL+F5 and ALT+F10, and you could just do anything with that.  Oh, those were the days.  So as for optimizing my workflow, this is why, I just did want to mention, this is why I spent the time before I started back on the work on SpinRite nailing down my development environment.  And I am so glad I did.  I'm now able to make an edit to SpinRite's source in Visual Studio in Windows.  I hit ALT+A, which is for Assemble, which builds the entire project by the time I've released the "A" key.  Then I turn to the keyboard attached to the utterly quiet ZimaBoard which is at my right elbow and type "g" on that keyboard, which is short for "go," which executes a DOS batch file to load and run SpinRite under Brett Salter's, who unfortunately we lost a couple years ago, DOS Periscope debugger from the directory it shares over the network with my Windows machine.  The whole experience is a joy.  And I'm going to keep at it.  After SpinRite 6.1, we're going to go to 7 and 7.1 and 7.2 with an equally perfect environment.



Oh, and speaking of ALT keys and so forth, Gordon Hlavenka, he said:  "Surely you must have known" - okay, Gordon.



LEO:  Surely.  Wait a minute, now, you didn't say it right.  "Surely you must have known."



STEVE:  That's right, "that at least since Windows 3.x CTRL+TAB will switch between child windows of an app."



LEO:  Surely.



STEVE:  "Since the invention of browser tabs, this has also worked to switch between tabs.  In the same vein, CTRL+F4 will close a tab or child window.  These aren't limited to browsers, of course.  You can switch between Word documents, for instance."



And Gordon, yes.  And I use all of those constantly because, like our wonderful Paul, who is a Windows keyboard shortcut maven, I've been doing that, too.  My assembly code, as I mentioned, is broken into many files by function, so in Visual Studio I'll often be collecting too many open files that I'm no longer working in.  So CTRL+F4, I use that repeatedly to close them quickly.  But what I was talking about, just to be clear, was ALT+TAB, which in all of my previous experience only switches among top-level application windows.  But here again on Monday, here I was working on this.  I'm in Edge and using ALT+TAB to quickly jump among Edge's most recently viewed tabs.  It's great.  But it came as a surprise to me.  And there's another crucial difference between ALT+TAB and CTRL+TAB.  ALT+TAB uses an MRU explicitly, a most recently used list, so that ALT+TAB takes you back to where you most recently were; whereas CTRL+TAB always moves forward in strict round robin sequence.  So MRU is the most useful sequence for me.



What's this one?  Oh.  Ooh.  Simon, tweeting from @Talk_2Simon, he sent a link to a useful rant posted at Medium.com by a guy who discovered that PayPal's login dialog was far too helpful.  It turns out it's true.  It's really shocking, Leo, you're not going to believe this.



LEO:  Okay.



STEVE:  That completely separate and completely bypassing any and all other measures that a PayPal user may have set up on their account, including their password and any form of second-factor authentication, and probably including Passkeys, we'll see once it gets there, there's always the option of providing your email address to identify yourself, then clicking the "Login with one-time code" button which is always there and which will send a six-digit code via SMS to the user's phone.



We know that SMS is not secure.  So there's that.  But, maddeningly, there is nothing that any PayPal user can do to prevent this short-circuiting of any and all other means of login.  I use PayPal.  I use PayPal a lot.  So I have my account set up with a long and gnarly password that no sane person could possibly enter correctly, and a TOTP, right, a time-based one-time password.  And my trusty authenticator app has an entry for PayPal.  But all of that is for naught if an attacker can arrange to somehow intercept my phone's SMS messaging stream.  If so, they can log in with the code they receive and use nothing else.  Nothing.



The debate with PayPal's customer support has been raging over this for more than a year, and the only thing PayPal will say, over and over, is "This feature is permanently enabled for the protection of our customers."  Well, we know what this is about; right?  We've seen this enough on this podcast to understand that PayPal's less sophisticated users must be provided with a backdoor into their accounts.  You can't actually have security for something like this or users will be frustrated when they cannot arrange to access their own money, even when it's entirely their fault that they cannot.  So as a result, every PayPal user's security is reduced to this lowest common denominator in order to accommodate the few.



LEO:  Wow.  So just avoid getting SIM-jacked.



STEVE:  Yup.



LEO:  Do you think eSIMs, this new Apple eSIM saying - well, it's not new, and Apple doesn't own it, but Apple's put it in all their new iPhones - will help with that?  Or does that make it easier?



STEVE:  I don't know.  Because certainly there is the problem of physically swapping SIMs.  But the biggest problem is that, as we know, our telephone system is still actually not very secure.



LEO:  It's not secure.  SS7 is always - and always will be because they're never going to fix it, yeah.



STEVE:  It's a mess, yeah.  Okay.  A little comment, update on where I am with SpinRite.  I had a number of things to fix and finish in SpinRite since I last spoke of it.  Mostly, none of the new drivers had ever been exercised in the presence of unreadable sectors, which we talked about recently, my ability to deliberately create flaws.  Until now we've all been testing on fully readable drives.  Some of the documentation, it turned out, in the official AHCI controller specification to which I had blindly written code because I had no choice at that point, was revealed to be more ambiguous than I knew.  So that necessitated that I reengineer the means for determining which sectors failed amid a 32,000-sector read.



So I did that.  I've tested the crap out of it now.  It's now working beautifully.  Then, with the ability to induce and locate defective sectors, I was able to work through SpinRite's entire data recovery system.  Now that's all done and working, as well, and tested.  Then I turned my attention to the detailed event logging system which needed major reworking to bring it into alignment with the various things that can now be reported since SpinRite has direct access to the system's mass storage hardware.  And all that's done, too.



Next up is performing the same defective sector location testing for SpinRite's four other drivers:  the new bus mastering DMA driver, its new IDE hardware driver, and then both its basic and extended BIOS drivers.  And I've already started to work on those.  So I reasonably expect to be finished with that by the end of this week.



Then the final thing I need to do is revisit SpinRite's command-line options to update them where needed.  There are some additional features due to the fact that I have hardware access now, like I can determine drives' serial numbers.  And so it'd be nice to be able to allow the user to specify which drive they want to use through the command line by specifying the serial number of a drive they know they want to test.  So anyway, that way, even if the drive should move around in the drive listing as other drives come and go, or as the BIOS reorders drives, the user will always be able to select the drive they intend.



At that point SpinRite 6.1 will be feature complete, so we're close.  And it will have been finished with some confidence, and I'll consider it to be at alpha stage.  So I'll update GRC's server to enable our testers to obtain their personally licensed test releases of it, and we'll eliminate any problems that I have missed and that have been missed by all of our previous testing.



I haven't mentioned it before, but I received news about a month ago that the commercial embedded operating system I've chosen for SpinRite's future will be leaving the market at the end of the year.



LEO:  Oh, crap.  So you're moving away from FreeDOS.



STEVE:  Moving away from FreeDOS.  And this is something I've been worried about since I recognized it could happen at any time.  I'm not surprised by this, actually, since this On Time RTOS-32 OS is old and mature, and there is now so much competition in the embedded processor market.  Intel x86 chips are much more oriented to high-end desktop processing, and there are a huge number of lower end chips that make much more sense for embedded applications.  So I strongly suspect that the licensing revenue for this thing has dried up quite a while ago, and the guys finally decided, okay, it's just not worth keeping this up.



Now, I'm not put off by the idea of a lack of support since I was planning to purchase the source code for this commercial system anyway.



LEO:  Oh, oh, okay.



STEVE:  So that I could fully customize it and extend it in any way that I might need for several more decades that I plan to be working on this.  But this end-of-the-year deadline means that I need to make sure that it's what I want, and to commit to it before it disappears.  Which means bellying up to the bar and buying a very expensive source code license for this entire commercial OS.  So while the SpinRite testing gang is pounding on the fresh 6.1 Alpha release, I'm going to overlap their testing with my own testing of On Time's OS to make sure that I like its development environment, and I'm able to dual boot my code both on BIOS and UEFI, which is the whole reason I'm doing this.  You know, I'll just create a very simple "Hello World" app to test that.  Since purchasing the source code for this commercial OS will be a significant investment, I need to make sure that, before I do it, it's going to be something that I'm going to want to use.



So when that's done, I'll incorporate any suggestions our testers have come up with.  I'll fix anything that they've found that's not working, and we'll move SpinRite to its pre-release beta stage.  And needless to say, it's feeling really great to see this project nearing its long-awaited conclusion.



LEO:  All right, Steve.  I think it is time to enter GCHQ.



STEVE:  So as I said, this week's topic began as a Twitter DM from Jonathan Z. Simon.  He said:  "Steve, thought you might appreciate this, the thoughts of the departing Technical Director of the U.K. National Cyber Security Centre."  And as we know, I did appreciate what a 20-year veteran and Technical Director of the U.K.'s NCSC, that's the National Cyber Security Centre, and GCHQ, whose name is Ian Levy, had to say about the valuable lessons he's learned.  And I know that our listeners will benefit from it, too.  His posting was long, so I've edited it down for size, but I've otherwise changed as little as possible.



His initial blog entry, or his final, rather, his final blog entry, he titled it humorously "So long and thanks for all the bits," of course in reference to Douglas Adams's Hitchhiker's Guide sequel.  So he said:  "It's with a heavy heart that I'm announcing that I'm leaving NCSC, GCHQ, and Crown Service.  Being Technical Director of the NCSC has been the best job in the world, and truly an honor.  I've spent more than two decades in GCHQ, with the last decade in this role, and its prior incarnations.  I've met and worked with some of the most amazing people, including some of the brightest people on the planet, and learned an awful lot. I've got to give a special mention to everyone in NCSC and wider GCHQ because they're awesome.  I've also had the pleasure of working with vendors, regulators, wider industry, academia, international partners, and a whole bunch of others.  I like to think I've done some good in this role, and I know I couldn't have accomplished as much without them.



He says:  "Regardless, there's a lot left to do.  So, I thought I'd leave by sharing 10 things I've learned, and one idea for the future."  And again, I've whittled those down.  We won't go through all of them because some of them just weren't needed.



He said:  "As a community, cybersecurity folk are simultaneously incredibly smart and incredibly dumb.  This superposition doesn't seem to be unique to our community, though.  In World War II, the Boeing B-17 Flying Fortress was the workhorse bomber of the U.S.  As you'd expect, a lot of them were lost during combat, but a lot were lost when they were landing at base.  Pilots were blamed.  The training was blamed.  Poor maintenance was blamed.  Even the quality of the runways was blamed.  None of this was supported by any evidence.



"After the war, someone actually did some research to try to understand the real problem.  Imagine you've been on a stressful bombing raid for 12 hours without sleep.  You're tired.  It's dark.  It's raining, and you're on final approach to your base.  You reach over to throw the switch that engages the landing gear and suddenly, with a lurch, your aircraft stalls and smashes into the ground, killing everyone.  This picture of a B-17 cockpit instrument panel shows what the pilot would see."



And at this point he actually shows a picture of the B-17 cockpit instrument panel.  There are two switches next to each other.  One switch he has labeled "Land safely."  The switch to its immediate right is labeled "Die horribly."  He says:  "There's no amount of training in the world that could compensate for this design flaw.  There's nothing to stop the most obvious error turning into a catastrophic outcome.  The 'land safely' switch, which operates the landing gear, and the 'die horribly' switch, which operates the flaps, are very close to each other and identical."



LEO:  Wow.



STEVE:  It turns out it was a UI flaw.



LEO:  Yeah.



STEVE:  In the layout of the panel that was killing all of these people who were landing their B-17 bombers.  "So," he says, "Blaming users for not being able to operate a terrible design safely.  Sound familiar?  But this investigation," he said, "led to something called 'shape coding' which persists as a design language today.  Most modern aircraft have a lever to operate the landing gear, and the little toggle on the end of the lever is wheel shaped so, when you grab it, you know what you've got hold of."



LEO:  Yeah, that's smart.



STEVE:  "The flaps control is generally a big square knob; and, guess what, they're not next to each other anymore.  The aircraft world has learned from its mistakes."



"In cybersecurity, we come up with exquisite solutions to incredibly hard problems, manage risks that would make most people's toes curl, and get computers to do things nobody thought was possible, or in some cases desirable.  But we also continue to place ridiculous demands on users."  He says:  "Take a deep breath, not to mention clicking links in emails or password policies."  He says:  "Implicitly expect arbitrary complex implementations of technology to be perfect and vulnerability-free in the long term, and then berate those who build the stuff we use when they fail to properly defend themselves from everything a hostile state can throw at them.



"I've always found this cognitive dissonance interesting, but I haven't found a way to reliably and easily work out when we're asking for daft things.  The nearest I've got is to try to determine where the security burden lies and whether it's reasonable to ask that party to shoulder it.  Is it reasonable to ask a 10-person software company to defend themselves from the Russians?  No.  Is it reasonable to ask a critical infrastructure company to not have their management systems connected to the Internet with a password of 'SecurePassword'?  Bloody right it is.



"The trick for making cybersecurity scalable in the long term is to get the various security burdens in the right place and incentivize the entities that need to manage those burdens properly.  Sometimes the obvious person to manage a burden won't be the optimal one, so we may want to shift things around so they scale better.  And let's be honest, we do suffer a lot from groupthink in cybersecurity.  So here's my plea.  The cybersecurity community should talk to people who aren't like us and actually listen to them.  Stop blaming people who don't have our 'l33t skillz' when something goes wrong.  Build stuff that works for most people, most of the time, rather than the easy or shiny thing.  And put ourselves in the shoes of our users and ask if we're really being sensible in our applications and expectations.  We haven't got that right yet."



And he says:  "We're treating the symptoms, not the cause."  He says:  "This month marks the 50th anniversary of memory safety vulnerabilities.  Back in October of 1972, a U.S. Air Force study first described a memory safety vulnerability," he says, "page 61, although the whole two-volume report is fascinating.  In the 1990s, Aleph One published the seminal work titled 'Smashing the Stack for Fun and Profit,' explaining how to exploit buffer overflows.  In the first three months of 2022, there were about 400 memory safety vulnerabilities reported to the National Vulnerability Database.



"As Anderson et al say in their report, this is the result of a fundamental flaw in the design of the hardware and software we use today, the exact same fundamental flaw they identified in 1972, 50 years ago.  Nothing has changed.  And what is our response to this issue?  Basically, we tell people to write better code and try harder.  Sometimes we give them coping mechanisms, which we give cool-sounding names like 'static and dynamic analysis' or 'taint tracking.'  But in the end we blame the programmer.  That's treating the symptoms, not the underlying cause, and once again harks back to the B-17 design problem.  I think we do that because fixing the real problem is hard.  Doing so breaks a bunch of existing stuff."  What was I saying last week about the operating system, which could be perfect, but it wouldn't run anything; right?  It would break everything.  Nothing to run.



"Doing so costs people money.  Doing so seems to be daft if you're in the commercial ecosystem."  Right, as I've also recently said.  You don't get paid for doing something secure.  "This is where," he says, "government can step in.  Under the Digital Secure By Design program, we're funding the creation of a memory safe microprocessor and associated tool chain.  If this works, it won't fix the stuff we already have, but at least it'll be possible to build systems that can't be exploited through memory safety errors without expecting every programmer to be perfect all the time.



"All of the Active Cyber Defense services are really treating the symptoms, rather than the underlying causes.  And I'm really proud of what we've achieved with the ACD program, and we've used it to force some systemic changes.  But even that program is about mitigating harm caused by the problems we see, rather than fixing the problems.  We really need to get to the root causes and solutions to some of these really thorny issues.  For example, one problem, in my opinion, is that it's too easy to set up free hosting for your cybercrime site.  There's no friction and no risk to dissuade would-be crims.  Hosting companies aren't incentivized to make it harder because, if one of them does it, they'll just lose customers to a competitor, which is commercial insanity.



"Should government regulate the provision of hostings?  Almost certainly not, at least in democratic countries.  But there's got to be some way of squaring this circle, and the many other apparent paradoxes that have dogged cybersecurity for years.  I've just not had the chance to work out how, so someone else needs to, hint, hint."  And he says:  "Much to learn you have, young Padawan."



He says:  "There have been a couple of big incidents in the last year or so that have rocked the cybersecurity world.  The attack on SolarWinds, now attributed to the Russian state, was hailed by some as a heinous, unacceptable attack.  The vulnerability in the widely deployed Log4j component caused mass panic, only some of which was justified, and has kicked off a load of kneejerk reactions around understanding software supply chains that aren't necessarily well thought through.  Both of these attacks remind me of Groundhog Day.



"The Log4j problem is equivalent to the problem we had with the Heartbleed attack in 2014, despite being a different sort of vulnerability.  The community had alighted on a good solution to a problem and reused the hell out of it.  But we hadn't put the right support in place to make sure that that component, OpenSSL, was developed and maintained like the security of the world depended on it because it did.



"The SolarWinds issue was a supply-chain attack, going after a supplier to get to your real target.  It's not the first time we've seen that, either.  Back in 2002, someone borked the distribution server for the most popular email server program, Sendmail; and for months, many, many servers on the Internet were running compromised code.  That same year, OpenSSH distribution was borked to compromise people who built that product to provide secure access to their enterprise."



He says:  "My point here is that we rarely learn from the past, or generalize from the point problems we see to the more general case, to get ahead of the attackers.  Going back to the aircraft analogy, if that industry managed risk and vulnerability the way we do in cybersecurity, there would be planes literally falling out of the sky.  We should learn from that sector, and the safety industry more generally.  The U.S. have set up the Cyber Safety Review Board, based on the National Transportation Safety Board, which will hopefully be a first step in systematizing learnings from incidents and the like, to drive change in the cybersecurity ecosystem.  At the moment, it's only looking at major incidents, but it's a start.  I hope the community supports this and similar efforts, and helps them scale and move from just looking at incidents to tackling some of the underlying issues we all know need fixing.  Without some sort of feedback loop, we're destined to be a bit dumb forever."



And finally, he says:  "Incentives really matter."  He says:  "Imagine you're a consumer who's about to switch broadband providers, and you want to compare two companies.  One offers to send you a 60-page document describing how it secures the PE- and P-nodes in its MPLS network, how the OLTs in the fiber network are managed, and how much effort it puts into making sure the ONT in your house is built and supported.  The other ones offers you free Netflix.  Of course everyone chooses the ISP with the free Netflix."



LEO:  Of course.



STEVE:  "In the telecoms sector, customers don't pay for security, so there's no incentive to do more for your customers, which is what would drive the sector to get better."  He says:  "That's where regulation can help and is one of the reasons that we, with DCMS, pushed so hard for what became the Telecoms Security Act.  The detailed security requirements in the Code of Practice make it clear what's expected of telecoms operators, and the fines Ofcom can levy when that's not met are material.  We're already seeing this drive better outcomes in the sector.



"If you ask anyone in the intelligence community who was responsible for the SolarWinds attack I referenced earlier, they'll say it was the Russian Foreign Intelligence Service, the SVR.  But Matt Stoller makes an interesting argument in his article that it's actually the ownership model for the company that's really the underlying issue.  The private equity house that owns SolarWinds acted like an entity that wanted to make money, rather than one that wanted to promote secure software development and operations.  Who knew?"



And Leo, if you remember thinking back to SolarWinds, there were some problems in the way they were managing their software that made no sense to us...



LEO:  Right, right.



STEVE:  ...at the time.  It's because they're owned by private equity that just wants to squeeze it for money.  He says:  "Now, I don't agree with everything Matt says in his article, but it's hard not to link the commercial approach of the company with the incentive and capability of the people in it.  It's then hard not to link the actions of those people, driven by their incentives and capabilities, with the set of circumstances that led to the attack being so easy for the Russians.  Of course, the blame still sits with the SVR, but did we accidentally make it easier for them?



"In both cases the companies are doing exactly what they're supposed to do, generate shareholder value.  But we implicitly expect these companies to manage our national security risk by proxy, often without even telling them.  Even in the best case, their commercial risk model is not the same as the national security risk model, and their commercial incentives are definitely not aligned with managing long-term national security.  In the likely case, it's worse.



"So I think we need to stop just shouting 'DO MORE SECURITY!' at companies and help incentivize them to do what we need long-term.  Sometimes that will be regulation, like the Telecoms Security Act, but not always.  Sometimes we're going to have - shock, horror - to pay them to do what we need when it's paranoid lunatic national security stuff.  But making the market demand the right sort of security and rewarding those who do it well has got to be a decent place to start.  Trying to manage cybersecurity completely devoid of the vendors' commercial contexts doesn't seem sensible to me."



Okay.  He says:  "I know I've been banging on about this forever, but the last few years have shown how important the mantra is:  Details matter.  Deep down, we all know how much details matter, but we also all seem to find it easy to fall back to generalizations and hyperbole.  It is critical that we get into the detail of things, whether deeply technical things that only three people on the planet care about, or discussions of policy that will affect everyone in the country.  If we don't, we eventually make a catastrophic error that will have real-world consequences."  So here it feels to me like this is the previous head of the NCSC who's talking about bureaucracy has a problem because it has a problem dealing with things that are detailed.



He says:  "While it's not pure cybersecurity, the somewhat polarized discussions about child safety and end-to-end encryption typify this problem for me, but I see it in many places.  I honestly think that getting into the real detail of hard problems is what helps us see pathways to solutions.  As I said at the start, there's a long way to go for cybersecurity, and a lot of hard problems still to solve.  Details are your friend."



And actually he then goes on at some length about what he calls his "grand unified theory of cyber," where everything is known about everything, about networks and software and functions and vulnerabilities and interactions, and where everything can be interconnected and graphed to drive decisions.  I didn't include it here because it's very long and because the creation of artificial sentience will likely occur first.



So anyway, I thought that sharing his recitation of the fundamental problems was useful.  And even though I know they will all have sounded familiar to anyone who follows the podcast, you know, if nothing else, we do appear to be moving toward a general consensus of the problems.  Although, yes, we can look back over the past 50 years, since the identification of the first memory-based vulnerability, and bemoan the fact that we also suffered from another one just yesterday, 50 years ago we didn't have any grasp of the problems we were going to be facing.  Today, at least we have that.  So it's a start.



LEO:  Yeah.



STEVE:  Even if all we can talk about is blue sky new operating systems that don't actually do anything.



LEO:  I do feel like we're making some progress.  I mean, I know that the landscape is littered with security problems.  But I feel like at least we're understanding what the issues are a little bit better, yeah.



STEVE:  Yeah.  And it has to be that bringing pressure to companies that need it is where the solution lies. 



LEO:  Well, you do that very well, as well as explain and elucidate what's going on in this world around us, Steve Gibson.  His website, GRC.com, the Gibson Research Corporation.  That's where you'll find SpinRite, the world's best mass storage recovery and maintenance utility.  6.0's the current version.  6.1 is in process, as you hear, very close.  If you buy 6.0 now, fear not, you will get a free upgrade to 6.1.  So you're not going to miss out.  Plus you get to participate in the development as we get closer.



He also has copies of the show there, GRC.com, including two unique copies.  He's got a 16Kb version for the bandwidth-impaired, and transcripts written by a human, Elaine Farris.  So those are great.  You can search those to find what you want, or read along as you listen, or use them however you like.  GRC.com.  Lots of other free stuff there, well worth checking out.



We have 64Kb audio at our site, TWiT.tv/sn.  We have a unique format, video.  If you want to watch, see all the blinking lights blink, that's also at TWiT.tv/sn, or on YouTube.  There's a dedicated YouTube channel.  And of course you can subscribe to either version of the show at the website.  Yeah, at the website we have links to the big name podcast clients.  But really any podcast client, if you search for Security Now! or TWiT, you should find it and be able to just press a button and subscribe to it at no charge.



After the fact, oh, I did mention all that.  Oh, I know what I didn't mention, when we do the show, which is Tuesdays around 1:30 Pacific, 4:30 Eastern.  We are going off daylight saving time on Sunday, so our new UTC time will be 21:30 UTC.  You figure out what time that it is in your local jurisdiction.  I don't know.  It's up to you on that one, 21:30.  And I mention that, not because - you can always download it at your leisure.  But if you want to watch us do it live, get the very first version of the show as we're recording it, you go to live.twit.tv at that time, you'll be able to watch that.  You can chat if you're watching live.  That's one of the advantages of live is interactivity.  The IRC is open to all at irc.twit.tv.  And of course the Discord for you Club TWiT members.  Join us next Tuesday.  Election Day, Steve.



STEVE:  Oh, my god, I know.



LEO:  Came up on us fast, yeah.



STEVE:  Boy, it's going to be interesting.



LEO:  Yeah.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#896

DATE:		November 8, 2022

TITLE:		Something for Everyone

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-896.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This pure news week we look at Dropbox's handling of a minor breach, and we follow up on last week's OpenSSL flaws.  The FTC has had it with a repeat offender, and we know how much total (reported) ransom was paid last year.  Akamai reports on phishing kits, we have some stats about what Initial Access Brokers charge, and we look at the mechanics of cyber bank heists.  Several more DeFi platforms defy belief, Russia is forced to move to Linux, the Red Cross wants a "please don't attack us" cyber seal, nutty Floridians get themselves indicted for a bold tax fraud scheme, is China cheating with zero-days, the NCSC will be scanning its citizenry, and more.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about how Dropbox properly handled a minor breach, and ask the question of whether you should ever trust a managed service provider.  More on the OpenSSL flaws.  The FTC going at it with Chegg.  I'm glad to see this.  And is China cheating with zero-days?  That and a whole lot more coming up next on Security Now!.  Stay tuned.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 896, recorded Tuesday, November 8th, 2022:  Something for Everyone.



It's time for Security Now!, the show where we cover you, your privacy, your security, how the Internet works, how computers work, with this guy, this genius right here, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.  We have - this is a Patch Tuesday.  This is the - it hasn't fallen on an Election Day since 2016.  Just a little bit of trivia there...



LEO:  Fascinating, yes.



STEVE:  ...for those who are following along.  And we owe today's show title to my wife.  Lorrie and I were out walking yesterday, and I was telling her what progress I had made so far.  She said, "So do you have a topic?"  And I said, "You know, I don't so far."  I said, "But that's okay.  Sometimes nothing really jumps out or stands out or needs special attention, and so I just call it like a busy news week or something."  And she said, "How about calling it 'Something for Everyone'?"  And I said, "I like that."



LEO:  I like it.



STEVE:  And so that's today's title, "Something for Everyone."  Because we just have all kinds of stuff.  We've got one of our pure news weeks.  We've got Dropbox's handling of a minor breach.  We follow up on last week's OpenSSL flaws.  The FCC has had it with a repeat offender.  And we're going to find out how much total reported ransom was paid last year to the ransomware denizens.  Akamai has reported on phishing kits, and that's some - it's, like, frightening.  We've got some stats about what Initial Access Brokers charge.  And we look at the mechanics of cyber bank heists, like how that's actually pulled off in the real world.  We've got several more DeFi platforms defying belief.  Russia is forced to move to Linux, finally.  The Red Cross wants a "Please don't attack us" cyber seal.  We've got nutty Floridians who have gotten themselves indicted in a bold tax fraud scheme that you just can't imagine they could have possibly thought they could have gotten away with.  And, well, because of indictments they didn't.  



LEO:  You know how that is, yeah.



STEVE:  That's right.  Also the question has been raised by Microsoft whether China is cheating with zero-days.  And in what I think is a fabulous idea that I hope the U.S. might adopt, the NCSC will be scanning the U.K.'s citizenry for vulnerabilities and working with them to remediate them.  And that's not all. There's more.  We've got a great Picture of the Week.  I've got some feedback from our listeners and a brief update on where SpinRite stands.



LEO:  Oh, wow.



STEVE:  So as I said, something for everyone.



LEO:  Something for everyone.  That sounds like an excellent show.  I'm looking forward to it.  I always do.  Do we have a picture?  I didn't even look.



STEVE:  We have a wonderful picture.  I will lead up while you're getting it ready.  Now, I'm tempted to call this the dumbest thing I've ever seen except that we've got two previous occupants for that slot.  One is the locked gate standing alone out in the middle of a meadow with a path running up to it.  And it's like, what is this locked gate doing out in the middle of nowhere?  Who's not going to walk around it?  And sure enough, there's like a dirt-trodden path on either side.



The other dumbest thing was that generator that had to be grounded, so someone stuck a piece of rebar into a pail of dirt and hooked the ground wire to the rebar.  And it's like, okay, I don't think that's quite what they had in mind when they said you need to ground this generator.



Okay, here we've got a very tall gate which looks like it's an electric gate.



LEO:  It's a good-looking gate.  It's very nice.



STEVE:  Nice-looking gate.  Got an intercom on the side so you can buzz the person, looks like maybe three different units are back there somewhere.  And you are not supposed to get in or out, presumably.  Unh-unh.  No.  The problem is that the genius who designed this gate used a series of horizontal bars.  And so I gave this the caption "Can't get in?  Hmm.  How about use the built-in ladder?"  Because, I mean, it's like designed for scaling the gate.  It's just, you know, hmm, I can't get in.  What should I do?  Oh, look, it's a ladder.



LEO:  How handy.  How convenient.



STEVE:  I mean, all they had to do was make them vertical, and then you'd just be like stuck.  You'd be looking like, you know, like prison bars.  But no, they built a ladder from the gate, and so it's quite easy.  Though this goes down, this is maybe the third dumbest thing that we've seen on the podcast where the...



LEO:  It's in the list, definitely.



STEVE:  Yeah, we are acquiring them over time.  Okay.  So last Tuesday, which was the first of November, Dropbox posted of their own experience titled "How We Handled a Recent Phishing Incident That Targeted Dropbox."  And the short version is I think they handled it pretty well.  But there are some lessons to be had surrounding the event.  Their announcement began with sort of the required "do not worry" disclaimer.  They said:  "We were recently the target of a phishing campaign that successfully accessed some of the code we store in GitHub.  No one's content, passwords, or payment information was accessed, and the issue was quickly resolved.  Our core apps and infrastructure were also unaffected, as access to this code is even more limited and strictly controlled.  We believe the risk to customers is minimal.  Because we take our commitment to security, privacy, and transparency seriously, we've notified those affected and are sharing more here."



Okay.  Then I skipped over a bunch of background.  And the part I wanted to share with our listeners was this.  They said:  "At Dropbox, we use GitHub to host our public repositories as well as some of our private repositories.  We also used CircleCI for select internal deployments."  CI is some automation technology, CI standing for Continuous Integration.  So they said:  "In early October, multiple Dropboxers received phishing emails impersonating CircleCI, with the intent of targeting our GitHub accounts.  A person can use their GitHub credentials," they explained, "to log into CircleCI."



They said:  "While our systems automatically quarantined some of these emails" - you know, phishing emails, right - "others landed in Dropboxers' inboxes.  These legitimate-looking emails directed employees to visit a fake CircleCI login page, enter their GitHub username and password, and then use their hardware authentication key to pass a One Time Password to the malicious site."  And as we know, all of this bypasses, you know, I mean, this approach will get around the use of one-time password authenticators.  So they said:  "This eventually succeeded, giving the threat actor access to one of our GitHub organizations, where they proceeded to copy 130 of our code repositories."  Whoops.



They said:  "These repositories included our own copies of third-party libraries slightly modified for use by Dropbox, internal prototypes, and some tools and configuration files used by the security team.  Importantly, they did not include code for our core apps or infrastructure.  Access to those repositories is even more limited and strictly controlled."  And finally:  "On the same day," they said, "we were informed of the suspicious activity."  They don't indicate how, but this is why you need to do network monitoring like, Leo, you were just talking about with that previous sponsor.



They said:  "The threat actor's access to GitHub was disabled.  Our security teams took immediate action to coordinate the rotation of all exposed developer credentials, and determine what customer data, if any, was accessed or stolen.  We also reviewed our logs and found no evidence of successful abuse.  To be sure, we hired outside forensic experts to verify our findings, and reported this event to the appropriate regulators and law enforcement."



Okay.  So there are three points that I wanted to highlight from this report.  The first is that we have yet another instance of a major security-savvy and network-savvy organization - you know, Dropbox; right?  I mean, they know their way around or they wouldn't still be around - being successfully attacked and breached, even in the face of knowing that this is going on.  Their email filters worked to prevent their employees from being subjected to this error-prone event mostly.  But those filters also failed just enough to allow bogus phishing attacks to reach their employees.



And notice that these were code developing employees, you know, not, for example, less sophisticated clerical or office workers who you might have in a huge organization that wouldn't be expected to be up to speed on computers.  You know, these are people who like log into CircleCI and GitHub, and they were fooled.  The point is phishing.  And we'll be talking about that several more times before the end of today's podcast.



The second point I want to make is the introduction of a new concept which I would term "the phishing email attack surface."  We're all familiar with the traditional concept of an attack surface; right?  The idea being that the more potential points of entry that exist, the greater the threat that any one of those might be inadvertently left open or somehow breachable.  So this new concept that I would call the phishing email attack surface uses this recent Dropbox experience as a perfect example, noticing that the more complex an organization's setup is, which is to say the greater number of ancillary services an organization employs, the greater is their phishing email attack surface.  There're just more things that have logons and authentication requirements and, again, more points of entry.



The modern trend is products as managed services, where companies are increasingly contracting out for an increasing number of services, rather than rolling their own in-house.  The theory of this is sound.  Why reinvent the same wheel over and over, especially when there's little additional value to be added by doing so?  Just contract for this or that service while focusing upon the company's core mission, rather than wasting time on developing and running all of those other things that are common to all companies.  Sounds great.



But recall all of the downstream damage that the breach at SolarWinds created.  SolarWinds was a provider of exactly this sort of outsourced services model.  And also remember all of those dental offices that were being breached, and the hospital services that were hit by crippling ransomware when their MSP, their managed service provider was breached.  The danger represented by managed service providers is exactly what I'm referring to here.



So I wanted to observe that we, as an industry, still have a serious problem with remote network services authentication.  The very fact that phishing emails even exists as a security issue demonstrates that this serious problem has not yet been solved.  So the more remote network MSP services an organization maintains, the greater their phishing email attack surface will be.



The third and final point I wanted to make was where Dropbox wrote, they said:  "On the same day we were informed of the suspicious activity, the threat actor's access to GitHub was disabled.  Our security teams took immediate action to coordinate the rotation of all exposed developer credentials and determine what customer data, if any, was accessed or stolen.  We also reviewed our logs and found no evidence of successful abuse."



To that I say bravo.  When we were all growing up, our elementary schools conducted periodic fire drills.  Without warning, alarms would sound throughout the school, and the entire school, class by class, would file out in an organized manner to previously designated locations.  While I was in school, those alarms never went off except for during drills.  But if someday they were to, the entire school was prepared.



My point is every organization must now be prepared for the possibility of a network breach.  So "breach drills" should become a thing that all responsible organizations conduct, just as fire drills were once that when we were in elementary school.  Just as when a school might be on fire, after a network intrusion we've seen the stats showing that time really can be of the essence.  So planning for a breach, including having some drills, should be something that responsible organizations do.  Dropbox's immediate response showed that they were ready and prepared for that eventuality.  And again, I think that is of crucial importance.



LEO:  I think it's also important to point out that that's probably why in many cases it's better to use an MSP than do it on your own.  I mean, if we were to count all the flaws that people introduce themselves by trying to do it themselves, that's going to far outweigh the number of exploits because MSP was taken advantage of; right?



STEVE:  I mean, I think that's a useful consideration.  The problem with an MSP is the single point of failure.  So a breach at SolarWinds...



LEO:  Gets everybody.  Yeah, yeah, yeah.



STEVE:  ...devastated, yes, so many clients.



LEO:  But I think about Bitwarden, for instance.  And some people, again, with Bitwarden, one of our sponsors and a password manager, host their own.  And they often say, well, why do you let Bitwarden host it?  Because, I always say because I think they're going to - yeah, I could host it myself.  I think they're more likely to keep it locked down than I am.  You know?



STEVE:  And backed up.



LEO:  Yeah.



STEVE:  You don't risk losing the cloud presence.  I mean, it certainly is a consideration.  I guess the thing to do would be...



LEO:  But you've got to trust them. 



STEVE:  As always, yeah, find some balance point, you know, for example, don't give no consideration to the security of the services that you're hiring.  At least, you know, have them run the gauntlet and demonstrate that it makes sense for you to put some portion of your security in their hands because you are.  You know, you are, when you're outsourcing a service, you're outsourcing the security of that service and that service's access back into your organization.  And that's what bit the hospitals and bit all those dental practices when their common MSP got hacked.



So it's just - I sort of wanted to put it on people's radar to consider that, you know, if Dropbox hadn't been using CircleCI, well, they wouldn't have been prone to the CircleCI phishing emails.  And so that couldn't have happened.  Maybe something else would have happened.  They would have gotten in some other way.  But that's the way it happened.  So it's very much like, you know, having exposed ports.  Each of those things represent some exposure; and that means, you know, an expanded attack surface.



Two weeks ago - as we talked about last week when it was one week ago, now it's two weeks ago - the OpenSSL project maintainers told the entire world that one week from then a critical vulnerability would be patched and necessarily revealed to the world.  So last week the severity, the good news was it was downgraded from critical to high.  Since there is some possibility that one of the two problems could be weaponized, the advice remains that everyone using any v3.x.x of OpenSSL, where those x's aren't 0 and 7, which is to say if you're using anything before 3.0.7, which contains the two fixes, that should be looked at.  So, okay.  Here's what we know now, as I suspected last week, we would find out what was going on.  Here's what the project maintainers wrote about the most serious of the two problems.  It's got a CVE-2022-3602, now rated at high severity.



They said:  "A buffer overrun" - which is of course where most of these problems begin.  "A buffer overrun can be triggered in the X.509 certificate verification, specifically in name constraint checking."  They said:  "Note that this occurs after certificate chain signature verification and requires either a CA to have signed the malicious certificate, or for the application to continue certificate verification despite failure to construct a path to a trusted issuer."  That meaning if it hadn't been signed.  "An attacker can craft a malicious email address to overflow four attacker-controlled bytes on the stack.  This buffer overflow could result in a crash, causing a denial of service" - meaning, you know, your service is denied because the thing crashed - "or potentially remote code execution.  Many platforms implement stack overflow protections which would mitigate against the risk of remote code execution.



"The risk may be further mitigated based on stack layout for any given platform and compiler.  Pre-announcements of the CVE described this issue as critical.  Further analysis based on some of the mitigating factors described above have led this to be downgraded to high.  Users are still encouraged to upgrade to a new version as soon as possible.  In a TLS client, this can be triggered by connecting to a malicious server.  In a TLS server, this can be triggered if the server requests client authentication, and a malicious client connects."



Okay.  So the second of the two problems - there were two that were related.  The second one is quite similar, but it only allows the attacker to overflow the stack with an arbitrary number of "dot," you know, period characters.  I think that's hex 46.  So the attacker's inability to overflow the stack with their own provided data, all they can do is dot characters, limits the practical danger to a denial of service that would result in a crash in OpenSSL.  But the reason the more serious of the two was initially felt to be critical is that the stack overflow can be of attacker-provided bytes, for attacker-provided bytes.  Which could be a jump or just enough code, for example, to elevate this task if it weren't already, or to bypass security checks, you know, whatever.



So what remains to be seen is whether anyone ever arranges to weaponize this attack.  There's no doubt that many vulnerable instances of OpenSSL v3 previous to 07 will remain out in the world for the foreseeable future.  They will have already been built into appliances that will never be updated.  It's a relief that the trouble cannot be induced in an OpenSSL-based TLS server without the server first requesting a certificate from a client.  That's unusual enough so as not to be a big issue.



But if an OpenSSL-based TLS client were to be induced into visiting a malicious server after this flaw were weaponized, that could result in the execution of code on the visiting client, thus compromising somebody who connects to a malicious server.  And that could pose sufficient inducement to cause, that is, the potential of that could be sufficient inducement to cause major exploit creating players to investigate its weaponization.  So we'll see if, a year or two from now, we're not talking about, whoops, remember that OpenSSL vulnerability that was downgraded to high, that should have been fixed wherever possible, well, you know, we'll see if that ends up happening.  It could.



Okay.  We're going to begin hearing of more instances of these sorts of reactions from the U.S. federal government; and, over time, it will become widely known that companies cannot simply ignore their security responsibilities with impunity.  On Halloween, the FTC's Business Blog post was titled "Multiple data breaches suggest educational technology company Chegg [C-H-E-G-G] didn't do its homework, alleges the FTC."  Now, we'll forgive the FTC for being cute about an educational company not doing its homework.  But the points made in their blog posting about this were instructive.



The FTC wrote:  "Chegg, Inc., sells educational products and services directly to high school and college students.  That includes renting textbooks, guiding customers in their search for scholarships, and offering online tutoring.  But according to the FTC, the ed tech company's lax security practices resulted in four separate data breaches in a span of just a few years, leading to the misappropriation of personal information about approximately 40 million consumers.



"The FTC complaint and some notable provisions in the proposed settlement suggest that it's time for a data security refresher course" - again with the educational approach - "at Chegg.  Are there lessons your company can learn, the FTC posits or wonders, from where the FTC says Chegg failed to make the grade?"



Okay.  Okay.  In the course of its business - so here's what happened.  California-based Chegg collected, they said, the FTC said, a treasure trove of personal information about many of its customers, including their religious affiliation, heritage, date of birth, sexual orientation, disabilities, and parents' income.



LEO:  Why do they have my sexual orientation in the first place?



STEVE:  Exactly.



LEO:  What the hell is that?



STEVE:  Exactly.



LEO:  They're doing textbooks.



STEVE:  Yes.  I know.  Even the Chegg employee in charge of cybersecurity described the data gathered as part of its scholarship search service as "very sensitive."



LEO:  Oh.  Yeah.  So you might - there might be a scholarship for queer scholars, something like that. 



STEVE:  Okay.



LEO:  So you'd have to give them that information, I guess, to find those scholarships.



STEVE:  In order to, yeah, to qualify, right.



LEO:  It is.  It's very sensitive.



STEVE:  Yes.



LEO:  Yeah.



STEVE:  And four breaches.  I mean, it's very sensitive, and they're not treating it responsibly.  But wait till you hear, Leo, it's unbelievable.  A key component of Chegg's information technology infrastructure was Simple Storage Service (S3).



LEO:  Oh, boy.



STEVE:  Uh-huh.



LEO:  S3 buckets can be secure, but they're often not.  They're often not.



STEVE:  Cloud service offered by Amazon Web Services (AWS) that Chegg used to store a substantial amount of customer and employee data.  The full complaint provides all the details, but the FTC cites a number of examples of what Chegg did and didn't do that were indicative of the company's lax security practices.  For example, the FTC alleges that Chegg allowed employees and third-party contractors to access the S3 databases with a single access key that provided full administrative privileges over all information.  Chegg did not require multifactor authentication for account access to the S3 databases.  Rather than encrypting the data, Chegg stored users' and employees' personal information in plaintext.



Until at least April of 2018, Chegg "protected" - they have that in air quotes - passwords with outdated cryptographic hash functions.  Until at least April 2020, Chegg failed to provide adequate data security training for employees and contractors.  Chegg didn't have processes in place for inventorying and deleting customers' and employees' personal information once there was no longer a business need to maintain it.  In other words, you know, it just kept accruing the data ad infinitum.  Chegg failed to monitor its networks adequately for unauthorized attempts to sneak in and illegally transfer sensitive data out of its systems.  In other words, across the board, your basic "do the minimum possible" laziness.



The report continues:  "Should it come as a surprise that the complaint recounts four separate episodes that led to the illegal exposure of personal information?  Incident 1 stemmed from a Chegg employee falling for a phishing attack that allowed a data thief access to the employee's direct deposit payroll information.  Incident 2 involved a former contractor who used Chegg's AWS credential, the one credential, to grab sensitive material from one of the company's S3 databases, information that ultimately found its way onto a public website.  Then came Incident 3, a phishing attack that took in a senior Chegg executive that allowed the intruder to bypass the company's multifactor email authentication system.  Once in the executive's email box, the intruder had access to personal information about consumers, including financial and medical information.  And Incident 4, a senior employee responsible for payroll fell for another phishing attack, thereby giving the intruder access to the company's payroll system.  The intruder left with the W-2 information of approximately 700 current and former employees, including their birthdates and Social Security numbers."



LEO:  Oh, god.



STEVE:  "In each of the four incidents cited in the complaint, the FTC alleges that Chegg had failed to take simple precautionary steps that would have likely helped prevent or detect the threat to consumer and employee data - for example, requiring employees to take data security training on the telltale signs of a phishing attempt."  Because they fell for it four times, and nobody ever learned any lessons.  No actions were taken as a consequence of those.



"To settle the case" - and, boy, have they gotten off easy - "Chegg has agreed to a comprehensive restructuring of its data protection practices.  As part of the proposed order, Chegg must follow a schedule that sets out the personal information it collects, why it collects the information, and when it will delete the data.  In addition, Chegg must give customers access to the information collected about them and honor requests to delete the data.  Chegg also must provide customers and employees with two-factor authentication or other authentication method to help protect their accounts."



So it's going to get better, but this is just, you know, this is just a toothpick in a haystack; right?  In this largely still unregulated industry, we're operating in a Wild West mode with nonexistent oversight until failures are egregious enough to bring governmental scrutiny.  And how many of these incidents were caused by employees falling for phishing schemes?  All four of them.  Even an exec did.  Yet there was no training provided.  The reason is none of those breaches directly affected Chegg's bottom line.  Oh, 40 million of their customers had highly sensitive data revealed?  "Well, we're very sorry about that."  Okay.  Right.



Well, I'm not one who believes in government overreach and having Uncle Sam rummaging around in our private corporate businesses.  But self-regulation isn't going to work here.  One solution would be to only provide tools that provide security.  Then at least security wouldn't need to be added on as an optional afterthought.  But as we all well know, we're not there yet.



LEO:  Everything you talk about on the show, Steve, is really a cautionary tale.  And I just imagine these CISOs and CIOs and IT folks listening, going, oh, boy.  Oh, boy.  Did we secure our S3 buckets today?  You know.  This is good.



STEVE:  Well, and we talked a couple weeks ago there was some survey, it was IBM who did the survey, of the stress that CISOs... 



LEO:  Oh, can you imagine.



STEVE:  ...are under.  I mean, it's just - it's not a - it's a horrible...



LEO:  Tough job.



STEVE:  It's a tough - yes.



LEO:  But a good job, important job.  Thank you for doing it.



STEVE:  It needs to be done.



LEO:  And we're glad you listen to Security Now! because that gives me some confidence that you're paying attention, which is good.



STEVE:  Okay.  FinCEN, which is the U.S. Financial Crimes Enforcement Network unit which is part of the U.S. Treasury Department, published a 10-page report detailing ransomware-related events as reported by banks and other financial institutions through the Bank Secrecy Act (BSA).  FinCEN said that in 2021, filings related to suspected ransomware payment substantially increased from 2020.



Okay.  So we're nearly a year behind, right, because that's the way these reports go.  Takes a while for them to filter through.  So not like this year.  We know this year was like a bang-up year, more so even than 2021.  Anyway, 2021 substantially increased over 2020.  2021 saw a reported $1.2 billion in known ransomware payments paid out.  The agency FinCEN estimates that roughly three quarters of these payments were made to ransomware gangs located in Russia.  And of course that's all the ones that we're talking about, the big guys, all of this is Russian to a large degree.  I've got a graph of the last few years of this.  But basically it is your - it's not quite exponential, but it's more than linear.  You know, it's more, yeah.



LEO:  That looks like a hockey stick.  It's a little hockey stick-y, yeah.



STEVE:  It's not good.



LEO:  Going up fast.



STEVE:  So, boy.  Yeah, we don't want Russia to be receiving our money.  And the problem is when there's this much money behind it, $1.2 billion in cryptocurrency transfers, that's called incentive.  And this is not what we want.



LEO:  By the way, that's why it's so low in the left-hand side of the chart.  You can really trace the success of ransomware to the rise of crypto.



STEVE:  Yes.  Unless you could get paid without getting caught, there was really no way to make this happen.  Remember it was Western Union transfers that was the way it was being done.



LEO:  Yeah, or you'd go down and buy money cards from the 7-Eleven; right?



STEVE:  Right, right.



LEO:  Sorry.



STEVE:  No, it's absolutely - it's been like the perfect storm where the bad guys realized, hey, this is great, we love this cryptocurrency stuff.  Let's just ask for some bitcoin.



Akamai published their third quarter, their Q3 Threat Report for this year, 2022, which they released on right smack dab on the end, on Halloween.  Since phishing has grown to become, by far, I mean, how many times have we spoken of it already in this 46 minutes, the most frequently detected first step in most successful attack scenarios.  What Akamai's report had to say about phishing, I thought, was telling.



They said:  "As covered in the Q2," that is, their previous quarter's 2022 report, "the overwhelming phishing landscape scale and magnitude is being enabled" - and this is news - "by the existence of phishing toolkits.  Phishing toolkits support the deployment and maintenance of phishing websites, driving even nontechnical scammers to join the phishing adversary landscape and run and execute phishing scams."  And anyone who's been listening to this podcast for long knows that's like the worst thing that we could hear, right, is you don't have to know anything now, increasingly, in order to pull off this, which is why there's so much of it.



They wrote:  "According to Akamai research that tracked 299 different phishing toolkits being used in the wild to launch new attack campaigns, during the third quarter of 2022, 2.01% of the tracked kits were reused on at least 63 distinct days.  53.2, so a little over half of the kits were reused to launch a new attack campaign on at least five distinct days.  And all 100% of the tracked kits were used on no fewer than three distinct days with the average toolkit reused on nine days during the third quarter of 2022."  So the bad guys are being fickle about their toolkits.  They jumping around trying different ones.  And they're not - these are not long-lived campaigns.  They're setting them up, sending out a bunch of emails, waiting for how long they would expect the email to take before somebody opened it and clicked on it.  And they wait five, six, seven, eight, nine days, and then they go, okay, time to do a different campaign.



They wrote:  "Further analysis on one of the most reused kits in the third quarter, counting the number of different domains used to deliver each kit, shows that kits that abuse Adobe and M&T Bank are top leading toolkits:  Adobe with more than 500 domains" - just during Q3, I know - "and M&T Bank with more than 400 domains."  Then they said:  "The reusing behavior of phishing toolkits is more evidence of the trend of the phishing landscape that continues to scale, moving to a phishing-as-a-service model and utilizing free Internet services.  Phishing attacks are more relevant than ever."



And it's interesting because their mention of utilizing free Internet services, remember, that was the one thing that the guy, the technical director of NCSC, who was the subject of last week's podcast, one of the things he said was I wish something could be done to limit free hosting services.  That is where so much of the problem is.  And at the same time he said, but what can you do in an open government...



LEO:  Can't shut them in, yeah.



STEVE:  Exactly.  But here, you know, utilizing free Internet services, the ability to just, you know, spin up free hosting and create free Internet service, that's a problem.  So, but think about that, 299 distinctly different phishing toolkits.  And as I said, what we've learned from observation is that the easier something is to do, the more it will be done.  The Log4j vulnerability never swept the world as was originally feared because it turned out that the nature of the vulnerability meant that there was no one-size-fit-all exploit for it available.  And if the script kiddies can't use something, then its use will be significantly curtailed.  But if script kiddies can use something, then a feeding frenzy is the result.  So on the front end it has never been easier to get into the phishing business.  And on the back end, there's a huge market for the services of the so-called Initial Access Brokers; right?  They're the ones who perform this, who develop initial access, and then resell it.



LEO:  Right.



STEVE:  So any credentials that a phishing campaign can manage to obtain will find a ready market among those who can turn them into devastating network attacks.



I do have one little bit of news before I talk about Initial Access Brokers, and that is that Akamai reported seeing - although this was in their admittedly very skewed sample set, which I'll explain - they saw a 40% increase, from 25% to 65%, in the use of DNS over TLS.  But that's not global.  That's their enterprise and their own small and medium-sized business customers.  But still, although this doesn't represent the world at large, currently more than 70% of all DNS remains over UDP.  But what I think will happen is, this will be a very gradual change.  As new systems are engineered from scratch, it's more likely that those new solutions will probably choose one of the encrypted forms of DNS, rather than old-school UDP.  So we can hope.  And it certainly says something that Akamai's own enterprise and small and medium-sized business customers really have started to adopt DNS over TLS.



Okay.  As for Initial Access Brokers, another third-quarter report came out from a threat intelligence firm Kela, K-E-L-A.  They published a report on the Initial Access Broker side of the network intrusion marketplace.  Kela's report stated that during just this third quarter, this past third quarter that just ended this year, they found over 570 unique network access listings for sale, with a cumulative requested price of approximately $4 million U.S.



Okay.  So just to be clear, someone responding and agreeing to purchase one of these 570 listings would be receiving, and this is something that's done through a Tor hidden service on the so-called dark web, they would be receiving the means to log into an unsuspected company's network with useful network privileges.  Within that set of 570 listings, the average price to purchase access was $2,800, and the median price was $1,350.  And prices have been rising since the second quarter.  The total number of listings remained almost unchanged between the second quarter and the third quarter, appearing at the rate of around 190 new access listings per month.



So think about that.  So there's a marketplace where people can go, and in fact as we'll get to it later, remember the numbskull Floridians, they actually went here, and they asked for access to CPA and tax preparer networks.  I mean, this marketplace is that specific.  You can go there, and you can say I want to get into the networks of these types of businesses, and you can purchase credentials that do that.  And new credentials are appearing at the rate of 190 listings per month.  That's 6.25 new listings per day, by the way.  So anyway, and the average price, $2,800 to purchase access to somebody's network.  And typically there's 570 of them up at any one time.



Wow.  Okay.  We will get to Florida in a minute.  I found an interesting little bit that shared some details about how bank heists work.  Although they don't receive a lot of coverage, over the past decade banks have not escaped ever-increasingly sophisticated cyberattacks.  Many banks have been hacked and have collectively lost billions of U.S. dollars in serious intrusions.  The two most notorious and successful threat actors that pulled off successful bank heists were a group called Carbanak, and also North Korea's Lazarus Group, which is an APT, an Advanced Persistent Threat group.  Lazarus we've talked about before.



The attack geography, interestingly enough, has been evolving over time.  Initial cyber heists tended to target organizations in North America and in Europe.  Once those regions were fully explored, and security began tightening up, there was a move into Asia and Latin America.  But as those banks also began to seriously upgrade their network defenses and security, movement has been now, more recently, in the direction of Africa, a region that has until now been left largely unscathed.



But a joint report published this week by security firm Group-IB and Orange's CERT team, a French-speaking cyber group tracked as, okay, we'll pronounce them "operator," although the "t" is a numeral "1," so "OPERA1ER," also known as Common Raven or the DESKTOP-group.  They've recently been wreaking havoc across the African continent, well, recently from 2018 through 2021.  This report covers nothing in this report since then.  But actions have continued.  The researchers said they linked this OPERA1ER group to 35 different intrusions at different organizations across 15 countries in Africa, with most of the attacks targeting banks.



Group-IB and the Orange researchers said that while the group used basic phishing attacks and off-the-shelf remote access trojans to gain an initial foothold in their victims' networks, once inside a network this OPERA1ER group has exhibited both restraint and patience.  Some intrusions lasted for months, as the group moved laterally across banking systems, observing, mapping the internal network topology, and patiently waiting before springing their attack.  The group's target was banking systems that handled money transfers.  And this is what I found so interesting.



The report explained:  "Once their network penetration had reached those most sensitive systems" - where the actual money transfers are managed - "the group would set a time for the heist and, working with a large network of some 400 money mules, would orchestrate a synchronized coordinated transfer of funds from the bank's larger legitimate accounts into the 400 mule accounts, with the money mules immediately withdrawing the stolen funds from their accounts via ATMs in a coordinated ATM cash-out before the bank's employees had the opportunity to react.  The mules would refresh the ATM's screens at the appointed time, waiting for their account balance to suddenly jump up.  Then they would drain the account for cash and quickly leave the area, thus of course bringing new meaning to the term 'decentralized finance.'  The Group-IB researchers said they had linked OPERA1ER intrusions to bank heists totaling $11 million, but the group is suspected of stealing more than $30 million total, though not all the incidents have been formally confirmed."



So anyway, I thought that was interesting.  The bad guys get in using phishing or remote access trojans, set up a presence in the networks, explore the networks, being quite patient, sometimes taking months until they determine what is there and get into a position where they're able to actually perform account funds transfers.  They then reach out to their network, obviously a pre-established network of 400 individuals who then at a prescribed time go to ATMs where their own mule accounts have suddenly become wealthy, and dump all the cash out of the ATM that they can and then take off and head somewhere else.  Wow.



Just to sort of keep an eye on DeFi, not to anyone's surprise, the DeFi platform Skyward Finance confirmed last Wednesday that a clever hacker had exploited a vulnerability in its smart contract system and made off with $3 million of cryptocurrency. And I guess at this point for us the proper expression would be, or the response, would be a yawn.  And the DeFi platform Solend (S-O-L-E-N-D) said it lost 1.26 million worth of cryptocurrency following an Oracle attack on its platform which targeted the Hubble (USDH) currency.  So it's hard to keep track of all these things these days.



Leo, you're going to love this one.  In a big "What in the world took them so long?" bit of news, the Russian Ministry of Digital Development surveyed the country's largest IT firms, Russia's largest IT firms, to obtain their recommendations for the best replacement for Windows across Russian government and private-sector networks.  The three contenders are all Linux-based operating systems, because what else could they be?



LEO:  Yeah.



STEVE:  They are, I mean, you're right, there is nothing else.



LEO:  Yeah, would they get Mac?  No, of course not.



STEVE:  No, no.  So they are the Astra Linux, ALT OS, and Red OS.



LEO:  Red OS is the Chinese one; isn't it?



STEVE:  Oh, that's interesting.



LEO:  China has its own Linux distribution the Chinese Communist Party recommends.



STEVE:  Yup, yup.  It would certainly make sense that it was Red OS.



LEO:  Red Linux, yeah.



STEVE:  And again, how many times have we, like, wondered, like what has taken them so long?  Like how could Russia be using  Windows?  It's just astonishing to me.



LEO:  They're often using pirated copies of Windows, and often using end-of-life pirated copies of Windows.  So it's hideously insecure.  The Chinese Linux is Kylin Linux, K-Y-L-I-N.  And it's specifically for the mainland China market.



STEVE:  Well, and get this.  It turns out that Russia would not have moved away from Windows but for their attack on Ukraine.  Reportedly, the Russian government is seeking a replacement only now, after Microsoft pulled out of Russia, stopped delivering security updates to Russian systems, and started blocking Russians' access to Windows installation files.  In other words, Microsoft left them with no choice.



LEO:  Yeah.



STEVE:  And so, okay, Linux.  Again, I wonder if - I guess I don't because they're moving to an open source operating system.  Our NSA probably knows all about Linux, just as well as it does Windows.  So it probably doesn't really make a difference one way or the other.



Okay, Leo, this one, wow.  We've all seen war stories where, in the midst of battle, prominently marked Red Cross trucks come barreling in carrying noncombatants wearing wide Red Cross armband emblems with the hope and expectation that all combatants in the area, no matter whose side they're on, will respect the Red Cross's global neutrality and allow them to care for the wounded.



In a bizarre - and, okay, I was going to say interesting, but I think bizarre wins - move, they're trying to do this in cyberspace.  After two years of study, last Thursday the International Committee for the Red Cross, the ICRC, has published their resulting report - again, took them two years -  titled "Digitalizing the Red Cross, Red Crescent, and Red Crystal Emblems:  Benefits, Risks, and Possible Solutions."



Okay.  In explaining their intention, they wrote:  "As societies digitalize, cyber operations are becoming a reality of armed conflict.  A growing number of states are developing military cyber capabilities, and their use during armed conflicts is likely to increase.  The ICRC (International Red Cross), has warned against the potential human cost of cyber operations and, in particular, the vulnerability of the medical sector and humanitarian organizations to cyber operations, both having been targeted in recent years.



"Against this background, the ICRC decided to investigate the idea of reflecting the internationally recognized distinctive Red Cross, Red Crescent, and Red Crystal emblems in the information and communication technology, i.e., a 'digital emblem.'  Since 2020 the ICRC has partnered with research institutions to explore the technological feasibility of developing a digital emblem, and convened a global group of experts to assess its potential, benefits and risks.  The idea and objective of a digital emblem was straightforward.  For over 150 years, the distinctive emblems have been used to convey a simple message:  In times of armed conflict, those who wear them, or facilities and objects marked with them, must be protected against harm."



Well, good luck.  I wonder whether during these past two years of study those working on this have noticed how many hospital networks have been cyber attacked?  You know, we're not dealing with declared hostilities in a battle theater where there's any sense of honor and conventions, Geneva or otherwise.  I'll be interested to see how this one plays out.  I mean, and what would prevent non-Red Cross organizations from putting up a Red Cross seal in order to protect themselves from attack?  I mean, it's just loony.  Okay.



Okay.  Last Tuesday, the Department of Justice's U.S. Attorney's Office for the Middle District of Florida posted a press release with the title "Band Of Cybercriminals Responsible for Computer Intrusions Nationwide Indicted for RICO Conspiracy That Netted Millions."  Okay.  And, now, that's 36 millions, to be precise.  Okay.  The alleged tax fraud crimes took place between 2015 through 2019.  DOJ officials said the group first purchased credentials from the dark web, allowing them to gain access to the internal networks of several Certified Public Accounting and tax preparation firms located across the U.S.



The group accessed the CPA and tax prep networks, stole the tax returns of thousands of taxpayers, created six tax preparation businesses in Florida and set up bank accounts and everything, I mean, full working businesses, and used those companies, those six tax preparation companies, to file more than 9,000 fraudulent tax returns in the victims' names and hijack tax refunds, directing them towards their own accounts.



And, surprise, surprise, somehow this was detected, and they didn't get away with it.  Now they're all facing on the order of 20 years behind bars for RICO charges and fraud and money laundering and, you know, interstate felonies and you name it.  I think what was most interesting and illuminating about this was the idea that I mentioned before that things are so well organized on the dark web that it's literally possible to search for network access by entity type.  It's like, "Yeah, I'd like to purchase network access credentials for CPA and tax prep firms in the U.S.  How much for how many?"  Wow.



This piece from Microsoft, I'm not sure about this.  Seems a little specious to me.  It appears to be the month for reporting, and Microsoft is also out with their annual Digital Defense Report.  The report contained a great many interesting tidbits, and buried among them was Microsoft's observation of an interesting change in China's profile.  The observation begins with Microsoft noting that China's advanced persistent threat actors have leveraged significantly more zero-day vulnerabilities during the past year than anyone else.



Now, although most, if not all APT groups rely upon zero-day vulnerabilities for their exploits, Microsoft said that it had noted Chinese threat actors had an increased number of zero-days over the past year.  And most interestingly, Microsoft believes that this sudden spike in zero-day exploits exclusively by Chinese threat actors is the direct result of a new law passed by the Chinese government last year.  We talked about this last summer.  The new law was passed in July of 2021, and it entered into effect in September of last year, 2021.  It requires all Chinese security researchers to first report any new vulnerabilities they find to a state security agency.



And yes, at the time this did raise some eyebrows.  It was roundly criticized within the security industry, while the Chinese government claimed that it only wanted to maintain an accurate catalog of vulnerabilities for the sake of making sure that local companies would not dodge responsibility for failing to patch vulnerabilities in time, thus leaving, obviously, Chinese users and government networks exposed to attacks.  Uh-huh.  Right.  And that sort of sounds like a reverse-engineered rationale.



To put a point on it, the new law also contains several generically-worded clauses that could be interpreted to suggest that the Chinese government was setting up a secret process through which its offensive cyber units would have access to this trove of privately reported, at the time unknown vulnerabilities, while simultaneously suppressing the work of the infosec community for the benefit of the country's espionage operations.  Although no solid evidence has come to light to support these theories, Microsoft appears to be sold on this narrative in its latest report.



They wrote:  "This new regulation might" - this is Microsoft writing.  "This new regulation might enable elements in the Chinese government to stockpile reported vulnerabilities toward weaponizing them.  The increased use of zero days over the last year from China-based actors likely reflects the first full year of China's vulnerability disclosure requirements for the Chinese security community, and a major step in the use of zero-day exploits as a state priority."



To put a little more meat on the bone, Microsoft listed five specific zero-days as possible examples of abuse:  two in Zoho ManageEngine, and one each in SolarWinds Serv-U, Atlassian Confluence, and Microsoft Exchange.  Were exploits of these five zero-days developed by Chinese APT threat actors after they were reported through Chinese in-house vulnerability disclosure rules?  We don't know.  Maybe.  On the other hand, would anyone be surprised to learn of zero-days in those applications?  Hasn't all of that software been repeatedly plagued by major vulnerabilities and zero-day exploits discovered by other researchers and exploited by other threat actors?  Of course.  Of that there could be no doubt.



So perhaps a more accurate and rounded assessment would be that we cannot blame Chinese APT actors for looking at what everyone else is looking at and discovering the same zero-days that others are finding.  Could they be getting a little help from the state's mandatory disclosure law?  Again, maybe.  But public evidence seems to be sorely lacking.  What I wondered, like maybe reading between the lines, is whether Microsoft actually knows more than they're able to disclose without revealing their own sources and methods which they need to keep secret.  Maybe this is a little bit of a shot across the bow saying read between the lines, China, because here's five zero-days that we think are suspicious.  Maybe they have grounds, and they just can't talk about it.



Okay.  So I love this idea.  I'll be interested to see what feedback I get from our listeners because not everyone might like it.  But it's interesting.  The U.K.'s cyber group, the NCSC, will be scanning its public network space, looking for known vulnerabilities.



LEO:  Hmm.



STEVE:  I think this is an interesting trend.  We were of course just talking about the U.K.'s GCHQ NCSC cyber division last week when we covered the retirement of its technical director after his 20 years of service.  And he certainly knew this was happening because this had to have been in the works for a while.  So it was with interest that I noted what I think is the NCSC's excellent plan to periodically scan its own U.K. IP space searching for known vulnerabilities which are accessible on the public Internet and reporting them for remediation to the owners of those IP addresses.  I think this is a terrific idea.



Okay.  So they have an information page which they titled "NCSC Scanning information."  It's not too long.  I'm just going to share this because it's sort of in a Q&A fashion.  They said:  "This page provides information on the NCSC's scanning activities.  You may have been referred here by information left by one of our scanning probes, if a system you own or administer has been scanned."



So they ask:  "Why is the NCSC carrying out scanning activities?"  They say:  "As part of the NCSC's mission to make the U.K. the safest place to live and do business online, we are building a data-driven view of 'the vulnerability of the UK.'  This directly supports the U.K. government cyber security strategy relating to understanding U.K. cyber risk.  This will help us to" - three things - "better understand the vulnerability and security of the U.K., help system owners understand their security posture on a day-to-day basis, and respond to shocks, like a widely exploited zero-day vulnerability."  That's interesting.  So they'll be on top of that.  When they find out something new like Heartbleed, for example, they would immediately scan the U.K.'s web servers and be proactive rather than passive.



Next question:  "How does the NCSC determine which systems to scan?"  They answer:  "These activities cover any Internet-accessible system that is hosted within the U.K. and vulnerabilities that are common or particularly important due to their high impact.  The NCSC uses the data we have collected to create an overview of the U.K.'s exposure to vulnerabilities following their disclosure, and track their remediation over time."  Boy, this just sounds wonderful to me.



Next question:  "How is scanning performed?  To identify whether a vulnerability exists on a system, we first need to identify the existence of specific associated protocols or services.  We do this by interacting with the system in much the same way a web browser or other network client typically would, and then analyzing the response that is received.  For example, we may be able to determine the existence of a vulnerability known to exist in version X of a type of commonly used web server software by making a web request to the URL" - and then they give an example - ".../login.html and detecting the value 'version X' in the content of the page that is returned.  If the vulnerability is then remediated in a subsequent version Y, we can identify this by similarly detecting the value 'version Y' in the response.  By repeating these requests on a regular basis, we maintain an up-to-date picture of vulnerabilities across the whole of the U.K."  Wow.



"What information does the NCSC collect and store?  We collect and store any data that a service returns in response to a request.  For web servers, this includes the full HTTP response, including headers, to a valid HTTP request.  For other services, this includes data that is sent by the server immediately after a connection has been established, like the SMP headers, for example, or a valid protocol handshake has been completed.  We also record other useful information for each request and response, such as the time and date of the request and the IP addresses of the source and destination endpoints.



"We design our requests to collect the smallest amount of technical information required to validate the presence/version and/or vulnerability of a piece of software.  We also design requests to limit the amount of personal data within the response.  In the unlikely event that we do discover information that is personal or otherwise sensitive, we take steps to remove the data and prevent it from being captured again in the future."



Question:  "How can I attribute activity on my systems to NCSC Scanning?"  They answer:  "All activity is performed on a schedule using standard and freely available network tools running within a dedicated cloud-hosted environment.  All connections are made using one of two IP addresses:  18.171.7.246 or 35.177.10.231."  And they said:  "Note that these IP addresses are also both assigned to 'scanner.scanning.service.ncsc.gov.uk' with both forward and reverse DNS records."  So that's very cool.  That means you could do a DNS lookup on scanner.scanning.service.ncsc.gov.uk, and it would return those two IPs.  Or if you did a reverse lookup on either of those IPs, that's the DNS that you would get to know what that was.



They said:  "Scan probes will also attempt to identify themselves as having originated from NCSC where possible, for example, by including the following header within all HTTP requests."  And the header is X-NCSC-Scan: NCSC Scanning agent.  And then they provide a URL to the page that I've been sharing so people can find out what that's about.



"What precautions and safety measures does the NCSC take when scanning?"  They answer:  "The NCSC is committed to conducting scanning activities in a safe and responsible manner.  As such, all our probes are verified by a senior technical professional and tested in our own environment before use.  We also limit how often we run scans to ensure we don't risk disrupting the normal operation of systems."



And finally:  "Can I opt out of having servers that I own or maintain being scanned?"  Answer:  "Yes.  Please contact scanning@ncsc.gov.uk with a list of IP addresses that you wish to exclude from any future scan activity, and we will endeavor to remove them as soon as possible once validated."



So, as I said, sign me up as a fan of this concept.  Given the sad and sorry state of so much consumer crap and unfortunately the patch latency of so many enterprises, all of which is hung out on the Internet to be attacked, I think this makes a huge amount of sense.  I mean, it's not like we're not all being scanned all over the place all the time anyway.  I mean, I referred to it, it was one of the first acronyms or abbreviations that I coined, and that was IBR because I started getting involved in Internet security, and I thought, what is all this packet noise?  And so it's Internet Background Radiation.  It's just random crap out on the Internet that hits all of our IPs from time to time.  So I think it would be great if the U.S. could take up similar responsibility and do something like this.  Or maybe defer to individual ISPs to like police the traffic on their own networks and inform their customers.



LEO:  Well, this was, you know, this was the big argument some years ago when spam - well, it's still a problem, but when it was really a problem.  All an ISP would have to do is block port 25, the SMTP port, and they would effectively kill spammers on their network.  And for a long time companies like Comcast, the biggest ISP in the U.S., wouldn't do it because they were afraid of the huge cost of tech support calls from people saying, well, I can't send my email anymore.  And they eventually did do it.  So ISPs, we've talked about this before.  ISPs could, without doing the scanning that the British are doing, do a lot to police the outbound traffic from their networks.



STEVE:  Yup.  And because they don't have to, they haven't done it.



LEO:  Yeah.  Yeah.



STEVE:  They have not been made to do it.



LEO:  Yeah.



STEVE:  Yeah.  And I think that their blocking of port 25 was also self-interest because they were getting complaints, like their network was sending all this spam.



LEO:  Right.



STEVE:  And it was, yeah, it was a customer in their network.  Cox, my cable provider, blocks port 25, so I have a way around that in order to contact my SMTP server at GRC.  But something has to be done.



Just a quick note about Twitter since I'm about to share two listener feedback tweets.  As my followers probably know, I have the blue verified check mark seal.  And like so many others who have commented, I'm not going to be paying anything for it.  I don't need any advanced features.  I'm not paying anything for it now, and I'm certainly not going to be paying $100 per year to keep it.



LEO:  Well, it would also devalue it because anybody who pays eight bucks regardless will get it.  So it no longer verifies that you are who you say you are.  It only means you paid eight bucks.



STEVE:  Right.



LEO:  So it completely devalues - it doesn't mean verified anymore.



STEVE:  Yeah.  So if it's taken away, I'll still be me.



LEO:  Yeah, I'm not paying either.  In fact, I got off Twitter.  I'm done with that.



STEVE:  I did note one thing in passing which I thought was interesting.  The Twitter alternative Mastodon reported that it had recently reached, not surprisingly, an all-time high of 655,000 active users after an influx of - get this - 230,000 new users just last week alone.



LEO:  It's up to a million now.



STEVE:  Wow.



LEO:  And I, you know, our server has a 7,000% increase in users, a 2,000% increase in interactions.



STEVE:  Wow.



LEO:  You should join the - can I put a plug in for TWiT.social?  I would love to have you.  We even have, you know, on TWiT.social we have a custom icon that's your head.  So I think you need to...



STEVE:  Well, all I really do with my Twitter account is tweet the link every week.



LEO:  Yeah, and you don't have to give up Twitter to do that.  But I suspect if you joined TWiT.social you would probably get in some very interesting conversations because people who listen to our show, many of them are there.  And the thing to understand about Mastodon is, you know, I'm running - it's federated.  So I'm running - it's like email.  I'm running a server.  But you can follow, and people can follow you from all over the Fediverse; right?  If you were - and I will give this to you, @steve@twit.social, everybody would know to follow you.  Or if you want to be SGgrc, whatever you want to be, you can be.



STEVE:  Well, I should be.  I don't want to get engaged in conversation.  That's not what I do.



LEO:  You don't have to.  It doesn't require it.  It's up to you.  I'm not going to push you into it, obviously.  In fact, one of the great things about Mastodon, I'm a little reluctant to promote that we do this because I don't want a whole influx of Twitter people in here.  I want people who, you know, are nice people.



STEVE:  Well, the good news is, Leo, the only people who are hearing this are the people who you do want.



LEO:  Are nice people, yes.



STEVE:  Are nice people.



LEO:  And that's a very good way of putting it, yes.  It's a safe space here.



STEVE:  And that's how I feel about GRC's newsgroups.  It's just it's a fabulous place where I'm able to get real work done.  I should mention that I will be firing up a mailing list finally.  I have to do it in order to announce SpinRite 6.1 to all of SpinRite 6.0's owners.



LEO:  Ooh, exciting.



STEVE:  So that has to happen.  So, and I'm going to - I'll create a number of different sublists and so forth.  And I'm thinking as Twitter becomes sort of an uncertain deal, and frankly there are an awful lot of our listeners who are like, they've always refused to be on Twitter.  So I will probably, one of the things that I'll do once I get a mailing system running is to just send out a short note every week, containing the show notes link because...



LEO:  Oh, that's a great idea, yeah.



STEVE:  Yeah.



LEO:  That's a great idea.



STEVE:  That way everyone will be able to get it.  So, okay.  Closing the Loop, two bits of feedback, as I said.  I wanted to note that it was fun to receive all of the feedback from my discussion of my preferred keyboards last week, Leo.  Not surprisingly, lots of people had opinions about keyboards.  There's lots of discussions going on in various places now.  So it turns out that I'm far from the only one who cares passionately about basically the way their primary device feels under their fingers.



David Stricker said:  "This week you talked about ALT+TAB acting as MRU," right, Most Recently Used.  He says:  "But CTRL+TAB as round robin.  Firefox has an option to set CTRL+TAB to act in MRU and is one of the main reasons I use it over Chromium-based browsers."  He said:  "I opened a bug with Chrome to allow MRU, and their response was simply 'Won't fix.'"  So he said:  "FF FTW."  So anyway, I just wanted to share with our listeners something I never knew, which is that there was an option in Firefox that would allow you to change the behavior of CTRL+TAB so that it is not round robin, but MRU.  And I would find that much preferable.



PCOwner said:  "Steve, what is the best commercial cloud storage, secure, encrypted?"  Okay, well, I know that there are many choices.  But I did want to mention I am still, just to renew, still a fan of Sync.com, who I haven't talked about for a while.  I've set up Sync to completely manage the file synchronization between my two locations, and it has never failed me.  It's completely TNO (Trust No One) end-to-end encrypted.  It has apps for iOS and Android, of course runs under Windows and Mac, presents a Sync directory under Windows and Mac, and allows for managed public link sharing despite the fact that it's end-to-end encrypted.  So it has all the features that you would expect from a mature, secure, encrypted, commercial cloud storage provider.



What I did was to move a bunch of subdirectories that already existed on my system under Sync's automatically synchronizing Sync directory.  So, for example, I have "c:\asm" where all of my assembly code work lives.  So I moved that entire directory under the new Sync directory.  Then I used Windows, there's a command in Windows, make link (mklink), which creates what's known as a junction point, you know, Linux refers to them as symbolic links or hard links.  This creates a junction point where the relocated directory used to be at "c:\asm."  This puts a link there so that all of the existing automation and batch files and everything that I have that expects my assembly language stuff to be at c:\asm, it's still there, as far as it's concerned, although it's actually under the Sync directory and now automatically synchronized between my multiple locations and available wherever I am.



The only feature missing, and they are painfully aware of it, is Linux client support.  But I expect that their evaluation of the market for Linux, I understand it's a skewed demographic here in this podcast audience, but Windows and Mac have such a high percentage of the total desktop share that they don't seem to be making much headway on a Linux client. 



LEO:  No, because this has been going on for years.



STEVE:  Yes.  And I did want to mention that without question for me, the best feature which I have used many times is that everything that is synchronized has full incremental versioning behind it, without the user ever needing to do anything.  Boy, is that a win.  And it has saved my bacon a couple times.  I was once doing file versioning myself locally, but now it's just all built into the system that I'm using to synchronize my locations, and it's great.  They have multiple plans, including a free 5GB plan that you can use to get your feet wet, and you can bump that, as I mentioned before when I talked about Sync, to a free 6GB if you go to Sync.com but use my affiliate code.



Actually, you can just go there in one jump.  It's grc.sc/sync, grc.sc/sync.  And that would give you an extra 1GB, and I get one added to my account, too.  So anyway, still bullish about Sync.  Again, I know that whenever I mention this, I get like  15 people all with different cloud sync providers, so I get it that there are alternatives.  But this is the one that I can vouch for.  And as I said, I've been using it, I use it every day, and it's never let me down.



And lastly - oh, boy, this is getting exciting - a quick update on where I am and what I'm doing when I'm not doing this podcast.  I finished all of SpinRite's data recovery driver testing.  All of it's working.  The oldest drivers for BIOS-interfaced drives ended up needing a bunch of updating.  That's all finished and tested.  As the final piece of work, I turned my attention to SpinRite's command-line interface and its built-in command-line help.  I updated everything in the online help with the new design.  The redesign of the way it's going to work is finished, so the help guide is updated to reflect that.



Now I'm in the midst of rewriting much of SpinRite's command-line processor to make it, well, to bring it up to speed with all of the other changes that SpinRite has undergone.  In the process of doing that, I needed to update SpinRite's "list" command which causes SpinRite to exit immediately after discovering and characterizing all of a system's mass storage devices which are accessible to it.  It dumps that list in tabular ASCII text to the DOS console.  For this new SpinRite, we also need a way of selecting drives through the command line.  I could have just used the old way of indicating which line item in the listed table we wanted.  But SpinRite power users use the command-line to automate SpinRite, and the ordering of drives could change over time if a drive was unplugged, or it went offline, or if a new drive was plugged into a lower numbered port, which would then get enumerated sooner and appear earlier in the table.



So a much more robust way of selecting drives is to allow a text match on any fields in the table.  Since that includes the drive's model number and its serial number, it'll be possible to positively lock selections to specific drives.  It'll also be possible to select multiple drives by class.  For example, since one of the table's columns is "type," it'll be possible to give SpinRite the command "type AHCI," which will cause SpinRite to pre-select all of the system's AHCI drives, but none others.



So that's where I stopped working Sunday evening to put the podcast together.  Tonight, well, probably not tonight because this is election night, so I will be in thrall.  But tomorrow morning, first thing in the morning, I'll probably still have the election on in the background, but I'll be working on SpinRite, getting that finished and tested and then out into the hands of our group.  So anyway, as I said to Lorrie during our walk yesterday, it's getting exciting.  And we have - I think we're up to 406 registered testers in our GitLab instance.  So we'll have a lot of people pounding on it, and we will move it as quickly as possible from Alpha into Beta.  At which point I'll be able to make it available widely.



LEO:  Yay.  Is that it?



STEVE:  That's it.



LEO:  There was, literally, something for everyone.  I was waiting for you to talk about the guy who had a billion dollars in crypto in his coffee can in his backyard.  Did you see that story?



STEVE:  I missed it.



LEO:  He had stolen it.  Let me see if I can find the details.



STEVE:  Oh, I did.  I didn't know, I didn't realize it was stolen.



LEO:  Oh, yeah, yeah.



STEVE:  I did hear something about someone who'd stolen a bunch of crypto.



LEO:  Yeah.  He'd stolen a bunch of crypto.  And he put it on a little board because it's, you know, it strikes me you could just write down the number of your wallet.  You don't need to actually...



STEVE:  Yes, you could, yes.



LEO:  But for some reason he decided to put it on a board.  Maybe he wasn't that sophisticated.  Anyway, he had a billion dollars' worth of bitcoin.  Was it a coffee can?  Or it was hidden.



STEVE:  And he got found? 



LEO:  Oh, yeah.  He got caught, and I think he's been arrested, yeah.  Anyway, I don't have the - we'll probably talk about it on TWiT on Sunday because it's just a great story.



STEVE:  Yeah.



LEO:  Mr. G.  If you like what you hear here, you've got to check out his website, GRC.com.  Yes, SpinRite's there, the world's finest mass storage maintenance and recovery utility.  6.0 is the current version; 6.1, as you heard, like just around the corner.  You'll get it for free if you buy 6.0 now.  You get an automatic upgrade.  So it's worth doing that.  You will want to have this software.  If you have a hard drive or an SSD, you've got to have SpinRite.



While you're there, check out the show.  Steve has two unique versions of the show, a 16Kb audio version and transcripts written by an actual human so they're actually legible.  And you can use those to search or just read along as you're listening.  He also has a 64Kb audio.  GRC.com.  You can leave him comments there.  As you heard, he doesn't really want to talk to you.  But if you want to leave a comment, go to GRC.com/comment, I'm sorry, feedback.  Yeah.  I don't blame you.  I don't.  I never read @ replies either.  Or you can go to Twitter, @SGgrc.  Sure I can't just sign you up, Steve, at TWiT.social?  It'd be so much easier.



STEVE:  I do reply to DMs.  I try to, you know, I mean, I'm present.  But extended conversations, everyone would rather have SpinRite than me. 



LEO:  Yes, get to work.  We have 64Kb audio.  We have video, too, at our website, TWiT.tv/sn.  There's a YouTube channel.  You can subscribe in your favorite podcast client, as well, and get it automatically, the minute it's available.  Some people like to watch live, like get the very freshest, hot off the podcast griddle version.  We do the show Tuesdays.  The time varies depending on how long MacBreak Weekly goes.  Somewhere  1:30 to 2:00 p.m. Pacific's what we're shooting for, 5:00 p.m. Eastern, 22:00 UTC.  Live.twit.tv is the stream.  There's audio and video streams there.  It's a nice thing to have in the background while you're working or whatever.



And if you're doing that, you might as well chat with us at irc.twit.tv.  Club members can also chat in the Discord.  And I guess, you know what, you could also comment on the TWiT.social there.  Steve won't see it, but I will.  Or on our Discourse, our forums at TWiT.community.  So there's quite a few ways to interact, either synchronously or asynchronously, with me and other listeners.  Don't expect Steve to get involved.  He's got something better to do.  More important.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




